{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 统计各个指标下面的仓库数量\n",
    "issues_closed: 71726 files\n",
    "change_requests_reviews: 59783 files\n",
    "participants: 73837 files\n",
    "attention: 73427 files\n",
    "issue_resolution_duration: 71665 files\n",
    "technical_fork: 73390 files\n",
    "issue_response_time: 72213 files\n",
    "activity: 73842 files\n",
    "issue_age: 72871 files\n",
    "change_request_age: 74382 files\n",
    "stars: 73255 files\n",
    "issues_new: 72214 files\n",
    "change_request_resolution_duration: 73474 files\n",
    "new_contributors: 71181 files\n",
    "change_requests_accepted: 71662 files\n",
    "change_request_response_time: 73105 files\n",
    "bus_factor: 73757 files\n",
    "code_change_lines_add: 73734 files\n",
    "code_change_lines_remove: 73727 files\n",
    "code_change_lines_sum: 73734 files\n",
    "change_requests: 73736 files\n",
    "inactive_contributors: 71438 files\n",
    "issue_comments: 73781 files\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folders(directory):\n",
    "    folder_counts = {}\n",
    "\n",
    "    for folder_name in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            file_count = len([name for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))])\n",
    "            folder_counts[folder_name] = file_count\n",
    "\n",
    "    return folder_counts\n",
    "\n",
    "def main():\n",
    "    data_dir = 'data/repo_data'\n",
    "\n",
    "    if os.path.exists(data_dir):\n",
    "        folder_file_counts = count_files_in_folders(data_dir)\n",
    "\n",
    "        for folder, count in folder_file_counts.items():\n",
    "            print(f\"{folder}: {count} files\")\n",
    "    else:\n",
    "        print(f\"Directory {data_dir} does not exist.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert JSON format to CSV format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def extract_data_from_json(repo_id, json_file_path, is_avg_structure=False):\n",
    "    \"\"\"从 JSON 文件中提取 yyyy-MM 格式的数据，返回字典格式的数据\"\"\"\n",
    "    # Check if the file is empty\n",
    "    if os.path.getsize(json_file_path) == 0:\n",
    "        print(f\"Empty file: {json_file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file: {json_file_path}\")\n",
    "        return None\n",
    "\n",
    "    if is_avg_structure:\n",
    "        # If it is an \"avg\" structure, only extract key-value pairs in the format of yyyy-MM under avg.\n",
    "        if 'avg' in data:\n",
    "            filtered_data = {k: v for k, v in data['avg'].items() if len(k) == 7 and k[4] == '-'}\n",
    "        else:\n",
    "            print(f\"No 'avg' key found in {json_file_path}\")\n",
    "            return None\n",
    "    else:\n",
    "        # therwise, extract the key-value pairs in the root directory in the yyyy-MM format\n",
    "        filtered_data = {k: v for k, v in data.items() if len(k) == 7 and k[4] == '-'}\n",
    "\n",
    "    if not filtered_data:\n",
    "        print(f\"No valid yyyy-MM data found in {json_file_path}\")\n",
    "        return None\n",
    "\n",
    "    filtered_data['repo_id'] = repo_id\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def process_and_merge_files(input_dir, is_avg_structure=False):\n",
    "    \"\"\"处理文件夹中的所有 JSON 文件，并将它们合并为一个 DataFrame\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.json'):\n",
    "            repo_id = os.path.splitext(file_name)[0]  # 从文件名中获取 repo_id\n",
    "            json_file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "            data = extract_data_from_json(repo_id, json_file_path, is_avg_structure)\n",
    "            if data:\n",
    "                all_data.append(data)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    if not df.empty:\n",
    "        time_columns = sorted([col for col in df.columns if col != 'repo_id'])\n",
    "\n",
    "        df = df[['repo_id'] + time_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_all_folders(base_dir, output_dir):\n",
    "    \"\"\"处理 repo_data 目录下的所有子文件夹，并将结果保存为 CSV 文件\"\"\"\n",
    "    avg_structure_folders = [\n",
    "        \"change_request_age\", \"change_request_resolution_duration\",\n",
    "        \"change_request_response_time\", \"issue_age\", \"issue_response_time\",\n",
    "        \"issue_resolution_duration\"\n",
    "    ]\n",
    "\n",
    "    for folder in os.listdir(base_dir):\n",
    "        input_dir = os.path.join(base_dir, folder)\n",
    "        output_file = os.path.join(output_dir, f\"{folder}_merged.csv\")\n",
    "\n",
    "        if os.path.exists(input_dir):\n",
    "            # Determine if it is a folder structure of the avg.\n",
    "            is_avg_structure = folder in avg_structure_folders\n",
    "\n",
    "            # Process and merge the JSON files in this folder.\n",
    "            merged_df = process_and_merge_files(input_dir, is_avg_structure)\n",
    "\n",
    "            if not merged_df.empty:\n",
    "                merged_df.to_csv(output_file, index=False)\n",
    "                print(f\"Merged data saved to {output_file}\")\n",
    "            else:\n",
    "                print(f\"No data to merge in folder: {folder}\")\n",
    "        else:\n",
    "            print(f\"Folder not found: {input_dir}\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    base_dir = 'data/repo_data_final'\n",
    "    output_dir = 'data/repo_analysis_final'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process all JSON files in the folders and merge them into a CSV file\n",
    "    process_all_folders(base_dir, output_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count the rows of each metric file.\n",
    "File: code_change_lines_sum_merged.csv, Rows: 72923\n",
    "File: code_change_lines_add_merged.csv, Rows: 73306\n",
    "File: technical_fork_merged.csv, Rows: 72787\n",
    "File: issue_resolution_duration_merged.csv, Rows: 71399\n",
    "File: bus_factor_merged.csv, Rows: 73213\n",
    "File: change_requests_reviews_merged.csv, Rows: 59613\n",
    "File: issue_age_merged.csv, Rows: 70541\n",
    "File: participants_merged.csv, Rows: 73288\n",
    "File: change_request_response_time_merged.csv, Rows: 72838\n",
    "File: stars_merged.csv, Rows: 72570\n",
    "File: change_request_age_merged.csv, Rows: 70840\n",
    "File: issue_response_time_merged.csv, Rows: 71958\n",
    "File: change_requests_merged.csv, Rows: 73350\n",
    "File: new_contributors_merged.csv, Rows: 70939\n",
    "File: code_change_lines_remove_merged.csv, Rows: 73207\n",
    "File: attention_merged.csv, Rows: 72703\n",
    "File: issues_closed_merged.csv, Rows: 71355\n",
    "File: change_request_resolution_duration_merged.csv, Rows: 73184\n",
    "File: inactive_contributors_merged.csv, Rows: 64250\n",
    "File: activity_merged.csv, Rows: 73816\n",
    "File: issue_comments_merged.csv, Rows: 73302\n",
    "File: issues_new_merged.csv, Rows: 71798\n",
    "File: change_requests_accepted_merged.csv, Rows: 71330\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def count_rows_in_csv_files(directory):\n",
    "    \"\"\"遍历目录中的所有CSV文件，统计每个文件中的行数\"\"\"\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            try:\n",
    "                # 读取CSV文件\n",
    "                df = pd.read_csv(file_path)\n",
    "                # 统计行数（包括表头）\n",
    "                row_count = df.shape[0]\n",
    "                print(f\"File: {file_name}, Rows: {row_count}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# 设置目录路径\n",
    "directory = 'data/repo_analysis_final'\n",
    "\n",
    "# 调用函数获取每个CSV文件的数据数量\n",
    "count_rows_in_csv_files(directory)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge two sets of tag data and take the intersection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 文件夹路径，仅使用bot_v2和bot_v3\n",
    "folders = ['data/bot_label_result_1/category', 'data/bot_label_result_2/category']\n",
    "output_folder = 'data/final/actor_category'\n",
    "\n",
    "# 要比较的文件列表\n",
    "files = [\n",
    "    \"CI_CD_Bots.csv\",\n",
    "    \"Code_Review_Bots.csv\",\n",
    "    \"Code_Security_Review_Bots.csv\",\n",
    "    \"Collaboration_and_Communication_Bots.csv\",\n",
    "    \"Configuration_Management_Bots.csv\",\n",
    "    \"Documentation_Generation_Bots.csv\",\n",
    "    \"Open_Source_Compliance_Inspection_Bots.csv\",\n",
    "    \"Periodic_Report_Bots.csv\",\n",
    "    \"Workflow_Control_Bots.csv\"\n",
    "]\n",
    "\n",
    "# 函数用于加载CSV并提取actor_id列\n",
    "def load_actor_ids(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return set(df['actor_id'])\n",
    "\n",
    "# 函数用于比较bot_v3与bot_v2的交集并保存到CSV文件\n",
    "def merge_intersections(folders, files, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file in files:\n",
    "        # 定义两个文件夹的路径\n",
    "        bot_v3_path = os.path.join(folders[0], file)\n",
    "        bot_v2_path = os.path.join(folders[1], file)\n",
    "\n",
    "        # 确保文件存在\n",
    "        if all(os.path.exists(p) for p in [bot_v3_path, bot_v2_path]):\n",
    "            # 加载actor_id\n",
    "            bot_v3_actors = load_actor_ids(bot_v3_path)\n",
    "            bot_v2_actors = load_actor_ids(bot_v2_path)\n",
    "\n",
    "            # 计算交集：bot_v3 与 bot_v2\n",
    "            intersection_v3_v2 = bot_v3_actors.intersection(bot_v2_actors)\n",
    "\n",
    "            # 保存交集到新的CSV文件\n",
    "            output_file = os.path.join(output_folder, file)\n",
    "            pd.DataFrame(list(intersection_v3_v2), columns=['actor_id']).to_csv(output_file, index=False)\n",
    "            print(f\"Saved merged actor_id to {output_file}\")\n",
    "\n",
    "# 运行函数，合并交集并输出结果\n",
    "merge_intersections(folders, files, output_folder)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count the final number of labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CI_CD_Bots.csv': 551, 'Code_Review_Bots.csv': 406, 'Code_Security_Review_Bots.csv': 44, 'Collaboration_and_Communication_Bots.csv': 77, 'Documentation_Generation_Bots.csv': 52, 'Open_Source_Compliance_Inspection_Bots.csv': 37, 'Periodic_Report_Bots.csv': 22, 'Workflow_Control_Bots.csv': 36}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义文件夹路径和文件名\n",
    "folder_path = 'data/final/actor_category'\n",
    "files = [\n",
    "    \"CI_CD_Bots.csv\",\n",
    "    \"Code_Review_Bots.csv\",\n",
    "    \"Code_Security_Review_Bots.csv\",\n",
    "    \"Collaboration_and_Communication_Bots.csv\",\n",
    "    \"Documentation_Generation_Bots.csv\",\n",
    "    \"Open_Source_Compliance_Inspection_Bots.csv\",\n",
    "    \"Periodic_Report_Bots.csv\",\n",
    "    \"Workflow_Control_Bots.csv\"\n",
    "]\n",
    "# 初始化字典以存储actor_id数量\n",
    "actor_counts = {}\n",
    "\n",
    "# 遍历每个文件，统计actor_id列中的唯一数量\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        actor_counts[file] = df['actor_id'].nunique()\n",
    "\n",
    "# 输出统计结果\n",
    "print(actor_counts)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T12:29:42.420775Z",
     "start_time": "2024-09-12T12:29:40.445863Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
