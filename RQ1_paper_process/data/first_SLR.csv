title,author,abstract,year,id
RABBIT: A tool for identifying bot accounts based on their recent GitHub event history,"Chidambaram, Natarajan and Mens, Tom and Decan, Alexandre","Collaborative software development through GitHub repositories frequently relies on bot accounts to automate repetitive and error-prone tasks. This highlights the need to have accurate and efficient bot identification tools. Several such tools have been proposed in the past, but they tend to rely on a substantial amount of historical data, or they limit themselves to a reduced subset of activity types, making them difficult to use at large scale. To overcome these limitations, we developed RABBIT, an open source command-line tool that queries the GitHub Events API to retrieve the recent events of a given GitHub account and predicts whether the account is a human or a bot. RABBIT is based on an XGBoost classification model that relies on six features related to account activities and achieves high performance, with an AUC, F1 score, precision and recall of 0.92. Compared to the state-of-the-art in bot identification, RABBIT exhibits a similar performance in terms of precision, recall and F1 score, while being more than an order of magnitude faster and requiring considerably less data. This makes RABBIT usable on a large scale, capable of processing several thousand accounts per hour efficiently.",2024,2
Bot detection in GitHub repositories,"Chidambaram, Natarajan and Mazrae, Pooya Rostami","Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the GrimoireLab software development analytics platform. In this work, we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using Kibana.",2022,4
Exploring How Software Developers Work with Mention Bot in GitHub,"Peng, Zhenhui and Yoo, Jeehoon and Xia, Meng and Kim, Sunghun and Ma, Xiaojuan","Recently, major software development platforms have started to provide automatic reviewer recommendation (ARR) services for pull requests, to improve the collaborative coding review process. However, the user experience of ARR is under-investigated. In this paper, we use a two-stage mixed-methods approach to study how software developers perceive and work with the Facebook mention bot, one of the most popular ARR bots in GitHub. Specifically, in Stage I, we conduct archival analysis on projects employing mention bot and a user survey to investigate the bot's performance. A year later, in Stage II, we revisit these projects and conduct additional surveys and interviews with three user groups: project owners, contributors and reviewers. Results show that developers appreciate mention bot saving their effort, but are bothered by its unstable setting and unbalanced workload allocation. We conclude with design considerations for improving ARR services.",2018,7
On the accuracy of bot detection techniques,"Golzadeh, Mehdi and Decan, Alexandre and Chidambaram, Natarajan","Development bots are often used to automate a wide variety of repetitive tasks in collaborative software development. Such bots are commonly among the most active project contributors in terms of commit activity. As such, tools that analyse contributor activity (e.g., for recognizing and giving credit to project members for their contributions) need to take into account the bots and exclude their activity. While there are a few techniques to detect bots in software repositories, these techniques are not perfect and may miss some bots or may wrongly identify some human accounts as bots. In this paper, we present an exploratory study on the accuracy of bot detection techniques on a set of 540 accounts from 27 GitHub projects. We show that none of the bot detection techniques are accurate enough to detect bots among the 20 most active contributors of each project. We show that combining these techniques drastically increases the accuracy and recall of bot detection. We also highlight the importance of considering bots when attributing contributions to humans, since bots are prevalent among the top contributors and responsible for large proportions of commits.",2022,8
Autonomy Is an Acquired Taste: Exploring Developer Preferences for GitHub Bots,"Ghorbani, Amir and Cassee, Nathan and Robinson, Derek and Alami, Adam and Ernst, Neil A. and Serebrenik, Alexander and W\k{a}sowski, Andrzej","Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",2023,9
Cataloging GitHub Repositories,"Sharma, Abhishek and Thung, Ferdian and Kochhar, Pavneet Singh and Sulistya, Agus and Lo, David","GitHub is one of the largest and most popular repository hosting service today, having about 14 million users and more than 54 million repositories as of March 2017. This makes it an excellent platform to find projects that developers are interested in exploring. GitHub showcases its most popular projects by cataloging them manually into categories such as DevOps tools, web application frameworks, and game engines. We propose that such cataloging should not be limited only to popular projects. We explore the possibility of developing such cataloging system by automatically extracting functionality descriptive text segments from readme files of GitHub repositories. These descriptions are then input to LDA-GA, a state-of-the-art topic modeling algorithm, to identify categories. Our preliminary experiments demonstrate that additional meaningful categories which complement existing GitHub categories can be inferred. Moreover, for inferred categories that match GitHub categories, our approach can identify additional projects belonging to them. Our experimental results establish a promising direction in realizing automatic cataloging system for GitHub.",2017,11
Towards an autonomous bot for automatic source code refactoring,"Wyrich, Marvin and Bogner, Justus","Continuous refactoring is necessary to maintain source code quality and to cope with technical debt. Since manual refactoring is inefficient and error-prone, various solutions for automated refactoring have been proposed in the past. However, empirical studies have shown that these solutions are not widely accepted by software developers and most refactorings are still performed manually. For example, developers reported that refactoring tools should support functionality for reviewing changes. They also criticized that introducing such tools would require substantial effort for configuration and integration into the current development environment.In this paper, we present our work towards the Refactoring-Bot, an autonomous bot that integrates into the team like a human developer via the existing version control platform. The bot automatically performs refactorings to resolve code smells and presents the changes to a developer for asynchronous review via pull requests. This way, developers are not interrupted in their workflow and can review the changes at any time with familiar tools. Proposed refactorings can then be integrated into the code base via the push of a button. We elaborate on our vision, discuss design decisions, describe the current state of development, and give an outlook on planned development and research activities.",2019,12
What to Expect from Code Review Bots on GitHub? A Survey with OSS Maintainers,"Wessel, Mairieli and Serebrenik, Alexander and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco A.","Software bots are used by Open Source Software (OSS) projects to streamline the code review process. Interfacing between developers and automated services, code review bots report continuous integration failures, code quality checks, and code coverage. However, the impact of such bots on maintenance tasks is still neglected. In this paper, we study how project maintainers experience code review bots. We surveyed 127 maintainers and asked about their expectations and perception of changes incurred by code review bots. Our findings reveal that the most frequent expectations include enhancing the feedback bots provide to developers, reducing the maintenance burden for developers, and enforcing code coverage. While maintainers report that bots satisfied their expectations, they also perceived unexpected effects, such as communication noise and newcomers' dropout. Based on these results, we provide a series of implications for bot developers, as well as insights for future research.",2020,13
Towards s/engineer/bot: principles for program repair bots,"van Tonder, Rijnard and Goues, Claire Le","Of the hundreds of billions of dollars spent on developer wages, up to 25% accounts for fixing bugs. Companies like Google save significant human effort and engineering costs with automatic bug detection tools, yet automatically fixing them is still a nascent endeavour. Very recent work (including our own) demonstrates the feasibility of automatic program repair in practice. As automated repair technology matures, it presents great appeal for integration into developer workflows. We believe software bots are a promising vehicle for realizing this integration, as they bridge the gap between human software development and automated processes. We envision repair bots orchestrating automated refactoring and bug fixing. To this end, we explore what building a repair bot entails. We draw on our understanding of patch generation, validation, and real world software development interactions to identify six principles that bear on engineering repair bots and discuss related design challenges for integrating human workflows. Ultimately, this work aims to foster critical focus and interest for making repair bots a reality.",2019,15
Conversational Bot for Newcomers Onboarding to Open Source Projects,"Dominic, James and Houser, Jada and Steinmacher, Igor and Ritter, Charles and Rodeghero, Paige","This paper targets the problems newcomers face when onboarding to open source projects and the low retention rate of newcomers. Open source software projects are becoming increasingly more popular. Many major companies have started building open source software. Unfortunately, many newcomers only commit once to an open source project before moving on to another project. Even worse, many novices struggle with joining open source communities and end up leaving quickly, sometimes before their first successful contribution. In this paper, we propose a conversational bot that would recommend projects to newcomers and assist in the onboarding to the open source community. The bot would be able to provide helpful resources, such as Stack Overflow related content. It would also be able to recommend human mentors. We believe that this bot would improve newcomers' experience by providing support not only during their first contribution, but by acting as an agent to engage them to the project.",2020,22
Painting the landscape of automotive software in GitHub,"Kochanthara, Sangeeth and Dajsuren, Yanja and Cleophas, Loek and van den Brand, Mark","The automotive industry has transitioned from being an electromechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook's front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing a first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing its characteristics and development styles.The landscape is defined by 15,000+ users contributing to ≈600 actively-developed automotive software projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics-related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe shifts along multiple dimensions, including preferred language from MATLAB to Python and prevalence of perception and decision-related software over traditional automotive software. This study witnesses open source automotive software boom in its infancy with many implications for future research and practice.",2022,23
Explainable software bot contributions: case study of automated bug fixes,"Monperrus, Martin","In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans.Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers.We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of ""dry patches"": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an ""explainable bug fix"": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples.In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation.",2019,29
TwiBot-20: A Comprehensive Twitter Bot Detection Benchmark,"Feng, Shangbin and Wan, Herun and Wang, Ningnan and Li, Jundong and Luo, Minnan","Twitter has become a vital social media platform while an ample amount of malicious Twitter bots exist and induce undesirable social effects. Successful Twitter bot detection proposals are generally supervised, which rely heavily on large-scale datasets. However, existing benchmarks generally suffer from low levels of user diversity, limited user information and data scarcity. Therefore, these datasets are not sufficient to train and stably benchmark bot detection measures. To alleviate these problems, we present TwiBot-20, a massive Twitter bot detection benchmark, which contains 229,573 users, 33,488,192 tweets, 8,723,736 user property items and 455,958 follow relationships. TwiBot-20 covers diversified bots and genuine users to better represent the real-world Twittersphere. TwiBot-20 also includes three modals of user information to support both binary classification of single users and community-aware approaches. To the best of our knowledge, TwiBot-20 is the largest Twitter bot detection benchmark to date. We reproduce competitive bot detection methods and conduct a thorough evaluation on TwiBot-20 and two other public datasets. Experiment results demonstrate that existing bot detection measures fail to match their previously claimed performance on TwiBot-20, which suggests that Twitter bot detection remains a challenging task and requires further research efforts.",2021,35
Automatic Core-Developer Identification on GitHub: A Validation Study,"Bock, Thomas and Alznauer, Nils and Joblin, Mitchell and Apel, Sven","Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods.",2023,39
FixMe: a GitHub bot for detecting and monitoring on-hold self-admitted technical debt,"Phaithoon, Saranphon and Wongnil, Supakarn and Pussawong, Patiphol and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee and Maipradit, Rungroj and Hata, Hideaki and Matsumoto, Kenichi","Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related ""On-hold SATD"", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN_YQ.",2022,41
"Mentions of Security Vulnerabilities on Reddit, Twitter and GitHub","Horawalavithana, Sameera and Bhattacharjee, Abhishek and Liu, Renhao and Choudhury, Nazim and O. Hall, Lawrence and Iamnitchi, Adriana","Activity on social media is seen as a relevant sensor for different aspects of the society. In a heavily digitized society, security vulnerabilities pose a significant threat that is publicly discussed on social media. This study presents a comparison of user-generated content related to security vulnerabilities on three digital platforms: two social media conversation channels (Reddit and Twitter) and a collaborative software development platform (GitHub). Our data analysis shows that while more security vulnerabilities are discussed on Twitter, relevant conversations go viral earlier on Reddit. We show that the two social media platforms can be used to accurately predict activity on GitHub.",2019,44
Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection,"Hays, Chris and Schutzman, Zachary and Raghavan, Manish and Walk, Erin and Zimmer, Philipp","Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near-perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules — shallow decision trees trained on a small number of features — achieve near-state-of-the-art performance on most available datasets and that bot detection datasets, even when combined together, do not generalize well to out-of-sample datasets. Our findings reveal that predictions are highly dependent on each dataset’s collection and labeling procedures rather than fundamental differences between bots and humans. These results have important implications for both transparency in sampling and labeling procedures and potential biases in research using existing bot detection tools for pre-processing.",2023,45
Community formation and detection on GitHub collaboration networks,"Moradi-Jamei, Behnaz and Kramer, Brandon L. and Calder\'{o}n, J. Bayo\'{a}n Santiago and Korkmaz, Gizem","This paper studies community formation in OSS collaboration networks. While most current work examines the emergence of small-scale OSS projects, our approach draws on a large-scale historical dataset of 1.8 million GitHub users and their repository contributions. OSS collaborations are characterized by small groups of users that work closely together, leading to the presence of communities defined by short cycles in the underlying network structure. To understand the impact of this phenomenon, we apply a pre-processing step that accounts for the cyclic network structure by using Renewal-Nonbacktracking Random Walks (RNBRW) and the strength of pairwise collaborations before implementing the Louvain method to identify communities within the network. Equipping Louvain with RNBRW and the contribution strength provides a more assertive approach for detecting small-scale teams and reveals nontrivial differences in community detection such as users' tendencies toward preferential attachment to more established collaboration communities. Using this method, we also identify key factors that affect community formation, including the effect of users' location and primary programming language, which was determined using a comparative method of contribution activities. Overall, this paper offers several promising methodological insights for both open-source software experts and network scholars interested in studying team formation.",2022,51
A Preliminary Study of Bots Usage in Open Source Community,"Wu, Xiaojun and Gao, Anze and Zhang, Yang and Wang, Tao and Tang, Yi","Bots are seen as a promising approach in software development, which help to deal with the ever-increasing complexity of modern software engineering and development. The number of bots in open source community, such as GitHub, has expanded substantially over the last three years. Due to its increasing popularity, it is essential to characterize the current usage of bots in practices. In this paper, we present an empirical study of bots usage in GitHub community. By analyzing 7,399 projects from GitHub, we find that 4,148 (56%) projects have used bots. Through automatic identification and manual detection, we collect a total of 196 bots. We then analyze and classify them into 4 categories and 14 topics. Finally, we discuss some raised implications for bots in current GitHub community.",2022,52
BotHunter: an approach to detect software bots in GitHub,"Abdellatif, Ahmad and Wessel, Mairieli and Steinmacher, Igor and Gerosa, Marco A. and Shihab, Emad","Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level.",2022,63
Coding together in a social network: collaboration among GitHub users,"Celi\'{n}ska, Dorota","In this article we investigate developers involved in the creation of Open Source software to identify which characteristics favor innovation in the Open Source community. The results of the analysis show that higher reputation in the community improves the probability of gaining collaborators to a certain degree, but developers are also driven by reciprocity. This is consistent with the concept of gift economy. A significant network effect exists and emerges from standardization, showing that developers using the most popular programming languages in the service are likely to have more collaborators. Providing additional information (valid URL to developer's homepage) improves the chances of finding coworkers. The results can be generalized for the population of mature users of GitHub.",2018,65
Enhancing developers’ support on pull requests activities with software bots,"Wessel, Mairieli","Software bots are employed to support developers' activities, serving as conduits between developers and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save development cost, time, and effort, the bots' presence can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers enhance existing bots. Toward this end, we are interviewing maintainers, contributors, and bot developers to understand the problems in the human-bot interaction and how they affect the collaboration in a project. Afterward, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.",2020,68
Leveraging Software Bots to Enhance Developers' Collaboration in Online Programming Communities,"Wessel, Mairieli","Software bots are applications that are integrated into human communication channels, serving as an interface between users and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save developers' costs, time, and effort, the interaction of these bots can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers to enhance existing bots, thereby improving the partnership with contributors and maintainers. Toward this end, we are interviewing developers to understand what are the problems on the human-bot interaction and how they affect human collaboration. Afterwards, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.",2020,71
"Bots for pull requests: the good, the bad, and the promising","Wessel, Mairieli and Abdellatif, Ahmad and Wiese, Igor and Conte, Tayana and Shihab, Emad and Gerosa, Marco A. and Steinmacher, Igor","Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (""the good""). However, their interactions can be disruptive and noisy and lead to information overload (""the bad""). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (""the promising""). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",2022,72
Don't Disturb Me: Challenges of Interacting with Software Bots on Open Source Software Projects,"Wessel, Mairieli and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco Aurelio","Software bots are used to streamline tasks in Open Source Software (OSS) projects' pull requests, saving development cost, time, and effort. However, their presence can be disruptive to the community. We identified several challenges caused by bots in pull request interactions by interviewing 21 practitioners, including project maintainers, contributors, and bot developers. In particular, our findings indicate noise as a recurrent and central problem. Noise affects both human communication and development workflow by overwhelming and distracting developers. Our main contribution is a theory of how human developers perceive annoying bot behaviors as noise on social coding platforms. This contribution may help practitioners understand the effects of adopting a bot, and researchers and tool designers may leverage our results to better support human-bot interaction on social coding platforms.",2021,74
The Power of Bots: Characterizing and Understanding Bots in OSS Projects,"Wessel, Mairieli and de Souza, Bruno Mendes and Steinmacher, Igor and Wiese, Igor S. and Polato, Ivanilton and Chaves, Ana Paula and Gerosa, Marco A.","Leveraging the pull request model of social coding platforms, Open Source Software (OSS) integrators review developers' contributions, checking aspects like license, code quality, and testability. Some projects use bots to automate predefined, sometimes repetitive tasks, thereby assisting integrators' and contributors' work. Our research investigates the usage and impact of such bots. We sampled 351 popular projects from GitHub and found that 93 (26%) use bots. We classified the bots, collected metrics from before and after bot adoption, and surveyed 228 developers and integrators. Our results indicate that bots perform numerous tasks. Although integrators reported that bots are useful for maintenance tasks, we did not find a consistent, statistically significant difference between before and after bot adoption across the analyzed projects in terms of number of comments, commits, changed files, and time to close pull requests. Our survey respondents deem the current bots as not smart enough and provided insights into the bots' relevance for specific tasks, challenges, and potential new features. We discuss some of the raised suggestions and challenges in light of the literature in order to help GitHub bot designers reuse and test ideas and technologies already investigated in other contexts.",2018,75
The Inconvenient Side of Software Bots on Pull Requests,"Wessel, Mairieli and Steinmacher, Igor","Software bots are applications that integrate their work with humans' tasks, serving as conduits between users and other tools. Due to their ability to automate tasks, bots have been widely adopted by Open Source Software (OSS) projects hosted on GitHub. Commonly, OSS projects use bots to automate a variety of routine tasks to save time from maintainers and contributors. Although bots can be useful for supporting maintainers' work, sometimes their comments are seen as spams, and are quickly ignored by contributors. In fact, the way that these bots interact on pull requests can be disruptive and perceived as unwelcoming. In this paper, we propose the concept of a meta-bot to deal with current problems on the human-bot interaction on pull requests. Besides providing additional value to this interaction, meta-bot will reduce interruptions and help maintainers and contributors stay aware of important information.",2020,76
Software bots in software engineering: benefits and challenges,"Wessel, Mairieli and Gerosa, Marco A. and Shihab, Emad","Software bots are becoming increasingly popular in software engineering (SE). In this tutorial, we define what a bot is and present several examples. We also discuss the many benefits bots provide to the SE community, including helping in development tasks (such as pull request review and integration) and onboarding newcomers to a project. Finally, we discuss the challenges related to interacting with and developing software bots.",2022,77
Defining and classifying software bots: a faceted taxonomy,"Lebeuf, Carlene and Zagalsky, Alexey and Foucault, Matthieu and Storey, Margaret-Anne","While bots have been around for many decades, recent technological advancements and the increasing adoption of language-based communication platforms have led to a surge of new software bots, which have become increasingly pervasive in our everyday lives. Although many novel bots are being designed and deployed, the terms used to describe them and their properties are vast, diverse, and often inconsistent. Even the concept of what is or is not a bot is unclear. This hinders our ability to study, understand, design, and classify bots.In this paper, we present a taxonomy of software bots, which focuses on the observable properties and behaviours of software bots, as well as the environments where bots are deployed and designed. We see this taxonomy as a focal point for a discussion in our community so that together we can deeply consider how to evaluate and understand existing bots, as well as how we may design more innovative and productive bots.",2019,79
"Expecting the unexpected: distilling bot development, challenges, and motivations","Pinheiro, Andr\'{e} M. and Rabello, Caio S. and Furtado, Leonardo B. and Pinto, Gustavo and de Souza, Cleidson R. B.","Software bots are becoming an increasingly popular tool in the software development landscape, which is particularly due to their potential of use in several different contexts. More importantly, software developers interested in transitioning to bot development may have to face challenges intrinsic related to bot software development. However, so far, it is still unclear what is the profile of bot developers, what motivate them, or what challenges do they face when dealing with bot development. To shed an initial light on this direction, we conducted a survey with 43 Github users who have been involved (showing their interest or actively contributing to) in bot software projects.",2019,83
Current and future bots in software development,"Erlenhov, Linda and de Oliveira Neto, Francisco Gomes and Scandariato, Riccardo and Leitner, Philipp","Bots that support software development (""DevBots"") are seen as a promising approach to deal with the ever-increasing complexity of modern software engineering and development. Existing DevBots are already able to relieve developers from routine tasks such as building project images or keeping dependencies up-to-date. However, advances in machine learning and artificial intelligence hold the promise of future, significantly more advanced, DevBots. In this paper, we introduce the terminology of contemporary and ideal DevBots. Contemporary DevBots represent the current state of practice, which we characterise using a facet-based taxonomy. We exemplify this taxonomy using 11 existing, industrial-strength bots. We further provide a vision and definition of future (ideal) DevBots, which are not only autonomous, but also adaptive, as well as technically and socially competent. These properties may allow ideal DevBots to act more akin to artificial team mates than simple development tools.",2019,84
BotWalk: Efficient Adaptive Exploration of Twitter Bot Networks,"Minnich, Amanda and Chavoshi, Nikan and Koutra, Danai and Mueen, Abdullah","We propose BotWalk, a near-real time adaptive Twitter exploration algorithm to identify bots exhibiting novel behavior. Due to suspension pressure, Twitter bots are constantly changing their behavior to evade detection. Traditional supervised approaches to bot detection are non-adaptive and thus cannot identify novel bot behaviors. We therefore devise an unsupervised approach, which allows us to identify bots as they evolve. We characterize users with a behavioral feature vector which consists of (well-studied in isolation) metadata-, content-, temporal-, and network-based features. We identify a random bot from our seed bank, populated initially by previously-labeled bots, gather this user's followers' features from Twitter in real time, and employ an unsupervised ensemble anomaly detection method in the multi-dimensional behavioral space. These potential bots are folded into the seed bank and the process is then repeated, with the new seeds' features allowing us to adaptively identify novel bot behavior. BotWalk allows for the identification of on average 6,000 potential bots a day. Our method allowed us to detect 7,995 previously undiscovered bots from a sample of 15 seed bots with a precision of 90%.",2017,89
Detecting Financial Bots on the Ethereum Blockchain,"Niedermayer, Thomas and Saggese, Pietro and Haslhofer, Bernhard","The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%. Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.",2024,90
The Roles Bots Play in Wikipedia,"Zheng, Lei (Nico) and Albano, Christopher M. and Vora, Neev M. and Mai, Feng and Nickerson, Jeffrey V.","Bots are playing an increasingly important role in the creation of knowledge in Wikipedia. In many cases, editors and bots form tightly knit teams. Humans develop bots, argue for their approval, and maintain them, performing tasks such as monitoring activity, merging similar bots, splitting complex bots, and turning off malfunctioning bots. Yet this is not the entire picture. Bots are designed to perform certain functions and can acquire new functionality over time. They play particular roles in the editing process. Understanding these roles is an important step towards understanding the ecosystem, and designing better bots and interfaces between bots and humans. This is important for understanding Wikipedia along with other kinds of work in which autonomous machines affect tasks performed by humans. In this study, we use unsupervised learning to build a nine category taxonomy of bots based on their functions in English Wikipedia. We then build a multi-class classifier to classify 1,601 bots based on labeled data. We discuss different bot activities, including their edit frequency, their working spaces, and their software evolution. We use a model to investigate how bots playing certain roles will have differential effects on human editors. In particular, we build on previous research on newcomers by studying the relationship between the roles bots play, the interactions they have with newcomers, and the ensuing survival rate of the newcomers.",2019,91
JITBot: an explainable just-in-time defect prediction bot,"Khanan, Chaiyakarn and Luewichana, Worawit and Pruktharathikoon, Krissakorn and Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee","Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explainability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/",2021,93
"A Case Study of Developer Bots: Motivations, Perceptions, and Challenges","Asthana, Sumit and Sajnani, Hitesh and Voyloshnikova, Elena and Acharya, Birendra and Herzig, Kim","Continuous integration and deployment (CI/CD) is now a widely adopted development model in practice as it reduces the time from ideas to customers. This adoption has also revived the idea of ""shifting left"" during software development -- a practice intended to find and prevent defects early in the software delivery process. To assist with that, engineering systems integrate developer bots in the development workflow to improve developer productivity and help them identify issues early in the software delivery process.  

In this paper, we present a case study of developer bots in Microsoft. We identify and analyze 23 developer bots that are deployed across 13,000 repositories and assist about 6,000 developers daily in their CI/CD software development workflows. We classify these bots across five major categories: Config Violation, Security, Data-privacy, Developer Productivity, and Code Quality. By conducting interviews and surveys with bot developers and bot users and by analyzing about half a million historical bot actions spanning over one and a half years, we present software workflows that motivate bot instrumentation, factors impacting their usefulness as perceived by bot users, and challenges associated with their use. Our findings echo existing issues with bots, such as noise, and illustrate new benefits (e.g., cross-team communication) and challenges (e.g., too many bots) for large software teams.",2023,96
Can Generative AI Bots Be Trusted?,"Denning, Peter J.",It will be a long road to learning how to use generative AI wisely.,2023,107
Classifying issues into custom labels in GitBot,"Park, Doje and Cho, Heetae and Lee, Seonah","GitBots are bots in Git repositories to automate repetitive tasks that occur in software development, testing and maintenance. GitBots are expected to perform the repetitive tasks that are normally done by humans, such as feedback on issue reports and answers to questions. However, studies on GitBots for labeling issue reports fall short of replacing developers' labeling tasks. Developers still manually attach labels to issues. In this paper, we introduce an issue labeling bot classifying issue reports into custom labels that developers define by themselves so that our bot could attach labels in a similar way to human behavior.",2022,118
Understanding automated code review process and developer experience in industry,"Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu","Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality.",2022,121
A unified code review automation for large-scale industry with diverse development environments,"Kim, Hyungjin and Kwon, Yonghwi and Kwon, Hyukin and Ryou, Yeonhee and Joh, Sangwoo and Kim, Taeksu and Kim, Chul-Joo","Code Review is an essential activity to ensure the quality of the software in the development process. Code Review Automation with various analyses can reduce human efforts of code review activities. However, it is a challenge to automate the code review process for large-scale companies such as Samsung Electronics due to their complex development environments: many kinds of products, various sizes of software, different version control systems, and diverse code review systems. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments. Our findings provide practical evidence that our system motivates developers in Samsung Electronics to improve code quality.",2022,123
Translating motivational interviewing for the HPV vaccine into a computable ontology model for automated AI conversational interaction,"Moore, Nicole and Amith, Muhammad and Neumann, Ana and Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui","Human papillomavirus (HPV) vaccinations are lower than expected. To protect the onset of head and neck cancers, innovative strategies to improve the rates are needed. Artificial intelligence may offer some solutions, specifically conversational agents to perform counseling methods. We present our efforts in developing a dialogue model for automating motivational interviewing (MI) to encourage HPV vaccination. We developed a formalized dialogue model for MI using an existing ontology-based framework to manifest a computable representation using OWL2. New utterance classifications were identified along with the ontology that encodes the dialogue model. Our work is available on GitHub under the GPL v.3. We discuss how an ontology-based model of MI can help standardize/formalize MI counseling for HPV vaccine uptake. Our future steps will involve assessing MI fidelity of the ontology model, operationalization, and testing the dialogue model in a simulation with live participants.",2024,135
On the Use of Tests for Software Supply Chain Threats,"Hejderup, Joseph","Development teams are increasingly investing in automating the updating of third-party libraries to limit the patch time of zero-day exploits such as the Equifax breach. GitHub bots such as Dependabot and Renovate build such functionality by leveraging existing test infrastructure in repositories to test and evaluate new library updates. However, two recent studies suggest that test suites in projects lack effectiveness and coverage to reliably find regressions in third-party libraries. Adequate test coverage and effectiveness are critical in discovering new vulnerabilities and weaknesses from third-party libraries. The recent Log4Shell incident exemplifies this, as projects will likely not have adequate tests for logging libraries. This position paper discusses the weaknesses and challenges of current testing practices and techniques from a supply chain security perspective. We highlight two key challenges that researchers and practitioners need to address: (1) the lack of resources and best practices for testing the uses of third-party libraries and (2) enhancing the reliability of automating library updates.",2022,140
Towards Automated Detection of Unethical Behavior in Open-Source Software Projects,"Win, Hsu Myat and Wang, Haibo and Tan, Shin Hwei","Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholders’ perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8% average true positive rate (up to 100% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.",2023,141
Wide-spectrum characterization of long-running political phenomena on social media: the brexit case,"Calisir, Emre and Brambilla, Marco","In this study, we propose a wide-spectrum analysis of long-running political events on social media, with reference to an interesting real-world international case: the so-called Brexit, the process through which the United Kingdom activated the option of leaving the European Union. In this study, we model the users participating in 33 months of Twitter debate, covering their behaviour and demographics. By using publicly shared tweets, we developed a stance classification model to evaluate the change of stance over time. We also extracted the key topics of the long-running debate, studying which political side have discussed them most and what is the general sentiment on each. We also revealed the participation of bot accounts, and we found that the higher the bot score, the more likely the account is in a pro-Leave position. We conclude our study with a temporal and comparative analysis of politicians' social media accounts.",2020,142
Exploit those code reviews! bigger data for deeper learning,"Heum\""{u}ller, Robert and Nielebock, Sebastian and Ortmeier, Frank","Modern code review (MCR) processes are prevalent in most organizations that develop software due to benefits in quality assurance and knowledge transfer. With the rise of collaborative software development platforms like GitHub and Bitbucket, today, millions of projects share not only their code but also their review data. Although researchers have tried to exploit this data for more than a decade, most of that knowledge remains a buried treasure. A crucial catalyst for many advances in deep learning, however, is the accessibility of large-scale standard datasets for different learning tasks. This paper presents the ETCR (Exploit Those Code Reviews!) infrastructure for mining MCR datasets from any GitHub project practicing pull-request-based development. We demonstrate its effectiveness with ETCR-Elasticsearch, a dataset of &gt;231𝑘 review comments for &gt;47𝑘 Java file revisions in &gt;40𝑘 pull-requests from the Elasticsearch project. ETCR is designed with the challenge of deep learning in mind. Compared to previous datasets, ETCR datasets include all information for linking review comments to nodes in the respective program’s Abstract Syntax Tree.",2021,146
GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation,"Zhang, Jing and Zhang, Xiaokang and Zhang-Li, Daniel and Yu, Jifan and Yao, Zijun and Ma, Zeyao and Xu, Yiqi and Wang, Haohua and Zhang, Xiaohan and Lin, Nianyi and Lu, Sunrui and Li, Juanzi and Tang, Jie","We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics. Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. All the source code is available on Github.",2023,148
Toward an empirical theory of feedback-driven development,"Beller, Moritz","Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are pre- rather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial framework on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.",2018,149
Towards offensive language detection and reduction in four Software Engineering communities,"Cheriyan, Jithin and Savarimuthu, Bastin Tony Roy and Cranefield, Stephen","Software Engineering (SE) communities such as Stack Overflow have become unwelcoming, particularly through members’ use of offensive language. Research has shown that offensive language drives users away from active engagement within these platforms. This work aims to explore this issue more broadly by investigating the nature of offensive language in comments posted by users in four prominent SE platforms – GitHub, Gitter, Slack and Stack Overflow (SO). It proposes an approach to detect and classify offensive language in SE communities by adopting natural language processing and deep learning techniques. Further, a Conflict Reduction System (CRS), which identifies offence and then suggests what changes could be made to minimize offence has been proposed. Beyond showing the prevalence of offensive language in over 1 million comments from four different communities which ranges from 0.07% to 0.43%, our results show promise in successful detection and classification of such language. The CRS system has the potential to drastically reduce manual moderation efforts to detect and reduce offence in SE communities.",2021,153
The forgotten case of the dependency bugs: on the example of the robot operating system,"Fischer-Nielsen, Anders and Fu, Zhoulai and Su, Ting and W\k{a}sowski, Andrzej","A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.The paper presents a definition and a taxonomy of dependency bugs extracted from data. It describes multiple facets of these bugs and estimates that as many as 15% (!) of all reported bugs are dependency bugs. We show that lightweight tools can find dependency bugs efficiently, although it is challenging to decide which tools to build and difficult to build general tools. We present the research problem to the community, and posit that it should be feasible to eradicate it from software development practice.",2020,156
Effectiveness of code contribution: from patch-based to pull-request-based tools,"Zhu, Jiaxin and Zhou, Minghui and Mockus, Audris","Code contributions in Free/Libre and Open Source Software projects are controlled to maintain high-quality of software. Alternatives to patch-based code contribution tools such as mailing lists and issue trackers have been developed with the pull request systems being the most visible and widely available on GitHub. Is the code contribution process more effective with pull request systems? To answer that, we quantify the effectiveness via the rates contributions are accepted and ignored, via the time until the first response and final resolution and via the numbers of contributions. To control for the latent variables, our study includes a project that migrated from an issue tracker to the GitHub pull request system and a comparison between projects using mailing lists and pull request systems. Our results show pull request systems to be associated with reduced review times and larger numbers of contributions. However, not all the comparisons indicate substantially better accept or ignore rates in pull request systems. These variations may be most simply explained by the differences in contribution practices the projects employ and may be less affected by the type of tool. Our results clarify the importance of understanding the role of tools in effective management of the broad network of potential contributors and may lead to strategies and practices making the code contribution more satisfying and efficient from both contributors' and maintainers' perspectives.",2016,159
"""@alex, this fixes #9"": Analysis of Referencing Patterns in Pull Request Discussions","Chopra, Ashish and Mo, Morgan and Dodson, Samuel and Beschastnikh, Ivan and Fels, Sidney S. and Yoon, Dongwook","Pull Requests (PRs) are a frequently used method for proposing changes to source code repositories. When discussing proposed changes in a PR discussion, stakeholders often reference a wide variety of information objects for establishing shared awareness and common ground. Previous work has not considered how the referential behavior impacts collaborative software development via PRs. This knowledge gap is the major barrier in evaluating the current support for referencing in PRs and improving them. We conducted an explorative analysis of textasciitilde7K references, collected from 450 public PRs on GitHub, and constructed taxonomies of referent types and expressions. Using our annotated dataset, we identified several patterns in the use of references. Referencing source code elements was prevalent but the authoring interface lacks support for it. Three classes of contextual factors influence referencing behaviors: referent type, discussion thread, and project attributes. Referencing patterns may indicate PR outcomes (e.g., merged PRs frequently reference issues, users, and tests). We conclude with design implications to support more effective referencing in PR discussion interfaces.",2021,160
An Experience Report on Technical Debt in Pull Requests: Challenges and Lessons Learned,"Karmakar, Shubhashis and Codabux, Zadia and Vidoni, Melina","Background: GitHub is a collaborative platform for global software development, where Pull Requests (PRs) are essential to bridge code changes with version control. However, developers often trade software quality for faster implementation, incurring Technical Debt (TD). When developers undertake reviewers’ roles and evaluate PRs, they can often detect TD instances, leading to either PR rejection or discussions. Aims: We investigated whether Pull Request Comments (PRCs) indicate TD by assessing three large-scale repositories: Spark, Kafka, and React. Method: We combined manual classification with automated detection using machine learning and deep learning models. Results: We classified two datasets and found that 37.7 and 38.7% of PRCs indicate TD, respectively. Our best model achieved F1 = 0.85 when classifying TD during the validation phase. Conclusions: We faced several challenges during this process, which may hint that TD in PRCs is discussed differently from other software artifacts (e.g., code comments, commits, issues, or discussion forums). Thus, we present challenges and lessons learned to assist researchers in pursuing this area of research.",2022,167
Motivation Research Using Labeling Functions,"Amit, Idan and Feitelson, Dror G.","Motivation is an important factor in software development. However, it is a subjective concept that is hard to quantify and study empirically. In order to use the wealth of data available about real software development projects in GitHub, we represent the motivation of developers using labeling functions. These are validated heuristics that need only be better than a guess, computable on a dataset. We define four labeling functions for motivation based on behavioral cues like working in diverse hours of the day. We validated the functions by agreement with respect to a developers survey, per person behavior, and temporal changes. We then apply them to 150 thousand developers working on GitHub projects. Using the identification of motivated developers, we measure developer performance gaps. We show that motivated developers have up to 70% longer activity period, produce up to 300% more commits, and invest up to 44% more time per commit.",2024,168
"""Looks Good To Me ;-)"": Assessing Sentiment Analysis Tools for Pull Request Discussions","Coutinho, Daniel and Cito, Luisa and Lima, Maria Vit\'{o}ria and Arantes, Beatriz and Alves Pereira, Juliana and Arriel, Johny and Godinho, Jo\~{a}o and Martins, Vinicius and Lib\'{o}rio, Paulo V\'{\i}tor C. F. and Leite, Leonardo and Garcia, Alessandro and Assun\c{c}\~{a}o, Wesley K. G. and Steinmacher, Igor and Baffa, Augusto and Fonseca, Baldoino","Modern software development relies on cloud-based collaborative platforms (e.g., GitHub and GitLab). In these platforms, developers often employ a pull-based development approach, proposing changes via pull requests and engaging in communication via asynchronous message exchanges. Since communication is key for software development, studies have linked different types of sentiments embedded in the communication to their effects on software projects, such as bug-inducing commits or the non-acceptance of pull requests. In this context, sentiment analysis tools are paramount to detect the sentiment of developers’ messages and prevent potentially harmful impact. Unfortunately, existing state-of-the-art tools vary in terms of the nature of their data collection and labeling processes. Yet, there is no comprehensive study comparing the performance and generalizability of existing tools utilizing a dataset that was designed and systematically curated to this end, and in this specific context. Therefore, in this study, we design a methodology to assess the effectiveness of existing sentiment analysis tools in the context of pull request discussions. For that, we created a dataset that contains ≈ 1.8K manually labeled messages from 36 software projects. The messages were labeled by 19 experts (neuroscientists and software engineers), using a novel and systematic manual classification process designed to reduce subjectivity. By applying these existing tools to the dataset, we observed that while some tools ]perform acceptably, their performance is far from ideal, especially when classifying negative messages. This is interesting since negative sentiment is often related to a critical or unfavorable opinion. We also observed that some messages have characteristics that can make them harder to classify, causing disagreements between the experts and possible misclassifications by the tools, requiring more attention from researchers. Our contributions include valuable resources to pave the way to develop robust and mature sentiment analysis tools that capture/anticipate potential problems during software development.",2024,181
Understanding Regular Expression Denial of Service (ReDoS): Insights from LLM-Generated Regexes and Developer Forums,"Siddiq, Mohammed Latif and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva","Regular expression Denial of Service (ReDoS) represents an algorithmic complexity attack that exploits the processing of regular expressions (regexes) to produce a denial-of-service attack. This attack occurs when a regex's evaluation time scales polynomially or exponentially with input length, posing significant challenges for software developers. The advent of Large Language Models (LLMs) has revolutionized the generation of regexes from natural language prompts, but not without its risks. Prior works showed that LLMs can generate code with vulnerabilities and security smells. In this paper, we examined the correctness and security of regexes generated by LLMs as well as the characteristics of LLM-generated vulnerable regexes. Our study also examined ReDoS patterns in actual software projects, aligning them with corresponding regex equivalence classes and algorithmic complexity. Moreover, we analyzed developer discussions on GitHub and StackOverflow, constructing a taxonomy to investigate their experiences and perspectives on ReDoS. In this study, we found that GPT-3.5 was the best LLM to generate regexes that are both correct and secure. We also observed that LLM-generated regexes mainly have polynomial ReDoS vulnerability patterns, and it is consistent with vulnerable regexes found in open source projects. We also found that developers' main discussions around insecure regexes is related to mitigation strategies to remove vulnerable regexes.",2024,186
Massive Cross-Platform Simulations of Online Social Networks,"Muri\'{c}, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\'{e}s and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio","As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.",2020,191
Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks,"Weiss, Michael and Tonella, Paolo","Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remote-only model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.",2023,193
The Human Side of Fuzzing: Challenges Faced by Developers during Fuzzing Activities,"Nourry, Olivier and Kashiwa, Yutaro and Lin, Bin and Bavota, Gabriele and Lanza, Michele and Kamei, Yasutaka","Fuzz testing, also known as fuzzing, is a software testing technique aimed at identifying software vulnerabilities. In recent decades, fuzzing has gained increasing popularity in the research community. However, existing studies led by fuzzing experts mainly focus on improving the coverage and performance of fuzzing techniques. That is, there is still a gap in empirical knowledge regarding fuzzing, especially about the challenges developers face when they adopt fuzzing. Understanding these challenges can provide valuable insights to both practitioners and researchers on how to further improve fuzzing processes and techniques.We conducted a study to understand the challenges encountered by developers during fuzzing. More specifically, we first manually analyzed 829 randomly sampled fuzzing-related GitHub issues and constructed a taxonomy consisting of 39 types of challenges (22 related to the fuzzing process itself, 17 related to using external fuzzing providers). We then surveyed 106 fuzzing practitioners to verify the validity of our taxonomy and collected feedback on how the fuzzing process can be improved. Our taxonomy, accompanied with representative examples and highlighted implications, can serve as a reference point on how to better adopt fuzzing techniques for practitioners, and indicates potential directions researchers can work on toward better fuzzing approaches and practices.",2023,200
PhishinWebView: Analysis of Anti-Phishing Entities in Mobile Apps with WebView Targeted Phishing,"Choi, Yoonjung and Lee, Woonghee and Hur, Junbeom","Despite the relentless efforts on developing anti-phishing techniques, phishing attacks continue to proliferate, often incorporating evasion techniques to bypass detection. While recent studies have continuously enhanced our understanding of their evasion techniques in desktop environments, few studies have been conducted to explore how the phishing attack is being handled in mobile environments, specifically WebView.In this study, we systematically evaluate the blocking processes of anti-phishing entities in individual apps in the real world by designing the phishing attack tailored to WebView. Specifically, we select eight well-known apps using WebView, and report 80 typical phishing sites (without evasion techniques) and 130 user-agent-specific phishing sites (accessible exclusively via each app's WebView). For scalable analysis, we develop an autonomous evaluation framework and investigate accessibility of both apps and Safe Browsing entities. As a result, we find that user-agent-specific (UA-specific) phishing sites successfully evade blocking across all of the eight Android apps. We also investigate accessing strategies of anti-phishing crawlers of both the apps and Safe Browsing entities; and find that only two apps' crawlers can access UA-specific phishing sites without any subsequent actions such as blocking the link. Based on our experiment results, we present security recommendations to take proactive phishing cautions using link preview bots. To the best of our knowledge, this is the first study that explores how the WebView environments handle phishing attacks and disclose their limitation in the real world.",2024,217
Properties of Fairness Measures in the Context of Varying Class Imbalance and Protected Group Ratios,"Brzezinski, Dariusz and Stachowiak, Julia and Stefanowski, Jerzy and Szczech, Izabela and Susmaga, Robert and Aksenyuk, Sofya and Ivashka, Uladzimir and Yasinskyi, Oleksandr","Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.",2024,223
Example Driven Code Review Explanation,"Rahman, Shadikur and Koana, Umme Ayman and Nayebi, Maleknaz","Background: Code reviewing is an essential part of software development to ensure software quality. However, the abundance of review tasks and the intensity of the workload for reviewers negatively impact the quality of the reviews. The short review text is often unactionable. Aims: We propose the Example Driven Review Explanation (EDRE) method to facilitate the code review process by adding additional explanations through examples. EDRE recommends similar code reviews as examples to further explain a review and help a developer to understand the received reviews with less communication overhead. Method: Through an empirical study in an industrial setting and by analyzing 3,722 Code reviews across three open-source projects, we compared five methods of data retrieval, text classification, and text recommendation. Results: EDRE using TF-IDF word embedding along with an SVM classifier can provide practical examples for each code review with 92% F-score and 90% Accuracy. Conclusions: The example-based explanation is an established method for assisting experts in explaining decisions. EDRE can accurately provide a set of context-specific examples to facilitate the code review process in software teams.",2022,224
Enabling Collaborative Data Science Development with the Ballet Framework,"Smith, Micah J. and Cito, J\""{u}rgen and Lu, Kelvin and Veeramachaneni, Kalyan","While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects.",2021,237
Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study,"Tufano, Rosalia and Mastropaolo, Antonio and Pepe, Federica and Dabic, Ozren and Di Penta, Massimiliano and Bavota, Gabriele","Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.",2024,243
What makes a good commit message?,"Tian, Yingchen and Zhang, Yuxia and Stol, Klaas-Jan and Jiang, Lin and Liu, Hui","A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an ""audit trail"" by which developers can understand how the source code of a project has changed---and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a ""good"" commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether ""good"" commit messages can be automatically identified; such automation could prompt developers to write better commit messages.",2022,246
Blackmarket-Driven Collusion on Online Media: A Survey,"Dutta, Hridoy Sankar and Chakraborty, Tanmoy","Online media platforms have enabled users to connect with individuals and organizations, and share their thoughts. Other than connectivity, these platforms also serve multiple purposes, such as education, promotion, updates, and awareness. Increasing, the reputation of individuals in online media (aka social reputation) is thus essential these days, particularly for business owners and event managers who are looking to improve their publicity and sales. The natural way of gaining social reputation is a tedious task, which leads to the creation of unfair ways to boost the reputation of individuals artificially. Several online blackmarket services have developed a thriving ecosystem with lucrative offers to attract content promoters for publicizing their content online. These services are operated in such a way that most of their inorganic activities are going unnoticed by the media authorities, and the customers of the blackmarket services are less likely to be spotted. We refer to such unfair ways of bolstering social reputation in online media as collusion. This survey is the first attempt to provide readers a comprehensive outline of the latest studies dealing with the identification and analysis of blackmarket-driven collusion in online media. We present a broad overview of the problem, definitions of the related problems and concepts, the taxonomy of the proposed approaches, and a description of the publicly available datasets and online tools, and we discuss the outstanding issues. We believe that collusive entity detection is a newly emerging topic in anomaly detection and cyber-security research in general, and the current survey will provide readers with an easy-to-access and comprehensive list of methods, tools, and resources proposed so far for detecting and analyzing collusive entities on online media.",2022,255
Nudge: Accelerating Overdue Pull Requests toward Completion,"Maddila, Chandra and Upadrasta, Sai Surya and Bansal, Chetan and Nagappan, Nachiappan and Gousios, Georgios and van Deursen, Arie","Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge’s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.",2023,265
Automated testing of software that uses machine learning APIs,"Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yifan and Hoffmann, Henry and Maire, Michael and Lu, Shan","An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.",2022,269
Mining software repositories with a collaborative heuristic repository,"Babii, Hlib and Prenner, Julian Aron and Stricker, Laurin and Karmakar, Anjan and Janes, Andrea and Robbes, Romain","Many software engineering studies or tasks rely on categorizing software engineering artifacts. In practice, this is done either by defining simple but often imprecise heuristics, or by manual labelling of the artifacts. Unfortunately, errors in these categorizations impact the tasks that rely on them. To improve the precision of these categorizations, we propose to gather heuristics in a collaborative heuristic repository, to which researchers can contribute a large amount of diverse heuristics for a variety of tasks on a variety of SE artifacts. These heuristics are then leveraged by state-of-the-art weak supervision techniques to train high-quality classifiers, thus improving the categorizations. We present an initial version of the heuristic repository, which we applied to the concrete task of commit classification.",2021,272
Challenges in the collaborative development of a complex mathematical software and its ecosystem,"Zimmermann, Th\'{e}o","This is a contribution to the OpenSym 2018 Doctoral Symposium. This paper describes my PhD objectives. As an insider in the Coq development team, I've worked at making the release process of the Coq proof assistant smoother and more automated, at opening the development to external contributions, and at shaping the ecosystem around Coq. I'm intending to evaluate how well-known software engineering techniques and results about open source software communities apply in the specific case of the proof assistant I'm studying.",2018,274
Geographic diversity in public code contributions: an exploratory large-scale study over 50 years,"Rossi, Davide and Zacchiroli, Stefano","We conduct an exploratory, large-scale, longitudinal study of 50 years of commits to publicly available version control system repositories, in order to characterize the geographic diversity of contributors to public code and its evolution over time. We analyze in total 2.2 billion commits collected by Software Heritage from 160 million projects and authored by 43 million authors during the 1971--2021 time period. We geolocate developers to 12 world regions derived from the United Nation geoscheme, using as signals email top-level domains, author names compared with names distributions around the world, and UTC offsets mined from commit metadata.We find evidence of the early dominance of North America in open source software, later joined by Europe. After that period, the geographic diversity in public code has been constantly increasing. We also identify relevant historical shifts related to the UNIX wars, the increase of coding literacy in Central and South Asia, and broader phenomena like colonialism and people movement across countries (immigration/emigration).",2022,290
Towards Collaborative Continuous Benchmarking for HPC,"Pearce, Olga and Scott, Alec and Becker, Gregory and Haque, Riyaz and Hanford, Nathan and Brink, Stephanie and Jacobsen, Doug and Poxon, Heidi and Domke, Jens and Gamblin, Todd","Benchmarking is integral to procurement of HPC systems, communicating HPC center workloads to HPC vendors, and verifying performance of the delivered HPC systems. Currently, HPC benchmarking is manual and challenging at every step, posing a high barrier to entry, and hampering reproducibility of the benchmarks across different HPC systems. In this paper, we propose collaborative continuous benchmarking to enable functional reproducibility, automation, and community collaboration in HPC benchmarking. Recent progress in HPC automation allows us to consider previously unimaginable large-scale improvements to the HPC ecosystem. We define the minimal requirements for collaborative continuous benchmarking and develop a common language to streamline the interactions between HPC centers, vendors, and researchers. We demonstrate the initial implementation of collaborative continuous benchmarking, and introduce an open source continuous benchmarking repository, Benchpark, for community collaboration. We believe collaborative continuous benchmarking will help overcome the human bottleneck in HPC benchmarking, enabling better evaluation of our systems and enabling a more productive collaboration within the HPC community.",2023,301
Investigating Reflection in Undergraduate Software Development Teams: An Analysis of Online Chat Transcripts,"Hundhausen, Christopher and Conrad, Phill and Adesope, Olusola and Tariq, Ahsun and Sbai, Samir and Lu, Andrew","Metacognition is widely acknowledged as a key soft skill in collaborative software development. The ability to plan, monitor, and reflect on cognitive and team processes is crucial to the efficient and effective functioning of a software team. To explore students' use of reflection--one aspect of metacognition--in undergraduate team software projects, we analyzed the online chat channels of teams participating in agile software development projects in two undergraduate courses that took place exclusively online (n = 23 teams, 117 students, and 4,915 chat messages). Teams' online chats were dominated by discussions of work completed and to be done; just two percent of all chat messages showed evidence of reflection. A follow-up analysis of chat vignettes centered around reflection messages (n = 63) indicates that three-fourths of the those messages were prompted by a course requirement; just 14% arose organically within the context of teams' ongoing project work. Based on our findings, we identify opportunities for computing educators to increase, through pedagogical and technological interventions, teams' use of reflection in team software projects.",2023,307
"GitWaterFlow: a successful branching model and tooling, for achieving continuous delivery with multiple version branches","Rayana, Rayene Ben and Killian, Sylvain and Trangez, Nicolas and Calmettes, Arnaud","Collaborative software development presents organizations with a near-constant flow of day-to-day challenges, and there is no available off-the-shelf solution that covers all needs. This paper provides insight into the hurdles that Scality's Engineering team faced in developing and extending a sophisticated storage solution, while coping with ever-growing development teams, challenging - and regularly shifting - business requirements, and non-trivial new feature development. The authors present a novel combination of a Git-based Version Control and Branching model with a set of innovative tools dubbed GitWaterFlow to cope with the issues encountered, including the need to both support old product versions and to provide time-critical delivery of bug fixes. In the spirit of Continuous Delivery, Scality Release Engineering aims to ensure high quality and stability, to present short and predictable release cycles, and to minimize development disruption. The team's experience with the GitWaterFlow model suggests that the approach has been effective in meeting these goals in the given setting, with room for unceasing fine-tuning and improvement of processes and tools.",2016,311
An Information System for Law Integrating Ontological Bases with a Legal Reasoner Chatbot,"Monteiro, Lucas Henrique de Assis and Rodrigues, Cleyton M\'{a}rio de Oliveira and Sousa, A\^{e}da Monalliza Cunha de","Context: The Semantic Web aims to assign meanings to resources available on the internet so that humans and computers can understand them. It can be used in the most diverse contexts, facilitating the development of systems where expert knowledge is formalized through logical-mathematical resources, mitigating potential inconsistencies, and promoting more human-friendly interaction services. Problem: The existence of semantic anomalies (use of rhetorical language, polysemy and inaccuracies) in the Brazilian Legal Domain enables the use of Semantic Web standards and technologies to mitigate these problems. Solution: This work deals with the development of an Information System that uses resources from the Semantic Web for the formal representation and the realization of legal inferences about Crimes Against Property. SI Theory: The Behavioral Decision Theory was approached, mainly in the incorporation of real patterns of decision making. Method: Bibliographic and documentary research methods were used to list the main concepts related to the Criminal Types investigated. The research is prescriptive and has a quali-quantitative approach. Summary of Results: A prototype system is presented, integrating ontologies of Brazilian Law with a chatbot that enables interaction with users in natural language, as well as performing reasoning tasks based on the knowledge formalized in these ontologies. Contributions and Impact in the IS area: The research will contribute to the automation of decision-making processes involving crimes against property, serving as an aid for professionals or law students and for legal simulations by ordinary people. Furthermore, it will serve as a reference for the development of other information systems with similar objectives in other contexts.",2022,320
ThingNet: a lightweight real-time mirai IoT variants hunter through CPU power fingerprinting,"Li, Zhuoran and Zhao, Dan","Internet of Things (IoT) devices have become attractive targets of cyber criminals, whereas attackers have been leveraging these vulnerable devices most notably via the infamous Mirai-based botnets, accounting for nearly 90% of IoT malware attacks in 2020. In this work, we propose a robust, universal and non-invasive Mirai-based malware detection engine employing a compact deep neural network architecture. Our design allows programmatic collection of CPU power footprints with integrated current sensors under various device states, such as idle, service and attack. A lightweight online inference model is deployed in the CPU for on-the-fly classification. Our model is robust against noisy environment with a lucid design of noise reduction function. This work appears to be the first step towards a viable CPU malware detection engine based on power fingerprinting. The extensive simulation study under ARM architecture that is widely used in IoT devices, demonstrates a high detection accuracy of 99.1% at a speed less than 1ms. By analyzing Mirai-based infection under distinguishable phases for power feature extraction, our model has further demonstrated an accuracy of 96.3% on model-unknown variants detection.",2022,341
CVEfixes: automated collection of vulnerabilities and their fixes from open-source software,"Bhandari, Guru and Naseer, Amara and Moonen, Leon","Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.",2021,373
"A wiki for Mizar: motivation, considerations, and initial prototype","Urban, Josef and Alama, Jesse and Rudnicki, Piotr and Geuvers, Herman","Formal mathematics has so far not taken full advantage of ideas from collaborative tools such as wikis and distributed version control systems (DVCS). We argue that the field could profit from such tools, serving both newcomers and experts alike. We describe a preliminary system for such collaborative development based on the Git DVCS. We focus, initially, on the Mizar system and its library of formalized mathematics.",2010,383
Challenges in Chatbot Development: A Study of Stack Overflow Posts,"Abdellatif, Ahmad and Costa, Diego and Badran, Khaled and Abdalkareem, Rabe and Shihab, Emad","Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development.Therefore, in this paper, we examine the Q&amp;A website, Stack Overflow, to provide insights on the topics that chatbot developers are interested and the challenges they face. In particular, we leverage topic modeling to understand the topics that are being discussed by chatbot developers on Stack Overflow. Then, we examine the popularity and difficulty of those topics. Our results show that most of the chatbot developers are using Stack Overflow to ask about implementation guidelines. We determine 12 topics that developers discuss (e.g., Model Training) that fall into five main categories. Most of the posts belong to chatbot development, integration, and the natural language understanding (NLU) model categories. On the other hand, we find that developers consider the posts of building and integrating chatbots topics more helpful compared to other topics. Specifically, developers face challenges in the training of the chatbot's model. We believe that our study guides future research to propose techniques and tools to help the community at its early stages to overcome the most popular and difficult topics that practitioners face when developing chatbots.",2020,384
A Survey of Multi-modal Knowledge Graphs: Technologies and Trends,"Liang, Wanying and Meo, Pasquale De and Tang, Yong and Zhu, Jia","In recent years, Knowledge Graphs (KGs) have played a crucial role in the development of advanced knowledge-intensive applications, such as recommender systems and semantic search. However, the human sensory system is inherently multi-modal, as objects around us are often represented by a combination of multiple signals, such as visual and textual. Consequently, Multi-modal Knowledge Graphs (MMKGs), which combine structured knowledge representation with multiple modalities, represent a powerful extension of KGs. Although MMKGs can handle certain types of tasks (e.g., visual query answering) or queries that standard KGs cannot process, and they can effectively tackle some standard problems (e.g., entity alignment), we lack a widely accepted definition of MMKG. In this survey, we provide a rigorous definition of MMKGs along with a classification scheme based on how existing approaches address four fundamental challenges: representation, fusion, alignment, and translation, which are crucial to improving an MMKG. Our classification scheme is flexible and allows for easy incorporation of new approaches, as well as a comparison of two approaches in terms of how they address one of the fundamental challenges mentioned above. As the first comprehensive survey of MMKG, this article aims at inspiring and provide a reference for relevant researchers in the field of Artificial Intelligence.",2024,388
Analyzing the Evolution and Maintenance of ML Models on Hugging Face,"Casta\~{n}o, Joel and Silverio, Mart\'{\i}nez-Fern\'{a}ndez and Franch, Xavier and Bogner, Justus","Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF - aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.",2024,394
Recommending Who to Follow in the Software Engineering Twitter Space,"Sharma, Abhishek and Tian, Yuan and Sulistya, Agus and Wijedasa, Dinusha and Lo, David","With the advent of social media, developers are increasingly using it in their software development activities. Twitter is one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape.” Unfortunately, due to the general-purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets.To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domain-related tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over 1 month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-Measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-Measure by at least 7.63%.",2018,396
ITA-ELECTION-2022: A Multi-Platform Dataset of Social Media Conversations Around the 2022 Italian General Election,"Pierri, Francesco and Liu, Geng and Ceri, Stefano","Online social media play a major role in shaping public discourse and opinion, especially during political events. We present the first public multi-platform dataset of Italian-language political conversations, focused on the 2022 Italian general election taking place on September 25th. Leveraging public APIs and a keyword-based search, we collected millions of posts published by users, pages and groups on Facebook, Instagram and Twitter, along with metadata of TikTok and YouTube videos shared on these platforms, over a period of four months. We augmented the dataset with a collection of political ads sponsored on Meta platforms, and a list of social media handles associated with political representatives. Our data resource will allow researchers and academics to further our understanding of the role of social media in the democratic process.",2023,423
Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications,"Gamage, Dilrukshi and Ghasiya, Piyush and Bonagiri, Vamshi and Whiting, Mark E. and Sasahara, Kazutoshi","Deepfakes are synthetic content generated using advanced deep learning and AI technologies. The advancement of technology has created opportunities for anyone to create and share deepfakes much easier. This may lead to societal concerns based on how communities engage with it. However, there is limited research available to understand how communities perceive deepfakes. We examined deepfake conversations on Reddit from 2018 to 2021—including major topics and their temporal changes as well as implications of these conversations. Using a mixed-method approach—topic modeling and qualitative coding, we found 6,638 posts and 86,425 comments discussing concerns of the believable nature of deepfakes and how platforms moderate them. We also found Reddit conversations to be pro-deepfake and building a community that supports creating and sharing deepfake artifacts and building a marketplace regardless of the consequences. Possible implications derived from qualitative codes indicate that deepfake conversations raise societal concerns. We propose that there are implications for Human Computer Interaction (HCI) to mitigate the harm created from deepfakes.",2022,450
Pair programming conversations with agents vs. developers: challenges and opportunities for SE community,"Robe, Peter and Kuttal, Sandeep K. and AuBuchon, Jake and Hart, Jacob","Recent research has shown feasibility of an interactive pair-programming conversational agent, but implementing such an agent poses three challenges: a lack of benchmark datasets, absence of software engineering specific labels, and the need to understand developer conversations. To address these challenges, we conducted a Wizard of Oz study with 14 participants pair programming with a simulated agent and collected 4,443 developer-agent utterances. Based on this dataset, we created 26 software engineering labels using an open coding process to develop a hierarchical classification scheme. To understand labeled developer-agent conversations, we compared the accuracy of three state-of-the-art transformer-based language models, BERT, GPT-2, and XLNet, which performed interchangeably. In order to begin creating a developer-agent dataset, researchers and practitioners need to conduct resource intensive Wizard of Oz studies. Presently, there exists vast amounts of developer-developer conversations on video hosting websites. To investigate the feasibility of using developer-developer conversations, we labeled a publicly available developer-developer dataset (3,436 utterances) with our hierarchical classification scheme and found that a BERT model trained on developer-developer data performed ~10% worse than the BERT trained on developer-agent data, but when using transfer-learning, accuracy improved. Finally, our qualitative analysis revealed that developer-developer conversations are more implicit, neutral, and opinionated than developer-agent conversations. Our results have implications for software engineering researchers and practitioners developing conversational agents.",2022,456
CC2Vec: distributed representations of code changes,"Hoang, Thong and Kang, Hong Jin and Lo, David and Lawall, Julia","Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.",2020,462
"Socio-technical Affordances for Stigmergic Coordination Implemented in MIDST, a Tool for Data-Science Teams","Crowston, Kevin and Saltz, Jeff S. and Rezgui, Amira and Hegde, Yatish and You, Sangseok","We present a conceptual framework for socio-technical affordances for stigmergic coordination, that is, coordination supported by a shared work product. Based on research on free/libre open source software development, we theorize that stigmergic coordination depends on three sets of socio-technical affordances: the visibility and combinability of the work, along with defined genres of work contributions. As a demonstration of the utility of the developed framework, we use it as the basis for the design and implementation of a system, MIDST, that supports these affordances and that we thus expect to support stigmergic coordination. We describe an initial assessment of the impact of the tool on the work of project teams of three to six data-science students that suggests that the tool was useful but also in need of further development. We conclude with plans for future research and an assessment of theory-driven system design.",2019,466
Detection of algorithmically generated domain names used by botnets: a dual arms race,"Spooren, Jan and Preuveneers, Davy and Desmet, Lieven and Janssen, Peter and Joosen, Wouter","Malware typically uses Domain Generation Algorithms (DGAs) as a mechanism to contact their Command and Control server. In recent years, different approaches to automatically detect generated domain names have been proposed, based on machine learning. The first problem that we address is the difficulty to systematically compare these DGA detection algorithms due to the lack of an independent benchmark. The second problem that we investigate is the difficulty for an adversary to circumvent these classifiers when the machine learning models backing these DGA-detectors are known. In this paper we compare two different approaches on the same set of DGAs: classical machine learning using manually engineered features and a 'deep learning' recurrent neural network. We show that the deep learning approach performs consistently better on all of the tested DGAs, with an average classification accuracy of 98.7% versus 93.8% for the manually engineered features. We also show that one of the dangers of manual feature engineering is that DGAs can adapt their strategy, based on knowledge of the features used to detect them. To demonstrate this, we use the knowledge of the used feature set to design a new DGA which makes the random forest classifier powerless with a classification accuracy of 59.9%. The deep learning classifier is also (albeit less) affected, reducing its accuracy to 85.5%.",2019,480
Deceiving Portable Executable Malware Classifiers into Targeted Misclassification with Practical Adversarial Examples,"Kucuk, Yunus and Yan, Guanhua","Due to voluminous malware attacks in the cyberspace, machine learning has become popular for automating malware detection and classification. In this work we play devil's advocate by investigating a new type of threats aimed at deceiving multi-class Portable Executable (PE) malware classifiers into targeted misclassification with practical adversarial samples. Using a malware dataset with tens of thousands of samples, we construct three types of PE malware classifiers, the first one based on frequencies of opcodes in the disassembled malware code (opcode classifier), the second one the list of API functions imported by each PE sample (API classifier), and the third one the list of system calls observed in dynamic execution (system call classifier). We develop a genetic algorithm augmented with different support functions to deceive these classifiers into misclassifying a PE sample into any target family. Using an Rbot malware sample whose source code is publicly available, we are able to create practical adversarial samples that can deceive the opcode classifier into targeted misclassification with a successful rate of 75%, the API classifier with a successful rate of 83.3%, and the system call classifier with a successful rate of 91.7%.",2020,482
Dynamic Malware Analysis in the Modern Era—A State of the Art Survey,"Or-Meir, Ori and Nissim, Nir and Elovici, Yuval and Rokach, Lior","Although malicious software (malware) has been around since the early days of computers, the sophistication and innovation of malware has increased over the years. In particular, the latest crop of ransomware has drawn attention to the dangers of malicious software, which can cause harm to private users as well as corporations, public services (hospitals and transportation systems), governments, and security institutions. To protect these institutions and the public from malware attacks, malicious activity must be detected as early as possible, preferably before it conducts its harmful acts. However, it is not always easy to know what to look for—especially when dealing with new and unknown malware that has never been seen. Analyzing a suspicious file by static or dynamic analysis methods can provide relevant and valuable information regarding a file's impact on the hosting system and help determine whether the file is malicious or not, based on the method's predefined rules. While various techniques (e.g., code obfuscation, dynamic code loading, encryption, and packing) can be used by malware writers to evade static analysis (including signature-based anti-virus tools), dynamic analysis is robust to these techniques and can provide greater understanding regarding the analyzed file and consequently can lead to better detection capabilities. Although dynamic analysis is more robust than static analysis, existing dynamic analysis tools and techniques are imperfect, and there is no single tool that can cover all aspects of malware behavior. The most recent comprehensive survey performed in this area was published in 2012. Since that time, the computing environment has changed dramatically with new types of malware (ransomware, cryptominers), new analysis methods (volatile memory forensics, side-channel analysis), new computing environments (cloud computing, IoT devices), new machine-learning algorithms, and more. The goal of this survey is to provide a comprehensive and up-to-date overview of existing methods used to dynamically analyze malware, which includes a description of each method, its strengths and weaknesses, and its resilience against malware evasion techniques. In addition, we include an overview of prominent studies presenting the usage of machine-learning methods to enhance dynamic malware analysis capabilities aimed at detection, classification, and categorization.",2019,494
Worldwide gender differences in public code contributions: and how they have been affected by the COVID-19 pandemic,"Rossi, Davide and Zacchiroli, Stefano","Gender imbalance is a well-known phenomenon observed throughout sciences which is particularly severe in software development and Free/Open Source Software communities. Little is know yet about the geography of this phenomenon in particular when considering large scales for both its time and space dimensions.We contribute to fill this gap with a longitudinal study of the population of contributors to publicly available software source code. We analyze the development history of 160 million software projects for a total of 2.2 billion commits contributed by 43 million distinct authors over a period of 50 years. We classify author names by gender using name frequencies and author geographical locations using heuristics based on email addresses and time zones. We study the evolution over time of contributions to public code by gender and by world region.For the world overall, we confirm previous findings about the low but steadily increasing ratio of contributions by female authors. When breaking down by world regions we find that the long-term growth of female participation is a world-wide phenomenon. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women's ability to contribute to public code has been more hindered than that of men.Software developers around the world work together to produce publicly available software (or public code). They do so using public identities and disclosing information about their work that include their names and when a software change was made. We use this information to characterize the gender gap in public code, that is, the difference in participation to public software development between men and women. Specifically, we study the development history of 160 million pieces of public software, developed over a period of 50 years by 43 million authors. We characterize the gender gap on this corpus over time and by world region. To determine author genders we rely on public data about name frequencies by gender around the world. To determine author locations we use email addresses, name frequencies around the world, and the timezone associated to each software change. We confirm that the gender gap in public code is huge. Female authors are only 8.1% of the total and have authored only 13.5% software versions. The gender gap is however shrinking, with women participation having increased steadily over the past 12 years. This improvement is a global phenomenon, observable in most world regions. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women have been more hindered than men in their ability to contribute to public code.",2022,501
Patterns of Effort Contribution and Demand and User Classification based on Participation Patterns in NPM Ecosystem,"Dey, Tapajit and Ma, Yuxing and Mockus, Audris","Background: Open source requires participation of volunteer and commercial developers (users) in order to deliver functional high-quality components. Developers both contribute effort in the form of patches and demand effort from the component maintainers to resolve issues reported against it. Open source components depend on each other directly and transitively, and evidence suggests that more effort is required for reporting and resolving the issues reported further upstream in this supply chain. Aim: Identify and characterize patterns of effort contribution and demand throughout the open source supply chain and investigate if and how these patterns vary with developer activity; identify different groups of developers; and predict developers' company affiliation based on their participation patterns. Method: 1,376,946 issues and pull-requests created for 4433 NPM packages with over 10,000 monthly downloads and full (public) commit activity data of the 272,142 issue creators is obtained and analyzed and dependencies on NPM packages are identified. Fuzzy c-means clustering algorithm is used to find the groups among the users based on their effort contribution and demand patterns, and Random Forest is used as the predictive modeling technique to identify their company affiliations. Result: Users contribute and demand effort primarily from packages that they depend on directly with only a tiny fraction of contributions and demand going to transitive dependencies. A significant portion of demand goes into packages outside the users' respective supply chains (constructed based on publicly visible version control data). Three and two different groups of users are observed based on the effort demand and effort contribution patterns respectively. The Random Forest model used for identifying the company affiliation of the users gives a AUC-ROC value of 0.68, and variables representing aggregate participation patterns proved to be the important predictors. Conclusion: Our results give new insights into effort demand and supply at different parts of the supply chain of the NPM ecosystem and its users and suggests the need to increase visibility further upstream.",2019,506
Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection,"Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario","Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",2023,533
Detecting Anomalous Computation with RNNs on GPU-Accelerated HPC Machines,"Zou, Pengfei and Li, Ang and Barker, Kevin and Ge, Rong","This paper presents a workload classification framework that accurately discriminates illicit computation from authorized workloads on GPU-accelerated HPC systems at runtime. As such systems become increasingly powerful and widely-adopted, attackers have begun to run illicit and for-profit programs that typically require extremely high computing capability to be successful, depriving mission-critical and authorized workloads of execution cycles and increasing risks of data leaking and empowered attacks. Traditional measures on CPU hosts are oblivious to such attacks. Our classification framework leverages the distinctive signatures between illicit and authorized GPU workloads, and explores machine learning methods and workload profiling to classify them. We face multiple challenges in designing the framework: achieving high detection accuracy, maintaining low profiling and inference overhead, and overcoming the limitation of lacking data types and volumes typically required by deep learning models. To address these challenges, we use lightweight, non-intrusive, high-level workload profiling, collect multiple sequences of easily obtainable multimodal input data, and build recurrent neural networks (RNNs) to learn from history for online anomalous workload detection. Evaluation results on three generations of GPU machines demonstrate that the workload classification framework can tell apart the illicit workloads with a high accuracy of over 95%. The collected dataset, detection framework, and neural network models are released on github1.",2020,544
Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform,"Edge, Darren and Larson, Jonathan and White, Christopher","The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.",2018,556
Origins of the D programming language,"Bright, Walter and Alexandrescu, Andrei and Parker, Michael","As its name suggests, the initial motivation for the D programming language was to improve on C and C++ while keeping their spirit. The D language was to preserve those languages' efficiency, low-level access, and Algol-style syntax. The areas D set out to improve focused initially on rapid development, convenience, and simplifying the syntax without hampering expressiveness.  The genesis of D has its peculiarities, as is the case with many other languages. Walter Bright, D's creator, is a mechanical engineer by education who started out working for Boeing designing gearboxes for the 757. He was programming games on the side, and in trying to make his game Empire run faster, became interested in compilers. Despite having no experience, Bright set out in 1982 to implement a compiler that produced better code than those on the market at the time.  This interest materialized into a C compiler, followed by compilers for C++, Java, and JavaScript. Best known of these would be the Zortech C++ compiler, the first (and to date only) C++-to-native compiler developed by a single person. The D programming language began in 1999 as an effort to pull the best features of these languages into a new one. Fittingly, D would use the by that time mature C/C++ back end (optimizer and code generator) that had been under continued development and maintenance since 1982.  Between 1999 and 2006, Bright worked alone on the D language definition and its implementation, although a steadily increasing volume of patches from users was incorporated. The new language would be based on the past successes of the languages he'd used and implemented, but would be clearly looking to the future. D started with choices that are obvious today but were less clear winners back in the 1990s: full support for Unicode, IEEE floating point, 2s complement arithmetic, and flat memory addressing (memory is treated as a linear address space with no segmentation). It would do away with certain compromises from past languages imposed by shortages of memory (for example, forward declarations would not be required). It would primarily appeal to C and C++ users, as expertise with those languages would be readily transferrable. The interface with C was designed to be zero cost.  The language design was begun in late 1999. An alpha version appeared in 2001 and the initial language was completed, somewhat arbitrarily, at version 1.0 in January 2007. During that time, the language evolved considerably, both in capability and in the accretion of a substantial worldwide community that became increasingly involved with contributing. The front end was open-sourced in April 2002, and the back end was donated by Symantec to the open source community in 2017. Meanwhile, two additional open-source back ends became mature in the 2010s: `gdc` (using the same back end as the GNU C++ compiler) and `ldc` (using the LLVM back end).  The increasing use of the D language in the 2010s created an impetus for formalization and development management. To that end, the D Language Foundation was created in September 2015 as a nonprofit corporation overseeing work on D's definition and implementation, publications, conferences, and collaborations with universities.",2020,560
Security Aspects of Behavioral Biometrics for Strong User Authentication,"Jancok, Vladimir and Ries, Michal","Efficient user identification and authentication are fundamental for securing access to systems processing sensitive data. This paper provides an analysis of current research in the field of user identification and identity verification with a focus on the behavioral biometrics supported by Machine Learning. It identifies the methods for user modeling with the potential of application in real-world scenarios such as strong authentication and fraud detection domain. This paper further elaborates on the current state-of-the-art approaches, feature extraction, and classification methods. We describe our experimental setup and provide an evaluation of our method in the selected deployment. We focus on user interactions in a controlled web environment. We performed classification experiments with the machine learning models on various datasets showing promising results in the robustness and proving relevance as a modern non-intrusive security measure.",2022,566
Rethinking data-driven networking with foundation models: challenges and opportunities,"Le, Franck and Srivatsa, Mudhakar and Ganti, Raghu and Sekar, Vyas","Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking.",2022,568
Where do Databases and Digital Forensics meet? A Comprehensive Survey and Taxonomy,"Seufitelli, Danilo B. and Brand\~{a}o, Michele A. and Fernandes, Ayane C. A. and Siqueira, Kayque M. and Moro, Mirella M.","We present a systematic literature review and propose a taxonomy for research at the intersection of Digital Forensics and Databases. The merge between these two areas has become more prolific due to the growing volume of data and mobile apps on the Web, and the consequent rise in cyber attacks. Our review has identified 91 relevant papers. The taxonomy categorizes such papers into: Cyber-Attacks (subclasses SQLi, Attack Detection, Data Recovery) and Criminal Intelligence (subclasses Forensic Investigation, Research Products, Crime Resolution). Overall, we contribute to better understanding the intersection between digital forensics and databases, and open opportunities for future research and development with potential for significant social, economic, and technical-scientific contributions.",2023,572
OpenRank Leaderboard: Motivating Open Source Collaborations Through Social Network Evaluation in Alibaba,"Zhao, Shengyu and Xia, Xiaoya and Fitzgerald, Brian and Li, Xiaozhou and Lenarduzzi, Valentina and Taibi, Davide and Wang, Rong and Wang, Wei and Tian, Chunqi","Open source has revolutionized how software development is carried out, with a growing number of individuals and organizations contributing to open source projects. As the importance of open source continues to grow, companies also expect to grow thriving and sustainable open source communities with continued contributions and better collaborations. In this study, we applied the contribution leaderboard to seven open source projects initiated by Alibaba. We conducted a case study to investigate the perceptions and facts regarding how to motivate collaboration through gamification. Specifically, we employed a social network algorithm, OpenRank, to evaluate and steer developers' contributions. We validated the effectiveness of OpenRank by comparing it with other evaluation metrics and surveying developers. Through semi-structured interviews and project metric analysis, we found that the OpenRank Leaderboard can promote transparent communication environments, a better community atmosphere, and improved collaboration behavior.",2024,603
Method overloading the circuit,"Meiklejohn, Christopher and Stark, Lydia and Celozzi, Cesare and Ranney, Matt and Miller, Heather","Circuit breakers are frequently deployed in microservice applications to improve their reliability. They achieve this by short circuiting RPC invocations issued to overloaded or failing services, thereby relieving pressure on those services and allowing them to recover. In this paper, we systematically examine the state of the art in industrial circuit breakers designs. We first present a taxonomy of existing, open-source circuit breaker designs and implementations based on a systematic mapping study. We then examine the relationship between these circuit breaker designs and application reliability. We make a clear case that incorrect application of circuit breakers to an application can hurt reliability in the process of trying to improve it. To address the deficiencies in the state of the art, we propose two new circuit breaker designs and provide guidance on how to properly structure microservice applications for the best circuit breaker use. Finally, we identify several open challenges in circuit breaker usage and design for future researchers.",2022,619
SONAR: Automatic Detection of Cyber Security Events over the Twitter Stream,"Le Sceller, Quentin and Karbab, ElMouatez Billah and Debbabi, Mourad and Iqbal, Farkhund","Everyday, security experts face a growing number of security events that affecting people well-being, their information systems and sometimes the critical infrastructure. The sooner they can detect and understand these threats, the more they can mitigate and forensically investigate them. Therefore, they need to have a situation awareness of the existing security events and their possible effects. However, given the large number of events, it can be difficult for security analysts and researchers to handle this flow of information in an adequate manner and answer the following questions in near-real time: what are the current security events? How long do they last? In this paper, we will try to answer these issues by leveraging social networks that contain a massive amount of valuable information on many topics. However, because of the very high volume, extracting meaningful information can be challenging. For this reason, we propose SONAR: an automatic, self-learned framework that can detect, geolocate and categorize cyber security events in near-real time over the Twitter stream. SONAR is based on a taxonomy of cyber security events and a set of seed keywords describing type of events that we want to follow in order to start detecting events. Using these seed keywords, it automatically discovers new relevant keywords such as malware names to enhance the range of detection while staying in the same domain. Using a custom taxonomy describing all type of cyber threats, we demonstrate the capabilities of SONAR on a dataset of approximately 47.8 million tweets related to cyber security in the last 9 months. SONAR could efficiently and effectively detect, categorize and monitor cyber security related events before getting on the security news, and it could automatically discover new security terminologies with their event. Additionally, SONAR is highly scalable and customizable by design; therefore we could adapt SONAR framework for virtually any type of events that experts are interested in.",2017,638
"Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review","Araujo, Hugo and Mousavi, Mohammad Reza and Varshosaz, Mahsa","We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.",2023,641
Gotta Catch ’em All: A Multistage Framework for Honeypot Fingerprinting,"Srinivasa, Shreyas and Pedersen, Jens Myrup and Vasilomanolakis, Emmanouil","Honeypots are decoy systems that lure attackers by presenting them with a seemingly vulnerable system. They provide an early detection mechanism as well as a method for learning how adversaries work and think. However, over the past years, several researchers have shown methods for fingerprinting honeypots. This significantly decreases the value of a honeypot; if an attacker is able to recognize the existence of such a system, they can evade it. In this article, we revisit the honeypot identification field, by providing a holistic framework that includes state-of-the-art and novel fingerprinting components. We decrease the probability of false positives by proposing a rigid multi-step approach for labeling a system as a honeypot. We perform extensive scans covering 2.9 billion addresses of the IPv4 space and identify a total of 21,855 honeypot instances. Moreover, we present several interesting side findings such as the identification of around 355,000 non-honeypot systems that represent potentially misconfigured or unpatched vulnerable servers (e.g., SSH servers with default password configurations and vulnerable versions). We ethically disclose our findings to network administrators about the default configuration and the honeypot developers about the gaps in implementation that lead to possible honeypot fingerprinting. Last, we discuss countermeasures against honeypot fingerprinting techniques.",2023,664
Teaching Software Development for Real-World Problems using a Microservice-Based Collaborative Problem-Solving Approach,"Lau, Yi Meng and Koh, Christian Michael and Jiang, Lingxiao","Experienced and skillful software developers are needed in organizations to develop software products effective for their business with shortened time-to-market. Such developers will not only need to code but also be able to work in teams and collaboratively solve real-world problems that organizations are facing. It is challenging for educators to nurture students to become such developers with strong technical, social, and cognitive skills.Towards addressing the challenge, this study presents a Collaborative Software Development Project Framework for a course that focuses on learning microservices architectures and developing a software application for a real-world business. Students get to work in teams to solve a real-world problem of their own choice. They are given opportunities to recognize that the software development process goes beyond writing code and that social and cognitive skills in engaging with each other are also essential. By adopting microservices architectures in the course, students learn to break down the functionalities of their applications into smaller pieces of code with standardized interfaces that can be developed, tested, and deployed independently. This not only helps students to learn various technical skills needed for developing and implementing the functionalities needed by the application in the form of microservices but also facilitates task allocation and coordination among their team members and provides a platform for them to solve problems collaboratively. Upon completion of their projects, students are also asked to reflect on their development process and encouraged to think beyond the basics for better software design and development approaches.The course curriculum incorporates the framework, especially for the student team projects. The earlier teaching weeks introduce a combination of concepts and lab exercises to students as the building blocks. The survey studies show that the framework is effective in enhancing the students' learning of technical, social, and cognitive skills, while further improvements, such as closer collaboration with other courses, can be done to improve a holistic learning curriculum.",2024,670
Workflow Integration Alleviates Identity and Access Management in Serverless Computing,"Sankaran, Arnav and Datta, Pubali and Bates, Adam","As serverless computing continues to revolutionize the design and deployment of web services, it has become an increasingly attractive target to attackers. These adversaries are developing novel tactics for circumventing the ephemeral nature of serverless functions, exploiting container reuse optimizations and achieving lateral movement by “living off the land” provided by legitimate serverless workflows. Unfortunately, the traditional security controls currently offered by cloud providers are inadequate to counter these new threats. In this work, we propose will.iam,1 a workflow-aware access control model and reference monitor that satisfies the functional requirements of the serverless computing paradigm. will.iam encodes the protection state of a serverless application as a permissions graph that describes the permissible transitions of its workflows, associating web requests with a permissions set at the point of ingress according to a graph-based labeling state. By proactively enforcing the permissions requirements of downstream workflow components, will.iam is able to avoid the costs of partially processing unauthorized requests and reduce the attack surface of the application. We implement the will.iam framework in Go and evaluate its performance as compared to recent related work against the well-established Nordstrom “Hello, Retail!” application. We demonstrate that will.iam imposes minimal burden to requests, averaging 0.51% overhead across representative workflows, but dramatically improves performance when handling unauthorized requests (e.g., DDoS attacks) as compared to past solutions. will.iam thus demonstrates an effective and practical alternative for authorization in the serverless paradigm.",2020,691
Dr.Emotion: Disentangled Representation Learning for Emotion Analysis on Social Media to Improve Community Resilience in the COVID-19 Era and Beyond,"Ju, Mingxuan and Song, Wei and Sun, Shiyu and Ye, Yanfang and Fan, Yujie and Hou, Shifu and Loparo, Kenneth and Zhao, Liang","During the pandemic caused by coronavirus disease (COVID-19), social media has played an important role by enabling people to discuss their experiences and feelings of this global crisis. To help combat the prolonged pandemic that has exposed vulnerabilities impacting community resilience, in this paper, based on our established large-scale COVID-19 related social media data, we propose and develop an integrated framework (named Dr.Emotion) to learn disentangled representations of social media posts (i.e., tweets) for emotion analysis and thus to gain deep insights into public perceptions towards COVID-19. In Dr.Emotion, for given social media posts, we first post-train a transformer-based model to obtain the initial post embeddings. Since users may implicitly express their emotions in social media posts which could be highly entangled with other descriptive information in the post content, to address this challenge for emotion analysis, we propose an adversarial disentangler by integrating emotion-independent (i.e., sentiment-neutral) priors of the posts generated by another post-trained transformer-based model to separate and disentangle the implicitly encoded emotions from the content in latent space for emotion classification at the first attempt. Extensive experimental studies are conducted to fully evaluate Dr.Emotion and promising results demonstrate its performance in emotion analysis by comparison with the state-of-the-art baseline methods. By exploiting our developed Dr.Emotion, we further perform emotion analysis over a large number of social media posts and provide in-depth investigation from both temporal and geographical perspectives, based on which additional work can be conducted to extract and transform the constructive ideas, experiences and support into actionable information to improve community resilience in responses to a variety of crises created by COVID-19 and well beyond.",2021,693
On the use of DGAs in malware: an everlasting competition of detection and evasion,"Spooren, Jan and Preuveneers, Davy and Desmet, Lieven and Janssen, Peter and Joosen, Wouter","Malware typically makes use of Domain Generation Algorithms (DGAs) as a mechanism to contact their Command and Control server. In recent years, different approaches to automatically detect generated domain names have been proposed, based on machine learning. The first problem that we address is the difficulty to systematically compare these DGA detection algorithms due to the lack of an independent benchmark. The second problem that we investigate is the difficulty for an adversary to circumvent these classifiers when the machine learning models backing these DGA-detectors are known. In this paper we compare two different approaches on the same set of DGAs: classical machine learning using manually engineered features and a 'deep learning' recurrent neural network. We show that the deep learning approach performs consistently better on all of the tested DGAs, with an average classification accuracy of 98.7% versus 93.8% for the manually engineered features. We demonstrate that the deep learning solution yields better results even when only 10,000 malicious samples are available. We also show that one of the dangers of manual feature engineering is that DGAs can adapt their strategy, based on knowledge of the features used to detect them. To demonstrate this, we use the knowledge of the used feature set to design a new DGA which makes the Random Forest classifier powerless with a classification accuracy of 57.3%. The deep learning classifier is also (albeit less) affected, reducing its accuracy to 78.9%.",2019,701
Code review practices for refactoring changes: an empirical study on OpenStack,"AlOmar, Eman Abdullah and Chouchen, Moataz and Mkaouer, Mohamed Wiem and Ouni, Ali","Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the code base. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.",2022,711
Source Code Recommender Systems: The Practitioners' Perspective,"Ciniselli, Matteo and Pascarella, Luca and Aghajani, Emad and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele","The automatic generation of source code is one of the long-lasting dreams in software engineering research. Several techniques have been proposed to speed up the writing of new code. For example, code completion techniques can recommend to developers the next few tokens they are likely to type, while retrieval-based approaches can suggest code snippets relevant for the task at hand. Also, deep learning has been used to automatically generate code statements starting from a natural language description. While research in this field is very active, there is no study investigating what the users of code recommender systems (i.e., software practitioners) actually need from these tools. We present a study involving 80 software developers to investigate the characteristics of code recommender systems they consider important. The output of our study is a taxonomy of 70 ""requirements"" that should be considered when designing code recommender systems. For example, developers would like the recommended code to use the same coding style of the code under development. Also, code recommenders being ""aware"" of the developers' knowledge (e.g., what are the framework/libraries they already used in the past) and able to customize the recommendations based on this knowledge would be appreciated by practitioners. The taxonomy output of our study points to a wide set of future research directions for code recommenders.",2023,729
Proactive detection of collaboration conflicts,"Brun, Yuriy and Holmes, Reid and Ernst, Michael D. and Notkin, David","Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results.First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems.Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations.Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts.",2011,753
Unsupervised Anomaly Detectors to Detect Intrusions in the Current Threat Landscape,"Zoppi, Tommaso and Ceccarelli, Andrea and Capecchi, Tommaso and Bondavalli, Andrea","Anomaly detection aims at identifying unexpected fluctuations in the expected behavior of a given system. It is acknowledged as a reliable answer to the identification of zero-day attacks to such extent, several ML algorithms that suit for binary classification have been proposed throughout years. However, the experimental comparison of a wide pool of unsupervised algorithms for anomaly-based intrusion detection against a comprehensive set of attacks datasets was not investigated yet. To fill such gap, we exercise 17 unsupervised anomaly detection algorithms on 11 attack datasets. Results allow elaborating on a wide range of arguments, from the behavior of the individual algorithm to the suitability of the datasets to anomaly detection. We conclude that algorithms as Isolation Forests, One-Class Support Vector Machines, and Self-Organizing Maps are more effective than their counterparts for intrusion detection, while clustering algorithms represent a good alternative due to their low computational complexity. Further, we detail how attacks with unstable, distributed, or non-repeatable behavior such as Fuzzing, Worms, and Botnets are more difficult to detect. Ultimately, we digress on capabilities of algorithms in detecting anomalies generated by a wide pool of unknown attacks, showing that achieved metric scores do not vary with respect to identifying single attacks.",2021,757
Data-Driven Governance in Crises: Topic Modelling for the Identification of Refugee Needs,"Sprenkamp, Kilian and Zavolokina, Liudmila and Angst, Mario and Dolata, Mateusz","The war in Ukraine and the following refugee crisis have recently again highlighted the need for effective refugee management across European countries. Refugee management contemporarily mostly relies on top-down management approaches by governments. These often lead to suboptimal policies for refugees and highlight a need to better identify and integrate refugee needs into management. Here, we show that modern applications of Natural Language Processing (NLP) allow for the effective analysis of large text corpora linked to refugee needs, making it possible to complement top-down approaches with bottom-up knowledge centered around the current needs of the refugee population. By following a Design Science Research Methodology, we utilize 58 semi-structured stakeholder interviews within Switzerland to develop design requirements for NLP applications for refugee management. Based on the design requirements, we developed R2G – “Refugees to Government”, an application based on state-of-the-art topic modeling to identify refugee needs bottom-up through Telegram data. We evaluate R2G with a dedicated workshop held with stakeholders from the public sector and civil society. Thus, we contribute to the ongoing discourse on how to design refugee management applications and showcase how topic modeling can be utilized for data-driven governance during refugee crises.",2023,770
Automating developer chat mining,"Pan, Shengyi and Bao, Lingfeng and Ren, Xiaoxue and Xia, Xin and Lo, David and Li, Shanping","Online chatrooms are gaining popularity as a communication channel between widely distributed developers of Open Source Software (OSS) projects. Most discussion threads in chatrooms follow a Q&amp;A format, with some developers (askers) raising an initial question and others (respondents) joining in to provide answers. These discussion threads are embedded with rich information that can satisfy the diverse needs of various OSS stakeholders. However, retrieving information from threads is challenging as it requires a thread-level analysis to understand the context. Moreover, the chat data is transient and unstructured, consisting of entangled informal conversations. In this paper, we address this challenge by identifying the information types available in developer chats and further introducing an automated mining technique. Through manual examination of chat data from three chatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories and create a labeled dataset with 2,959 threads. We propose a classification approach (named F2Chat) to structure the vast amount of threads based on the information type automatically, helping stakeholders quickly acquire their desired information. F2Chat effectively combines handcrafted non-textual features with deep textual features extracted by neural models. Specifically, it has two stages with the first one leveraging the siamese architecture to pretrain the textual feature encoder, and the second one facilitating an in-depth fusion of two types of features. Evaluation results suggest that our approach achieves an average F1-score of 0.628, which improves the baseline by 57%. Experiments also verify the effectiveness of our identified non-textual features under both intra-project and cross-project validations.",2022,793
Dissecting Click Fraud Autonomy in the Wild,"Zhu, Tong and Meng, Yan and Hu, Haotian and Zhang, Xiaokuan and Xue, Minhui and Zhu, Haojin","Although the use of pay-per-click mechanisms stimulates the prosperity of the mobile advertisement network, fraudulent ad clicks result in huge financial losses for advertisers. Extensive studies identify click fraud according to click/traffic patterns based on dynamic analysis. However, in this study, we identify a novel click fraud, named humanoid attack, which can circumvent existing detection schemes by generating fraudulent clicks with similar patterns to normal clicks. We implement the first tool ClickScanner to detect humanoid attacks on Android apps based on static analysis and variational AutoEncoders (VAEs) with limited knowledge of fraudulent examples. We define novel features to characterize the patterns of humanoid attacks in the apps' bytecode level. ClickScanner builds a data dependency graph (DDG) based on static analysis to extract these key features and form a feature vector. We then propose a classification model only trained on benign datasets to overcome the limited knowledge of humanoid attacks.We leverage ClickScanner to conduct the first large-scale measurement on app markets (i.e., 120,000 apps from Google Play and Huawei AppGallery) and reveal several unprecedented phenomena. First, even for the top-rated 20,000 apps, ClickScanner still identifies 157 apps as fraudulent, which shows the prevalence of humanoid attacks. Second, it is observed that the ad SDK-based attack (i.e., the fraudulent codes are in the third-party ad SDKs) is now a dominant attack approach. Third, the manner of attack is notably different across apps of various categories and popularities. Finally, we notice there are several existing variants of the humanoid attack. Additionally, our measurements demonstrate the proposed ClickScanner is accurate and time-efficient (i.e., the detection overhead is only 15.35% of those of existing schemes).",2021,797
NivaDuck - A Scalable Pipeline to Build a Database of Political Twitter Handles for India and the United States,"Panda, Anmol and Gonawela, A’ndre and Acharyya, Sreangsu and Mishra, Dibyendu and Mohapatra, Mugdha and Chandrasekaran, Ramgopal and Pal, Joyojeet","We present a scalable methodology to identify Twitter handles of politicians in a given region and test our framework in the context of Indian and US politics. The main contribution of our work is the list of the curated Twitter handles of 18500 Indian and 8000 US politicians. Our work leveraged machine learning-based classification and human verification to build a data set of Indian politicians on Twitter. We built NivaDuck, a highly precise, two-staged classification pipeline that leverages Twitter description text and tweet content to identify politicians. For India, we tested NivaDuck’s recall using Twitter handles of the members of the Indian parliament while for the US we used state and local level politicians in California state and San Diego county respectively. We found that while NivaDuck has lower recall scores, it produces large, diverse sets of politicians with precision exceeding 90 percent for the US dataset. We discuss the need for an ML-based, scalable method to compile such a dataset and its myriad use cases for the research community and its wide-ranging utilities for research in political communication on social media.",2020,820
Deep Learning in Sentiment Analysis: Recent Architectures,"Abdullah, Tariq and Ahmet, Ahmed","Humans are increasingly integrated with devices that enable the collection of vast unstructured opinionated data. Accurately analysing subjective information from this data is the task of sentiment analysis (an actively researched area in NLP). Deep learning provides a diverse selection of architectures to model sentiment analysis tasks and has surpassed other machine learning methods as the foremast approach for performing sentiment analysis tasks. Recent developments in deep learning architectures represent a shift away from Recurrent and Convolutional neural networks and the increasing adoption of Transformer language models. Utilising pre-trained Transformer language models to transfer knowledge to downstream tasks has been a breakthrough in NLP.This survey applies a task-oriented taxonomy to recent trends in architectures with a focus on the theory, design and implementation. To the best of our knowledge, this is the only survey to cover state-of-the-art Transformer-based language models and their performance on the most widely used benchmark datasets. This survey paper provides a discussion of the open challenges in NLP and sentiment analysis. The survey covers five years from 1st July 2017 to 1st July 2022.",2022,825
Is historical data an appropriate benchmark for reviewer recommendation systems? a case study of the gerrit community,"Gauthier, Ian X. and Lamothe, Maxime and Mussbacher, Gunter and McIntosh, Shane","Reviewer recommendation systems are used to suggest community members to review change requests. Like several other recommendation systems, it is customary to evaluate recommendations using held out historical data. While history-based evaluation makes pragmatic use of available data, historical records may be: (1) overly optimistic, since past assignees may have been suboptimal choices for the task at hand; or (2) overly pessimistic, since ""incorrect"" recommendations may have been equal (or even better) choices.In this paper, we empirically evaluate the extent to which historical data is an appropriate benchmark for reviewer recommendation systems. We replicate the cHRev and WLRRec approaches and apply them to 9,679 reviews from the Gerrit open source community. We then assess the recommendations with members of the Gerrit reviewing community using quantitative methods (personalized questionnaires about their comfort level with tasks) and qualitative methods (semi-structured interviews).We find that history-based evaluation is far more pessimistic than optimistic in the context of Gerrit review recommendations. Indeed, while 86% of those who had been assigned to a review in the past felt comfortable handling the review, 74% of those labelled as incorrect recommendations also felt that they would have been comfortable reviewing the changes. This indicates that, on the one hand, when reviewer recommendation systems recommend the past assignee, they should indeed be considered correct. Yet, on the other hand, recommendations labelled as incorrect because they do not match the past assignee may have been correct as well.Our results suggest that current reviewer recommendation evaluations do not always model the reality of software development. Future studies may benefit from looking beyond repository data to gain a clearer understanding of the practical value of proposed recommendations.",2022,869
A Time-Aware Exploration of RecSys15 Challenge Dataset,"Pamp\'{\i}n, Humberto Jes\'{u}s Corona and Peleteiro, Ana","E-commerce is currently one of the main applications of recommender systems, since it generates vast amounts of data that can be used to make predictions and analyse users's behaviour. In this paper we present an overview of the public dataset used for the RecSys Challenge 2015. We describe the basic statistical properties of this dataset and how events (clicks and purchases) are distributed over products (items) and users (sessions). We also present a time-aware analysis of the dataset, with the aim to better understand the change of user behaviour within time cycles, and how it affects the activity in user purchases. We further study the relation between categories, the solely type of metadata present in this completely anonymised dataset. We are interested both in how these categories are distributed and how users and items interact with them. Finally, along the paper we explain the implications that the results obtained from our analysis may have when building models for the challenge.",2016,876
iLFQA: A Platform for Efficient and Accurate Long-Form Question Answering,"Butler, Rhys and Duggirala, Vishnu Dutt and Banaei-Kashani, Farnoush","We present an efficient and accurate long-form question-answering platform, dubbed iLFQA (i.e., short for intelligent Long-Form Question Answering). The purpose of iLFQA is to function as a platform which accepts unscripted questions and efficiently produces semantically meaningful, explanatory, and accurate long-form responses. iLFQA consists of a number of modules for zero-shot classification, text retrieval, and text generation to generate answers to questions based on an open-domain knowledge base. iLFQA is unique in the question answering space because it is an example of a deployable and efficient long-form question answering system. Question answering systems exist in many forms, but long-form question answering remains relatively unexplored, and to the best of our knowledge none of the existing long-form question answering systems are shown to be sufficiently efficient to be deployable. We have made the source code and implementation details of iLFQA available for the benefit of researchers and practitioners in this field. With this demonstration, we present iLFQA as an open-domain, deployable, and accurate open-source long-form question answering platform.",2022,884
A Meta-Study of Software-Change Intentions,"Kr\""{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia","Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system, many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state-of-the-art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs; and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.",2024,933
Sec2vec: Anomaly Detection in HTTP Traffic and Malicious URLs,"Gniewkowski, Mateusz and Maciejewski, Henryk and Surmacz, Tomasz and Walentynowicz, Wiktor","In this paper, we show how methods known from Natural Language Processing (NLP) can be used to detect anomalies in HTTP requests and malicious URLs. Most of the current solutions focusing on a similar problem are either rule-based or trained using manually selected features. Modern NLP methods, however, have great potential in capturing a deep understanding of samples and therefore improving the classification results. Other methods, which rely on a similar idea, often ignore the interpretability of the results, which is so important in machine learning. We are trying to fill this gap. In addition, we show to what extent the proposed solutions are resistant to concept drift. In our work, we compare three different vectorization methods: simple BoW, fastText, and the current state-of-the-art language model RoBERTa. The obtained vectors are later used in the classification task. In order to explain our results, we utilize the SHAP method. We evaluate the feasibility of our methods on four different datasets: CSIC2010, UNSW-NB15, MALICIOUSURL, and ISCX-URL2016. The first two are related to HTTP traffic, the other two contain malicious URLs. The results we show are comparable to others or better, and most importantly - interpretable.",2023,958
Sequential Action Patterns in Collaborative Ontology-Engineering Projects: A Case-Study in the Biomedical Domain,"Walk, Simon and Singer, Philipp and Strohmaier, Markus","Within the last few years the importance of collaborative ontology-engineering projects, especially in the biomedical domain, has drastically increased. This recent trend is a direct consequence of the growing complexity of these structured data representations, which no single individual is able to handle anymore. For example, the World Health Organization is currently actively developing the next revision of the International Classification of Diseases (ICD), using an OWL-based core for data representation and Web 2.0 technologies to augment collaboration. This new revision of ICD consists of roughly 50,000 diseases and causes of death and is used in many countries around the world to encode patient history, to compile health-related statistics and spendings. Hence, it is crucial for practitioners to better understand and steer the underlying processes of how users collaboratively edit an ontology. Particularly, generating predictive models is a pressing issue as these models may be leveraged for generating recommendations in collaborative ontology-engineering projects and to determine the implications of potential actions on the ontology and community. In this paper we approach this task by (i) exploring whether regularities and common patterns in user action sequences, derived from change-logs of five different collaborative ontology-engineering projects from the biomedical domain, exist. Based on this information we (ii) model the data using Markov chains of varying order, which are then used to (iii) predict user actions in the sequences at hand.",2014,964
One General Teacher for Multi-Data Multi-Task: A New Knowledge Distillation Framework for Discourse Relation Analysis,"Jiang, Congcong and Qian, Tieyun and Liu, Bing","Automatically identifying the discourse relations can help many downstream NLP tasks such as reading comprehension and machine translation. It can be categorized into explicit and implicit discourse relation recognition (EDRR and IDRR). Due to the lack of connectives, IDRR remains to be a big challenge. A good number of methods have been developed to combine explicit data with implicit ones under the multi-task learning framework. However, the difference in linguistic property and class distribution makes it hard to directly optimize EDRR and IDRR with multi-task learning. In this paper, we take the first step to exploit the knowledge distillation (KD) technique for discourse relation analysis. Our target is to train &lt;italic&gt;a focused single-data single-task student&lt;/italic&gt; with the help of &lt;italic&gt;a general multi-data multi-task teacher&lt;/italic&gt;. Specifically, we first train one teacher for both the top and second level relation classification tasks with explicit and implicit data. We then transfer the feature embeddings and soft labels from the teacher network to the student network. Moreover, we develop an adaptive knowledge distillation module to reduce the number of hyper-parameters and also to stimulate the potential of the student on autonomous learning. Extensive experimental results on the popular PDTB dataset proves that our model achieves a new state-of-the-art performance. We also show the effectiveness of our proposed KD architecture through detailed analysis.",2023,977
Social network analysis in open source software peer review,"Yang, Xin","Software peer review (aka. code review) is regarded as one of the most important approaches to keep software quality and productivity. Due to the distributed collaborations and communication nature of Open Source Software (OSS), OSS review differs from traditional industry review. Unlike other related works, this study investigated OSS peer review pro- cesses from social perspective by using social network anal- ysis (SNA). We analyzed the review history from three typi- cal OSS projects. The results provide hints on relationships among the OSS reviewers which can help to understand how developers work and communicate with each other.",2014,1003
Hershel: Single-Packet OS Fingerprinting,"Shamsi, Zain and Nandwani, Ankur and Leonard, Derek and Loguinov, Dmitri and Shamsi, Zain and Nandwani, Ankur and Leonard, Derek and Loguinov, Dmitri","Traditional TCP/IP fingerprinting tools e.g., nmap are poorly suited for Internet-wide use due to the large amount of traffic and intrusive nature of the probes. This can be overcome by approaches that rely on a single SYN packet to elicit a vector of features from the remote server. However, these methods face difficult classification problems due to the high volatility of the features and severely limited amounts of information contained therein. Since these techniques have not been studied before, we first pioneer stochastic theory of single-packet OS fingerprinting, build a database of 116 OSs, design a classifier based on our models, evaluate its accuracy in simulations, and then perform OS classification of 37.8M hosts from an Internet-wide scan.",2016,1014
Meta-stasis of the Internet,"Desai, Bipin C.","This paper offers a brief history of the information age in order to demonstrate how the loss of user control and the increase in certain forms of automation have metastasized into imminent and ongoing threats to social order and the democratic way of life. The internet was established after a number of developments which included the interconnection of computers without extensive need of action by the users. It led to the introduction of user communication sub-systems such as text, email, file sharing and systems for searching for files. The so called information age is said to be marked by the adaption of a hypertext transport protocol in the last decade of the twentieth century. The information age was marked by a number of meetings which included the first of the world wide web conference in April 1994 followed by the second (Oct. 1994) and the third(April 1995) in quick succession. Other, by invitation only, meetings which dealt with issue of this era were held in Denver, OH(Metadata) and (America in the Age of Information)Bethesda, MD. However, in just under three decades this information age has meta-stasis-ed into a form that is a threat to our social order and democratic way of life while fostering division. Enormous wealth has been garnered by just a few corporations and individuals at the expense of the harm it is doing to people all over the globe. This is the result of the spreading of fake-news and favouring angry content that result in civil strife and loss of lives. It has led to divisiveness and autocratic governments. Some so called democracies are in name only with the same people continuing in their ’elected’ position from term to term, ad infinitum. Just as in the metastasis of a cancer, until it is checked, this transformed internet will destroy some vital parts of our everyday existence: our privacy and liberty while promoting an inegalitarian spirit.",2022,1060
Constructive Code Review: Managing the Impact of Interpersonal Conflicts in Practice,"Wurzel Goncalves, Pavlina and S. V. Goncalves, Joao and Bacchelli, Alberto","Code review is an activity where developers receive feedback on their code contributions from other developers. The frequent and potentially negative feedback developers receive makes code review prone to interpersonal conflicts. There is a consensus about such behavior being anti-social and leading to negative outcomes for the code, team, project, and even the company. However, these conflicts are a naturally occurring phenomenon that can lead to reaping the benefits of code review if managed well. Interpersonal conflicts in code review are not necessarily an issue to avoid, but rather to be managed.In this study, we survey developers in two companies - Adnovum - working predominantly on closed-source projects - and in Red Hat open source projects. Based on a set of 154 respondents, we have found that 77% of developers sometimes experience interpersonal conflicts in code review, even though mostly not very frequently. These conflicts pose some degree of a problem to 64% of developers. However, developers are rather successful in deriving constructive outcomes in the face of conflicts - 24% of developers report that conflicts have more positive than negative outcomes. While they are highly successful in deriving positive outcomes for code quality and maintainability, the motivation of developers and their communication and collaboration in a team has the most potential to be harmed by conflicts. The most effective strategy to have constructive rather than destructive conflicts is managing work effort and re-assigning tasks in the team to reduce stress in review. Data and materials: https://zenodo.org/records/10477537.",2024,1073
Leveraging Change Intents for Characterizing and Identifying Large-Review-Effort Changes,"Wang, Song and Bansal, Chetan and Nagappan, Nachiappan and Philip, Adithya Abraham","Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. In most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis.In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes regarding review effort---changes with large review effort. Specifically, we first propose a feedback-driven and heuristics-based approach to obtain change intents. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on a large-scale project from Microsoft and three large-scale open source projects, i.e., Qt, Android, and OpenStack. Our results show that, (i) code changes with some intents are more likely to be LRE changes, (ii) machine learning based prediction models can efficiently help identify LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. The tool developed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process.",2019,1090
Collaborative Security: A Survey and Taxonomy,"Meng, Guozhu and Liu, Yang and Zhang, Jie and Pokluda, Alexander and Boutaba, Raouf","Security is oftentimes centrally managed. An alternative trend of using collaboration in order to improve security has gained momentum over the past few years. Collaborative security is an abstract concept that applies to a wide variety of systems and has been used to solve security issues inherent in distributed environments. Thus far, collaboration has been used in many domains such as intrusion detection, spam filtering, botnet resistance, and vulnerability detection. In this survey, we focus on different mechanisms of collaboration and defense in collaborative security. We systematically investigate numerous use cases of collaborative security by covering six types of security systems. Aspects of these systems are thoroughly studied, including their technologies, standards, frameworks, strengths and weaknesses. We then present a comprehensive study with respect to their analysis target, timeliness of analysis, architecture, network infrastructure, initiative, shared information and interoperability. We highlight five important topics in collaborative security, and identify challenges and possible directions for future research. Our work contributes the following to the existing research on collaborative security with the goal of helping to make collaborative security systems more resilient and efficient. This study (1) clarifies the scope of collaborative security, (2) identifies the essential components of collaborative security, (3) analyzes the multiple mechanisms of collaborative security, and (4) identifies challenges in the design of collaborative security.",2015,1095
Identifying bot activity in GitHub pull request and issue comments,M. Golzadeh; A. Decan; E. Constantinou; T. Mens,"Development bots are used on Github to automate repetitive activities. Such bots communicate with human actors via issue comments and pull request comments. Identifying such bot comments allows to prevent bias in socio-technical studies related to software development. To automate their identification, we propose a classification model based on natural language processing. Starting from a balanced ground-truth dataset of 19,282 PR and issue comments, we encode the comments as vectors using a combination of the bag of words and TF-IDF techniques. We train a range of binary classifiers to predict the type of comment (human or bot) based on this vector representation. A multinomial Naive Bayes classifier provides the best results. Its performance on a test set containing 50% of the data achieves an average precision, recall, and F1 score of 0.88. Although the model shows a promising result on the pull request and issue comments, further work is required to generalize the model on other types of activities, like commit messages and code reviews.",2021,1148
On the Accuracy of Bot Detection Techniques,M. Golzadeh; A. Decan; N. Chidambaram,"Development bots are often used to automate a wide variety of repetitive tasks in collaborative software development. Such bots are commonly among the most active project contributors in terms of commit activity. As such, tools that analyse contributor activity (e.g., for recognizing and giving credit to project members for their contributions) need to take into account the bots and exclude their activity. While there are a few techniques to detect bots in software repositories, these techniques are not perfect and may miss some bots or may wrongly identify some human accounts as bots. In this paper, we present an exploratory study on the accuracy of bot detection techniques on a set of 540 accounts from 27 GitHub projects. We show that none of the bot detection techniques are accurate enough to detect bots among the 20 most active contributors of each project. We show that combining these techniques drastically increases the accuracy and recall of bot detection. We also highlight the importance of considering bots when attributing contributions to humans, since bots are prevalent among the top contributors and responsible for large proportions of commits.",2022,1149
Software Bots in Software Engineering: Benefits and Challenges,M. Wessel; M. A. Gerosa; E. Shihab,"Software bots are becoming increasingly popular in software engineering (SE). In this tutorial, we define what a bot is and present several examples. We also discuss the many benefits bots provide to the SE community, including helping in development tasks (such as pull request review and integration) and onboarding newcomers to a project. Finally, we discuss the challenges related to interacting with and developing software bots.",2022,1153
Towards an Autonomous Bot for Automatic Source Code Refactoring,M. Wyrich; J. Bogner,"Continuous refactoring is necessary to maintain source code quality and to cope with technical debt. Since manual refactoring is inefficient and error-prone, various solutions for automated refactoring have been proposed in the past. However, empirical studies have shown that these solutions are not widely accepted by software developers and most refactorings are still performed manually. For example, developers reported that refactoring tools should support functionality for reviewing changes. They also criticized that introducing such tools would require substantial effort for configuration and integration into the current development environment. In this paper, we present our work towards the Refactoring-Bot, an autonomous bot that integrates into the team like a human developer via the existing version control platform. The bot automatically performs refactorings to resolve code smells and presents the changes to a developer for asynchronous review via pull requests. This way, developers are not interrupted in their workflow and can review the changes at any time with familiar tools. Proposed refactorings can then be integrated into the code base via the push of a button. We elaborate on our vision, discuss design decisions, describe the current state of development, and give an outlook on planned development and research activities.",2019,1156
Bot Detection in GitHub Repositories,N. Chidambaram; P. R. Mazrae,"Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the GrimoireLab software development analytics platform. In this work, we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using Kibana.",2022,1157
A Dataset of Bot and Human Activities in GitHub,N. Chidambaram; A. Decan; T. Mens,"Software repositories hosted on GitHub frequently use development bots to automate repetitive, effort intensive and error-prone tasks. To understand and study how these bots are used, state-of-the-art bot identification tools have been developed to detect bots based on their comments in commits, issues and pull requests. Given that bots can be involved in many other activity types, there is a need to consider more activities that they are carrying out in the software repositories they are involved in. We therefore propose a curated dataset of such activities carried out by bots and humans involved in GitHub repositories. The dataset was constructed by identifying 24 high-level activity types that could be extracted from 15 lower-level event types that were queried from GitHub’s event stream API for all considered bots and humans. The proposed dataset contains around 834K activities performed by 385 bots and 616 humans involved in GitHub repositories, during an observation period ranging from 25 November 2022 to 9 March 2023. By analysing the activity patterns of bots and humans, this dataset could lead to better bot identification tools and empirical studies on how bots play a role in collaborative software development.",2023,1158
Detecting Bot on GitHub Leveraging Transformer-based Models: A Preliminary Study,J. Zhang; X. Wu; Y. Zhang; S. Xu,"Bots are prevalent contributors in collaborative software development, necessitating accurate detection techniques. This preliminary study aims to leveraging public datasets and Transformer-based models (i.e., BERT, CodeBERT, RoBERTa, BART, and PLBART) for the bot detection task. Our experimental result reveals that CodeBERT achieves the highest performance, with an impressive accuracy score of 94.1%.",2023,1160
BotHunter: An Approach to Detect Software Bots in GitHub,A. Abdellatif; M. Wessel; I. Steinmacher; M. A. Gerosa; E. Shihab,"Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level.",2022,1161
Suggestion Bot: Analyzing the Impact of Automated Suggested Changes on Code Reviews,N. Palvannan; C. Brown,"Peer code reviews are crucial for maintaining the quality of the code in software repositories. Developers have introduced a number of software bots to help with the code review process. Despite the benefits of automating code review tasks, many developers face challenges interacting with these bots due to non-comprehensive feedback and disruptive notifications. In this paper, we analyze how incorporating a bot in software development cycle will decrease turnaround time of pull request. We created a bot called “SUGGESTION BOT” to automatically review the code base using GitHub’s suggested changes functionality in order to solve this issue. A preliminary comparative empirical investigation between the utilization of this bot and manual review procedures was also conducted in this study. We evaluate SUGGESTION BOT concerning its impact on review time and also analyze whether the comments given by the bot are clear and useful for users. Our results provide implications for the design of future systems and improving human-bot interactions for code review.",2023,1163
On Twitter Bots Behaving Badly: A Manual and Automated Analysis of Python Code Patterns on GitHub,A. Millimaggi; F. Daniel,"Bots, i.e., algorithmically driven entities that behave like humans in online communications, are increasingly infiltrating social conversations on the Web. If not properly prevented, this presence of bots may cause harm to the humans they interact with. This article aims to understand which types of abuse may lead to harm and whether these can be considered intentional or not. We manually review a dataset of 60 Twitter bot code repositories on GitHub, derive a set of potentially abusive actions, characterize them using a taxonomy of abstract code patterns, and assess the potential abusiveness of the patterns. The article then describes the design and implementation of a code pattern recognizer and uses the pattern recognizer to automatically analyze a dataset of 786 Python bot code repositories. The study does not only reveal the existence of 28 communication-specific code patterns - which could be used to assess the harmfulness of bot code - but also their consistent presence throughout all studied repositories.",2019,1164
Together or Apart? Investigating a mediator bot to aggregate bot’s comments on pull requests,E. Ribeiro; R. Nascimento; I. Steinmacher; L. Xavier; M. Gerosa; H. de Paula; M. Wessel,"Software bots connect users and tools, streamlining the pull request review process in social coding platforms. However, bots can introduce information overload into developers’ communication. Information overload is especially problematic for newcomers, who are still exploring the project and may feel overwhelmed by the number of messages. Inspired by the literature of other domains, we designed and evaluated FunnelBot, a bot that acts as a mediator between developers and other bots in the repository. We conducted a within-subject study with 25 newcomers to capture their perceptions and preferences. Our results provide insights for bot developers who want to mitigate noise and create bots for supporting newcomers, laying a foundation for designing better bots.",2022,1165
Classifying Issues into Custom Labels in GitBot,D. Park; H. Cho; S. Lee,"GitBots are bots in Git repositories to automate repetitive tasks that occur in software development, testing and maintenance. Git-Bots are expected to perform the repetitive tasks that are normally done by humans, such as feedback on issue reports and answers to questions. However, studies on GitBots for labeling issue reports fall short of replacing developers' labeling tasks. Developers still manually attach labels to issues. In this paper, we introduce an issue labeling bot classifying issue reports into custom labels that developers define by themselves so that our bot could attach labels in a similar way to human behavior.",2022,1166
"Bots for Pull Requests: The Good, the Bad, and the Promising",M. Wessel; A. Abdellatif; I. Wiese; T. Conte; E. Shihab; M. A. Gerosa; I. Steinmacher,"Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (“the good”). However, their interactions can be disruptive and noisy and lead to information overload (“the bad”). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (“the promising”). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",2022,1167
Effects of Adopting Code Review Bots on Pull Requests to OSS Projects,M. Wessel; A. Serebrenik; I. Wiese; I. Steinmacher; M. A. Gerosa,"Software bots, which are widely adopted by Open Source Software (OSS) projects, support developers on several activities, including code review. However, as with any new technology adoption, bots may impact group dynamics. Since understanding and anticipating such effects is important for planning and management, we investigate how several activity indicators change after the adoption of a code review bot. We employed a regression discontinuity design on 1,194 software projects from GitHub. Our results indicate that the adoption of code review bots increases the number of monthly merged pull requests, decreases monthly non-merged pull requests, and decreases communication among developers. Practitioners and maintainers may leverage our results to understand, or even predict, bot effects on their projects' social interactions.",2020,1169
Understanding the Impact of Bots on Developers Sentiment and Project Progress,A. Gao; S. Chen; T. Wang; J. Deng,"Software developers advance the project process by contributing and discussing on the code platform. Software bot acts as an assistant to help developers deal with repetitive tasks. In this paper, we explore whether the adoption of bots has an impact on developer sentiment and projects progress. We collected issues, pull requests and comments from GitHub popular projects. And we found that human users had significantly reduced positive sentiment in bot-created issues and PRs. The average merging time of the bot-created PRs is significantly shorter than human-created ones. The average solving time of bot-commented issues is significantly longer than human-created ones.",2022,1170
BDGOA: A bot detection approach for GitHub OAuth Apps,Z. Liao; X. Huang; B. Zhang; J. Wu; Y. Cheng,"As various software bots are widely used in open source software repositories, some drawbacks are coming to light, such as giving newcomers non-positive feedback and misleading empirical studies of software engineering researchers. Several techniques have been proposed by researchers to perform bot detection, but most of them are limited to identifying bots performing specific activities, let alone distinguishing between GitHub App and OAuth App. In this paper, we propose a bot detection technique for OAuth App, named BDGOA. 24 features are used in BDGOA, which can be divided into three dimensions: account information, account activity, and text similarity. To better explore the behavioral features, we define a fine-grained classification of behavioral events and introduce self-similarity to quantify the repeatability of behavioral sequence. We leverage five machine learning classifiers on the benchmark dataset to conduct bot detection, and finally choose random forest as the classifier, which achieves the highest F1-score of 95.83%. The experimental results comparing with the state-of-the-art approaches also demonstrate the superiority of BDGOA.",2023,1171
Recognizing Bot Activity in Collaborative Software Development,M. Golzadeh; T. Mens; A. Decan; E. Constantinou; N. Chidambaram,"Using popular open source projects on GitHub, we provide evidence that bots are regularly among the most active contributors, even though GitHub does not explicitly acknowledge their presence. This poses a problem for techniques that analyze human contributor activity.",2022,1172
"Human, bot or both? A study on the capabilities of classification models on mixed accounts",N. Cassee; C. Kitsanelis; E. Constantinou; A. Serebrenik,"Several bot detection algorithms have recently been discussed in the literature, as software bots that perform maintenance tasks have become more popular in recent years. State-of-the-art techniques detect bots based on a binary classification, where a GitHub account is either a human or a bot. However, this conceptualisation of bot detection as an account-level binary classification problem fails to account for ‘mixed accounts’, accounts that are shared between a human and a bot, and that therefore exhibit both bot and human activity. By using binary classification models for bot detection, researchers might hence mischaracterize both human and bot behavior in software maintenance. This calls for conceptualisation of bot detection through a comment-level classification. However, the single such approach solely investigates a small number of mixed account comments. The nature of mixed accounts on GitHub is thus yet unknown, and the absence of appropriate datasets make this a difficult problem to study. In this paper, we investigate three comment-level classification models and we evaluate these classifiers on a manually labeled dataset of mixed accounts. We find that the best classifiers based on these classification models achieve a precision and recall between 88% and 96%. However, even the most accurate comment-level classifier cannot accurately detect mixed accounts; rather, we find that textual content alone, or textual content combined with templates used by bots, are very effective features for the detection of both bot and mixed accounts. Our study calls for more accurate bot detection techniques capable of identifying mixed accounts, and as such supporting more refined insights in software maintenance activities performed by humans and bots on social coding sites.",2021,1173
FixMe: A GitHub Bot for Detecting and Monitoring On-Hold Self-Admitted Technical Debt,S. Phaithoon; S. Wongnil; P. Pussawong; M. Choetkiertikul; C. Ragkhitwetsagul; T. Sunetnanta; R. Maipradit; H. Hata; K. Matsumoto,"Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related ""On-hold SATD"", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN_YQ.",2021,1175
Guidelines for Developing Bots for GitHub,M. Wessel; A. Zaidman; M. A. Gerosa; I. Steinmacher,"Projects on GitHub rely on the automation provided by software development bots. Nevertheless, the presence of bots can be annoying and disruptive to the community. Backed by multiple studies with practitioners, this article provides guidelines for developing and maintaining software bots.",2023,1177
"Expecting the Unexpected: Distilling Bot Development, Challenges, and Motivations",A. M. Pinheiro; C. S. Rabello; L. B. Furtado; G. Pinto; C. R. B. de Souza,"Software bots are becoming an increasingly popular tool in the software development landscape, which is particularly due to their potential of use in several different contexts. More importantly, software developers interested in transitioning to bot development may have to face challenges intrinsic related to bot software development. However, so far, it is still unclear what is the profile of bot developers, what motivate them, or what challenges do they face when dealing with bot development. To shed an initial light on this direction, we conducted a survey with 43 Github users who have been involved (showing their interest or actively contributing to) in bot software projects.",2019,1178
Autonomy Is An Acquired Taste: Exploring Developer Preferences for GitHub Bots,A. Ghorbani; N. Cassee; D. Robinson; A. Alami; N. A. Ernst; A. Serebrenik; A. Wąsowski,"Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",2023,1179
How Does Bot Affect Developer’s Sentiment: An Empirical Study on GitHub Issues and PRs,A. Gao; Y. Zhang; T. Wang; S. Chen; J. Deng,"Software bots act as assistants to help developers in GitHub issues and pull requests (PRs) solution. In this paper, we explore whether software bots affect developer’s sentiment through an empirical study. We find that developers express more neutral sentiment in bot-involved projects and have more positive sentiment in bot-involved issues and PRs. Further, we find developers are less positive in issues and PRs created by bots. To Figure out how developer’s sentiment looks like when mention bots, we use a combination of quantitative and qualitative analyses to explore the different sentiment of developers toward bots and explain the reasons.",2022,1180
"Exploring Activity and Contributors on GitHub: Who, What, When, and Where",X. Xia; Z. Weng; W. Wang; S. Zhao,"Apart from being a code hosting platform, GitHub is the place where large-scale open collaborations and contributions happen. Every minute, thousands of developers are submitting code, having discussions of issues or pull requests, with all user behaviors recorded in the GitHub Event Stream (GES). Exploration of the activities in the GES could help understand who is active, the way they work, the time when they are active and even their location. To this end, a large-scale analysis was initially performed based on the 0.86 billion event records generated in 2020. We extracted 902K active contributors out of 14 million GitHub accounts by observing their activity distribution, then explored their behavior distribution, active time in the day and week, and estimated time zone distributions on the basis of their circadian activity rhythm. To go deeper, a case study of 79 projects in CNCF and contrast analyses of different project maturity levels were conducted. Our results showed that from a macro perspective, bots are increasingly more active and can serve numerous projects. Contributors work on weekdays, and are globally more inclined toward the daytime working hours in the Americas and Europe. The time zone distribution also reveals that UTC+2 and UTC-4 have the most active contributors. A critical discovery was the validation and quantification of a high bus factor risk exists in the OSS ecosystem. Whether from a large group point of view or within specific projects, a rather small group of OSS contributors (less than 20%) undertook the majority of the work. The GES can provide a wealth of information about open source software (OSS). Our findings provide insights into global GitHub collaboration behaviors and may be of help for researchers and practitioners to further understand modern OSS ecosystem.",2022,1189
AI-Powered Code Review Assistant for Streamlining Pull Request Merging,C. Adapa; S. S. Avulamanda; A. R. K. Anjana; A. Victor,"WatsonX, a comprehensive data and AI platform, adeptly addresses contemporary challenges by meticulously training, validating, tuning, and deploying data to drive impactful business outcomes. The intricate task of timely merging Pull Requests (PRs) poses a significant challenge for software development teams, directly influencing business operations. This paper introduces an innovative solution leveraging AI, particularly harnessing generative AI techniques with the Falcon40-B model through the platform. The AI bot facilitates an initial PR review, offering insightful feedback on code formatting, best practices, and minor issues and streamlines collaboration by automatically assigning and notifying PR reviewers. The overarching goal is the continuous evolution of this AI bot into an intelligent reviewer, capable of assessing code from a functional standpoint. The implementation of this solution holds the promise of significantly enhancing PR management and expediting the entire development workflow.",2024,1190
JITBot: An Explainable Just-In-Time Defect Prediction Bot,C. Khanan; W. Luewichana; K. Pruktharathikoon; J. Jiarpakdee; C. Tantithamthavorn; M. Choetkiertikul; C. Ragkhitwetsagul; T. Sunetnanta,"Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explain-ability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/.",2020,1192
Conversation Clustering Adaptation for Intent Recognition,M. Lew; A. Obuchowski; E. Kacprzak; A. Pluwak,"With the increasing presence of NLU tools such as automatic dialogue systems, achieving high accuracy of intent recognition in chatbots becomes an especially important problem to tackle. This issue cannot be solved without sufficient training data, but the scarcity of labelled training data often poses a major challenge to the development of real-life chatbots. Therefore, methods utilizing unlabelled data resources have been recently gaining interest. One of most notable approaches is the use of pre-trained encoders based on language models. Trained for general purposes, they benefit from further domain adjustments. In our work we offer an approach that can increase the model’s accuracy for text classification, which can serve as an alternative for standard methods of domain adaptation. Our approach consists of a combination of methods: a clustering approach, similar to intent induction; an encoder domain adaptation on a cluster classification task, similar to intent recognition using unlabelled data; and model fine-tuning on labelled datasets. In this approach unlabelled data becomes complementary to labelled data, reducing the time needed for corpus building. We evaluate our approach on: 1) the public WebApp dataset and 2) a demanding real-life banking domain dataset, achieving 0.97 and 0.93 accuracy respectively. This approach, called Conversation Clustering Adaptation (CCA), when applied to an encoder, increases the accuracy of intent recognition up by to 12.4pp and exceeds current state-of-the-art methods while benefiting from the use of additional training data. We share our code at https://github.com/michal-lew/cca.",2021,1195
Toward an Empirical Theory of Feedback-Driven Development,M. Beller,"Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are prerather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial theory on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.",2018,1200
"GeeSolver: A Generic, Efficient, and Effortless Solver with Self-Supervised Learning for Breaking Text Captchas",R. Zhao; X. Deng; Y. Wang; Z. Yan; Z. Han; L. Chen; Z. Xue; Y. Wang,"Although text-based captcha, which is used to differentiate between human users and bots, has faced many attack methods, it remains a widely used security mechanism and is employed by some websites. Some deep learning-based text captcha solvers have shown excellent results, but the labor-intensive and time-consuming labeling process severely limits their viability. Previous works attempted to create easy-to-use solvers using a limited collection of labeled data. However, they are hampered by inefficient preprocessing procedures and inability to recognize the captchas with complicated security features.In this paper, we propose GeeSolver, a generic, efficient, and effortless solver for breaking text-based captchas based on self-supervised learning. Our insight is that numerous difficult-to-attack captcha schemes that ""damage"" the standard font of characters are similar to image masks. And we could leverage masked autoencoders (MAE) to improve the captcha solver to learn the latent representation from the ""unmasked"" part of the captcha images. Specifically, our model consists of a ViT encoder as latent representation extractor and a well-designed decoder for captcha recognition. We apply MAE paradigm to train our encoder, which enables the encoder to extract latent representation from local information (i.e., without masking part) that can infer the corresponding character. Further, we freeze the parameters of the encoder and leverage a few labeled captchas and many unlabeled captchas to train our captcha decoder with semi-supervised learning.Our experiments with real-world captcha schemes demonstrate that GeeSolver outperforms the state-of-the-art methods by a large margin using a few labeled captchas. We also show that GeeSolver is highly efficient as it can solve a captcha within 25 ms using a desktop CPU and 9 ms using a desktop GPU. Besides, thanks to latent representation extraction, we successfully break the hard-to-attack captcha schemes, proving the generality of our solver. We hope that our work will help security experts to revisit the design and availability of text-based captchas. The code is available at https://github.com/NSSL-SJTU/GeeSolver.",2023,1206
"Factoring Expertise, Workload, and Turnover Into Code Review Recommendation",F. Hajari; S. Malmir; E. Mirsaeedi; P. C. Rigby,"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload. We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review. Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover. Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover. Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing the risk of knowledge loss from turnover. Recent work, WhoDo, that considers developer workload, assigns developers that are not sufficiently committed to the project and we see an increase in files at risk to turnover. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover, but they unacceptably reduce the overall expertise during reviews. Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer. In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge. For the projects we study, we are able to globally increase expertise during reviews, $+3$+3%, reduce workload concentration, $-12$−12%, and reduce the files at risk, $-28$−28%. We make our scripts and data available in our replication package [1]. Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes [2].",2024,1209
The VALU3S ECSEL project: Verification and validation of automated systems safety and security,J.A. Agirre and L. Etxeberria and R. Barbosa and S. Basagiannis and G. Giantamidis and T. Bauer and E. Ferrari and M. {Labayen Esnaola} and V. Orani and J. Öberg and D. Pereira and J. Proença and R. Schlick and A. Smrčka and W. Tiberti and S. Tonetta and M. Bozzano and A. Yazici and B. Sangchoolie,"Manufacturers of automated systems and their components have been allocating an enormous amount of time and effort in R&D activities, which led to the availability of prototypes demonstrating new capabilities as well as the introduction of such systems to the market within different domains. Manufacturers need to make sure that the systems function in the intended way and according to specifications. This is not a trivial task as system complexity rises dramatically the more integrated and interconnected these systems become with the addition of automated functionality and features to them. This effort translates into an overhead on the V&V (verification and validation) process making it time-consuming and costly. In this paper, we present VALU3S, an ECSEL JU (joint undertaking) project that aims to evaluate the state-of-the-art V&V methods and tools, and design a multi-domain framework to create a clear structure around the components and elements needed to conduct the V&V process. The main expected benefit of the framework is to reduce time and cost needed to verify and validate automated systems with respect to safety, cyber-security, and privacy requirements. This is done through identification and classification of evaluation methods, tools, environments and concepts for V&V of automated systems with respect to the mentioned requirements. VALU3S will provide guidelines to the V&V community including engineers and researchers on how the V&V of automated systems could be improved considering the cost, time and effort of conducting V&V processes. To this end, VALU3S brings together a consortium with partners from 10 different countries, amounting to a mix of 25 industrial partners, 6 leading research institutes, and 10 universities to reach the project goal.",2021,1239
"Comprehensive review on intelligent security defences in cloud: Taxonomy, security issues, ML/DL techniques, challenges and future trends",Mohamad Mulham Belal and Divya Meena Sundaram,"Nowadays, machine learning and deep learning algorithms are used in recent studies as active security techniques instead of traditional ones to secure the cloud environment based on pre-trained data. In this paper, a literature review on machine and deep learning based defences against attacks and security issues in cloud computing is provided. A taxonomy of all different types of attacks and threats as per cloud security alliance (CSA) layers; and the general defences against cloud attacks is shown in this review as well as the reasons which let the traditional security techniques fail to satisfy the desired security level are discussed. Forty-two case studies are selected based on seven quality assessment standards and then, analyzed to answer seven research questions which help to protect cloud environments from various attacks, issues, and challenges. The analysis of case studies shows a description of the most common security issues in cloud; machine learning and deep learning models that are applied, datasets models, performance metrics, machine learning and deep learning based countermeasures and defences that are developed to prevent security issues. Finally, the future scope and open challenges in cloud computing security based on machine and deep learning are discussed as well.",2022,1243
Phoma and related genera,B.C. Sutton,"The type species of Cyphellopycnis Tehon & Stout and Alveophoma Alcalde, genera described as characterized by multi-ostiolate pycnidia, are compared with Phoma herbarum Westend, var. lactaria var. nov., a multi-ostiolate variety isolated from the rubber tubing of an automatic milking machine. Cyphellopycnis is shown to be congeneric with Phomopsis (Sacc.) Sacc, the type species C. pastinacae Tehon & Stout being antedated by P. diachenii Sacc. Alveophoma is distinct from Phomopsis and Phoma Sacc. on account of the sympodial development of the conidiophores. As a result of comparative studies on several hyaline-spored pycnidial fungi related to Phoma, it is suggested that separation of genera on the basis of conidium and conidiophore development will provide a more satisfactory approach to the classification of pycnidial fungi.",1964,1251
Multi-classification approaches for classifying mobile app traffic,Giuseppe Aceto and Domenico Ciuonzo and Antonio Montieri and Antonio Pescapé,"The growing usage of smartphones in everyday life is deeply (and rapidly) changing the nature of traffic traversing home and enterprise networks, and the Internet. Different tools and middleboxes, such as performance enhancement proxies, network monitors and policy enforcement devices, base their functions on the knowledge of the applications generating the traffic. This requirement is tightly coupled to an accurate traffic classification, being exacerbated by the (daily) expanding set of apps and the moving-target nature of mobile traffic. On the top of that, the increasing adoption of encrypted protocols (such as TLS) makes classification even more challenging, defeating established approaches (e.g., Deep Packet Inspection). To this end, in this paper we aim to improve the performance of classification of mobile apps traffic by proposing a multi-classification (viz. fusion) approach, intelligently-combining outputs from state-of-the-art classifiers proposed for mobile and encrypted traffic classification. Under this framework, four classes of different combiners (differing in whether they accept soft or hard classifiers' outputs, the training requirements, and the learning philosophy) are taken into account and compared. The present approach enjoys modularity, as any classifier may be readily plugged-in/out to improve performance further. Finally, based on a dataset of (true) users' activity collected by a mobile solutions provider, our results demonstrate that classification performance can be improved according to all considered metrics, up to +9.5% (recall score) with respect to the best state-of-the-art classifier. The proposed system is also capitalized to validate a novel pre-processing of traffic traces, here developed, and assess performance sensitivity to traffic object (temporal) segmentation, before actual classification.",2018,1258
Chapter 7 - The Carotenoid Group,J.B. Davis,"Publisher Summary
This chapter discusses the basic constitution, classification, and nomenclature; occurrence, function, and isolation; structure and synthesis; and properties of carotenoids. A carotenoid is still easily recognized by its methyl(or modified methyl)-substituted polyene chain. The C50 and C45 carotenoids, which barely fall within the classical definition of carotenoids, are considered as straightforward di-and monoalkylated derivatives of the more usual C40 skeleton. There are several methods of converting a carotenoid with all its double bonds trans into an equilibrium mixture of cis/trans isomers. The most commonly used method involves briefly exposing a solution of the compound containing a trace of iodine to light. Many of the mono- and bi-cyclic carotenoids contain one or more chiral centers. These are usually at C(3), C(6), C(5), or associated with the allene group. In all 3-substituted carotenoids, the configuration at the 3-position is R (the -OH substituent is of the “β” type using steroid nomenclature), and in all carotenoids containing α-type end-group that at the 6-position is also R (the-H is “α”).",1975,1346
MalHyStack: A hybrid stacked ensemble learning framework with feature engineering schemes for obfuscated malware analysis,Kowshik Sankar Roy and Tanim Ahmed and Pritom Biswas Udas and Md. Ebtidaul Karim and Sourav Majumdar,"Since the advent of malware, it has reached a toll in this world that exchanges billions of data daily. Millions of people are victims of it, and the numbers are not decreasing as the year goes by. Malware is of various types in which obfuscation is a special kind. Obfuscated malware detection is necessary as it is not usually detectable and is prevalent in the real world. Although numerous works have already been done in this field so far, most of these works still need to catch up at some points, considering the scope of exploration through recent extensions. In addition to that, the application of a hybrid classification model is yet to be popularized in this field. Thus, in this paper, a novel hybrid classification model named, MalHyStack, has been proposed for detecting such obfuscated malware within the network. This proposed working model is built incorporating a stacked ensemble learning scheme, where conventional machine learning algorithms namely, Extremely Randomized Trees Classifier (ExtraTrees), Extreme Gradient Boosting (XgBoost) Classifier, and Random Forest are used in the first layer which is then followed by a deep learning layer in the second stage. Before utilizing the classification model for malware detection, an optimum subset of features has been selected using Pearson correlation analysis which improved the accuracy of the model by more than 2 % for multiclass classification. It also reduces time complexity by approximately two and three times for binary and multiclass classification, respectively. For evaluating the performance of the proposed model, a recently published balanced dataset named CIC-MalMem-2022 has been used. Utilizing this dataset, the overall experimental results of the proposed model represent a superior performance when compared to the existing classification models.",2023,1394
Traffic safety evaluation in Northwestern Federal District using sentiment analysis of Internet users’ reviews,Yaroslav Seliverstov and Svyatoslav Seliverstov and Igor Malygin and Oleg Korolev,"The paper addresses the task of analyzing traffic safety in the Northwestern Federal District according to the reviews published in the Web. To accomplish the task, the authors developed a system of automatic review classification based on a sentiment classifier. They analyzed open source libraries for data mining, developed a web crawler using Scrapy framework, written in Python 3, and collected reviews. They also considered the methods of text vectorization and lemmatization and their application in the Scikit-Learn library: Bag-of-Words, N-gram, CountVectorizer, and TF-IDF Vectorizer. For the purpose of classification, the authors used the naïve Bayes algorithm and a linear classifier model with stochastic gradient descent optimization. A base of tagged Twitter reviews was used as a training set. The classifier was trained using cross-validation and ShuffleSplit strategies. The authors also tested and compared the classification results for different classifiers. As a result of validation, the best model was determined. The developed system was applied to analyze the quality of roads in the Northwestern Federal District. Based on the outcome, the roads were marked-up in color to illustrate the results of the research.",2020,1399
Anomaly-based cyberattacks detection for smart homes: A systematic literature review,Juan Ignacio Iturbe Araya and Helena Rifà-Pous,"Smart homes, leveraging IoT technology to interconnect various devices and appliances to the internet, enable remote monitoring, automation, and control. However, collecting sensitive personal and business data assets renders smart homes a target for cyberattacks. Anomaly detection is a promising approach for identifying malicious behavior in smart homes. Yet, the current literature primarily discusses IoT-related cyberattacks and gives limited attention to detecting anomalies specific to the smart home context. Furthermore, there is a lack of datasets that accurately represent the complexity inherent in a smart home environment in terms of users with varying levels of expertise and diverse, evolving types of devices. Therefore, this paper presents a systematic literature review (SLR) that focuses on using anomaly detection to identify cyberattacks in smart home environments. The SLR includes an adapted taxonomy that classifies existing anomaly detection methods and a critical analysis of the current state of knowledge and future research challenges. Our findings show a growing interest in detecting cyberattacks with anomaly-based models in smart homes using centralized and network-based features. Ensemble and deep learning techniques are popular methods for detecting these anomalies. However, the limited diversity of cyberattacks in existing datasets and the absence of comprehensive datasets representing the complexity of smart home environments call for further research to improve the generalizability of detection models.",2023,1403
An effective convolutional neural network based on SMOTE and Gaussian mixture model for intrusion detection in imbalanced dataset,Hongpo Zhang and Lulu Huang and Chase Q. Wu and Zhanbo Li,"Network Intrusion Detection System (NIDS) is a key security device in modern networks to detect malicious activities. However, the problem of imbalanced class associated with intrusion detection dataset limits the classifier’s performance for minority classes. To improve the detection rate of minority classes while ensuring efficiency, we propose a novel class imbalance processing technology for large-scale dataset, referred to as SGM, which combines Synthetic Minority Over-Sampling Technique (SMOTE) and under-sampling for clustering based on Gaussian Mixture Model (GMM). We then design a flow-based intrusion detection model, SGM-CNN, which integrates imbalanced class processing with convolutional neural network, and investigate the impact of different numbers of convolution kernels and different learning rates on model performance. The advantages of the proposed model are verified using the UNSW-NB15 and CICIDS2017 datasets. The experimental results show that i) for binary classification and multiclass classification on the UNSW-NB15 dataset, SGM-CNN achieves a detection rate of 99.74% and 96.54%, respectively; ii) for 15-class classification on the CICIDS2017 dataset, it achieves a detection rate of 99.85%. We compare five imbalanced processing methods and two classification algorithms, and conclude that SGM-CNN provides an effective solution to imbalanced intrusion detection and outperforms the state-of-the-art intrusion detection methods.",2020,1405
Using machine learning to identify common flaws in CAPTCHA design: FunCAPTCHA case analysis,Carlos Javier Hernández-Castro and María D. R-Moreno and David F. Barrero and Stuart Gibson,"Human Interactive Proofs (HIPs 11Human Interaction Proof, or also Human Interactive Proof. or CAPTCHAs 22Completely Automated Public Turing test to tell Computers and Humans Apart.) have become a first-level security measure on the Internet to avoid automatic attacks or minimize their effects. All the most widespread, successful or interesting CAPTCHA designs put to scrutiny have been successfully broken. Many of these attacks have been side-channel attacks. New designs are proposed to tackle these security problems while improving the human interface. FunCAPTCHA is the first commercial implementation of a gender classification CAPTCHA, with reported improvements in conversion rates. This article finds weaknesses in the security of FunCAPTCHA and uses simple machine learning (ML) analysis to test them. It shows a side-channel attack that leverages these flaws and successfully solves FunCAPTCHA on 90% of occasions without using meaningful image analysis. This simple yet effective security analysis can be applied with minor modifications to other HIPs proposals, allowing to check whether they leak enough information that would in turn allow for simple side-channel attacks.",2017,1410
Utilizing Twitter data for analysis of chemotherapy,Ling Zhang and Magie Hall and Dhundy Bastola,"Objective
Twitter has become one of the most popular social media platforms that offers real-world insights to healthy behaviors. The purpose of this study was to assess and compare perceptions about chemotherapy of patients and health-care providers through analysis of chemo-related tweets.
Materials and methods
Cancer-related Twitter accounts and their tweets were obtained through using Tweepy (Python library). Multiple text classification algorithms were tested to identify the models with best performance in classifying the accounts into individual and organization. Chemotherapy-specific tweets were extracted from historical tweetset, and the content of these tweets was analyzed using topic model, sentiment analysis and word co-occurrence network.
Results
Using the description in Twitter users’ profiles, the accounts related with cancer were collected and coded as individual or organization. We employed Long Short Term Memory (LSTM) network with GloVe word embeddings to identify the user into individuals and organizations with accuracy of 85.2%. 13, 273 and 14,051 publicly available chemotherapy-related tweets were retrieved from individuals and organizations, respectively. The content of the chemo-related tweets was analyzed by text mining approaches. The tweets from individual accounts pertained to personal chemotherapy experience and emotions. In contrast with the personal users, professional accounts had a higher proportion of neutral tweets about side effects. The information about the assessment of response to chemotherapy was deficient from organizations on Twitter.
Discussion
Examining chemotherapy discussions on Twitter provide new lens into content and behavioral patterns associated with treatments for cancer patients. The methodology described herein allowed us to collect relatively large number of health-related tweets over a greater time period and exploit the potential power of social media, which provide comprehensive view on patients’ perceptions of chemotherapy.
Conclusion
This study sheds light on using Twitter data as a valuable healthcare data source for helping oncologists (organizations) in understanding patients’ experiences while undergoing chemotherapy, in developing personalize therapy plans, and a supplement to the clinical electronic medical records (EMRs).",2018,1430
Discovering differential features: Adversarial learning for information credibility evaluation,Lianwei Wu and Yuan Rao and Ambreen Nazir and Haolin Jin,"A series of deep learning approaches extract a large number of credibility features to detect fake news on the Internet. However, these extracted features still suffer from many irrelevant and noisy features that restrict severely the performance of the approaches. In this paper, we propose a novel model based on Adversarial Networks and inspirited by the Shared-Private model (ANSP), which aims at reducing common, irrelevant features from the extracted features for information credibility evaluation. Specifically, ANSP involves two tasks: one is to prevent the binary classification of true and false information for capturing common features relying on adversarial networks guided by reinforcement learning. Another extracts credibility features (henceforth, private features) from multiple types of credibility information and compares with the common features through two strategies, i.e., orthogonality constraints and KL-divergence for making the private features more differential. Experiments first on two six-label LIAR and Weibo datasets demonstrate that ANSP achieves state-of-the-art performance, boosting the accuracy by 2.1%, 3.1%, respectively and then on four-label Twitter16 validate the robustness of the model with 1.8% performance improvements.",2020,1433
"Test flakiness’ causes, detection, impact and responses: A multivocal review",Amjed Tahir and Shawn Rasheed and Jens Dietrich and Negar Hashemi and Lu Zhang,"Flaky tests (tests with non-deterministic outcomes) pose a major challenge for software testing. They are known to cause significant issues, such as reducing the effectiveness and efficiency of testing and delaying software releases. In recent years, there has been an increased interest in flaky tests, with research focusing on different aspects of flakiness, such as identifying causes, detection methods and mitigation strategies. Test flakiness has also become a key discussion point for practitioners (in blog posts, technical magazines, etc.) as the impact of flaky tests is felt across the industry. This paper presents a multivocal review that investigates how flaky tests, as a topic, have been addressed in both research and practice. Out of 560 articles we reviewed, we identified and analysed a total of 200 articles that are focused on flaky tests (composed of 109 academic and 91 grey literature articles/posts) and structured the body of relevant research and knowledge using four different dimensions: causes, detection, impact and responses. For each of those dimensions, we provide categorization and classify existing research, discussions, methods and tools With this, we provide a comprehensive and current snapshot of existing thinking on test flakiness, covering both academic views and industrial practices, and identify limitations and opportunities for future research.",2023,1450
A model to detect domain names generated by DGA malware,T Divya and P.P Amritha and Sangeetha Viswanathan,"Command and control(C&C) servers are being more frequently used in cyberattacks in recent years. A malware-infected machine is controlled and directed by an attacker using a command-and-control server in order to steal data from the network. To hide their servers, attackers commonly employ a domain generation algorithm that generates domain names for them by concatenating words from word lists. Some of the algorithmically-generated domain names are used to connect to the C&C server. With the emergence of sophisticated domain generation algorithms, detecting such domains has become a challenge, which in turn poses a severe danger to computer networks. In this paper, we are proposing a concept called centrality, which is used as one of the features to analyze the words in the domain names generated by the domain generation algorithm malware. For classification, we are using Naïve Bayes, KNN, SVM, Decision Trees, Random Forest and logistic regression. Experimental results showed that Random Forest gave the highest classification accuracy rate of 88.64% and Naive Bayes gave the lowest accuracy of 44.32%.",2022,1455
Towards filtering undesired short text messages using an online learning approach with semantic indexing,Renato M. Silva and Tulio C. Alberto and Tiago A. Almeida and Akebo Yamakami,"The popularity and reach of short text messages commonly used in electronic communication have led spammers to use them to propagate undesired content. This is often composed by misleading information, advertisements, viruses, and malwares that can be harmful and annoying to users. The dynamic nature of spam messages demands for knowledge-based systems with online learning and, therefore, the most traditional text categorization techniques can not be used. In this study, we introduce the MDLText, a text classifier based on the minimum description length principle, to the context of filtering undesired short text messages. The proposed approach supports incremental learning and, therefore, its predictive model is scalable and can adapt to continuously evolving spamming techniques. It is also fast, with computational cost increasing linearly with the number of samples and features, which is very desirable for expert systems applied to real-time electronic communication. In addition to the dynamic nature of these messages, they are also short and usually poorly written, rife with slangs, symbols, and abbreviations that difficult text representation, learning, and filtering. In this scenario, we also investigated the benefits of using text normalization and semantic indexing techniques. We showed these techniques can improve the text content quality and, consequently, enhance the performance of the expert systems for spamming detection. Based on these findings, we propose a new hybrid ensemble approach that combines the predictions obtained by the classifiers using the original text samples along with their variations created by applying text normalization and semantic indexing techniques. It has the advantages of being independent of the classification method and the results indicated it is efficient to filter undesired short text messages.",2017,1468
New optimization algorithms for neural network training using operator splitting techniques,Cristian Daniel Alecsa and Titus Pinţa and Imre Boros,"In the following paper we present a new type of optimization algorithms adapted for neural network training. These algorithms are based upon sequential operator splitting technique for some associated dynamical systems. Furthermore, we investigate through numerical simulations the empirical rate of convergence of these iterative schemes toward a local minimum of the loss function, with some suitable choices of the underlying hyper-parameters. We validate the convergence of these optimizers using the results of the accuracy and of the loss function on the MNIST, MNIST-Fashion and CIFAR 10 classification datasets.",2020,1472
Web of Things Semantic Interoperability in Smart Buildings,Amir Laadhar and Junior Dongo and Søren Enevoldsen and Frédéric Revaz and Dominique Gabioud and Torben Bach Pedersen and Martin Meyer and Brian Nielsen and Christian Thomsen,"Buildings are the largest energy consumers in Europe and are responsible for approximately 40% of EU energy consumption and 36% of the greenhouse gas emissions in Europe. Two-thirds of the building consumption is for residential buildings. To achieve energy efficiency, buildings are being integrated with IoT devices through the use of smart IoT services. For instance, a smart space heating service reduces energy consumption by dynamically heating apartments based on indoor and outdoor temperatures. The W3C recommends the use of the Web of Things (WoT) standard to enable IoT interoperability on the Web. However, in the context of a smart building, the ability to search and discover building metadata and IoT devices available in the WoT ecosystems remains a challenge due to the limitation of the current WoT Discovery, which only includes a directory containing only IoT devices metadata without including building metadata. Integrating the IoT device's metadata with building metadata in the same directory can provide better discovery capabilities to the IoT services providers. In this paper, we integrate building metadata into the W3C WoT Discovery through the construction of a Building Description JSON-LD file. This Building Description is integrated into the W3C WoT Discovery and based on the domOS Common Ontology (dCO) to achieve semantic interoperability in smart residential buildings for the WoT IoT ecosystem within the Horizon 2020 domOS project. This integration results in a Thing and Building Description Directory. dCO integrates the SAREF core ontology with the Thing Description ontology, devices, and building metadata. We have implemented and validated the WoT discovery on top of a WoT Thing and Building Description Directory. The WoT Discovery implementation is also made available for the WoT community.",2022,1478
Intelligent IoT-BOTNET attack detection model with optimized hybrid classification model,Balaganesh Bojarajulu and Sarvesh Tanwar and Thipendra Pal Singh,"The botnet have developed into a severe risk to Internet of Things (IoT) systems as a result of manufacturers ‘insufficient security policies and end users’ lack of security awareness. By default, several ports are open and user credentials are left unmodified. ML and DL strategies have been suggested in numerous latest research for identifying and categorising botnet assaults in the IoT context, but still, it has a few issues like high error susceptibility, working only with a large amount of data, poor quality, and data acquisition. This research provided use of a brand-new IoT botnet detector built on an improved hybrid classifier. The proposed work's main components are ""pre-processing, feature extraction, feature selection, and attack detection."" Following that, the improved Information Gain (IIG) model is used to choose the most reliable characteristics from the received information. To detect an attack, a hybrid classifier is utilized which can be constructed by integrating the optimized Bi-GRU with the Recurrent Neural Network (RNN). To increase the detection accuracy of IoT-BOTNETS, a novel hybrid optimization approach called SMIE (Slime Mould with Immunity Evolution) is created by conceptually integrating two conventional optimization modes: Coronavirus herd immunity optimizer (CHIO) and the Slime mould algorithm. The final output of the hybrid classifier displays the presence or absence of IoT-BOTNET attacks. The projected model's accuracy is 97%, which is 22.6%, 18.5%, 27.8%, 22.6%, and 24.8% higher than the previous models like GWO+ HC, SSO+ HC, WOA+ HC, SMA+ HC, and CHIO+ HC, respectively.",2023,1479
"Nature and classification of waterlain glaciogenic sediments, exemplified by Pleistocene, Late Paleozoic and Late Precambrian deposits",C.P. Gravenor and V. {von Brunn} and A. Dreimanis,"This study of waterlain glaciogenic sediments is designed to present both a review and new information on glaciogenic subaquatic deposits of differing age in a number of localities in North and South America and South Africa. The Late Paleozoic glaciogenic deposits of the Parana´Basin in Brazil and the Karoo Basin of South Africa are singled out for special attention as they show a reasonably complete lateral sequence of terrestrial to off-shore glaciogenic sedimentation. Although the environment of subaquatic glaciogenic sedimentation varies from one area to the next, certain common elements are found which can be used to develop a generalized model for both glaciomarine and glaciolacustrine sedimentation. For descriptive purposes, the model is divided into two broad categories: a shelf facies and a basinal facies. The shelf facies is marked by massive diamicton(ite) which may be 200 m or more in thickness and which is frequently overlain by a complex of clastic sediments consisting primarily of gravity and fluid flows. The basinal facies is marked by products of subaquatic slumps and more distal turbidites and glaciomarine sediments. New terminology is introduced. The massive diamicton(ite), which is diagnostic of the shelf facies, probably represents deposition from the base of active ice in a subaquatic environment and is termed undermelt diamicton(ite). The gravity and fluid flows which are usually found overlying undermelt diamicton(ite) and in the basinal facies are subdivided into six categories: glaciogenic subaquatic outwash, glaciogenic suspension flow, glaciogenic chaotic debris flow, glaciogenic subaquatic debris flow, glaciogenic slurry flow and glaciogenic turbidity flow. The relative abundance of undermelt diamicton(ite) and the various types of gravity and fluid flows can be used to define inner shelf, outer shelf, inner basin and outer basin facies of glaciomarine sedimentation.",1984,1533
Towards a taxonomy of code review smells,Emre Doğan and Eray Tüzün,"Context:
Code review is a crucial step of the software development life cycle in order to detect possible problems in source code before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the code review process, many companies and open source software (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective code review process. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are code review (CR) smells.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight OSS projects.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the OSS projects are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.",2022,1579
A proactive malicious software identification approach for digital forensic examiners,Muhammad Ali and Stavros Shiaeles and Nathan Clarke and Dimitrios Kontogeorgis,"Digital investigators often get involved with cases, which seemingly point the responsibility to the person to which the computer belongs, but after a thorough examination malware is proven to be the cause, causing loss of precious time. Whilst Anti-Virus (AV) software can assist the investigator in identifying the presence of malware, with the increase in zero-day attacks and errors that exist in AV tools, this is something that cannot be relied upon. The aim of this paper is to investigate the behaviour of malware upon various Windows operating system versions in order to determine and correlate the relationship between malicious software and OS artifacts. This will enable an investigator to be more efficient in identifying the presence of new malware and provide a starting point for further investigation. The study analysed several versions of the Windows operating systems (Windows 7, 8.1 and 10) and monitored the interaction of 90 samples of malware (across three categories of the most prevalent (Trojan, Worm, and Bot) and 90 benign samples through the Windows Registry. Analysis of the interactions has provided a rich source of knowledge about how various forms of malware interact with key areas of the Registry. Using this knowledge, the study sought to develop an approach to predict the presence and type of malware present through an analysis of the Registry. To this end, different classifiers such as Neural Network, Random forest, Decision tree, Boosted tree and Logistic regression were tested. It was observed that Boosted tree was resulting in a correct classification of over 72% – providing the investigator with a simple approach to determining which type of malware might be present independent and faster than an Antivirus. The modelling of these findings and their integration in an application or forensic analysis within an existing tool would be useful for digital forensic investigators.",2019,1580
Morphology of the decrementing expiratory neurons in the brainstem of the rat,Yoshiaki Saito and Ikuko Tanaka and Kazuhisa Ezure,"In anesthetized and artificially-ventilated rats, the morphological properties of decrementing expiratory (E-DEC) neurons were studied using intracellular recording and labeling with Neurobiotin. Sixteen E-DEC neurons were successfully labeled; ten of which were cranial motoneurons located in the facial (FN) and ambiguus (NA) nuclei. Two interneurons were labeled in the Bötzinger complex (BOT) and the ventral respiratory group (VRG) rostral to the obex, and the remaining four in the VRG caudal to the obex. All the interneurons had extensive intramedullary collaterals within the ventrolateral medulla. Terminal-like boutons were distributed ventral to the NA at the level of the BOT, both ventral to and within the NA at the level rostral to the obex and largely within the cell column tentatively designed as the ambiguous-retroambiguus complex (NA/NRA) caudal to the obex. The four interneurons in the NA/NRA had axons projecting to the spinal cord as well. The extensive intramedullary projections suggest that these E-DEC interneurons of the BOT and the VRG play a significant role in respiration. The simultaneous projections from the caudal E-DEC neurons to both the spinal cord and the NA suggest that these neurons also play integrative roles in non-respiratory behaviors including vocalization, swallowing and defecation.",2002,1583
Constructing a meta-model for assembly tolerance types with a description logic based approach,Yanru Zhong and Yuchu Qin and Meifa Huang and Wenlong Lu and Liang Chang,"There is a critical requirement for semantic interoperability among heterogeneous computer-aided tolerancing (CAT) systems with the sustainable growing demand of collaborative product design. But current data exchange standard for exchanging tolerance information among these systems can only exchange syntaxes and cannot exchange semantics. Semantic interoperability among heterogeneous CAT systems is difficult to be implemented only with this standard. To address this problem, some meta-models of tolerance information supporting semantic interoperability and an interoperability platform based on these meta-models should be constructed and developed, respectively. This paper mainly focuses on the construction of a meta-model for assembly tolerance types with a description logic ALC(D) based approach. Description logics, a family of knowledge representation languages for authoring ontologies, are well-known for having rigorous logic-based semantics which supports semantic interoperability. ALC(D) can provide a formal method to describe the research objects and the relations among them. In this formal method, constraint relations among parts, assembly feature surfaces and geometrical features are defined with some ALC(D) assertional axioms, and the meta-model of assembly tolerance types is constructed through describing the spatial relations between geometrical features with some ALC(D) terminological axioms. Besides, ALC(D) can also provide a highly efficient reasoning algorithm to automatically detect the inconsistency of the knowledge base, a finite set of assertional and terminological axioms. With this reasoning algorithm, assembly tolerance types for each pair of geometrical features are generated automatically through detecting the inconsistencies of the knowledge base. An application example is provided to illustrate the process of generating assembly tolerance types.",2014,1597
Brainstem and spinal projections of augmenting expiratory neurons in the rat,Kazuhisa Ezure and Ikuko Tanaka and Yoshiaki Saito,"There are two types of expiratory neurons with augmenting firing patterns (E-AUG neurons), those in the Bötzinger complex (BOT) and those in the caudal ventral respiratory group (cVRG). We studied their axonal projections morphologically using intracellular labeling of single E-AUG neurons with Neurobiotin, in anesthetized, paralyzed and artificially-ventilated rats. BOT E-AUG neurons (n=11) had extensive axonal projections to the brainstem, but E-AUG neurons (n=5) of the cVRG sent axons that descended the contralateral spinal cord without medullary collaterals. In addition to these somewhat expected characteristics, the present study revealed a number of new projection patterns of the BOT E-AUG neurons. First, as compared with the dense projections to the ipsilateral brainstem, those to the contralateral side were sparse. Second, several BOT E-AUG neurons sent long ascending collaterals to the pons, which included an axon that reached the ipsilateral parabrachial and Kölliker–Fuse nuclei and distributed boutons. Third, conspicuous projections from branches of these ascending collaterals to the area dorsolateral to the facial nucleus were found. Thus, the present study has shown an anatomical substrate for the extensive inhibitory projections of single BOT E-AUG neurons to the areas spanning the bilateral medulla and the pons.",2003,1602
"A survey on DoS/DDoS attacks mathematical modelling for traditional, SDN and virtual networks",Juan Fernando Balarezo and Song Wang and Karina Gomez Chavez and Akram Al-Hourani and Sithamparanathan Kandeepan,"Denial of Service and Distributed Denial of Service (DoS/DDoS) attacks have been one of the biggest threats against communication networks and applications throughout the years. Modelling DoS/DDoS attacks is necessary to get a better understanding of their behaviour at each step of the attack process, from the Botnet recruitment up to the dynamics of the attack. A deeper understanding of DoS/DDoS attacks would lead to the development of more efficient solutions and countermeasures to mitigate their impact. In this survey, we present a classification approach for existing DoS/DDoS models in different kinds of networks; traditional networks, Software Defined Networks (SDN) and virtual networks. In addition, this article provides a thorough review and comparison of the existing attack models, in particular we explain, analyze and simulate different aspects of three prominent models; congestion window, queuing, and epidemic models (same model used for corona virus spread analysis). Furthermore, we quantify the damage of DoS/DDoS attacks at three different levels; protocol (Transmission Control Protocol-TCP), device’s resources (bandwidth, CPU, memory), and network (infection and recovery speed).",2022,1603
D-Score: An expert-based method for assessing the detectability of IoT-related cyber-attacks,Yair Meidan and Daniel Benatar and Ron Bitton and Dan Avraham and Asaf Shabtai,"IoT devices are known to be vulnerable to various cyber-attacks, such as data exfiltration and the execution of flooding attacks as part of a DDoS attack. When it comes to detecting such attacks using network traffic analysis, it has been shown that some attack scenarios are not always equally easy to detect if they involve different IoT models. That is, when targeted at some IoT models, a given attack can be detected rather accurately, while when targeted at others the same attack may result in too many false alarms. In this research, we attempt to explain this variability of IoT attack detectability and devise a risk assessment method capable of addressing a key question: how easy is it for an anomaly-based network intrusion detection system to detect a given cyber-attack involving a specific IoT model? In the process of addressing this question we (a) investigate the predictability of IoT network traffic, (b) present a novel taxonomy for IoT attack detection which also encapsulates traffic predictability aspects, (c) propose an expert-based attack detectability estimation method which uses this taxonomy to derive a detectability score (termed ‘D-Score’) for a given combination of IoT model and attack scenario, and (d) empirically evaluate our method while comparing it with a data-driven method.",2023,1621
A survey of security issues for cloud computing,Minhaj Ahmad Khan,"High quality computing services with reduced cost and improved performance have made cloud computing a popular paradigm. Due to its flexible infrastructure, net centric approach and ease of access, the cloud computing has become prevalent. Its widespread usage is however being diminished by the fact that the cloud computing paradigm is yet unable to address security issues which may in turn aggravate the quality of service as well as the privacy of customers' data. In this paper, we present a survey of security issues in terms of security threats and their remediations. The contribution aims at the analysis and categorization of working mechanisms of the main security issues and the possible solutions that exist in the literature. We perform a parametric comparison of the threats being faced by cloud platforms. Moreover, we compare various intrusion detection and prevention frameworks being used to address security issues. The trusted cloud computing and mechanisms for regulating security compliance among cloud service providers are also analyzed. Since the security mechanisms continue to evolve, we also present the future orientation of cloud security issues and their possible countermeasures.",2016,1624
Common vulnerability scoring system prediction based on open source intelligence information sources,Philipp Kühn and David N. Relke and Christian Reuter,"The number of newly published vulnerabilities is constantly increasing. Until now, the information available when a new vulnerability is published is manually assessed by experts using a Common Vulnerability Scoring System (CVSS) vector and score. This assessment is time consuming and requires expertise. Various works already try to predict CVSS vectors or scores using machine learning based on the textual descriptions of the vulnerability to enable faster assessment. However, for this purpose, previous works only use the texts available in databases such as National Vulnerability Database. With this work, the publicly available web pages referenced in the National Vulnerability Database are analyzed and made available as sources of texts through web scraping. A Deep Learning based method for predicting the CVSS vector is implemented and evaluated. The present work provides a classification of the National Vulnerability Database’s reference texts based on the suitability and crawlability of their texts. While we identified the overall influence of the additional texts is negligible, we outperformed the state-of-the-art with our Deep Learning prediction models.",2023,1631
Large-scale intent analysis for identifying large-review-effort code changes,Song Wang and Chetan Bansal and Nachiappan Nagappan,"Context: Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. Change intents have been studied for years to help developers understand the rationale behind code commits. However, in most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis. Objective: In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes—changes with large review effort. Method: Specifically, we first propose a feedback-driven and heuristics-based approach to identify change intents of code changes. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on four large-scale projects, one from Microsoft and three are open source projects, i.e., Qt, Android, and OpenStack. Results: Our results show that, (i) code changes with some intents (i.e., Feature and Refactor) are more likely to be LRE changes, (ii) machine learning based prediction models are applicable for identifying LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. Conclusion: The change intent analysis and its application on LRE identification proposed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process. To show how to deploy our approaches in real-world practice, we report a case study of developing and deploying the intent analysis system in Microsoft. Moreover, we also evaluate the usefulness of our approaches by using a questionnaire survey. The feedback from developers demonstrate its practical value.",2021,1648
A systematic literature review and taxonomy of modern code review,Nicole Davila and Ingrid Nunes,"Context:
Modern Code Review (MCR) is a widely known practice of software quality assurance. However, the existing body of knowledge of MCR is currently not understood as a whole.
Objective:
Our goal is to identify the state of the art on MCR, providing a structured overview and an in-depth analysis of the research done in this field.
Methods:
We performed a systematic literature review, selecting publications from four digital libraries.
Results:
A total of 139 papers were selected and analyzed in three main categories. Foundational studies are those that analyze existing or collected data from the adoption of MCR. Proposals consist of techniques and tools to support MCR, while evaluations are studies to assess an approach or compare a set of them.
Conclusion:
The most represented category is foundational studies, mainly aiming to understand the motivations for adopting MCR, its challenges and benefits, and which influence factors lead to which MCR outcomes. The most common types of proposals are code reviewer recommender and support to code checking. Evaluations of MCR-supporting approaches have been done mostly offline, without involving human subjects. Five main research gaps have been identified, which point out directions for future work in the area.",2021,1656
Triage Software Update Impact via Release Notes Classification,Solomon Berhe and Vanessa Kan and Omhier Khan and Nathan Pader and Ali Zain Farooqui and Marc Maynard and Foutse Khomh,"In the rapidly evolving domain of Industry 4.0, effective management of software updates is crucial for maintaining system continuity and security. This paper presents a novel machine learning-based approach for a prompt and effective triage of software updates, leveraging an evaluation of six release note classifiers to categorize updates by component type, release type, and security risk. Our methodology, tested on a dataset of 1,000 release notes commonly encountered in Industry 4.0 ecosystems, demonstrates Logistic Regression as the most accurate classifier. The findings not only highlight the practical applicability of our approach in real-world data but also set the foundation for future enhancements to streamline the machine learning triage process further.",2024,1665
A machine learning approach for detecting fast flux phishing hostnames,Thomas Nagunwa and Paul Kearney and Shereen Fouad,"Attackers are increasingly using Fast Flux Service Networks (FFSNs), networks of compromised machines, to host phishing websites. In FFSNs, the machines rapidly change such that blacklisting them does not entirely stop the networks from operating the websites. This increases the longevity of the websites thus becoming more harmful. Existing solutions for detecting the websites are limited with relatively low or moderate prediction performances, high prediction time and use of less diversified features which increases their susceptibility to detection evasions. This paper proposes a Machine Learning (ML) based approach for detecting phishing websites hosted in FFSNs using a novel set of 56 features. Compared with previous works, the approach achieves high accuracy, a low detection time and uses highly diversified features to enhance resilience to detection evasion. The effectiveness of the features for prediction was evaluated in the context of binary and multi-class classification tasks using multiple traditional and deep learning ML algorithms. The proposed approach achieves an accuracy of 98.42% and 97.81% for binary and multi-class classification tasks respectively. Our results showed that temporal and DNS based features are the strongest predictors while network and host related features are the weakest. Our approach is a significant step towards tracking of core components of FFSNs with an aim of shutting down the entire phishing ecosystem.",2022,1714
Chapter 12 - Mobile health (mHealth),Sriram Iyengar,"Mobile health (mHealth), the use of mobile devices such as cell phones to support health, is poised to greatly impact healthcare in many ways. Smartphones are ubiquitous in developed countries and are rapidly being adopted in developing countries. Since they are highly portable and very powerful computing and communication devices with large storage and excellent graphic capabilities, they have the capability to provide clinical decision support, deliver media-rich medical advice, and support documentation of care even when there is no connectivity. The emerging discipline of persuasive technology provides a conceptual framework for developing effective smartphone systems for healthcare. In developing countries, smartphones can serve as powerful support tools for healthcare professionals including community health workers. Google, Apple, and other third parties provide systems for the development of smartphone systems. There are many new mobile technologies for the health including activity sensors and smartwatches.",2020,1715
An approach to the taxonomy of Vallisneria L. (Hydrocharitaceae),Richard M. Lowden,"Taxonomic decisions presented in this study of Vallisneria are founded on the consistency of comparable staminate and pistillate floral structures considering the geography and dioecious nature of the genus. Field studies, realized in southern Europe, eastern North America, Central America, northern South America and the Greater Antilles, formed the basis for the development of the present knowledge of characteris. Umbel and spike-like inflorescences were discovered from three localities in the Americas. Staminodia were encountered in the female flowers of Vallisneria spiralis. Two species including four varieties are recognized, mapped and illustrated. Infraspecific taxa are V. spiralis L. var. spiralis, V. spiralis var. denseserrulata Makino, V. americana Michaux var. americana and V. americana var. biwaensis (Miki) Lowden, comb. nov. These taxa are delineated according to the degree of connation of fertile filaments in the staminate flower and adnation of staminodia to stigma—style surfaces in the pistillate flower. Both species converge along what appears to be a continuous gradient in floral variation.",1982,1722
"A comprehensive survey on IoT attacks: Taxonomy, detection mechanisms and challenges",Tinshu Sasi and Arash Habibi Lashkari and Rongxing Lu and Pulei Xiong and Shahrear Iqbal,"The Internet of Things (IoT) has set the way for the continuing digitalization of society in various manners during the past decade. The IoT is a vast network of intelligent devices exchanging data online. The security component of IoT is crucial given its rapid expansion as a new technology paradigm since it may entail safety-critical procedures and the online storage of sensitive data. Unfortunately, security is the primary challenge when adopting Internet of Things (IoT) technologies. As a result, manufacturers’ and academics’ top priority now is improving the security of IoT devices. A substantial body of literature on the subject encompasses several issues and potential remedies. However, most existing research fails to offer a comprehensive perspective on attacks inside the IoT. Hence, this survey aims to establish a structure to guide researchers by categorizing attacks in the taxonomy according to various factors such as attack domains, attack threat type, attack executions, software surfaces, IoT protocols, attacks based on device property, attacks based on adversary location and attacks based on information damage level. This is followed by a comprehensive analysis of the countermeasures offered in academic literature. In this discourse, the countermeasures proposed for the most significant security attacks in the IoT are investigated. Following this, a comprehensive classification system for the various domains of security research in the IoT and Industrial Internet of Things (IIoT) is developed, accompanied by their respective remedies. In conclusion, the study has revealed several open research areas pertinent to the subject matter.",2023,1732
Reactions on Twitter towards Australia's proposed import restriction on nicotine vaping products: a thematic analysis,Tianze Sun and Carmen C.W. Lim and Coral Gartner and Jason P. Connor and Wayne D. Hall and Janni Leung and Daniel Stjepanović and Gary C.K. Chan,"Objective
In June 2020, the Australian Government announced that personal importation of nicotine vaping products (NVP) would be prohibited, pending a 12‐month classification and regulation review by the Therapeutic Goods Administration. This brief report examines the themes of responses on Twitter to this announcement.
Methods
Simple random sampling was used to retrieve tweets containing keywords from 19 to 26 June 2020. Tweets were manually coded and descriptive statistics calculated for themes and policy position.
Results
The vast majority of the 1,168 tweets were anti‐policy. Themes included: criticism towards government (59.8%), activism against NVP restriction (38%), potential adverse consequences (30.8%) and support for NVP restriction (1.4%). Tweets that identified potential adverse consequences of NVP restriction cited: smoking relapse for individuals currently using NVPs (75.6%); the impact of policy enforcement (8.6%); illicit market (8.3%); panic buying (3.6%); difficulty obtaining prescriptions (2.8%); and impacts on NVP businesses (2.8%).
Conclusion
Tweets predominately objected to the policy announcement. Approximately three‐quarters of tweets that cited potential adverse consequences of the policy mentioned smoking relapse as their primary concern.
Implications for public health
User‐generated content on Twitter was primarily used to lobby against the proposed policy, which was subsequently amended.",2021,1764
Selecting and combining complementary feature representations and classifiers for hate speech detection,Rafael M.O. Cruz and Woshington V. {de Sousa} and George D.C. Cavalcanti,"Hate speech is a major issue in social networks due to the high volume of data generated daily. Recent works demonstrate the usefulness of machine learning (ML) in dealing with the nuances required to distinguish between hateful posts from just sarcasm or offensive language. Many ML solutions for hate speech detection have been proposed by either changing how features are extracted from the text or the classification algorithm employed. However, most works consider only one type of feature extraction and classification algorithm. This work argues that a combination of multiple feature extraction techniques and different classification models is needed. We propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme. Source code, figures and dataset splits can be found in the GitHub repository: https://github.com/Menelau/Hate-Speech-MCS.",2022,1765
An analysis of discussions in collaborative knowledge engineering through the lens of Wikidata,Elisavet Koutsiana and Gabriel Maia Rocha Amaral and Neal Reeves and Albert Meroño-Peñuela and Elena Simperl,"We study discussions in Wikidata, the world’s largest open-source collaborative knowledge graph (KG). This is important because it helps KG community managers understand how discussions are used and inform the design of collaborative practices and support tools. We follow a mixed-methods approach with descriptive statistics, thematic analysis, and statistical tests to investigate how much discussions in Wikidata are used, what they are used for, and how they support knowledge engineering (KE) activities. The study covers three core sources of discussion, the talk pages that accompany Wikidata items and properties, and a general-purpose communication page. Our findings show low use of discussion capabilities and a power-law distribution similar to other KE projects such as Schema.org. When discussions are used, they are mostly about KE activities, including activities that span across the entire KE lifecycle from conceptualisation and implementation to maintenance and taxonomy building. We hope that the findings will help Wikidata devise improved practices and capabilities to encourage the use of discussions as a tool to collaborate, improve editor engagement, and engineer better KGs.",2023,1769
"The evolution of IoT Malwares, from 2008 to 2019: Survey, taxonomy, process simulator and perspectives",Benjamin Vignau and Raphaël Khoury and Sylvain Hallé and Abdelwahab Hamou-Lhadj,"The past decade has seen a rapidly growing interest in IoT-connected devices. But as is usually the case with computer systems and networks, malicious individuals soon realized that these objects could be exploited for criminal purposes. The problem is particularly salient since the firmware used in many Internet connected devices was developed without taking into consideration the expertise and best security practices gained over the past several years by programmers in other areas. Consequently, multiple attacks on IoT devices took place over the last decade, culminating in the largest ever recorded DDoS attack, the Mirai botnet, which took advantage of weaknesses in the security of the IoT. In this survey, we seek to shed light on the evolution of the IoT malware. We compare the characteristic features of 28 of the most widespread IoT malware programs of the last decade and propose a novel methodology for classifying malware based on its behavioral features. Our study also highlights the common practice of feature reuse across multiple malware programs.",2021,1771
BeCAPTCHA-Mouse: Synthetic mouse trajectories and improved bot detection,Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez,"We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions, and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36%, proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.",2022,1772
IoT Botnet Forensics: A Comprehensive Digital Forensic Case Study on Mirai Botnet Servers,Xiaolu Zhang and Oren Upton and Nicole Lang Beebe and Kim-Kwang Raymond Choo,"Internet of Things (IoT) bot malware is relatively new and not yet well understood forensically, despite its potential role in a broad range of malicious cyber activities. For example, it was abused to facilitate the distributed denial of service (DDoS) attack that took down a significant portion of the Internet on October 21, 2016, keeping millions of people from accessing over 1200 websites, including Twitter and NetFlix for nearly an entire day. The widespread adoption of an estimated 50 billion IoT devices, as well as the increasing interconnectivity of those devices to traditional networks, not to mention to one another with the advent of fifth generation (5G) networks, underscore the need for IoT botnet forensics. This study is the first published, comprehensive digital forensic case study on one of the most well known families of IoT bot malware - Mirai. Past research has largely studied the botnet architecture and analyzed the Mirai source code (and that of its variants) through traditional static and dynamic malware analysis means, but has not fully and forensically analyzed infected devices or Mirai network devices. In this paper, we set up a fully functioning Mirai botnet network architecture and conduct a comprehensive forensic analysis on the Mirai botnet server. We discuss forensic artifacts left on the attacker's terminal, command and control (CNC) server, database server, scan receiver and loader, as well as the network packets therefrom. We discuss how a forensic investigator might acquire some of these artifacts remotely, without direct physical access to the botnet server itself. This research provides findings tactically useful to forensic investigators, not only from the perspective of what data can be obtained (e.g., IP addresses of bot members), but also important information about which device they should target for acquisition and investigation to obtain the most investigatively useful information.",2020,1780
Neural networks based domain name generation,Zheng Wang and Yang Guo,"Domain generation algorithm (DGA) is used by botnets to build a stealthy command and control (C&C) communication channel between the C&C server and the bots. A DGA can periodically produce a large number of pseudo-random algorithmically generated domains (AGDs), a few of which direct the bots to the C&C server. AGD detection algorithms provide a lightweight, promising solution in response to the existing DGA techniques. In the constantly evolving attacker–defender game, attackers may seek more advanced DGA techniques to gain a better chance of evading detection by defenders. In this paper, we propose a new DGA, namely a neural networks-based domain name generation (NDG) architecture. NDG is based on a variational autoencoder (VAE), where the encoder and decoder networks use stacked gated convolutional neural networks (GCNNs) to learn the contextual structure hierarchically. NDG is experimentally validated using a set of state-of-the-art AGD detection algorithms. The existing DGAs of different classes following a DGA taxonomy are used to benchmark NDG. NDG shows the best overall anti-detection performance among all tested DGAs. We also demonstrate that NDG is effective in benchmarking AGD detection algorithms.",2021,1783
Detecting and mitigating DDoS attacks with moving target defense approach based on automated flow classification in SDN networks,Marcos Aurélio Ribeiro and Mauro Sergio {Pereira Fonseca} and Juliana {de Santi},"The Distributed Denial of Service (DDoS) coordinates synchronized attacks on systems on the Internet using a set of infected hosts (bots). Bots are programmed to attack a determined target by firing a lot of synchronized requests, causing slowness or unavailability of the service. This type of attack has recently grown in magnitude, diversity, and economic cost. Thus, this paper presents a DDoS detection and mitigation architecture based on Software Defined Networking (SDN). It considers the Moving Target Defense (MTD) approach, redirecting malicious floods to expendable low-capacity servers to protect the main server while discouraging the attacker. The redirecting decision is based on a sensor, that employs Machine Learning (ML) algorithms for flow classification. When malicious flows are detected, the sensor notifies the SDN controller to include them in the malicious hosts lists and to realize the redirection. The validation and evaluation of the proposed architecture are conducted by simulation. Results considering different classification models (probabilistic, linear model, neural networks, and trees) and attack types indicate that the proposed architecture is efficient in detecting and mitigating DDoS attacks in approximately 3 seconds.",2023,1784
Automating ontology engineering support activities with OnToology,Ahmad Alobaid and Daniel Garijo and María Poveda-Villalón and Idafen Santana-Perez and Alba Fernández-Izquierdo and Oscar Corcho,"Due to the increasing uptake of semantic technologies, ontologies are now part of a good number of information systems. As a result, software development teams that have to combine ontology engineering activities with software development practices are facing several challenges, since these two areas have evolved, in general, separately. In this paper we present OnToology, an approach to manage ontology engineering support activities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based application that builds on top of Git-based environments and integrates existing semantic web technologies. We have validated OnToology against a set of representative requirements for ontology development support activities in distributed environments, and report on a survey of the system to assess its usefulness and usability.",2019,1785
Using Linked Building Data for managing temporary construction items,Alexander Schlachter and Mads Holten Rasmussen and Jan Karlshøj,"For decades, the construction industry has experienced poor productivity due to challenges such as increasing project complexity and a fragmented project environment. Even though some technological innovations around Building Information Modeling (BIM) might have the potential to overcome these challenges, data integration across disciplines, companies and software solutions is yet to be solved entirely. Trending advancements try to enrich existing BIM data using Linked Data technologies to semantically describe the building information and facilitate data integration. By that, project data from different data sources is made available in an accessible format, so project participants can use it for their planning efforts. In this paper we explore the use of Linked Building Data (LBD) on a specific use case to answer the question of how the planning of Temporary Construction Items (TCIs) can be improved by integrating data and automating the demand calculation. A literature review concludes that TCIs only experience little attention in the current planning of construction projects but have a critical impact on the outcome of a project. Thus, the objective of this paper is to develop standard ontologies to provide a semantically rich terminology of the data and to propose a framework for TCI consideration within a BIM based project delivery system. A prototype solution is developed, taking formwork as a TCI representative. The result is a process for automatically creating a TCI utilization plan that quantifies the precise time- and location-based on-site TCI demand by integrating data from BIM, Location-Based Scheduling (LBS) and TCI information. Based on the results of prototyping and findings from expert interviews, this research integrates the solution into the process of construction and finally proposes two implementation scenarios for the solution – one being based on the current industry situation and one exploring the future vision of a more integrated and decentralized project delivery in the construction industry.",2022,1793
The evolution of Mirai botnet scans over a six-year period,Antonia Affinito and Stefania Zinno and Giovanni Stanco and Alessio Botta and Giorgio Ventre,"The proliferation of Internet of Things devices has resulted in an increase in security vulnerabilities and network attacks. The Mirai botnet is a well-known example of a network used for malicious activities, detected for the first time by the white-hat research group in August 2016. Since then, Mirai initiated massive DDoS attacks by scanning for and exploiting vulnerabilities in network devices. In this paper, we investigate the evolution of the Mirai botnet over a six-year period, analyzing the TCP SYN packets using Mirai signature, i.e. with TCP sequence number equal to the destination IP address. Our analysis stands out as we extensively investigate the evolution of Mirai scans over a prolonged six-year period (2016–2022). Our findings reveal that the Mirai signature is still implemented by malicious actors today, in contrast with previous works. Moreover, we observe that the number of hijacked devices and TCP SYN packets involved in the scanning phase have increased over time. We also confirm that cybercriminals generally target Telnet port 23, followed by fewer requests on Telnet port 2323. Conversely, the number of probes on the SSH ports decreases over time, followed by a subsequent increase in 2022. Lastly, we identify several ports that had not been contacted until 2018 but have since received a large number of TCP SYN packets that verify the Mirai’s signature. These ports are linked with the emergence of new variants of the Mirai botnet.",2023,1795
Robotic Process Automation and Artificial Intelligence in Industry 4.0 – A Literature review,Jorge Ribeiro and Rui Lima and Tiago Eckhardt and Sara Paiva,"Taking into account the technological evolution of the last decades and the proliferation of information systems in society, today we see the vast majority of services provided by companies and institutions as digital services. Industry 4.0 is the fourth industrial revolution where technologies and automation are asserting themselves as major changes. Robotic Process Automation (RPA) has numerous advantages in terms of automating organizational and business processes. Allied to these advantages, the complementary use of Artificial Intelligence (AI) algorithms and techniques allows to improve the accuracy and execution of RPA processes in the extraction of information, in the recognition, classification, forecasting and optimization of processes. In this context, this paper aims to present a study of the RPA tools associated with AI that can contribute to the improvement of the organizational processes associated with Industry 4.0. It appears that the RPA tools enhance their functionality with the objectives of AI being extended with the use of Artificial Neural Network algorithms, Text Mining techniques and Natural Language Processing techniques for the extraction of information and consequent process of optimization and of forecasting scenarios in improving the operational and business processes of organizations.",2021,1807
Real-time bot infection detection system using DNS fingerprinting and machine-learning,Vicente Quezada and Fabian Astudillo-Salinas and Luis Tello-Oquendo and Paul Bernal,"In today’s cyberattacks, botnets are used as an advanced technique to generate sophisticated and coordinated attacks. Infected systems connect to a command and control (C&C) server to receive commands and attack. Thus, detecting infected hosts makes it possible to protect the network’s resources and prevent them from illicit activities toward third parties. This research elaborates on the design, implementation, and results of a bot infection detection system based on Domain Name System (DNS) traffic events for a network corporation. An infection detection feasibility analysis is performed by creating fingerprints. The traces are generated from a numerical analysis of 13 attributes. These attributes are obtained from the DNS logs of a DNS server. It looks for fingerprint anomalies using Isolation Forest to label a host as infected or not. In addition, on the traces cataloged as anomalous, a search will be carried out for queries to domains generated by Domain Generation Algorithms (DGA). Then, Random Forest generates a model that detects future bot infections on hosts. The devised system integrates the ELK stack and Python. This integration facilitates the management, transformation, and storage of events, generation of fingerprints, machine learning application, and analysis of fingerprint classification results with a precision greater than 99%.",2023,1809
UTL_DGA22 - a dataset for DGA botnet detection and classification,Tong Anh Tuan and Nguyen Viet Anh and Tran Thi Luong and Hoang Viet Long,"The DGA botnet prevention is a burning topic in cybersecurity, with two problems: detection and classification. The DGA botnet dataset plays an essential role in the research allowing researchers to evaluate their proposed solutions. This study introduces a new dataset on DGA botnets named UTL_DGA22. Our proposed dataset not only inherits previous datasets' results but also has got own advantages. First, our new dataset includes only domain records and no other raw network traffic, helping to address the DGA botnet problem. Second, we removed duplicated botnet DGA families and added new botnet families for a total of 76 DGA botnet families presented. Third, we propose a valuable set of attributes as input for classification algorithms. Our experiments using the proposed features with several machine learning algorithms have had good results. It shows that our proposed attributes are firmly suitable for the input of the DGA botnet solution. Finally, we carefully compiled the dataset and attribute description documents to make it easy for researchers to use. The UTL_DGA22 dataset can serve as a database for researchers to develop their algorithms while objectively evaluating different solutions.",2023,1813
GPTSniffer: A CodeBERT-based classifier to detect source code written by ChatGPT,Phuong T. Nguyen and Juri {Di Rocco} and Claudio {Di Sipio} and Riccardo Rubei and Davide {Di Ruscio} and Massimiliano {Di Penta},"Since its launch in November 2022, ChatGPT has gained popularity among users, especially programmers who use it to solve development issues. However, while offering a practical solution to programming problems, ChatGPT should be used primarily as a supporting tool (e.g., in software education) rather than as a replacement for humans. Thus, detecting automatically generated source code by ChatGPT is necessary, and tools for identifying AI-generated content need to be adapted to work effectively with code. This paper presents GPTSniffer– a novel approach to the detection of source code written by AI – built on top of CodeBERT. We conducted an empirical study to investigate the feasibility of automated identification of AI-generated code, and the factors that influence this ability. The results show that GPTSniffer can accurately classify whether code is human-written or AI-generated, outperforming two baselines, GPTZero and OpenAI Text Classifier. Also, the study shows how similar training data or a classification context with paired snippets helps boost the prediction. We conclude that GPTSniffer can be leveraged in different contexts, e.g., in software engineering education, where teachers use the tool to detect cheating and plagiarism, or in development, where AI-generated code may require peculiar quality assurance activities.",2024,1819
Effectiveness of an ensemble technique based on the distributivity equation in detecting suspicious network activity,Ewa Rak and Jaromir Sarzyński and Rafał Rak,"With the growing complexity and frequency of cyber threats, there is a pressing need for more effective defense mechanisms. Machine learning offers the potential to analyze vast amounts of data and identify patterns indicative of malicious activity, enabling faster and more accurate threat detection. Ensemble methods, by incorporating diverse models with varying vulnerabilities, can increase resilience against adversarial attacks. This study covers the usage and evaluation of the relevance of an innovative approach of ensemble classification for identifying intrusion threats on a large CICIDS2017 dataset. The approach is based on the distributivity equation that appropriately aggregates the underlying classifiers. It combines various standard supervised classification algorithms, including Multilayer Perceptron Network, k-Nearest Neighbors, and Naive Bayes, to create an ensemble. Experiments were conducted to evaluate the effectiveness of the proposed hybrid ensemble method. The performance of the ensemble approach was compared with individual classifiers using measures such as accuracy, precision, recall, F-score, and area under the ROC curve. Additionally, comparisons were made with widely used state-of-the-art ensemble models, including the soft voting method (Weighted Average Probabilities), Adaptive Boosting (AdaBoost), and Histogram-based Gradient Boosting Classification Tree (HGBC) and with existing methods in the literature using the same dataset, such as Deep Belief Networks (DBN), Deep Feature Learning via Graph (Deep GFL). Based on these experiments, it was found that some ensemble methods, such as AdaBoost and Histogram-based Gradient Classification Tree, do not perform reliably for the specific task of identifying network attacks. This highlights the importance of understanding the context and requirements of the data and problem domain. The results indicate that the proposed hybrid ensemble method outperforms traditional algorithms in terms of classification precision and accuracy, and offers insights for improving the effectiveness of intrusion detection systems.",2024,1821
Twitter social bots: The 2019 Spanish general election data,Javier Pastor-Galindo and Mattia Zago and Pantaleone Nespoli and Sergio {López Bernal} and Alberto {Huertas Celdrán} and Manuel {Gil Pérez} and José A. Ruipérez-Valiente and Gregorio {Martínez Pérez} and Félix {Gómez Mármol},"The term social bots refer to software-controlled accounts that actively participate in the social platforms to influence public opinion toward desired directions. To this extent, this data descriptor presents a Twitter dataset collected from October 4th to November 11th, 2019, within the context of the Spanish general election. Starting from 46 hashtags, the collection contains almost eight hundred thousand users involved in political discussions, with a total of 5.8 million tweets. The proposed data descriptor is related to the research article available at [1]. Its main objectives are: i) to enable worldwide researchers to improve the data gathering, organization, and preprocessing phases; ii) to test machine-learning-powered proposals; and, finally, iii) to improve state-of-the-art solutions on social bots detection, analysis, and classification. Note that the data are anonymized to preserve the privacy of the users. Throughout our analysis, we enriched the collected data with meaningful features in addition to the ones provided by Twitter. In particular, the tweets collection presents the tweets’ topic mentions and keywords (in the form of political bag-of-words), and the sentiment score. The users’ collection includes one field indicating the likelihood of one account being a bot. Furthermore, for those accounts classified as bots, it also includes a score that indicates the affinity to a political party and the followers/followings list.",2020,1822
Dataset of open-source software developers labeled by their experience level in the project and their associated software metrics,Quentin Perez and Christelle Urtado and Sylvain Vauttier,"Developers are extracted from 17 open-source projects from GitHub. Projects are chosen that use the java programming language, the Spring framework and Maven/Gradle build tools. Along with these developers, 24 software engineering metrics are extracted for each of them. These metrics are either calculated by analyzing the source code or relative to project management metadata. Each of these developers then are manually searched for in professional social media such as LinkedIn or Twitter to be labeled with their experience level in their project. Outliers are statistically detected and manually re-assigned when needed. The resulting dataset contains 703 anonymized developers qualified by their 24 project-related software engineering metrics and labeled for their experience. It is suitable for empirical software engineering studies that need to connect developers’ level of experience to tangible software engineering metrics.",2023,1823
Algorithmically generated malicious domain names detection based on n-grams features,Alessandro Cucchiarelli and Christian Morbidoni and Luca Spalazzi and Marco Baldi,"Botnets are one of the major cyber infections used in several criminal activities. In most botnets, a Domain Generation Algorithm (DGA) is used by bots to make DNS queries aimed at establishing the connection with the Command and Control (C&C) server. The identification of such queries by monitoring the network DNS traffic is then crucial for bot detection. In this paper we present a methodology to detect DGA generated domain names based on a supervised machine learning process, trained with a dataset of known benign and malicious domain names. The proposed approach represents the domain names through a set of features which express the similarity between the 2-grams and 3-grams in a single unclassified domain name and those in domain names known as malicious or benign. We used the Kullback-Leibner divergence and the Jaccard Index to estimate the similarity, and we tested different machine learning algorithms to classify each domain name as benign or DGA-based (with both binary and multi-class approach). The results of our experiments demonstrate that the proposed methodology, which only exploits lexical features of domain names, attains a good level of accuracy and results in a general model able to classify previously unseen domains in an effective way. It is also able to outperform some of the state-of-the-art featur eless classification methods based on deep learning.",2021,1834
A ground-truth dataset and classification model for detecting bots in GitHub issue and PR comments,Mehdi Golzadeh and Alexandre Decan and Damien Legay and Tom Mens,"Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots.",2021,1837
A review of amplification-based distributed denial of service attacks and their mitigation,Salih Ismail and Hani Ragab Hassen and Mike Just and Hind Zantout,"The rise of Distributed Denial of Service (DDoS) attacks have been steady in terms of the frequency and the impact of the attack. Traditionally, the attackers required control of a huge amount of resources to launch an attack. This has changed with the use of reflectors and amplifiers in DDoS attacks. A recent shift consisted of using other protocols than the traditional NTP and DNS protocols which were heavily used for ADDoS. In this paper, we review and organize amplification-based DDoS (ADDoS) attacks and associated countermeasures into a new taxonomy. Furthermore, we present a modus operandi of ADDoS attacks and analyze how it differs from traditional DDoS attacks. We also investigate how accessible ADDoS are for attackers with average resources. We survey readily available open-source scripts on GitHub and also the ADDoS features available in hire-to-DDoS platforms. We believe that accessibility and low-cost of hire-to-DDoS platforms are the major reasons for the increase of amplification-based DDoS attacks. Lastly, we provide a list of future directions that might be interesting for the community to focus on.",2021,1841
"Multi-labeling of complex, multi-behavioral malware samples",P. García-Teodoro and J.A. Gómez-Hernández and A. Abellán-Galera,"The use of malware samples is usually required to test cyber security solutions. For that, the correct typology of the samples is of interest to properly estimate the exhibited performance of the tools under evaluation. Although several malware datasets are publicly available at present, most of them are not labeled or, if so, only one class or tag is assigned to each malware sample. We defend that just one label is not enough to represent the usual complex behavior exhibited by most of current malware. With this hypothesis in mind, and based on the varied classification generally provided by automatic detection engines per sample, we introduce here a simple multi-labeling approach to automatically tag the usual multiple behavior of malware samples. In the paper, we first analyze the coherence between the behaviors exhibited by a specific number of well-known malware samples dissected in the literature and the multiple tags provided for them by our labeling proposal. After that, the automatic multi-labeling scheme is executed over four public Android malware datasets, the different results and statistics obtained regarding their composition and representativeness being discussed. We share in a GitHub repository the multi-labeling tool developed, for public usage.",2022,1846
The influence of social media affordances on drug dealer posting behavior across multiple social networking sites (SNS),Michael Robert Haupt and Raphael Cuomo and Jiawei Li and Matthew Nali and Tim K. Mackey,"Social media has been documented as widely used for initiating online sales of illicit drugs such as opioids. However, not much is known about how affordances of social networking sites (SNS) influence how dealers advertise their supplies. To explore this topic, social media posts across 5 online platforms (Google Groups, Instagram, Twitter, Reddit, and Tumblr) were collected during 2020–2021. Biterm topic modeling (BTM) was used to identify signal posts specifically associated with the illegal online sale of opioids from drug selling social media accounts. Posts were analyzed by conducting a word count for drug names or slang terms associated with 5 categories: Opioids, Non-Opioid Prescription Controlled Drugs (e.g., Xanax, Valium), Other Illicit Drugs (e.g., Meth, Cocaine), Synthetic Opioids (Fentanyl), and Synthetic Marijuana. Number of mentions per post were calculated for each drug category and compared across platforms. Identifiers (e.g., publicly available email address) associated with posts were used to track dealers across different user accounts. Platforms with affordances for longer messages (e.g., Tumblr) had higher concentrations of drug mentions per post and higher variety of drug type mentions compared to SNS platforms Instagram and Twitter. Google Groups had the most drug mentions per post across all 5 categories. Additionally, each identifier was associated with multiple user accounts on a given platform. These results indicate that affordances of anonymity and message length may influence how drug dealers advertise their services on different platforms. Public health implications and strategies to counteract drug dealers and illicit drug diversion via SNS are also discussed.",2022,1849
A generic framework for federated CDEs applied to Issue Management,Jeroen Werbrouck and Oliver Schulz and Jyrki Oraskari and Erik Mannens and Pieter Pauwels and Jakob Beetz,"This paper analyses the requirements for managing interoperable building data in a federated Common Data Environment (CDE). We discuss the need for generic (meta)data storage patterns, semantic query interfaces, decentral authentication, data aggregation, and adaptation and prove that their combination is feasible with current-day technologies. We illustrate the mechanisms of such federated CDE by considering the topic of digital Issue Management, one of the primary functions of a CDE. In an exemplary data flow process, we show how generic (federated, Semantic Web-based) data patterns for Issue Management can be aggregated and restructured to match existing industry standards like buildingSMART’s BIM Collaboration Format (BCF) API. Finally, we show the methodology is compatible with current-day practice by implementing this process in a proof of concept. The main contribution of this research is a generic, federated framework for project-related, interdisciplinary collaboration for CDEs.",2023,1853
Applying machine learning to assess emotional reactions to video game content streamed on Spanish Twitch channels,Noemí Merayo and Rosalía Cotelo and Rocío Carratalá-Sáez and Francisco J. Andújar,"This research explores for the first time the application of machine learning to detect emotional responses in video game streaming channels, specifically on Twitch, the most widely used platform for broadcasting content. Analyzing sentiment in gaming contexts is difficult due to the brevity of messages, the lack of context, and the use of informal language, which is exacerbated in the gaming environment by slang, abbreviations, memes, and jargon. First, a novel Spanish corpus was created from chat messages on Spanish video game Twitch channels, manually labeled for polarity and emotions. It is noteworthy as the first Spanish corpus for analyzing social responses on Twitch. Secondly, machine learning algorithms were used to classify polarity and emotions offering promising evaluations. The methodology followed in this work consists of three main steps: (1) Extracting Twitch chat messages from Spanish streamers’ channels related to gaming events and gameplays; (2) Processing and selecting the messages to form the corpus and manually annotating polarity and emotions; and (3) Applying machine learning models to detect polarity and emotions in the created corpus. The results have shown that a Bidirectional Encoder Representation from Transformers (BERT) based model excels with 78% accuracy in polarity detection, while deep learning and Random Forest models reach around 70%. For emotion detection, the BERT model performs best with 68%, followed by deep learning with 55%. It is worth noting that emotion detection is more challenging due to the subjective interpretation of emotions in the complex communicative context of video gaming on platforms such as Twitch. The use of supervised learning techniques, together with the rigorous corpus labeling process and the subsequent corpus pre-processing methodology, has helped to mitigate these challenges, and the algorithms have performed well. The main limitations of the research involve category and video game representation balance. Finally, it is important to stress that the integration of machine learning in video games and on Twitch is innovative, by allowing the identification of viewers’ emotions on streamers’ channels. This innovation could bring benefits such as a better understanding of audience sentiment, improving content and audience retention, providing personalized recommendations and detecting toxic behavior in chats.",2024,1854
A unifying framework for the systematic analysis of Git workflows,Julio César {Cortés Ríos} and Suzanne M. Embury and Sukru Eraslan,"Context:
Git is a popular distributed version control system that provides flexibility and robustness for software development projects. Several workflows have been proposed to codify the way project contributors work collaboratively with Git. Some workflows are highly prescriptive while others allow more leeway but do not provide the same level of code quality assurance, thus, preventing their comparison to determine the most suitable for a specific set of requirements, or to ascertain if a workflow is being properly followed.
Objective:
In this paper, we propose a novel feature-based framework for describing Git workflows, based on a study of 26 existing instances. The framework enables workflows’ comparison, to discern how, and to what extent, they exploit Git capabilities for collaborative software development.
Methods:
The framework uses feature-based modelling to map Git capabilities, regularly expressed as contribution guidelines, and a set of features that can be impartially applied to all the workflows considered. Through this framework, each workflow was characterised based on their publicly available descriptions. The characterisations were then vectorised and processed using hierarchical clustering to determine workflows’ similarities and to identify which features are most popular, and more relevant for discriminatory purposes.
Results:
Comparative analysis evidenced that some workflows claiming to be closely related, when described and then characterised, turned out to have more differences than similarities. The analysis also showed that most workflows focus on the branching and code integration strategies, whilst others emphasise subtle differences from other popular workflows or describe a specific development route and are, thus, widely reused.
Conclusion:
The characterisation and clustering analysis demonstrated that our framework can be used to compare and analyse Git workflows.",2022,1859
PBCNN: Packet Bytes-based Convolutional Neural Network for Network Intrusion Detection,Lian Yu and Jingtao Dong and Lihao Chen and Mengyuan Li and Bingfeng Xu and Zhao Li and Lin Qiao and Lijun Liu and Bei Zhao and Chen Zhang,"Network intrusion detection system (IDS) protects the target network from the threats of data breaches and the insecurity of people’s privacy. However, most of existing researches on network intrusion detection cannot fulfil effectively the protection of targets, especially, depending heavily on the statistical features that are manually designed with domain experts’ knowledge and experiences, and failing to address the few sample data problem. Network traffic has a hierarchical structure, i.e., byte-packet-flow, which is similar to phrase-sentence-article in an article. This paper proposes a hierarchical packet byte-based CNN, called PBCNN, where the first level extracts abstract features automatically from bytes in a packet in raw Pcap files, and then the second level further constructs the representation from packets in a flow or session, instead of using feature-ready CSV files, to make full use of original data information. Multiple convolution-pooling modules are cascaded with byte-friendly sizes of multiple filters, and one-layer TextCNN to obtain the representation of traffic flow, feeding the representation to 3 layers of fully connected networks for intrusion classification. PBCNN-based few shot learning is applied to improve the detection reliability of network attack categories with the few sample problem. Several experiments are performed and the results show that the evaluation metrics are superior to the existing researches in regard to CIC-IDS2017 and CSE-CIC-IDS2018 datasets.",2021,1864
Public sentiment analysis and topic modeling regarding COVID-19 vaccines on the Reddit social media platform: A call to action for strengthening vaccine confidence,Chad A. Melton and Olufunto A. Olusanya and Nariman Ammar and Arash Shaban-Nejad,"Background
The COVID-19 pandemic fueled one of the most rapid vaccine developments in history. However, misinformation spread through online social media often leads to negative vaccine sentiment and hesitancy.
Methods
To investigate COVID-19 vaccine-related discussion in social media, we conducted a sentiment analysis and Latent Dirichlet Allocation topic modeling on textual data collected from 13 Reddit communities focusing on the COVID-19 vaccine from Dec 1, 2020, to May 15, 2021. Data were aggregated and analyzed by month to detect changes in any sentiment and latent topics.
Results
Polarity analysis suggested these communities expressed more positive sentiment than negative regarding the vaccine-related discussions and has remained static over time. Topic modeling revealed community members mainly focused on side effects rather than outlandish conspiracy theories.
Conclusion
Covid-19 vaccine-related content from 13 subreddits show that the sentiments expressed in these communities are overall more positive than negative and have not meaningfully changed since December 2020. Keywords indicating vaccine hesitancy were detected throughout the LDA topic modeling. Public sentiment and topic modeling analysis regarding vaccines could facilitate the implementation of appropriate messaging, digital interventions, and new policies to promote vaccine confidence.",2021,1865
A systematic comparison and evaluation of building ontologies for deploying data-driven analytics in smart buildings,Zhangcheng Qiang and Stuart Hands and Kerry Taylor and Subbu Sethuvenkatraman and Daniel Hugo and Pouya {Ghiasnezhad Omran} and Madhawa Perera and Armin Haller,"Ontologies play a critical role in data exchange, information integration, and knowledge sharing across diverse smart building applications. Yet, semantic differences between the prevailing building ontologies hamper their purpose of bringing data interoperability and restrict the ability to reuse building ontologies in real-world applications. In this paper, we propose and adopt a framework to conduct a systematic comparison and evaluation of four popular building ontologies (Brick Schema, RealEstateCore, Project Haystack, and Digital Buildings) from both axiomatic design and assertions in a use case, namely the Terminological Box (TBox) evaluation and the Assertion Box (ABox) evaluation. In the TBox evaluation, we use the SQuaRE-based Ontology Quality Evaluation (OQuaRE) framework and concede that Project Haystack and Brick Schema are more compact with respect to the ontology axiomatic design. In the ABox evaluation, we apply an empirical study with sample building data that suggests Brick Schema and RealEstateCore have greater completeness and expressiveness in capturing the main concepts and relations within the building domain. The results indicate that there is no universal building ontology for integrating Linked Building Data (LBD). We also discuss ontology compatibility and investigate building ontology design patterns (ODPs) to support ontology matching, alignment, and harmonisation.",2023,1878
"AI techniques for IoT-based DDoS attack detection: Taxonomies, comprehensive review and research challenges",Bindu Bala and Sunny Behal,"Distributed Denial of Service (DDoS) attacks in IoT networks are one of the most devastating and challenging cyber-attacks. The number of IoT users is growing exponentially due to the increase in IoT devices over the past years. Consequently, DDoS attack has become the most prominent attack as vulnerable IoT devices are becoming victims of it. In the literature, numerous techniques have been proposed to detect IoT-based DDoS attacks. However, techniques based on Artificial Intelligence (AI) have proven to be effective in the detection of cyber-attacks in comparison to other alternative techniques. This paper presents a systematic literature review of AI-based tools and techniques used for analysis, classification, and detection of the most threatening, prominent, and dreadful IoT-based DDoS attacks between the years 2019 to 2023. A comparative study of real datasets having IoT traffic features has also been illustrated. The findings of this systematic review provide useful insights into the existing research landscape for designing AI-based models to detect IoT-based DDoS attacks specifically. Additionally, the study sheds light on IoT botnet lifecycle, various botnet families, the taxonomy of IoT-based DDoS attacks, prominent tools used to launch DDoS attack, publicly available IoT datasets, the taxonomy of AI techniques, popular software available for ML/DL modeling, a list of numerous research challenges and future directions that may aid in the development of novel and reliable methods for identifying and categorizing IoT-based DDoS attacks.",2024,1879
"Distributed denial of service attack prediction: Challenges, open issues and opportunities",Anderson Bergamini {de Neira} and Burak Kantarci and Michele Nogueira,"Distributed Denial of Service (DDoS) attack is one of the biggest cyber threats. DDoS attacks have evolved in quantity and volume to evade detection and increase damage. Changes during the COVID-19 pandemic have left traditional perimeter-based security measures vulnerable to attackers that have diversified their activities by targeting health services, e-commerce, and educational services. DDoS attack prediction searches for signals of attack preparation to warn about the imminence of the attack. Prediction is necessary to handle high-volumetric DDoS attacks and to increase the time to defend against them. This survey article presents the classification of studies from the literature comprising the current state-of-the-art on DDoS attack prediction. It highlights the results of this extensive literature review categorizing the works by prediction time, architecture, employed methodology, and the type of data utilized to predict attacks. Further, this survey details each identified study and, finally, it emphasizes the research opportunities to evolve the DDoS attack prediction state-of-the-art.",2023,1888
On Detecting and Classifying DGA Botnets and their Families,Tong Anh Tuan and Hoang Viet Long and David Taniar,"Botnets are a frequent threat to information systems on the Internet, capable of launching denial-of-service attacks, spreading spam and malware on a large scale. Detecting and preventing botnets is very important in cybersecurity. Previous studies have suggested anomaly-based, signature-based, or HoneyNet-based botnet detection solutions. This paper presents new solutions for detecting and classifying families of Domain Generation Algorithm (DGA) botnets. Our solution can be applied in practice to disable botnets even if they have infected the computer. Our works help solve two problems, including binary classification and multiclass classification, specifically: (1) Determining whether a domain name is malicious or benign; (2) For malicious domains, identify their DGA botnet family. We proposed two deep learning models called LA_Bin07 and LA_Mul07 by combining the LSTM network and Attention layer. Our evaluation used the UMUDGA dataset recently published in 2020, with 50 DGA botnet families. The experimental results show that the LA_Bin07 and LA_Mul07 models solve the DGA botnets problem for binary and multiclass classification problems with very high accuracy.",2022,1891
Detection of zero-day attacks: An unsupervised port-based approach,Agathe Blaise and Mathieu Bouet and Vania Conan and Stefano Secci,"Last years have witnessed more and more DDoS attacks towards high-profile websites, as the Mirai botnet attack on September 2016, or more recently the memcached attack on March 2018, this time with no botnet required. These two outbreaks were not detected nor mitigated during their spreading, but only at the time they happened. Such attacks are generally preceded by several stages, including infection of hosts or device fingerprinting; being able to capture this activity would allow their early detection. In this paper, we propose a technique for the early detection of emerging botnets and newly exploited vulnerabilities, which consists in (i) splitting the detection process over different network segments and retaining only distributed anomalies, (ii) monitoring at the port-level, with a simple yet efficient change-detection algorithm based on a modified Z-score measure. We argue how our technique, named Split-and-Merge, can ensure the detection of large-scale zero-day attacks and drastically reduce false positives. We apply the method on two datasets: the MAWI dataset, which provides daily traffic traces of a transpacific backbone link, and the UCSD Network Telescope dataset which contains unsolicited traffic mainly coming from botnet scans. The assumption of a normal distribution – for which the Z-score computation makes sense – is verified through empirical measures. We also show how the solution generates very few alerts; an extensive evaluation on the last three years allows identifying major attacks (including Mirai and memcached) that current Intrusion Detection Systems (IDSs) have not seen. Finally, we classify detected known and unknown anomalies to give additional insights about them.",2020,1894
A study of the relationship of malware detection mechanisms using Artificial Intelligence,Jihyeon Song and Sunoh Choi and Jungtae Kim and Kyungmin Park and Cheolhee Park and Jonghyun Kim and Ikkyun Kim,"Implementation of malware detection using Artificial Intelligence (AI) has emerged as a significant research theme to combat evolving various types of malwares. Researchers implement various detection mechanisms using shallow and deep learning models to counter new malware, and they continue to develop these mechanisms today. However, in the field of malware detection using AI, there are difficulties in collecting data, and it is difficult to compare research content and performance with related studies. Meanwhile, the number of well-organized papers is not sufficient to understand the overall research flow of these related studies. Before starting new research, researchers need to analyze the current state of research in the malware detection field they want to study. Therefore, based on these requirements, we present a summary of the general criteria related to malware detection and a classification table for detection mechanisms. Additionally, we have organized many studies in the field of various types of malware detection so that they can be viewed at a glance. We hope that the provided survey can help new researchers quickly understand the research flow in the field of AI-based malware detection and establish the direction for future research.",2024,1897
A semantic ontology for representing and quantifying energy flexibility of buildings,Han Li and Tianzhen Hong,"Energy flexibility of buildings can be an essential resource for a sustainable and reliable power grid with the growing variable renewable energy shares and the trend to electrify and decarbonize buildings. Traditional demand-side management technologies, advanced building controls, and emerging distributed energy resources (including electric vehicle, energy storage, and on-site power generation) enable the transition of the building stock to grid-interactive efficient buildings (GEBs) that operate efficiently to meet service needs and are responsive to grid pricing or carbon signals to achieve energy and carbon neutrality. Although energy flexibility has received growing attention from industry and the research community, there remains a lack of common ground for energy flexibility terminologies, characterization, and quantification methods. This paper presents a semantic ontology—EFOnt (Energy Flexibility Ontology)—that extends existing terminologies, ontologies, and schemas for building energy flexibility applications. EFOnt aims to serve as a standardized tool for knowledge co-development and streamlining energy flexibility related applications. We demonstrate potential use cases of EFOnt via two examples: (1) energy flexibility analytics with measured data from a residential smart thermostat dataset and a commercial building, and (2) modeling and simulation to evaluate energy flexibility of buildings. The compatibility of EFOnt with existing ontologies and the outlook of EFOnt's role in the building energy data tool ecosystem are discussed.",2022,1912
Multi-triage: A multi-task learning framework for bug triage,Thazin Win Win Aung and Yao Wan and Huan Huo and Yulei Sui,"Assigning developers and allocating issue types are two important tasks in the bug triage process. Existing approaches tackle these two tasks separately, which is time-consuming due to repetition of effort and negating the values of correlated information between tasks. In this paper, a multi-triage model is proposed that resolves both tasks simultaneously via multi-task learning (MTL). First, both tasks can be regarded as a classification problem, based on historical issue reports. Second, performances on both tasks can be improved by jointly interpreting the representations of the issue report information. To do so, a text encoder and abstract syntax tree (AST) encoder are used to extract the feature representation of bug descriptions and code snippets accordingly. Finally, due to the disproportionate ratio of class labels in training datasets, the contextual data augmentation approach is introduced to generate syntactic issue reports to balance the class labels. Experiments were conducted on eleven open-source projects to demonstrate the effectiveness of this model compared with state-of-the-art methods.",2022,1922
Including widespread geometry schemas into Linked Data-based BIM applied to built heritage,Mathias Bonduel and Anna Wagner and Pieter Pauwels and Maarten Vergauwen and Ralf Klein,"A reliable data exchange often including geometry-related data between stakeholders is crucial in construction projects. In this regard, data exchange frameworks built on Linked Data principles are very promising for combining disparate data sets. However, existing proposals to combine geometry and Linked Data either demand dedicated applications or support only a limited number of common geometry schemas. If any existing geometry schema could be used in a Linked Data context, error-prone geometry conversions are avoided and stakeholders do not need to invest in new geometry engines. In this paper, the applicability of Resource Description Framework (RDF) literals for including a wide variety of existing geometry schemas is studied and applied in a built heritage context. The uniform linking pattern and related terminology of the Ontology for Managing Geometry are used to implement this approach. Subsequently, the File Ontology for Geometry formats and Geometry Metadata Ontology are developed to ease the reuse of linked geometry descriptions. The effectiveness of the entire data structure is demonstrated in a built heritage case study project. The receiving party is able to create successfully a coordinated view using a demo web application on shared, but disparate, RDF data sets containing geometry descriptions.",2020,1923
The Westermo network traffic data set,Per Erik Strandberg and David Söderman and Alireza Dehlaghi-Ghadim and Miguel Leon and Tijana Markovic and Sasikumar Punnekkat and Mahshid Helali Moghadam and David Buffoni,"There is a growing body of knowledge on network intrusion detection, and several open data sets with network traffic and cyber-security threats have been released in the past decades. However, many data sets have aged, were not collected in a contemporary industrial communication system, or do not easily support research focusing on distributed anomaly detection. This paper presents the Westermo network traffic data set, 1.8 million network packets recorded in over 90 minutes in a network built up of twelve hardware devices. In addition to the raw data in PCAP format, the data set also contains pre-processed data in the form of network flows in CSV files. This data set can support the research community for topics such as intrusion detection, anomaly detection, misconfiguration detection, distributed or federated artificial intelligence, and attack classification. In particular, we aim to use the data set to continue work on resource-constrained distributed artificial intelligence in edge devices. The data set contains six types of events: harmless SSH, bad SSH, misconfigured IP address, duplicated IP address, port scan, and man in the middle attack.",2023,1933
Ontology for building permit authorities (OBPA) for advanced building permit processes,Judith Fauth and Sebastian Seiß,"Building permit processes lie on the divide between architecture, engineering, and construction (AEC) and public administration. To ensure consistent and effective digitization in building permitting, it is necessary to consider and merge both areas. Hence, for advanced building permit processes, foundations must be developed, which begins with understanding and formalizing building permit authorities’ organizational structures and processes. Therefore, this study developed an ontology that covers a semantic representation of a building permit authority along with a subprocess of the building permit process called the assignment process. The assignment process describes how and on what basis building applications are assigned to appropriate building officials. Proposing a semantic representation of the assignment process, tacit knowledge from previously conducted data sets was analyzed and implemented in the ontology. As a case study, a sample building permit authority was described and implemented in the ontology. On the one hand, the developed ontology serves as a basis for decision support for building permit processes, while on the other hand, it enables a fully automated assignment process in a building permit authority. The approach not only makes the assignment process more objective and transparent for all parties involved in the building permit process but also allows time and personnel capacities to be used in its other subprocesses.",2023,1935
Current practices and infrastructure for open data based research on occupant-centric design and operation of buildings,Mikkel B. Kjærgaard and Omid Ardakanian and Salvatore Carlucci and Bing Dong and Steven K. Firth and Nan Gao and Gesche Margarethe Huebner and Ardeshir Mahdavi and Mohammad Saiedur Rahaman and Flora D. Salim and Fisayo Caleb Sangogboye and Jens Hjort Schwee and Dawid Wolosiuk and Yimin Zhu,"Many new tools for improving the design and operation of buildings try to realize the potential of big data. In particular, data is an important element for occupant-centric design and operation as occupants’ presence and actions are affected by a high degree of uncertainty and, hence, are hard to model in general. For such research, data handling is an important challenge, and following an open science paradigm based on open data can increase efficiency and transparency of scientific work. This article reviews current practices and infrastructure for open data-driven research on occupant-centric design and operation of buildings. In particular, it covers related work on open data in general and for the built environment in particular, presents survey results for existing scientific practices, reviews technical solutions for handling data and metadata, discusses ethics and privacy protection and analyses principles for the sharing of open data. In summary, this study establishes the status quo and presents an outlook on future work for methods and infrastructures to support the open data community within the built environment.",2020,1936
"Blockchain for healthcare systems: Architecture, security challenges, trends and future directions",Andrew J and Deva Priya Isravel and K. Martin Sagayam and Bharat Bhushan and Yuichi Sei and Jennifer Eunice,"Blockchain has become popular in recent times through its data integrity and wide scope of applications. It has laid the foundation for cryptocurrencies such as Ripple, Bitcoin, Ethereum, and so on. Blockchain provides a platform for decentralization and trust in various applications such as finance, commerce, IoT, reputation systems, and healthcare. However, prevailing challenges like scalability, resilience, security and privacy are yet to be overcome. Due to rigorous regulatory constraints such as HIPAA, blockchain applications in the healthcare industry usually require more stringent authentication, interoperability, and record sharing requirements. This article presents an extensive study to showcase the significance of blockchain technology from both application and technical perspectives for healthcare domain. The article discusses the features and use-cases of blockchain in different applications along with the healthcare domain interoperability. The detailed working operation of the blockchain and the consensus algorithms are presented in the context of healthcare. An outline of the blockchain architecture, platforms, and classifications are discussed to choose the right platform for healthcare applications. The current state-of-the-art research in healthcare blockchain and available blockchain based healthcare applications are summarized. Furthermore, the challenges and future research opportunities along with the performance evaluation metrics in realizing the blockchain technology for healthcare are presented to provide insight for future research. We also layout the various security attacks on the blockchain protocol with the classifications of threat models and presented a comparative analysis of the detection and protection techniques. Techniques to enhance the security and privacy of the blockchain network is also discussed.",2023,1949
Detecting coherent explorations in SQL workloads,Verónika Peralta and Patrick Marcel and Willeme Verdeaux and Aboubakar Sidikhy Diakhaby,"This paper presents a proposal aiming at better understanding a workload of SQL queries and detecting coherent explorations hidden within the workload. In particular, our work investigates SQLShare (Jain et al., 2016), a database-as-a-service platform targeting scientists and data scientists with minimal database experience, whose workload was made available to the research community. According to the authors of (Jain et al., 2016), this workload is the only one containing primarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed this workload by extracting features that characterize SQL queries and we investigate three different machine learning approaches to use these features to separate sequences of SQL queries into meaningful explorations. The first approach is unsupervised and based only on similarity between contiguous queries. The second approach uses transfer learning to apply a model trained over a dataset where ground truth is available. The last approach uses weak labeling to predict the most probable segmentation from heuristics meant to label a training set. We ran several tests over various query workloads to evaluate and compare the proposed methods.",2020,1954
Machine-Learning-enhanced systemic risk measure: A Two-Step supervised learning approach,Ruicheng Liu and Chi Seng Pun,"This paper explores ways to improve the existing systemic risk measures by incorporating machine learning algorithms into the measurement. We aim to overcome the shortcomings of existing methods that rely on restricted modeling and are difficult to tap into various data resources. To this end, this paper unifies a dynamic quantification framework for systemic risk and links it to a two-step supervised learning problem, which allows for hierarchical structure of the systemic event and the return dependence. We leverage the generalization and predictive powers of machine learning to statistically model the tail events and the co-movements of the equity returns during the shocks to the macro-economy. Our results show that most machine learning algorithms enhance the systemic risk measure’s predictive power. Numerous comparative and sensitivity backtesting studies for United States and Hong Kong markets are conducted, from which we recommend the best machine learning algorithm for systemic risk measurement.",2022,1961
"A Comprehensive Survey on Attacks, Security Issues and Blockchain Solutions for IoT and IIoT",Jayasree Sengupta and Sushmita Ruj and Sipra {Das Bit},"In recent years, the growing popularity of Internet of Things (IoT) is providing a promising opportunity not only for the development of various home automation systems but also for different industrial applications. By leveraging these benefits, automation is brought about in the industries giving rise to the Industrial Internet of Things (IIoT). IoT is prone to several cyberattacks and needs challenging approaches to achieve the desired security. Moreover, with the emergence of IIoT, the security vulnerabilities posed by it are even more devastating. Therefore, in order to provide a guideline to researchers, this survey primarily attempts to classify the attacks based on the objects of vulnerability. Subsequently, each of the individual attacks is mapped to one or more layers of the generalized IoT/IIoT architecture followed by a discussion on the countermeasures proposed in literature. Some relevant real-life attacks for each of these categories are also discussed. We further discuss the countermeasures proposed for the most relevant security threats in IIoT. A case study on two of the most important industrial IoT applications is also highlighted. Next, we explore the challenges brought by the centralized IoT/IIoT architecture and how blockchain can effectively be used towards addressing such challenges. In this context, we also discuss in detail one IoT specific Blockchain design known as Tangle, its merits and demerits. We further highlight the most relevant Blockchain-based solutions provided in recent times to counter the challenges posed by the traditional cloud-centered applications. The blockchain-related solutions provided in the context of two of the most relevant applications for each of IoT and IIoT is also discussed. Subsequently, we design a taxonomy of the security research areas in IoT/IIoT along with their corresponding solutions. Finally, several open research directions relevant to the focus of this survey are identified.",2020,1963
Deep learning for neuroimaging-based diagnosis and rehabilitation of Autism Spectrum Disorder: A review,Marjane Khodatars and Afshin Shoeibi and Delaram Sadeghi and Navid Ghaasemi and Mahboobeh Jafari and Parisa Moridian and Ali Khadem and Roohallah Alizadehsani and Assef Zare and Yinan Kong and Abbas Khosravi and Saeid Nahavandi and Sadiq Hussain and U. Rajendra Acharya and Michael Berk,"Accurate diagnosis of Autism Spectrum Disorder (ASD) followed by effective rehabilitation is essential for the management of this disorder. Artificial intelligence (AI) techniques can aid physicians to apply automatic diagnosis and rehabilitation procedures. AI techniques comprise traditional machine learning (ML) approaches and deep learning (DL) techniques. Conventional ML methods employ various feature extraction and classification techniques, but in DL, the process of feature extraction and classification is accomplished intelligently and integrally. DL methods for diagnosis of ASD have been focused on neuroimaging-based approaches. Neuroimaging techniques are non-invasive disease markers potentially useful for ASD diagnosis. Structural and functional neuroimaging techniques provide physicians substantial information about the structure (anatomy and structural connectivity) and function (activity and functional connectivity) of the brain. Due to the intricate structure and function of the brain, proposing optimum procedures for ASD diagnosis with neuroimaging data without exploiting powerful AI techniques like DL may be challenging. In this paper, studies conducted with the aid of DL networks to distinguish ASD are investigated. Rehabilitation tools provided for supporting ASD patients utilizing DL networks are also assessed. Finally, we will present important challenges in the automated detection and rehabilitation of ASD and propose some future works.",2021,1972
Towards ESCO 4.0 – Is the European classification of skills in line with Industry 4.0? A text mining approach,Filippo Chiarello and Gualtiero Fantoni and Terence Hogarth and Vito Giordano and Liga Baltina and Irene Spada,"ESCO is a multilingual classification of Skills, Competences, Qualifications, and Occupations created by the European Commission to improve the supply of information on skills demand in the labour market. It is designed to assist individuals, employers, universities and training providers by giving them up to date and standardized information on skills. Rapid technological change means that ESCO needs to be updated in a timely manner. Evidence is presented here of how text-mining techniques can be applied to the analysis of data on emerging skill needs arising from Industry 4.0 to ensure that ESCO provides information which is current. The alignment between ESCO and Industry 4.0 technological trends is analysed. Using text mining techniques, information is extracted on Industry 4.0 technologies from: (i) two versions of ESCO (v1.0 - v1.1.); and (ii) from the 4.0 related scientific literature. These are then compared to identify potential data gaps in ESCO. The findings demonstrate that text mining applied on scientific literature to extract technology trends, can help policy makers to provide more up-to-date labour market intelligence.",2021,1977
Fighting post-truth using natural language processing: A review and open challenges,Estela Saquete and David Tomás and Paloma Moreda and Patricio Martínez-Barco and Manuel Palomar,"Post-truth is a term that describes a distorting phenomenon that aims to manipulate public opinion and behavior. One of its key engines is the spread of Fake News. Nowadays most news is rapidly disseminated in written language via digital media and social networks. Therefore, to detect fake news it is becoming increasingly necessary to apply Artificial Intelligence (AI) and, more specifically Natural Language Processing (NLP). This paper presents a review of the application of AI to the complex task of automatically detecting fake news. The review begins with a definition and classification of fake news. Considering the complexity of the fake news detection task, a divide-and-conquer methodology was applied to identify a series of subtasks to tackle the problem from a computational perspective. As a result, the following subtasks were identified: deception detection; stance detection; controversy and polarization; automated fact checking; clickbait detection; and, credibility scores. From each subtask, a PRISMA compliant systematic review of the main studies was undertaken, searching Google Scholar. The various approaches and technologies are surveyed, as well as the resources and competitions that have been involved in resolving the different subtasks. The review concludes with a roadmap for addressing the future challenges that have emerged from the analysis of the state of the art, providing a rich source of potential work for the research community going forward.",2020,1981
Agriculture 4.0 and beyond: Evaluating cyber threat intelligence sources and techniques in smart farming ecosystems,Hang Thanh Bui and Hamed Aboutorab and Arash Mahboubi and Yansong Gao and Nazatul Haque Sultan and Aufeef Chauhan and Mohammad Zavid Parvez and Michael Bewong and Rafiqul Islam and Zahid Islam and Seyit A. Camtepe and Praveen Gauravaram and Dineshkumar Singh and M. {Ali Babar} and Shihao Yan,"The digitisation of agriculture, integral to Agriculture 4.0, has brought significant benefits while simultaneously escalating cybersecurity risks. With the rapid adoption of smart farming technologies and infrastructure, the agricultural sector has become an attractive target for cyberattacks. This paper presents a systematic literature review that assesses the applicability of existing cyber threat intelligence (CTI) techniques within smart farming infrastructures (SFIs). We develop a comprehensive taxonomy of CTI techniques and sources, specifically tailored to the SFI context, addressing the unique cyber threat challenges in this domain. A crucial finding of our review is the identified need for a virtual Chief Information Security Officer (vCISO) in smart agriculture. While the concept of a vCISO is not yet established in the agricultural sector, our study highlights its potential significance. The implementation of a vCISO could play a pivotal role in enhancing cybersecurity measures by offering strategic guidance, developing robust security protocols, and facilitating real-time threat analysis and response strategies. This approach is critical for safeguarding the food supply chain against the evolving landscape of cyber threats. Our research underscores the importance of integrating a vCISO framework into smart farming practices as a vital step towards strengthening cybersecurity. This is essential for protecting the agriculture sector in the era of digital transformation, ensuring the resilience and sustainability of the food supply chain against emerging cyber risks.",2024,1984
Next generation antivirus endowed with bitwise morphological extreme learning machines,Sidney M.L. Lima and Danilo M. Souza and Ricardo P. Pinheiro and Sthéfano H.M.T. Silva and Petrônio G. Lopes and Rafael D.T. {de Lima} and Jemerson R. {de Oliveira} and Thyago de A. Monteiro and Sérgio M.M. Fernandes and Edison de Q. Albuquerque and Washington W.A. {da Silva} and Wellington P. {dos Santos},"Background and Objective
Every second, on average, 8 (eight) new malware are created. So, our goal is to propose an antivirus, endowed with artificial intelligence, able of identifying malwares through models based on fast training and high-performance neural networks.
Methods
Our NGAV (Next Generation Antivirus) is equipped with an authorial ELM (Extreme Learning Morphological) machine. Our bmELMs (Bitwise-Morphological ELMs) are inspired by the image processing theory of Mathematical Morphology. We claim that bmELMs are able to adapt in any machine learning dataset. Inspired by Mathematical Morphology, our bmELMs are capable of modeling any form present at the decisions boundaries of neural networks.
Results
Our bmELMs results are compared with classical ELMs and evaluated through widely used classification metrics. Our antivirus, provided with Bitwise-Morphology, achieves an average accuracy of 97.88%, 93.07%, 93.07% and 91.74% in malware detection of PE (Portable Executable), Java, JavaScript and PHP, respectively.
Conclusions
Our NGAV enables high performance, large capacity of parallelism, and simple, low-power architecture with low power consumption. We concluded that our Bitwise-Morphology assists to the main requirements for the proper operation and confection of antivirus in hardware.",2021,1987
An evaluation of agile Ontology Engineering Methodologies for the digital transformation of companies,Daniele Spoladore and Elena Pessot,"Ontologies are increasingly recognised among the key enablers of the digital transformation of knowledge management processes, but still with a low level of adoption in manufacturing companies. Because ontologies and underlying technologies are complex, Ontology Engineering Methodologies (OEMs) provide a set of guidelines to move from an informal to a formal representation of the company’s knowledge base. This study evaluates three agile OEMs, i.e. UPONLite, SAMOD and RapidOWL, in terms of their process and outcome features, i.e. the OEM steps and the expected quality of the ontological models produced. The assessment is performed from the viewpoint of developers of ontology-based technologies in real industrial use cases. Results show that the three agile OEMs reflect different features to effectively support the digital transformation of companies' knowledge management; thus, they cannot be interchangeable. UPONLite is more effective in contexts where there is a lack of skills in OE, with the need for a structured approach in involving domain experts and generating documentation. SAMOD requires a more extended development period, but with several cycles that allow to map different types of knowledge and enable a “try-and-learn” approach. Conversely, RapidOWL lacks a structured sequence of modelling activities and encourages developers to be creative, but at the same time requires higher expertise in OE. Thus, companies and personnel dedicated to OE should choose the methodology according to the main aims guiding their digitalisation process, the current development status, and the level of expertise.",2022,1994
"Cyber ranges and security testbeds: Scenarios, functions, tools and architecture",Muhammad Mudassar Yamin and Basel Katt and Vasileios Gkioulos,"The first line of defense against cyber threats and cyber crimes is to be aware and get ready, e.g., through cyber security training. Training can have two forms, the first is directed towards security professionals and aims at improving understanding of the latest threats and increasing skill levels in defending and mitigating against them. The second form of training, which used to attract less attention, aims at increasing cyber security awareness among non-security professionals and the general public. Conducting such training programs requires dedicated testbeds and infrastructures that help realizing and executing the training scenarios and provide a playground for the trainees. A cyber range is an environment that aims at providing such testbeds. The purpose of this paper is to study the concept of a cyber range, and provide a systematic literature review that covers unclassified cyber ranges and security testbeds. In this study we develop a taxonomy for cyber range systems and evaluate the current literature focusing on architecture and scenarios, but including also capabilities, roles, tools and evaluation criteria. The results of this study can be used as a baseline for future initiatives towards the development and evaluation of cyber ranges in accordance with existing best practices and lessons learned from contemporary research and developments.",2020,2007
"Network attacks: Taxonomy, tools and systems",N. Hoque and Monowar H. Bhuyan and R.C. Baishya and D.K. Bhattacharyya and J.K. Kalita,"To prevent and defend networks from the occurrence of attacks, it is highly essential that we have a broad knowledge of existing tools and systems available in the public domain. Based on the behavior and possible impact or severity of damages, attacks are categorized into a number of distinct classes. In this survey, we provide a taxonomy of attack tools in a consistent way for the benefit of network security researchers. This paper also presents a comprehensive and structured survey of existing tools and systems that can support both attackers and network defenders. We discuss pros and cons of such tools and systems for better understanding of their capabilities. Finally, we include a list of observations and some research challenges that may help new researchers in this field based on our hands-on experience.",2014,2011
Imbalanced data classification: A KNN and generative adversarial networks-based hybrid approach for intrusion detection,Hongwei Ding and Leiyang Chen and Liang Dong and Zhongwang Fu and Xiaohui Cui,"With the continuous emergence of various network attacks, it is becoming more and more important to ensure the security of the network. Intrusion detection, as one of the important technologies to ensure network security, has been widely studied. However, class imbalance leads to a challenging problem, that is, the normal data is much more than the attack data. Class imbalance will lead to the deviation of decision boundary, which makes higher value attack data classification error. In the face of imbalanced data, how to make the classification model classify more effectively is called imbalanced learning problem. In this study, we propose a tabular data sampling method to solve the imbalanced learning problem, which aims to balance the normal samples and attack samples. Firstly, for normal samples, on the premise of minimizing the loss of sample information, the K-nearest neighbor method is used for effective undersampling. Then, we design a tabular auxiliary classifier generative adversarial networks model (TACGAN) for attack sample oversampling. TACGAN model is an extension of ACGAN model. We add two loss functions in the generator to measure the information loss between real data and generated data, which makes TACGAN more suitable for the generation of tabular data. Finally, the normal data after undersampling and the attack data after oversampling are mixed to balance the data. We have carried out verification experiments on three real intrusion detection data sets. Experimental results show that the proposed method achieves excellent results in Accuracy, F1, AUC and Recall.",2022,2040
Using network-based text analysis to analyze trends in Microsoft's security innovations,Tabitha L. James and Lara Khansa and Deborah F. Cook and Olga Bruyaka and Kellie B. Keeling,"As the use of networked computers and digital data increase, so have the reports of data compromise and malicious cyber-attacks. Increased use and reliance on technologies complicate the process of providing information security. This expanding complexity in supplying data security requirements coupled with the increased recognition of the value of information, have led to the need to quickly advance the information security area. In this paper, we examine the maturation of the information security area by analyzing the innovation activity of one of the largest and most ubiquitous information technology companies, Microsoft. We conduct a textual analysis of their patent application activity in the information security domain since the early 2000's using a novel text analysis approach based on concepts from social network analysis and algorithmic classification. We map our analysis to focal areas in information security and examine it against Microsoft's own history, in order to determine the depth and breadth of Microsoft's innovations. Our analysis shows the relevance of using a network-based text analysis. Specifically, we find that Microsoft has increasingly emphasized topics that fall into the identity and access management area. We also show that Microsoft's innovations in information security showed tremendous growth after their Trustworthy Computing Initiative was announced. In addition, we are able to determine areas of focus that correspond to Microsoft's major vulnerabilities. These findings indicate that while Microsoft is still actively, albeit not always successfully, fighting vulnerabilities in their products, they are quite vigorously and broadly innovating in the information security area.",2013,2070
Chapter 5 - Windows Forensic Analysis,Ryan D. Pittman and Dave Shaver,"Publisher Summary
This chapter provides technical methods and techniques to help practitioners extract and interpret data of investigative value from computers running Windows operating systems. An important aspect of conducting advanced forensic analysis is understanding the mechanisms underlying fundamental operations on Windows systems such as the boot process, file creation and deletion, and use of removable storage media. By understanding how to aggregate and correlate data on Windows systems, digital investigators are better able to get the “big picture” (such as an overall theory of user action and a timeline), as well as overcoming specific technical obstacles. It is not surprising that the majority of systems that digital investigators are called upon to examine run a Windows operating system. Whether investigating child pornography, intellectual property theft, or Internet Relay Chat (IRC) bot infection, it is a safe bet that knowledge of Windows operating systems, and its associated artifacts, will aid investigators in their task. It is important for forensic examiners to understand the Windows startup process for a number of reasons beyond simply interrupting the boot process to view and document the CMOS configuration. Ever since examiners figured out that there might be more to a file than meets the eye, they have been interested in Metadata, the information that describes or places data in context, without being part of the data that is the primary focus of the user. There are two types of metadata: file system metadata and application (or file) metadata.",2010,2084
EAD-DNN: Early Alzheimer's disease prediction using deep neural networks,Preethi Thangavel and Yuvaraj Natarajan and K.R. {Sri Preethaa},"Early Alzheimer’s disease (EAD) diagnosis enables individuals to take preventative actions before irreversible brain damage occurs. Memory and thinking skills get worse in alzheimer disease, making it hard to do basic things. The abnormal buildup of amyloid and tau proteins in and around brain cells is thought to cause it. When amyloid builds up, it forms plaques around brain cells. Inside brain cells, tau tangles form when it accumulates. Healthy brain cells are damaged by the tangles and plagues, which causes them to shrink. The hippocampus, a part of the brain that aids in memory formation, appears to be the location of this damage. There are currently no methods that give the most accurate results and suggestions. With the methods we have now, alzheimer disease is not found early. So, we said that the Early Alzheimer’s disease - Deep Neural Network (EAD-DNN) method has found a way to predict alzheimer disease earlier. The Magnetic Resonance Imaging (MRI) dataset in the Comma Separated Value (CSV) format has been used by the EAD-DNN method. Convolutional Neural Network (CNN), the deep Residual Network (ResNet) has been used to train the MRI image dataset. This ResNet model can get more information from network levels with the help of Deep ResNet.The modified adam optimization has selected the best feature information from MRI scans of alzheimer patients and transferred it to another area while keeping the most important data. Using the EAD-DNN approach, a multi-class classification has been carried out. The extensive experiments show that the suggested method can achieve an accuracy rate of 98%.",2023,2094
A two-tiered framework for anomaly classification in IoT networks utilizing CNN-BiLSTM model,Yue Guan and Morteza Noferesti and Naser Ezzati-Jivan,"The paper introduces ACS-IoT, an Anomaly Classification System for IoT networks, structured as a two-tiered framework. In the first, it employs a decision tree classifier for anomaly detection. In the second, a CNN-BiLSTM model is utilized for more profound analysis and classification of anomaly types. To address data imbalance, SMOTE is used, and feature selection is enhanced with PSO. The approach showcases strong practical applicability in real-world industrial settings, achieving an accuracy of 88%, precision of 89%, recall of 88%, and F1-score of 88% for multi-class classification, surpassing other machine learning approaches by at least 6% in all metrics.",2024,2099
IoT botnet detection via power consumption modeling,Woosub Jung and Hongyang Zhao and Minglong Sun and Gang Zhou,"Many IoT botnets that exploit vulnerabilities of IoT devices have emerged recently. After taking over control of IoT devices, the botnets generate tremendous traffic to attack target nodes. It is also a threat to the smart health area since they have used IoT devices more and more. To detect the malicious IoT botnets, many researchers have proposed botnet detection systems; however, these are not easily applicable to resource-constrained IoT devices. Moreover, since the botnet's early stage makes marginal differences in terms of traffic, it is hard to detect when they first attack the victim nodes. However, we observe that the IoT botnets generate distinguishable power consumption patterns. Thus, we aim to classify whether the IoT device is affected by malign behaviors through power consumption patterns so that we can protect the healthcare ecosystem from the malicious IoT botnets. We propose a CNN-based deep learning model that consists of a data processing module as well as an 8-layer CNN. Prior to applying the CNN model, we segment and normalize the collected power consumption data to help our CNN model to achieve higher accuracy. The 8-layer CNN classifies the processed data into four classes including a botnet class, which is our primary target. To demonstrate the performance, we run self-evaluation, cross-device-evaluation, leave-one-device-out, and leave-one-botnet-out tests on three common types of IoT devices, which are Security Camera, Router, and Voice Assistant devices. The self-tests achieve up to 96.5% classification accuracy whereas the cross-evaluation tests perform about 90% accuracy. Leave-one-out tests also introduce higher than 90% accuracy for botnet detection.",2020,2100
Seeking and sharing datasets in an online community of data enthusiasts,Besiki Stvilia and Leila Gibradze,"This study examined discussions of the r/Datasets community on Reddit. It identified three activities in which the community engaged: question answering, data sharing, and community building. Members of the community used 21 types of data and information sources in their activities. The findings of this research enhance our understanding of the activity structures, data and information sources used, and challenges and problems encountered when users search for, share, and make sense of datasets on the web, outside the traditional information and data ecosystems. Data librarians and curators can use the findings of this study in the design of their data management and reference services. The typology of data sources and the metadata model developed through this study can be used in annotating and categorizing data sources and informing the design of metadata schemas and vocabularies for datasets.",2022,2103
Turning captchas against humanity: Captcha-based attacks in online social media,Mauro Conti and Luca Pajola and Pier Paolo Tricomi,"Nowadays, people generate and share massive amounts of content on online platforms (e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook users posted around 150 thousand photos every minute. Content moderators constantly monitor these online platforms to prevent the spreading of inappropriate content (e.g., hate speech, nudity images). Based on deep learning (DL) advances, Automatic Content Moderators (ACM) help human moderators handle high data volume. Despite their advantages, attackers can exploit weaknesses of DL components (e.g., preprocessing, model) to affect their performance. Therefore, an attacker can leverage such techniques to spread inappropriate content by evading ACM. In this work, we analyzed 4600 potentially toxic Instagram posts, and we discovered that 44% of them adopt obfuscations that might undermine ACM. As these posts are reminiscent of captchas (i.e., not understandable by automated mechanisms), we coin this threat as Captcha Attack (CAPA). Our contributions start by proposing a CAPA taxonomy to better understand how ACM is vulnerable to obfuscation attacks. We then focus on the broad sub-category of CAPA using textual Captcha Challenges, namely CC-CAPA, and we empirically demonstrate that it evades real-world ACM (i.e., Amazon, Google, Microsoft) with 100% accuracy. Our investigation revealed that ACM failures are caused by the OCR text extraction phase. The training of OCRs to withstand such obfuscation is therefore crucial, but huge amounts of data are required. Thus, we investigate methods to identify CC-CAPA samples from large sets of data (originated by three OSN – Pinterest, Twitter, Yahoo-Flickr), and we empirically demonstrate that supervised techniques identify target styles of samples almost perfectly. Unsupervised solutions, on the other hand, represent a solid methodology for inspecting uncommon data to detect new obfuscation techniques.",2023,2111
DODFMiner: An automated tool for Named Entity Recognition from Official Gazettes,Gabriel M.C. Guimarães and Felipe X.B. {da Silva} and Andrei L. Queiroz and Ricardo M. Marcacini and Thiago P. Faleiros and Vinicius R.P. Borges and Luís P.F. Garcia,"Official gazettes are documents published by governments to publicize their actions, spanning long periods of time and making an important transparency mechanism. These documents have information on laws, contracts, and bidding processes, as well as on civil servants and their careers in public service. Automatic information extraction of these documents may contribute to public transparency, with two tasks being especially useful: the classification of the different segments of these documents, the so called acts; and the Named Entity Recognition (NER) within the acts. The variety of official gazettes and their patterns brings up the necessity of constructing different tools for specific gazettes. In this paper, we propose DODFMiner, a command-line interface tool to classify acts and extract named entities from the Official Gazette of the Federal District. The tool follows a 3-step approach: the pre-processing of the input data; text classification using rule-based systems with regular expressions; and NER with Machine Learning algorithms. It allows users to input JSON files and receive CSV as output, providing information that allows users to track government procurements through years, contracts duration and total amount, among others. We also propose a set of experiments to support the choice of models included in the tool, covering the classification and NER steps. Text classification achieved a mean F1-score of 0.778, while to the NER, we compared 3 different architectures, CRF with a mean F1-score of 0.851, CNN-biLSTM-CRF with 0.787 and CNN-CNN-LSTM with 0.841.",2024,2119
Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers,Marco Siino and Ilenia Tinnirello and Marco {La Cascia},"With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models – including modern Transformers – get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique – in place of the worst – can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Naïve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing.",2024,2127
Issues and challenges in DNS based botnet detection: A survey,Manmeet Singh and Maninder Singh and Sanmeet Kaur,"Cybercrimes are evolving on a regular basis and as such these crimes are becoming a greater threat day by day. Earlier these threats were very general and unorganized. In the last decade, these attacks have become highly sophisticated in nature. This higher level of coordination is possible mainly due to botnets, which are clusters of infected hosts controlled remotely by an attacker (botmaster). The number of infected machines is continuously rising, thereby resulting in botnets with over a million infected machines. This powerful capability gives the botmaster a lethal weapon to launch various security attacks. As a result, botnet detection techniques received greater research focus. The Domain Name System (DNS) is a large scale distributed database on the Internet, which is being abused as a botnet communication channel. While there are numerous survey and review papers on botnet detection, there are two survey papers on DNS-based botnet detection which are neither comprehensive nor take into consideration various parameters vital for effective comparison. This survey presents a new classification for DNS-based botnet detection techniques and provides a deep analysis of each technique within the category.",2019,2133
Classification of user attitudes in Twitter -beginners guide to selected Machine Learning libraries,Marta Sokolowska and Maciej Mazurek and Marcin Majer and Michal Podpora,"This paper presents an interesting use case for learning as well as teaching basics of Machine Learning systems. Starting from a brief historical outline of the ML, the authors propose and compare a set of popular ML libraries in an interesting exemplary implementation, to present their usability. The paper also describes text classification methods, the aim of which is to distinguish positive and negative labels of particular messages within the Twitter social network. The study is summarized by a brief comparison of the quality of the classification of the libraries and methods used, as an assessment of their suitability. Final thoughts on the importance of teaching ML are included.",2019,2139
Intercepting Hail Hydra: Real-time detection of Algorithmically Generated Domains,Fran Casino and Nikolaos Lykousas and Ivan Homoliak and Constantinos Patsakis and Julio Hernandez-Castro,"A crucial technical challenge for cybercriminals is to keep control over the potentially millions of infected devices that build up their botnets, without compromising the robustness of their attacks. A single, fixed C&C server, for example, can be trivially detected either by binary or traffic analysis and immediately sink-holed or taken-down by security researchers or law enforcement. Botnets often use Domain Generation Algorithms (DGAs), primarily to evade take-down attempts. DGAs can enlarge the lifespan of a malware campaign, thus potentially enhancing its profitability. They can also contribute to hindering attack accountability. In this work, we introduce HYDRAS, the most comprehensive and representative dataset of Algorithmically-Generated Domains (AGD) available to date. The dataset contains more than 100 DGA families, including both real-world and adversarially designed ones. We analyse the dataset and discuss the possibility of differentiating between benign requests (to real domains) and malicious ones (to AGDs) in real-time. The simultaneous study of so many families and variants introduces several challenges; nonetheless, it alleviates biases found in previous literature employing small datasets which are frequently overfitted, exploiting characteristic features of particular families that do not generalise well. We thoroughly compare our approach with the current state-of-the-art and highlight some methodological shortcomings in the actual state of practice. The outcomes obtained show that our proposed approach significantly outperforms the current state-of-the-art in terms of both classification performance and efficiency.",2021,2141
Reinforcement learning based web crawler detection for diversity and dynamics,Yang Gao and Zunlei Feng and Xiaoyang Wang and Mingli Song and Xingen Wang and Xinyu Wang and Chun Chen,"Crawler detection is always an important research topic in network security. With the development of web technology, crawlers are constantly updating and changing, and their types are becoming diverse. The diversity and dynamics of crawlers pose significant challenges for feature applicability and model robustness. Existing crawler detection methods can only detect a limited number of crawlers by predefined rules and can not cover all types of crawlers; worse, they can be completely invalidated by the emergence of new types of crawlers. In this paper, we propose a reinforcement learning based web crawler detection method for diversity and dynamics (WC3D), which is composed of a feature selector and a session classifier. The feature selector selects the appropriate feature set for different types of crawlers with deep deterministic policy gradient. The session classifier makes crawler detection and provides rewards to the feature selector. The two modules are trained jointly to optimize the feature selection and session classification processes. Extensive experiments demonstrate the existence of crawler diversity and that the proposed method is still highly robust against the new type of crawlers and achieves state-of-the-art performance even without considering the dynamics of the crawlers.",2023,2143
Discovering emerging business ideas based on crowdfunded software projects,Won Sang Lee and So Young Sohn,"User-centered innovation has attracted considerable interest for exploiting emerging business ideas. We suggest a novel framework for discovering emerging business ideas and their combination with user-centered innovation in the software industry based on design thinking processes. We apply topic modeling to projects on Kickstarter which is one of the largest crowdfunding platforms in the world. We adopt conjoint analysis to find which topics are most preferred upon the platform in terms of the amount of funding that they have received. From our findings, the convergence of smart assistant services with various domains, such as tutoring mathematics and seeking job opportunities, is recommended as an emerging idea for software businesses. We also find that the ideas preferred in the US are different from those preferred in other countries. Our findings can be exploited effectively for decision support in establishing a new business model. Finally, this study contributes to discovering emerging business ideas by connecting user-centered innovation with a design thinking perspective.",2019,2145
"Adaptive gamification in Collaborative systems, a systematic mapping study",María {Dalponte Ayastuy} and Diego Torres and Alejandro Fernández,"Mass collaboration mediated by technology is now commonplace (Wikipedia, Quora, TripAdvisor). Online, mass collaboration is also present in science in the form of Citizen Science. These collaboration models, which have a large community of contributors coordinated to pursue a common goal, are known as Collaborative systems. This article introduces a study of the published research on the application of adaptive gamification to collaborative systems. The study focuses on works that explicitly discuss an approach of personalization or adaptation of the gamification elements in this type of system. It employs a systematic mapping design in which a categorical structure for classifying the research results is proposed based on the topics that emerged from the papers review. The main contributions of this paper are a formalization of the adaptation strategies and the proposal of a new taxonomy for gamification elements adaptation. The results evidence the lack of research literature in the study of adapting gamification in the field of collaborative systems. Considering the underlying cultural diversity in those projects, the adaptability of gamification design and strategies is a promissory research field.",2021,2152
"A survey on Blockchain solutions in DDoS attacks mitigation: Techniques, open challenges and future directions",Rajasekhar Chaganti and Bharat Bhushan and Vinayakumar Ravi,"With the proliferation of new technologies such as the Internet of Things (IoT) and Software-Defined Networking (SDN) in recent years, the Distributed Denial of Service (DDoS) attack vector has broadened and opened new opportunities for more sophisticated DDoS attacks on the targeted victims. The new attack vector includes unsecured and vulnerable IoT devices connected to the internet, and denial of service vulnerabilities like southbound channel saturation in the SDN architecture. Given the high-volume and pervasive nature of these attacks, it is beneficial for stakeholders to collaborate in detecting and mitigating the denial of service attacks promptly. Blockchain technology is considered to improve the security aspects owing to the decentralized design, secured distributed storage, and privacy. A thorough exploration and classification of blockchain techniques used for DDoS attack mitigation are not explored in the prior art. This paper reviews and categorizes state-of-the-art DDoS mitigation solutions based on blockchain technology. The DDoS mitigation techniques are classified based on the solution deployment location i.e. network-based, near attacker location, near victim location, and hybrid solutions in the network architecture with emphasis on the IoT and SDN architectures. Additionally, based on our study, the research challenges and future directions to implement the blockchain based DDoS mitigation solutions are discussed.",2023,2166
Beware of the hierarchy — An analysis of ontology evolution and the materialisation impact for biomedical ontologies,Romana Pernisch and Daniele Dell’Aglio and Abraham Bernstein,"Ontologies are becoming a key component of numerous applications and research fields. But knowledge captured within ontologies is not static. Some ontology updates potentially have a wide ranging impact; others only affect very localised parts of the ontology and their applications. Investigating the impact of the evolution gives us insight into the editing behaviour but also signals ontology engineers and users how the ontology evolution is affecting other applications. However, such research is in its infancy. Hence, we need to investigate the evolution itself and its impact on the simplest of applications: the materialisation. In this work, we define impact measures that capture the effect of changes on the materialisation. In the future, the impact measures introduced in this work can be used to investigate how aware the ontology editors are about consequences of changes. By introducing five different measures, which focus either on the change in the materialisation with respect to the size or on the number of changes applied, we are able to quantify the consequences of ontology changes. To see these measures in action, we investigate the evolution and its impact on materialisation for nine open biomedical ontologies, most of which adhere to the EL++ description logic. Our results show that these ontologies evolve at varying paces but no statistically significant difference between the ontologies with respect to their evolution could be identified. We identify three types of ontologies based on the types of complex changes which are applied to them throughout their evolution. The impact on the materialisation is the same for the investigated ontologies, bringing us to the conclusion that the effect of changes on the materialisation can be generalised to other similar ontologies. Further, we found that the materialised concept inclusion axioms experience most of the impact induced by changes to the class inheritance of the ontology and other changes only marginally touch the materialisation.",2021,2177
Validation of a flow cytometric in vitro DNA repair (UDS) assay in rat hepatocytes,Jules R. Selden and Frank Dolbeare and James H. Clair and Judith E. Miller and Katherine McGettigan and John A. DiJohn and Gary R. Dysart and John G. DeLuca,"An in vitro flow cytometric (FCM) DNA repair assay has been developed and validated by comparison to conventional autoradiography (ARG). Both assays measure unscheduled DNA synthesis (UDS). Cultures of hepatocytes from young male Sprague-Dawley rats were exposed to a battery of 26 chemicals plus bromodeoxyuridine (BrdUrd) or 3H-thymidine (3H-dT) for 18–20 h before harvest. Selection of test chemicals was based upon both their genotoxicity classifications and carcinogenicity bioassay results in male rats. DNA repair in chemically treated cultures was detected flow cytometrically by measuring the uptake of BrdUrd in non-replicating (G1, G2, mitotic and 4C) cells. Intracellular levels of incorporated BrdUrd were visualized by immunochemical labeling with fluorescein isothiocyanate (FITC), and total cellular DNA content was simultaneously estimated by counterstaining samples with the nucleic acid intercalator, propidium iodide (PI). Information was obtained from 104 cells/sample. Since repairing cells incorporate significantly less BrdUrd per unit of time than replicating cells, low intensity BrdUrd-FITC fluorescent signals from repairing cells are readily discriminated from high intensity signals from replicating cells when displayed on linear univariate histograms. Further distinction between repairing and replicating cells was achieved by displaying the DNA contents of all cells on linear bivariate histograms. Thus, repairing cells were resolved without subjecting these cultures to agents which suppress replicative synthesis (e.g, hydroxyurea). Results from these concurrent FCM and ARG investigations include the following: (1) conclusions (DNA repair positive or negative) were in agreement, with one exception, cinnamyl anthranilate, for which cytotoxic doses produced a positive FCM response, but lack of intact hepatocytes in parallel ARG preparations prevented analysis; (2) similar sensitivities for most of the positive chemicals were reported; (3) a high correlation (85%) exists between the reported genotoxicity classification and these DNA repair results in the absence of overt cytotoxicity; (4) a poor correlation exists between these DNA repair results and hepatocarcinogenesis (only 4/11 liver carcinogens tested positive) or overall carcinogenesis in the male rat (only 9/21 carcinogens tested positive). This FCM assay provides a rapid, sensitive, safe and reliable means of identifying agents which induce DNA repair in mammalian cells.",1994,2204
The cyanobacteriales: A legitimate order based on the type strain Cyanobacterium stanieri?,R. Rippka and G. Cohen-Bazire,"Summary
As a logical consequence of the definition of a bacterium (Stanier and van Niel, 1962), R. Y. Stanier created the name «cyanobacteria as a replacement for «blue-green algae. As such, cyanobacteria entered the 8th issue of Bergey's Manual of Determinative Bacteriology 1974 as members of the Procaryotae Murray 1968, this kingdom being composed of two divisions, Cyanobacteria and Bacteria. An even tighter integration of cyanobacteria with other bacteria was proposed by Gibbons and Murray (1978) for the next edition of Bergey's Manual. These authors suggested that the cyanobacteria be integrated as an order Cyanobacteriales in the class Photobacteria. However, this proposal was doomed to failure by constrants imposed under present rules of the Bacteriological Code (Lapage et al., 1976), one of which is that the type of an order is the genus upon whose name the higher taxon is based. A genus Cyanobacterium did not exist when Gibbons and Murray made their proposal, and a subsequent special request by the same authors for an exemption from this rule was not granted (Judicial Commission of the International Committee on Systematic Bacteriology, Holt, 1978). We present here a revised classification for unicellular cyanobacteria dividing in one plane wherein we propose, among other changes, the creation of two new genera, Cyanobium and Cyanobaceterium. With the creation of the latter genus, the requirement for recognition of cyanobacteria as a legal order Cyanobacteriales under the Bacteriological Code should be fulfilled. We suggest that the type species of this genus be Cyanobacterium stanieri, in honor of the late Roger Y. Stanier.
Résumé
La définition d'une «bactérie proposée par Stanier et van Niel en 1962 [48] a eu pour conséquence logique la création, par R. Y. Stanier, du nom «cyanobactéries pour remplacer celui d' «algues bleues. C'est sous cette dénomination que ces organismes entrèrent dans la troisième édition du manuel de détermination bactériologique de Bergey en 1974 [4]. Dans ce manuel, les cyanobactéries étaient traitées comme membres des Procaryotae Murray 1968, cet embranchement étant constitué par deux groupes: les Cyanobactéries et les Bactéries. Un rapproachement encore plus étroit des cyanobactéries des autres bactéries a été proposé par Gibbons et Murray [16] pour la prochaine édition du manuel de Bergey. Ces auteurs ont proposé que les cyanobactéries soient intégrées dans la classe des Photobactéries, en tant qu'ordre des Cyanobacteriales. Cette proposition fut refusée comme ne satisfaisant pas les règles du Code Bactériologigue [28], l'une de ces règles étant que le type d'un ordre est le genre sous le nom duquel le taxon d'ordre plus élevé est basé. Le genre Cyanobacterium n'existait pas lorsque Gibbons et Murray ont fait leur proposition, et une demande ultérieure d'exception à cette règle [17] fut également refusée. Nous proposons ici une nouvelle classification des cyanobactéries unicellulaires qui se divisent sur un seul plan. Entre autres changements, nous proposons la création de deux nouveaux genres: Cyanobium et Cyanobacterium. La création de ce dernier genre devrait lever l'objection qui empêchait de reconnaître les Cyanobacteriales comme ordre légitime suivant les règles du Code Bactériologique. Nous proposons que l'espèce type de ce genre soit appelée Cyanobacterium stanieri pour honorer la mémoire de Roger Y. Stanier.",1983,2227
"Computational intelligence intrusion detection techniques in mobile cloud computing environments: Review, taxonomy, and open research issues",Shahab Shamshirband and Mahdis Fathi and Anthony T. Chronopoulos and Antonio Montieri and Fabio Palumbo and Antonio Pescapè,"With the increasing utilization of the Internet and its provided services, an increase in cyber-attacks to exploit the information occurs. A technology to store and maintain user's information that is mostly used for its simplicity and low-cost services is cloud computing (CC). Also, a new model of computing that is noteworthy today is mobile cloud computing (MCC) that is used to reduce the limitations of mobile devices by allowing them to offload certain computations to the remote cloud. The cloud environment may consist of critical or essential information of an organization; therefore, to prevent this environment from possible attacks a security solution is needed. An intrusion detection system (IDS) is a solution to these security issues. An IDS is a hardware or software device that can examine all inside and outside network activities and recognize doubtful patterns that may demonstrate a network attack and automatically alert the network (or system) administrator. Because of the ability of an IDS to detect known/unknown (inside/outside) attacks, it is an excellent choice for securing cloud computing. Various methods are used in an intrusion detection system to recognize attacks more accurately. Unlike survey papers presented so far, this paper aims to present a comprehensive survey of intrusion detection systems that use computational intelligence (CI) methods in a (mobile) cloud environment. We firstly provide an overview of CC and MCC paradigms and service models, also reviewing security threats in these contexts. Previous literature is critically surveyed, highlighting the advantages and limitations of previous work. Then we define a taxonomy for IDS and classify CI-based techniques into single and hybrid methods. Finally, we highlight open issues and future directions for research on this topic.",2020,2254
Taxonomy induction based on a collaboratively built knowledge repository,Simone Paolo Ponzetto and Michael Strube,"The category system in Wikipedia can be taken as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexico-syntactic matching. The result is a large scale taxonomy. For evaluation we propose a method which (1) manually determines the quality of our taxonomy, and (2) automatically compares its coverage with ResearchCyc, one of the largest manually created ontologies, and the lexical database WordNet. Additionally, we perform an extrinsic evaluation by computing semantic similarity between words in benchmarking datasets. The results show that the taxonomy compares favorably in quality and coverage with broad-coverage manually created resources.",2011,2264
The Use of Large Language Model in Code Review Automation: An Examination of Enforcing SOLID Principles,"Martins, Gustavo F.
and Firmino, Emiliandro C. M.
and De Mello, Vinicius P.","Within the ever-evolving domain of software development, the practice of having teams located in different geographical locations presents distinct obstacles, including disparate time zones, language hurdles, and differing degrees of experience. This paper presents a novel approach to address these difficulties by using an automated GitHub bot that utilizes Large Language Models (LLMs) to enforce SOLID principles during code reviews. This bot, which incorporates advanced models such as OpenAI's GPT-4 and the locally deployable Mixtral, has the objective of delivering immediate and practical feedback. Its purpose is to improve the quality of code and make learning easier for developers, particularly those who are new to programming. The bot's structure enables effortless incorporation into GitHub, utilizing LLMs to examine code modifications and offer observations regarding adherence to SOLID principles. An important characteristic of this method is the incorporation of Mixtral, which may be operated on-site, providing advantages in terms of data confidentiality and operational adaptability, essential for global enterprises with strict privacy demands. Here, we explores the bot's architecture, its incorporation with LLMs, and its capacity to revolutionize code reviews by offering a secure, efficient, and instructive instrument for geographically dispersed software development teams.",2024,2296
CodeScan: A Supervised Machine Learning Approach to Open Source Code Bot Detection,"Gaurav, Vipul
and Singh, Shresth
and Srivastava, Avikant
and Shidnal, Sushila",Enhancing software productivity would help companies to cut their costs and increase profits. Software metrics rely heavily on the personal experiences and skills of managers in pattern recognition and rewards. Differentiating between actual human effort and machine-generated code can help drive an organization's decision-making process that is rewarding its employees and provide an assistive tool to the managers allowing effective monitoring without micromanagement that has a wide application in managing work from home and other virtual environments. The paper explores the insight into the quality of machine-generated bot code compared to actual human coding efforts. It uses machine learning techniques to identify patterns and gives intelligent insights that can be used as a performance metric for versioning systems and business intelligence. We successfully distinguished between a bot and human-written code with an F1-score of 0.945 using the Light Gradient Boosting Method.,2022,2297
Inside the Tool Set of Automation: Free Social Bot Code Revisited,"Assenmacher, Dennis
and Adam, Lena
and Frischlich, Lena
and Trautmann, Heike
and Grimme, Christian","Social bots have recently gained attention in the context of public opinion manipulation on social media platforms. While a lot of research effort has been put into the classification and detection of such automated programs, it is still unclear how technically sophisticated those bots are, which platforms they target, and where they originate from. To answer these questions, we gathered repository data from open source collaboration platforms to identify the status-quo of social bot development as well as first insights into the overall skills of publicly available bot code.",2020,2298
On Twitter Bots Behaving Badly: Empirical Study of Code Patterns on GitHub,"Millimaggi, Andrea
and Daniel, Florian","Bots, i.e., algorithmically driven entities that behave like humans in online communications, are increasingly infiltrating social conversations on the Web. If not properly prevented, this presence of bots may cause harm to the humans they interact with. This paper aims to understand which types of abuse may lead to harm and whether these can be considered intentional or not. We manually review a dataset of 60 Twitter bot code repositories on GitHub, derive a set of potentially abusive actions, characterize them using a taxonomy of abstract code patterns, and assess the potential abusiveness of the patterns. The study does not only reveal the existence of 31 communication-specific code patterns -- which could be used to assess the harmfulness of bot code -- but also their presence throughout all studied repositories.",2019,2301
Designing Philobot: A Chatbot for Mental Health Support with CBT Techniques,"Ge, Qi
and Liu, Lu
and Zhang, Hewei
and Li, Linfang
and Li, Xiaonan
and Zhu, Xinyi
and Liao, Lejian
and Song, Dandan","Mental health issues are a major concern for teenagers, and access to affordable and accessible support is critical for promoting positive mental health outcomes. Philobot is a chatbot designed to provide personalized mental health support for teenagers with cognitive behavioral therapy (CBT) practices. It also offers a chatting moodule that facilitates building a rapport between the virtual assistant and its users. This paper presents the design, implementation, and validation of Philobot's key features, including its fine-grained intent classification system, FAQ retrieval model, and response generation model using top-k sampling. Evaluation results show that Philobot is a promising tool for promoting positive mental health outcomes in teenagers.",2023,2302
The GitHub Development Workflow Automation Ecosystems,"Wessel, Mairieli
and Mens, Tom
and Decan, Alexandre
and Mazrae, Pooya Rostami","Large-scale software development has become a highly collaborative and geographically distributed endeavor, especially in open-source software development ecosystems and their associated developer communities. It has given rise to modern development processes (e.g., pull-based development) that involve a wide range of activities such as issue and bug handling, code reviewing, coding, testing, and deployment. These often very effort-intensive activities are supported by a wide variety of tools such as version control systems, bug and issue trackers, code reviewing systems, code quality analysis tools, test automation, dependency management, and vulnerability detection tools. To reduce the complexity of the collaborative development process, many of the repetitive human activities that are part of the development workflow are being automated by CI/CD tools that help to increase the productivity and quality of software projects. Social coding platforms aim to integrate all this tooling and workflow automation in a single encompassing environment. These social coding platforms gave rise to the emergence of development bots, facilitating the integration with external CI/CD tools and enabling the automation of many other development-related tasks. GitHub, the most popular social coding platform, has introduced GitHub Actions to automate workflows in its hosted software development repositories since November 2019. This chapter explores the ecosystems of development bots and GitHub Actions and their interconnection. It provides an extensive survey of the state of the art in this domain, discusses the opportunities and threats that these ecosystems entail, and reports on the challenges and future perspectives for researchers as well as software practitioners.",2023,2305
