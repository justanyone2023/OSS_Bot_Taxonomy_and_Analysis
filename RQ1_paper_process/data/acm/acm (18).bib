@inproceedings{10.1145/3528228.3528409,
author = {Farah, Juan Carlos and Spaenlehauer, Basile and Lu, Xinyang and Ingram, Sandy and Gillet, Denis},
title = {An exploratory study of reactions to bot comments on GitHub},
year = {2022},
isbn = {9781450393331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528228.3528409},
doi = {10.1145/3528228.3528409},
abstract = {The widespread use of bots to support software development makes social coding platforms such as GitHub a particularly rich source of data for the study of human-bot interaction. Software development bots are used to automate repetitive tasks, interacting with their human counterparts via comments posted on the various discussion interfaces available on such platforms. One type of interaction supported by GitHub involves reacting to comments using predefined emoji. To investigate how users react to bot comments, we conducted an observational study comprising 54 million GitHub comments, with a particular focus on comments that elicited the laugh reaction. The results from our analysis suggest that some reaction types are not equally distributed across human and bot comments and that a bot's design and purpose influence the types of reactions it receives. Furthermore, while the laugh reaction is not exclusively used to express laughter, it can be used to convey humor when a bot behaves unexpectedly. These insights could inform the way bots are designed and help developers equip them with the ability to recognize and recover from unanticipated situations. In turn, bots could better support the communication, collaboration, and productivity of teams using social coding platforms.},
booktitle = {Proceedings of the Fourth International Workshop on Bots in Software Engineering},
pages = {18–22},
numpages = {5},
keywords = {GitHub, bots, emoji, humor, laugh, reactions, social coding platforms},
location = {Pittsburgh, Pennsylvania},
series = {BotSE '22}
}

@inproceedings{10.1145/3524842.3528520,
author = {Chidambaram, Natarajan and Mazrae, Pooya Rostami},
title = {Bot detection in GitHub repositories},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528520},
doi = {10.1145/3524842.3528520},
abstract = {Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the GrimoireLab software development analytics platform. In this work, we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using Kibana.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {726–728},
numpages = {3},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3528228.3528408,
author = {Mohayeji, Hamid and Ebert, Felipe and Arts, Eric and Constantinou, Eleni and Serebrenik, Alexander},
title = {On the adoption of a TODO bot on GitHub: a preliminary study},
year = {2022},
isbn = {9781450393331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528228.3528408},
doi = {10.1145/3528228.3528408},
abstract = {Bots support different software maintenance and evolution activities, such as code review or executing tests. Recently, several bots have been proposed to help developers to keep track of postponed activities, expressed by means of TODO comments: e.g., TODO Bot automatically creates a GitHub issue when a TODO comment is added to a repository, increasing visibility of TODO comments. In this work, we perform a preliminary evaluation of the impact of the TODO Bot on software development practice. We conjecture that the introduction of the TODO Bot would facilitate keeping track of the TODO comments, and hence encourage developers to use more TODO comments in their code changes.To evaluate this conjecture, we analyze all the 2,208 repositories which have at least one GitHub issue created by the TODO Bot. Firstly, we investigate to what extent the bot is being used and describe the repositories using the bot. We observe that the majority (54%) of the repositories which adopted the TODO Bot are new, i.e., were created within less than one month of first issue created by the bot, and from those, more than 60% have the issue created within three days. We observe a statistically significant increase in the number of the TODO comments after the adoption of the bot, however with a small effect size. Our results suggest that the adoption of the TODO Bot encourages developers to introduce TODO comments rendering the postponed decisions more visible. Nevertheless, it does not speed up the process of addressing TODO comments or corresponding GitHub issues.},
booktitle = {Proceedings of the Fourth International Workshop on Bots in Software Engineering},
pages = {23–27},
numpages = {5},
keywords = {TODO, bots, code comments, technical debt},
location = {Pittsburgh, Pennsylvania},
series = {BotSE '22}
}

@inproceedings{10.1145/3643991.3644877,
author = {Chidambaram, Natarajan and Mens, Tom and Decan, Alexandre},
title = {RABBIT: A tool for identifying bot accounts based on their recent GitHub event history},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644877},
doi = {10.1145/3643991.3644877},
abstract = {Collaborative software development through GitHub repositories frequently relies on bot accounts to automate repetitive and error-prone tasks. This highlights the need to have accurate and efficient bot identification tools. Several such tools have been proposed in the past, but they tend to rely on a substantial amount of historical data, or they limit themselves to a reduced subset of activity types, making them difficult to use at large scale. To overcome these limitations, we developed RABBIT, an open source command-line tool that queries the GitHub Events API to retrieve the recent events of a given GitHub account and predicts whether the account is a human or a bot. RABBIT is based on an XGBoost classification model that relies on six features related to account activities and achieves high performance, with an AUC, F1 score, precision and recall of 0.92. Compared to the state-of-the-art in bot identification, RABBIT exhibits a similar performance in terms of precision, recall and F1 score, while being more than an order of magnitude faster and requiring considerably less data. This makes RABBIT usable on a large scale, capable of processing several thousand accounts per hour efficiently.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {687–691},
numpages = {5},
keywords = {GitHub events, classification model, bot identification},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3540250.3558922,
author = {He, Hao and Su, Haonan and Xiao, Wenxin and He, Runzhi and Zhou, Minghui},
title = {GFI-bot: automated good first issue recommendation on GitHub},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558922},
doi = {10.1145/3540250.3558922},
abstract = {To facilitate newcomer onboarding, GitHub recommends the use of "good first issue" (GFI) labels to signal issues suitable for newcomers to resolve. However, previous research shows that manually labeled GFIs are scarce and inappropriate, showing a need for automated recommendations. In this paper, we present GFI-Bot (accessible at https://gfibot.io), a proof-of-concept machine learning powered bot for automated GFI recommendation in practice. Project maintainers can configure GFI-Bot to discover and label possible GFIs so that newcomers can easily locate issues for making their first contributions. GFI-Bot also provides a high-quality, up-to-date dataset for advancing GFI recommendation research.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1751–1755},
numpages = {5},
keywords = {good first issue, onboarding, open-source software, software bot},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3387940.3391503,
author = {Golzadeh, Mehdi and Legay, Damien and Decan, Alexandre and Mens, Tom},
title = {Bot or not? Detecting bots in GitHub pull request activity based on comment similarity},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391503},
doi = {10.1145/3387940.3391503},
abstract = {Many empirical studies focus on socio-technical activity in social coding platforms such as GitHub, for example to study the onboarding, abandonment, productivity and collaboration among team members. Such studies face the difficulty that GitHub activity can also be generated automatically by bots of a different nature. It therefore becomes imperative to distinguish such bots from human users. We propose an automated approach to detect bots in GitHub pull request (PR) activity. Relying on the assumption that bots contain repetitive message patterns in their PR comments, we analyse the similarity between multiple messages from the same GitHub identity, using a clustering method that combines the Jaccard and Levenshtein distance. We empirically evaluate our approach by analysing 20,090 PR comments of 250 users and 42 bots in 1,262 GitHub repositories. Our results show that the method is able to clearly separate bots from human users.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {31–35},
numpages = {5},
keywords = {GitHub, bot activity, empirical analysis, pull requests, social coding, software repository mining, text similarity},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3202667.3202694,
author = {Peng, Zhenhui and Yoo, Jeehoon and Xia, Meng and Kim, Sunghun and Ma, Xiaojuan},
title = {Exploring How Software Developers Work with Mention Bot in GitHub},
year = {2018},
isbn = {9781450365086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3202667.3202694},
doi = {10.1145/3202667.3202694},
abstract = {Recently, major software development platforms have started to provide automatic reviewer recommendation (ARR) services for pull requests, to improve the collaborative coding review process. However, the user experience of ARR is under-investigated. In this paper, we use a two-stage mixed-methods approach to study how software developers perceive and work with the Facebook mention bot, one of the most popular ARR bots in GitHub. Specifically, in Stage I, we conduct archival analysis on projects employing mention bot and a user survey to investigate the bot's performance. A year later, in Stage II, we revisit these projects and conduct additional surveys and interviews with three user groups: project owners, contributors and reviewers. Results show that developers appreciate mention bot saving their effort, but are bothered by its unstable setting and unbalanced workload allocation. We conclude with design considerations for improving ARR services.},
booktitle = {Proceedings of the Sixth International Symposium of Chinese CHI},
pages = {152–155},
numpages = {4},
keywords = {Automatic reviewer recommendation services, mixed-methods, software development platform, user experience},
location = {Montreal, QC, Canada},
series = {ChineseCHI '18}
}

@inproceedings{10.1145/3528228.3528406,
author = {Golzadeh, Mehdi and Decan, Alexandre and Chidambaram, Natarajan},
title = {On the accuracy of bot detection techniques},
year = {2022},
isbn = {9781450393331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528228.3528406},
doi = {10.1145/3528228.3528406},
abstract = {Development bots are often used to automate a wide variety of repetitive tasks in collaborative software development. Such bots are commonly among the most active project contributors in terms of commit activity. As such, tools that analyse contributor activity (e.g., for recognizing and giving credit to project members for their contributions) need to take into account the bots and exclude their activity. While there are a few techniques to detect bots in software repositories, these techniques are not perfect and may miss some bots or may wrongly identify some human accounts as bots. In this paper, we present an exploratory study on the accuracy of bot detection techniques on a set of 540 accounts from 27 GitHub projects. We show that none of the bot detection techniques are accurate enough to detect bots among the 20 most active contributors of each project. We show that combining these techniques drastically increases the accuracy and recall of bot detection. We also highlight the importance of considering bots when attributing contributions to humans, since bots are prevalent among the top contributors and responsible for large proportions of commits.},
booktitle = {Proceedings of the Fourth International Workshop on Bots in Software Engineering},
pages = {1–5},
numpages = {5},
keywords = {GitHub, bot detection techniques, contributor attribution, empirical analysis, social coding platforms, software repository mining},
location = {Pittsburgh, Pennsylvania},
series = {BotSE '22}
}

@inproceedings{10.1109/ICSE48619.2023.00123,
author = {Ghorbani, Amir and Cassee, Nathan and Robinson, Derek and Alami, Adam and Ernst, Neil A. and Serebrenik, Alexander and W\k{a}sowski, Andrzej},
title = {Autonomy Is an Acquired Taste: Exploring Developer Preferences for GitHub Bots},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00123},
doi = {10.1109/ICSE48619.2023.00123},
abstract = {Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1405–1417},
numpages = {13},
keywords = {software bot, pull request, human aspects},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3287324.3293787,
author = {Hu, Zhewei and Gehringer, Edward},
title = {Use Bots to Improve GitHub Pull-Request Feedback},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293787},
doi = {10.1145/3287324.3293787},
abstract = {Rising enrollments make it difficult for instructors and teaching assistants to give adequate feedback on each student's work. In our software engineering course, we have 50-120 students each semester. Our course projects require students to submit GitHub pull requests as deliverables for their open-source software (OSS) projects. We have set up a static code analyzer and a continuous integration service on GitHub to help students check code style and functionality. However, these tools cannot enforce system-specific customized guidelines and do not explicitly display detailed information. In this study, we discuss how we bypass the limitations of existing tools by implementing three Internet bots. The Expertiza Bot can help detect violations of more than 35 system-specific guidelines. The Travis CI Bot can explicitly display instant test execution results on the GitHub pull-request page. The Code Climate Bot can insert pull-request comments to remind students to fix issues detected by the static code analyzer. These bots are either open source or free for OSS projects, and can be easily integrated with GitHub repositories. Our survey results show that more than 70% of students think the advice given by the bots is useful. We tallied the amount of feedback given by the bots and the teaching staff for each GitHub pull request. Results show that bots can provide significantly more feedback (six times more on average) than teaching staff. Bots can also offer more timely feedback than teaching staff and help student contributions avoid more than 33% system-specific guideline violations.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1262–1263},
numpages = {2},
keywords = {expertiza, internet bots, open-source curriculum, open-source software, software engineering},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/3084226.3084287,
author = {Sharma, Abhishek and Thung, Ferdian and Kochhar, Pavneet Singh and Sulistya, Agus and Lo, David},
title = {Cataloging GitHub Repositories},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084287},
doi = {10.1145/3084226.3084287},
abstract = {GitHub is one of the largest and most popular repository hosting service today, having about 14 million users and more than 54 million repositories as of March 2017. This makes it an excellent platform to find projects that developers are interested in exploring. GitHub showcases its most popular projects by cataloging them manually into categories such as DevOps tools, web application frameworks, and game engines. We propose that such cataloging should not be limited only to popular projects. We explore the possibility of developing such cataloging system by automatically extracting functionality descriptive text segments from readme files of GitHub repositories. These descriptions are then input to LDA-GA, a state-of-the-art topic modeling algorithm, to identify categories. Our preliminary experiments demonstrate that additional meaningful categories which complement existing GitHub categories can be inferred. Moreover, for inferred categories that match GitHub categories, our approach can identify additional projects belonging to them. Our experimental results establish a promising direction in realizing automatic cataloging system for GitHub.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {314–319},
numpages = {6},
keywords = {Genetic Algorithm, GitHub, Latent Dirichlet Allocation},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.1109/BotSE.2019.00015,
author = {Wyrich, Marvin and Bogner, Justus},
title = {Towards an autonomous bot for automatic source code refactoring},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00015},
doi = {10.1109/BotSE.2019.00015},
abstract = {Continuous refactoring is necessary to maintain source code quality and to cope with technical debt. Since manual refactoring is inefficient and error-prone, various solutions for automated refactoring have been proposed in the past. However, empirical studies have shown that these solutions are not widely accepted by software developers and most refactorings are still performed manually. For example, developers reported that refactoring tools should support functionality for reviewing changes. They also criticized that introducing such tools would require substantial effort for configuration and integration into the current development environment.In this paper, we present our work towards the Refactoring-Bot, an autonomous bot that integrates into the team like a human developer via the existing version control platform. The bot automatically performs refactorings to resolve code smells and presents the changes to a developer for asynchronous review via pull requests. This way, developers are not interrupted in their workflow and can review the changes at any time with familiar tools. Proposed refactorings can then be integrated into the code base via the push of a button. We elaborate on our vision, discuss design decisions, describe the current state of development, and give an outlook on planned development and research activities.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {24–28},
numpages = {5},
keywords = {bot, code smells, maintainability, refactoring, software evolution, software quality improvement},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3422392.3422459,
author = {Wessel, Mairieli and Serebrenik, Alexander and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco A.},
title = {What to Expect from Code Review Bots on GitHub? A Survey with OSS Maintainers},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422459},
doi = {10.1145/3422392.3422459},
abstract = {Software bots are used by Open Source Software (OSS) projects to streamline the code review process. Interfacing between developers and automated services, code review bots report continuous integration failures, code quality checks, and code coverage. However, the impact of such bots on maintenance tasks is still neglected. In this paper, we study how project maintainers experience code review bots. We surveyed 127 maintainers and asked about their expectations and perception of changes incurred by code review bots. Our findings reveal that the most frequent expectations include enhancing the feedback bots provide to developers, reducing the maintenance burden for developers, and enforcing code coverage. While maintainers report that bots satisfied their expectations, they also perceived unexpected effects, such as communication noise and newcomers' dropout. Based on these results, we provide a series of implications for bot developers, as well as insights for future research.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {457–462},
numpages = {6},
keywords = {code review, open source software, pull-based model, software bots},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2597073.2597074,
author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
title = {The promises and perils of mining GitHub},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597074},
doi = {10.1145/2597073.2597074},
abstract = {With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {92–101},
numpages = {10},
keywords = {Mining software repositories, bias, code reviews, git, github},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1109/BotSE.2019.00019,
author = {van Tonder, Rijnard and Goues, Claire Le},
title = {Towards s/engineer/bot: principles for program repair bots},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00019},
doi = {10.1109/BotSE.2019.00019},
abstract = {Of the hundreds of billions of dollars spent on developer wages, up to 25% accounts for fixing bugs. Companies like Google save significant human effort and engineering costs with automatic bug detection tools, yet automatically fixing them is still a nascent endeavour. Very recent work (including our own) demonstrates the feasibility of automatic program repair in practice. As automated repair technology matures, it presents great appeal for integration into developer workflows. We believe software bots are a promising vehicle for realizing this integration, as they bridge the gap between human software development and automated processes. We envision repair bots orchestrating automated refactoring and bug fixing. To this end, we explore what building a repair bot entails. We draw on our understanding of patch generation, validation, and real world software development interactions to identify six principles that bear on engineering repair bots and discuss related design challenges for integrating human workflows. Ultimately, this work aims to foster critical focus and interest for making repair bots a reality.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {43–47},
numpages = {5},
keywords = {automatic program repair, automation, bots, program transformation, refactoring, software quality},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3387940.3391505,
author = {Romero, Ricardo and Parra, Esteban and Haiduc, Sonia},
title = {Experiences Building an Answer Bot for Gitter},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391505},
doi = {10.1145/3387940.3391505},
abstract = {Software developers use modern chat platforms to communicate about the status of a project and to coordinate development and release efforts, among other things. Developers also use chat platforms to ask technical questions to other developers. While some questions are project-specific and require an experienced developer familiar with the system to answer, many questions are rather general and may have been already answered by other developers on platforms such as the Q&amp;A site StackOverflow.In this paper, we present GitterAns, a bot that can automatically detect when a developer asks a technical question in a chat and leverages the information present in Q&amp;A forums to provide the developer with possible answers to their question. The results of a preliminary study indicate promising results, with GitterAns achieving an accuracy of 0.78 in identifying technical questions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {66–70},
numpages = {5},
keywords = {Q&amp;A, bot, chat, communication, recommendation, social media, team communication platforms},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3427228.3427258,
author = {Du, Kun and Yang, Hao and Zhang, Yubao and Duan, Haixin and Wang, Haining and Hao, Shuang and Li, Zhou and Yang, Min},
title = {Understanding Promotion-as-a-Service on GitHub},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427258},
doi = {10.1145/3427228.3427258},
abstract = {As the world’s leading software development platform, GitHub has become a social networking site for programmers and recruiters who leverage its social features, such as star and fork, for career and business development. However, in this paper, we found a group of GitHub accounts that conducted promotion services in GitHub, called “promoters”, by performing paid star and fork operations on specified repositories. We also uncovered a stealthy way of tampering with historical commits, through which these promoters are able to fake commits retroactively. By exploiting such a promotion service, any GitHub user can pretend to be a skillful developer with high influence. To understand promotion services in GitHub, we first investigated the underground promotion market of GitHub and identified 1,023 suspected promotion accounts from the market. Then, we developed an SVM (Support Vector Machine) classifier to detect promotion accounts from all active users extracted from GH Archive ranging from 2015 to 2019. In total, we detected 63,872 suspected promotion accounts. We further analyzed these suspected promotion accounts, showing that (1) a hidden functionality in GitHub is abused to boost the reputation of an account by forging historical commits and (2) a group of small businesses exploit GitHub promotion services to promote their products. We estimated that suspicious promoters could have made a profit of $3.41 million and $4.37 million in 2018 and 2019, respectively.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {597–610},
numpages = {14},
keywords = {GitHub, Promoter Detection, Promotion-as-a-Service},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3530019.3530041,
author = {Das, Ajoy and Uddin, Gias and Ruhe, Guenther},
title = {An Empirical Study of Blockchain Repositories in GitHub},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3530041},
doi = {10.1145/3530019.3530041},
abstract = {Blockchain is a distributed ledger technique that guarantees the traceability of transactions. Blockchain is adopted in multiple domains like finance (e.g., cryptocurrency), healthcare, security, and supply chain. In the open-source software (OSS) portal GitHub, we observe a growing adoption of Blockchain-based solutions. Given the rapid emergence of Blockchain-based solutions in our daily life and the evolving cryptocurrency market, it is important to know the status quo, how developers generally interact in those repos, and how much freedom they have in applying code changes. We report an empirical study of 3,664 Blockchain software repositories from GitHub. We divide the Blockchain repositories into two categories: Tool (e.g., SDKs) and Applications (e.g., service/solutions developed using SDKs). The Application category is further divided into two sub-categories: Crypto and Non-Crypto applications. In all Blockchain repository categories, the contribution interactions on commits are the most common interaction type. We found that more organizations contributing to the Blockchain repos than individual users. The median numbers of internal and external users in tools are higher than the application repos. We observed a higher degree of collaboration (e.g., for maintenance efforts) among users in Blockchain tools than those in the application repos. Among the artifacts, issues have a greater number of interactions than commits and pull requests. Related to autonomy we found that less than half of total project contributions are autonomous. Our findings offer implications to Blockchain stakeholders, like developers to stay aware of OSS practices around Blockchain software.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {211–220},
numpages = {10},
keywords = {Bitcoin, Blockchain, Cryptocurrency, GitHub, Repositories},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/2970276.2970283,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K. and Redl, Jesse and Collins, Jason A.},
title = {CORRECT: code reviewer recommendation at GitHub for Vendasta technologies},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970283},
doi = {10.1145/2970276.2970283},
abstract = {Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (e.g., external library experience) of a developer but also her experience in certain specialized technologies (e.g., Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the request, mines two relevant histories, and then returns a ranked list of appropriate code reviewers for the request within the browser's context. Demo: https://www.youtube.com/watch?v=rXU1wTD6QQ0},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {792–797},
numpages = {6},
keywords = {Code reviewer recommendation, GitHub, cross-project experience, pull request, specialized technology experience},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1109/ICSE.2019.00079,
author = {Imtiaz, Nasif and Middleton, Justin and Chakraborty, Joymallya and Robson, Neill and Bai, Gina and Murphy-Hill, Emerson},
title = {Investigating the effects of gender bias on GitHub},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00079},
doi = {10.1109/ICSE.2019.00079},
abstract = {Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {700–711},
numpages = {12},
keywords = {GitHub, gender, open source},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3387940.3391534,
author = {Dominic, James and Houser, Jada and Steinmacher, Igor and Ritter, Charles and Rodeghero, Paige},
title = {Conversational Bot for Newcomers Onboarding to Open Source Projects},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391534},
doi = {10.1145/3387940.3391534},
abstract = {This paper targets the problems newcomers face when onboarding to open source projects and the low retention rate of newcomers. Open source software projects are becoming increasingly more popular. Many major companies have started building open source software. Unfortunately, many newcomers only commit once to an open source project before moving on to another project. Even worse, many novices struggle with joining open source communities and end up leaving quickly, sometimes before their first successful contribution. In this paper, we propose a conversational bot that would recommend projects to newcomers and assist in the onboarding to the open source community. The bot would be able to provide helpful resources, such as Stack Overflow related content. It would also be able to recommend human mentors. We believe that this bot would improve newcomers' experience by providing support not only during their first contribution, but by acting as an agent to engage them to the project.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {46–50},
numpages = {5},
keywords = {bot, newcomer, onboarding, open source software},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3510003.3510116,
author = {Shimada, Naomichi and Xiao, Tao and Hata, Hideaki and Treude, Christoph and Matsumoto, Kenichi},
title = {GitHub sponsors: exploring a new way to contribute to open source},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510116},
doi = {10.1145/3510003.3510116},
abstract = {GitHub Sponsors, launched in 2019, enables donations to individual open source software (OSS) developers. Financial support for OSS maintainers and developers is a major issue in terms of sustaining OSS projects, and the ability to donate to individuals is expected to support the sustainability of developers, projects, and community. In this work, we conducted a mixed-methods study of GitHub Sponsors, including quantitative and qualitative analyses, to understand the characteristics of developers who are likely to receive donations and what developers think about donations to individuals. We found that: (1) sponsored developers are more active than non-sponsored developers, (2) the possibility to receive donations is related to whether there is someone in their community who is donating, and (3) developers are sponsoring as a new way to contribute to OSS. Our findings are the first step towards data-informed guidance for using GitHub Sponsors, opening up avenues for future work on this new way of financially sustaining the OSS community.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1058–1069},
numpages = {12},
keywords = {GitHub sponsors, open source, sponsorship},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3524842.3528460,
author = {Kochanthara, Sangeeth and Dajsuren, Yanja and Cleophas, Loek and van den Brand, Mark},
title = {Painting the landscape of automotive software in GitHub},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528460},
doi = {10.1145/3524842.3528460},
abstract = {The automotive industry has transitioned from being an electromechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook's front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing a first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing its characteristics and development styles.The landscape is defined by 15,000+ users contributing to ≈600 actively-developed automotive software projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics-related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe shifts along multiple dimensions, including preferred language from MATLAB to Python and prevalence of perception and decision-related software over traditional automotive software. This study witnesses open source automotive software boom in its infancy with many implications for future research and practice.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {215–226},
numpages = {12},
keywords = {GitHub, automotive software, cyber-physical systems, mining software repositories, open source, safety critical, software engineering},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@article{10.1145/3593592,
author = {Hundhausen, Christopher and Conrad, Phill and Adesope, Olusola and Tariq, Ahsun},
title = {Combining GitHub, Chat, and Peer Evaluation Data to Assess Individual Contributions to Team Software Development Projects},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
url = {https://doi.org/10.1145/3593592},
doi = {10.1145/3593592},
abstract = {Assessing team software development projects is notoriously difficult and typically based on subjective metrics. To help make assessments more rigorous, we conducted an empirical study to explore relationships between subjective metrics based on peer and instructor assessments, and objective metrics based on GitHub and chat data. We studied 23 undergraduate software teams (n = 117 students) from two undergraduate computing courses at two North American research universities. We collected data on teams’ (a) commits and issues from their GitHub code repositories, (b) chat messages from their Slack and Microsoft Teams channels, (c) peer evaluation ratings from the CATME peer evaluation system, and (d) individual assignment grades from the courses. We derived metrics from (a) and (b) to measure both individual team members’ contributions to the team, and the equality of team members’ contributions. We then performed Pearson analyses to identify correlations among the metrics, peer evaluation ratings, and individual grades. We found significant positive correlations between team members’ GitHub contributions, chat contributions, and peer evaluation ratings. In addition, the equality of teams’ GitHub contributions was positively correlated with teams’ average peer evaluation ratings and negatively correlated with the variance in those ratings. However, no such positive correlations were detected between the equality of teams’ chat contributions and their peer evaluation ratings. Our study extends previous research results by providing evidence that (a) team members’ chat contributions, like their GitHub contributions, are positively correlated with their peer evaluation ratings; (b) team members’ chat contributions are positively correlated with their GitHub contributions; and (c) the equality of team’ GitHub contributions is positively correlated with their peer evaluation ratings. These results lend further support to the idea that combining objective and subjective metrics can make the assessment of team software projects more comprehensive and rigorous.},
journal = {ACM Trans. Comput. Educ.},
month = {jul},
articleno = {33},
numpages = {23},
keywords = {Software engineering education, collaborative software development, assessment, peer evaluation, Covid-19, online chat communication, GitHub, Slack, Microsoft Teams, CATME}
}

@inproceedings{10.1145/3368089.3409722,
author = {Brown, Chris and Parnin, Chris},
title = {Understanding the impact of GitHub suggested changes on recommendations between developers},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409722},
doi = {10.1145/3368089.3409722},
abstract = {Recommendations between colleagues are effective for encouraging developers to adopt better practices. Research shows these peer interactions are useful for improving developer behaviors, or the adoption of activities to help software engineers complete programming tasks. However, in-person recommendations between developers in the workplace are declining. One form of online recommendations between developers are pull requests, which allow users to propose code changes and provide feedback on contributions. GitHub, a popular code hosting platform, recently introduced the suggested changes feature, which allows users to recommend improvements for pull requests. To better understand this feature and its impact on recommendations between developers, we report an empirical study of this system, measuring usage, effectiveness, and perception. Our results show that suggested changes support code review activities and significantly impact the timing and communication between developers on pull requests. This work provides insight into the suggested changes feature and implications for improving future systems for automated developer recommendations, such as providing situated, concise, and actionable feedback.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1065–1076},
numpages = {12},
keywords = {Empirical software engineering, GitHub, developer recommendations, online programming communities, suggested changes},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3597503.3623340,
author = {Jamieson, Jack and Yamashita, Naomi and Foong, Eureka},
title = {Predicting open source contributor turnover from value-related discussions: An analysis of GitHub issues},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623340},
doi = {10.1145/3597503.3623340},
abstract = {Discussions about project values are important for engineering software that meets diverse human needs and positively impacts society. Because value-related discussions involve deeply held beliefs, they can lead to conflicts or other outcomes that may affect motivations to continue contributing to open source projects. However, it is unclear what kind of value-related discussions are associated with significant changes in turnover. We address this gap by identifying discussions related to important project values and investigating the extent to which those discussions predict project turnover in the following months. We collected logs of GitHub issues and commits from 52 projects that share similar ethical commitments and were identified as part of the DWeb (Decentralized Web) community. We identify issues related to DWeb's core values of respectfulness, freedom, broadmindedness, opposing centralized social power, equity &amp; equality, and protecting the environment. We then use Granger causality analysis to examine how changes in the proportion of discussions related to those values might predict changes in incoming and outgoing turnover. We found multiple significant relationships between value-related discussions and turnover, including that discussions about respectfulness predict an increase in contributors leaving and a decrease in new contributors, while discussions about social power predicted better contributor retention. Understanding these antecedents of contributor turnover is important for managing open source projects that incorporate human-centric issues. Based on the results, we discuss implications for open source maintainers and for future research.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {57},
numpages = {13},
keywords = {human values, turnover, open source, GitHub},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3528228.3528403,
author = {Chidambaram, Natarajan and Decan, Alexandre and Golzadeh, Mehdi},
title = {Leveraging predictions from multiple repositories to improve bot detection},
year = {2022},
isbn = {9781450393331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528228.3528403},
doi = {10.1145/3528228.3528403},
abstract = {Contemporary social coding platforms such as GitHub facilitate collaborative distributed software development. Developers engaged in these platforms often use machine accounts (bots) for automating effort-intensive or repetitive activities. Determining whether a contributor corresponds to a bot or a human account is important in socio-technical studies, for example to assess the positive and negative impact of using bots, analyse the evolution of bots and their usage, identify top human contributors, and so on. BoDeGHa is one of the bot detection tools that have been proposed in the literature. It relies on comment activity within a single repository to predict whether an account is driven by a bot or by a human. This paper presents preliminary results on how the effectiveness of BoDeGHa can be improved by combining the predictions obtained from many repositories at once. We found that doing this not only increases the number of cases for which a prediction can be made, but that many diverging predictions can be fixed this way. These promising, albeit preliminary, results suggest that the "wisdom of the crowd" principle can improve the effectiveness of bot detection tools.},
booktitle = {Proceedings of the Fourth International Workshop on Bots in Software Engineering},
pages = {6–9},
numpages = {4},
location = {Pittsburgh, Pennsylvania},
series = {BotSE '22}
}

@inproceedings{10.1145/3366423.3380272,
author = {Maldeniya, Danaja and Budak, Ceren and Robert Jr., Lionel P. and Romero, Daniel M.},
title = {Herding a Deluge of Good Samaritans: How GitHub Projects Respond to Increased Attention},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380272},
doi = {10.1145/3366423.3380272},
abstract = {Collaborative crowdsourcing is a well-established model of work, especially in the case of open source software development. The structure and operation of these virtual and loosely-knit teams differ from traditional organizations. As such, little is known about how their behavior may change in response to an increase in external attention. To understand these dynamics, we analyze millions of actions of thousands of contributors in over 1100 open source software projects that topped the GitHub Trending Projects page and thus experienced a large increase in attention, in comparison to a control group of projects identified through propensity score matching. In carrying out our research, we use the lens of organizational change, which considers the challenges teams face during rapid growth and how they adapt their work routines, organizational structure, and management style. We show that trending results in an explosive growth in the effective team size. However, most newcomers make only shallow and transient contributions. In response, the original team transitions towards administrative roles, responding to requests and reviewing work done by newcomers. Projects evolve towards a more distributed coordination model with newcomers becoming more central, albeit in limited ways. Additionally, teams become more modular with subgroups specializing in different aspects of the project. We discuss broader implications for collaborative crowdsourcing teams that face attention shocks.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2055–2065},
numpages = {11},
keywords = {GitHub, PSM, attention shocks, coordination, crowdsourcing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1109/BotSE.2019.00010,
author = {Monperrus, Martin},
title = {Explainable software bot contributions: case study of automated bug fixes},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00010},
doi = {10.1109/BotSE.2019.00010},
abstract = {In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans.Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers.We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of "dry patches": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an "explainable bug fix": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples.In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {12–15},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3598469.3598489,
author = {Eibl, Gregor and Thurnay, L\H{o}rinc},
title = {The promises and perils of open source software release and usage by government – evidence from GitHub and literature},
year = {2023},
isbn = {9798400708374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3598469.3598489},
doi = {10.1145/3598469.3598489},
abstract = {Abstract: Open Source Software (OSS) is extensively utilized in industry and government because it allows for open access to the source code and allows for external involvement in the software development process. There are several factors driving this movement in a government setting, making it difficult to assess the adoption's success. Through a study of billions of rows of GitHub activity data, this research analyzes the production of OSS by administrations in German-speaking countries in detail and analyses the motivating factors and challenges to OSS adoption through a literature review. Similar studies have been conducted in other nations, with somewhat different approaches, foci, and different ways to identify public GitHub users as well as insiders and outsiders of OSS projects. 16 consequences of OSS usage and development are listed in the paper. On GitHub, we found 1021 OSS projects run by public agencies in largly German-speaking nations. We then compiled a list of the most popular projects based on commits and the most active public agencies in terms of projects. The research also finds automatic contributions by bots, which have not been taken into account in the literature so far, and demonstrates highly substantial positive correlations between commits, forks, and stars as proxy for the popularity of these projects. This research introduces a new method for identifying government organizations in OSS platforms and illuminates the possible positive and negative effects of the public sector's release and adoption of open source software.},
booktitle = {Proceedings of the 24th Annual International Conference on Digital Government Research},
pages = {180–190},
numpages = {11},
keywords = {GitHub, barriers, benefits, citizen engagement, evidence, government, open source software},
location = {Gda?sk, Poland},
series = {dg.o '23}
}

@inproceedings{10.1145/3540250.3549115,
author = {Pandya, Prahar and Tiwari, Saurabh},
title = {CORMS: a GitHub and Gerrit based hybrid code reviewer recommendation approach for modern code review},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549115},
doi = {10.1145/3540250.3549115},
abstract = {Modern Code review (MCR) techniques are widely adopted in both open-source software platforms and organizations to ensure the quality of their software products. However, the selection of reviewers for code review is cumbersome with the increasing size of development teams. The recommendation of inappropriate reviewers for code review can take more time and effort to complete the task effectively. In this paper, we extended the baseline of reviewers' recommendation framework - RevFinder - to handle issues with newly created files, retired reviewers, the external validity of results, and the accuracies of the state-of-the-art RevFinder. Our proposed hybrid approach, CORMS, works on similarity analysis to compute similarities among file paths, projects/sub-projects, author information, and prediction models to recommend reviewers based on the subject of the change. We conducted a detailed analysis on the widely used 20 projects of both Gerrit and GitHub to compare our results with RevFinder. Our results reveal that on average, CORMS, can achieve top-1, top-3, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 45.1%, 67.5%, 74.6%, 79.9% and 0.58 for the 20 projects, consequently improves the RevFinder approach by 44.9%, 34.4%, 20.8%, 12.3% and 18.4%, respectively.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {546–557},
numpages = {12},
keywords = {Data Mining, Gerrit, GitHub, Modern Code Review (MCR), Reviewer Recommendations},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3387940.3391502,
author = {Dey, Tapajit and Vasilescu, Bogdan and Mockus, Audris},
title = {An Exploratory Study of Bot Commits},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391502},
doi = {10.1145/3387940.3391502},
abstract = {Background: Bots help automate many of the tasks performed by software developers and are widely used to commit code in various social coding platforms. At present, it is not clear what types of activities these bots perform and understanding it may help design better bots, and find application areas which might benefit from bot adoption. Aim: We aim to categorize the Bot Commits by the type of change (files added, deleted, or modified), find the more commonly changed file types, and identify the groups of file types that tend to get updated together. Method: 12,326,137 commits made by 461 popular bots (that made at least 1000 commits) were examined to identify the frequency and the type of files added/ deleted/ modified by the commits, and association rule mining was used to identify the types of files modified together. Result: Majority of the bot commits modify an existing file, a few of them add new files, while deletion of a file is very rare. Commits involving more than one type of operation are even rarer. Files containing data, configuration, and documentation are most frequently updated, while HTML is the most common type in terms of the number of files added, deleted, and modified. Files of the type "Markdown","Ignore List", "YAML", "JSON" were the types that are updated together with other types of files most frequently. Conclusion: We observe that majority of bot commits involve single file modifications, and bots primarily work with data, configuration, and documentation files. A better understanding if this is a limitation of the bots and, if overcome, would lead to different kinds of bots remains an open question.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {61–65},
numpages = {5},
keywords = {Automated Commits, Bots, Code Commits, social coding platforms},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1145/3624739,
author = {Khatoonabadi, Sayedhassan and Costa, Diego Elias and Mujahid, Suhaib and Shihab, Emad},
title = {Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624739},
doi = {10.1145/3624739},
abstract = {Pull Requests (PRs) that are neither progressed nor resolved clutter the list of PRs, making it difficult for the maintainers to manage and prioritize unresolved PRs. To automatically track, follow up, and close such inactive PRs, Stale bot was introduced by GitHub. Despite its increasing adoption, there are ongoing debates on whether using Stale bot alleviates or exacerbates the problem of inactive PRs. To better understand if and how Stale bot helps projects in their pull-based development workflow, we perform an empirical study of 20 large and popular open source projects. We find that Stale bot can help deal with a backlog of unresolved PRs, as the projects closed more PRs within the first few months of adoption. Moreover, Stale bot can help improve the efficiency of the PR review process as the projects reviewed PRs that ended up merged and resolved PRs that ended up closed faster after the adoption. However, Stale bot can also negatively affect the contributors, as the projects experienced a considerable decrease in their number of active contributors after the adoption. Therefore, relying solely on Stale bot to deal with inactive PRs may lead to decreased community engagement and an increased probability of contributor abandonment.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {36},
numpages = {43},
keywords = {Software development bots, pull request abandonment, pull-based development, modern code review, social coding platforms, open source software}
}

@inproceedings{10.1145/3025453.3025830,
author = {Long, Kiel and Vines, John and Sutton, Selina and Brooker, Phillip and Feltwell, Tom and Kirman, Ben and Barnett, Julie and Lawson, Shaun},
title = {"Could You Define That in Bot Terms"? Requesting, Creating and Using Bots on Reddit},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025830},
doi = {10.1145/3025453.3025830},
abstract = {Bots are estimated to account for well over half of all web traffic, yet they remain an understudied topic in HCI. In this paper we present the findings of an analysis of 2284 submissions across three discussion groups dedicated to the request, creation and discussion of bots on Reddit. We set out to examine the qualities and functionalities of bots and the practical and social challenges surrounding their creation and use. Our findings highlight the prevalence of misunderstandings around the capabilities of bots, misalignments in discourse between novices who request and more expert members who create them, and the prevalence of requests that are deemed to be inappropriate for the Reddit community. In discussing our findings, we suggest future directions for the design and development of tools that support more carefully guided and reflective approaches to bot development for novices, and tools to support exploring the consequences of contextually-inappropriate bot ideas.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {3488–3500},
numpages = {13},
keywords = {Reddit, bots, co-creation, online communities},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3459637.3482019,
author = {Feng, Shangbin and Wan, Herun and Wang, Ningnan and Li, Jundong and Luo, Minnan},
title = {TwiBot-20: A Comprehensive Twitter Bot Detection Benchmark},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482019},
doi = {10.1145/3459637.3482019},
abstract = {Twitter has become a vital social media platform while an ample amount of malicious Twitter bots exist and induce undesirable social effects. Successful Twitter bot detection proposals are generally supervised, which rely heavily on large-scale datasets. However, existing benchmarks generally suffer from low levels of user diversity, limited user information and data scarcity. Therefore, these datasets are not sufficient to train and stably benchmark bot detection measures. To alleviate these problems, we present TwiBot-20, a massive Twitter bot detection benchmark, which contains 229,573 users, 33,488,192 tweets, 8,723,736 user property items and 455,958 follow relationships. TwiBot-20 covers diversified bots and genuine users to better represent the real-world Twittersphere. TwiBot-20 also includes three modals of user information to support both binary classification of single users and community-aware approaches. To the best of our knowledge, TwiBot-20 is the largest Twitter bot detection benchmark to date. We reproduce competitive bot detection methods and conduct a thorough evaluation on TwiBot-20 and two other public datasets. Experiment results demonstrate that existing bot detection measures fail to match their previously claimed performance on TwiBot-20, which suggests that Twitter bot detection remains a challenging task and requires further research efforts.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4485–4494},
numpages = {10},
keywords = {benchmarking, social media, twitter API, twitter bot detection},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00009,
author = {Tan, Shin Hwei and Hu, Chunfeng and Li, Ziqiang and Zhang, Xiaowen and Zhou, Ying},
title = {GitHub-OSS fixit},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00009},
doi = {10.1109/ICSE-SEET52601.2021.00009},
abstract = {Many studies have shown the benefits of introducing open-source projects into teaching Software Engineering (SE) courses. However, there are several limitations of existing studies that limit the wide adaptation of open-source projects in a classroom setting, including (1) the selected project is limited to one particular project, (2) most studies only investigated on its effect on teaching a specific SE concept, and (3) students may make mistakes in their contribution which leads to poor quality code. Meanwhile, software companies have successfully launched programs like Google Summer of Code (GSoC) and FindBugs "fixit" to contribute to open-source projects. Inspired by the success of these programs, we propose GitHub-OSS Fixit, a team-based course project where students are taught to contribute to open-source Java projects by fixing bugs reported in GitHub. We described our course outline to teach students SE concepts by encouraging the usages of several automated program analysis tools. We also included the carefully designed instructions that we gave to students for participating in GitHub-OSS Fixit. As all lectures and labs are conducted online, we think that our course design could help in guiding future online SE courses. Overall, our survey results show that students think that GitHub-OSS Fixit could help them to improve many skills and apply the knowledge taught in class. In total, 154 students have submitted 214 pull requests to 24 different Java projects, in which 93 of them have been merged, and 46 have been closed by developers.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {1–10},
numpages = {10},
keywords = {open-source software, program repair, software engineering},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.1145/3524842.3528528,
author = {Moharil, Ambarish and Orlov, Dmitrii and Jameel, Samar and Trouwen, Tristan and Cassee, Nathan and Serebrenik, Alexander},
title = {Between JIRA and GitHub: ASFBot and its influence on human comments in issue trackers},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528528},
doi = {10.1145/3524842.3528528},
abstract = {Open-Source Software (OSS) projects have adopted various automations for repetitive tasks in recent years. One common type of automation in OSS is bots. In this exploratory case study, we seek to understand how the adoption of one particular bot (ASFBot) by the Apache Software Foundation (ASF) impacts the discussions in the issue-trackers of these projects. We use the SmartShark dataset to investigate whether the ASFBot affects (i) human comments mentioning pull requests and fixes in issue comments and (ii) the general human comment rate on issues. We apply a regression discontinuity design (RDD) on nine ASF projects that have been active both before and after the ASFBot adoption. Our results indicate (i) an immediate decrease in the number of median comments mentioning pull requests and fixes after the bot adoption, but the trend of a monthly decrease in this comment count is reversed, and (ii) no effect in the number of human comments after the bot adoption. We make an effort to gather first insights in understanding the impact of adopting the ASFBot on the commenting behavior of developers who are working on ASF projects.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {112–116},
numpages = {5},
keywords = {ASFBot, apache, bots, issue-trackers},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@article{10.1145/3626783,
author = {Li, Kai and Guan, Shixuan and Lee, Darren},
title = {Towards Understanding and Characterizing the Arbitrage Bot Scam In the Wild},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3626783},
doi = {10.1145/3626783},
abstract = {This paper presents the first comprehensive analysis of an emerging cryptocurrency scam named "arbitrage bot" disseminated on online social networks. The scam revolves around Decentralized Exchanges (DEX) arbitrage and aims to lure victims into executing a so-called "bot contract" to steal funds from them. To entice victims and convince them of this scheme, we found that scammers have flocked to publish YouTube videos to demonstrate plausible profits and provide detailed instructions and links to the bot contract.To collect the scam at a large scale, we developed a fully automated scam detection system namedCryptoScamHunter, which continuously collects YouTube videos and automatically detects scams. Meanwhile,CryptoScamHunter can download the source code of the bot contract from the provided links and extract the associated scam cryptocurrency address. Through deployingCryptoScamHunter from Jun. 2022 to Jun. 2023, we have detected 10,442 arbitrage bot scam videos published from thousands of YouTube accounts. Our analysis reveals that different strategies have been utilized in spreading the scam, including crafting popular accounts, registering spam accounts, and using obfuscation tricks to hide the real scam address in the bot contracts. Moreover, from the scam videos we have collected over 800 malicious bot contracts with source code and extracted 354 scam addresses. By further expanding the scam addresses with a similar contract matching technique, we have obtained a total of 1,697 scam addresses. Through tracing the transactions of all scam addresses on the Ethereum mainnet and Binance Smart Chain, we reveal that over 25,000 victims have fallen prey to this scam, resulting in a financial loss of up to 15 million USD.Overall, our work sheds light on the dissemination tactics and censorship evasion strategies adopted in the arbitrage bot scam, as well as on the scale and impact of such a scam on online social networks and blockchain platforms, emphasizing the urgent need for effective detection and prevention mechanisms against such fraudulent activity.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {52},
numpages = {29},
keywords = {arbitrage bot, blockchain, cryptocurrency scams, decentralized exchange}
}

@article{10.1145/3593803,
author = {Bock, Thomas and Alznauer, Nils and Joblin, Mitchell and Apel, Sven},
title = {Automatic Core-Developer Identification on GitHub: A Validation Study},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3593803},
doi = {10.1145/3593803},
abstract = {Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {138},
numpages = {29},
keywords = {Open-source software projects, developer classification, developer networks}
}

@inproceedings{10.5555/2820518.2820563,
author = {Hauff, Claudia and Gousios, Georgios},
title = {Matching GitHub developer profiles to job advertisements},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally "be seen" by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {362–366},
numpages = {5},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1109/ASE51524.2021.9678680,
author = {Phaithoon, Saranphon and Wongnil, Supakarn and Pussawong, Patiphol and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee and Maipradit, Rungroj and Hata, Hideaki and Matsumoto, Kenichi},
title = {FixMe: a GitHub bot for detecting and monitoring on-hold self-admitted technical debt},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678680},
doi = {10.1109/ASE51524.2021.9678680},
abstract = {Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related "On-hold SATD", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN_YQ.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1257–1261},
numpages = {5},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/3643673,
author = {S\"{u}l\"{u}n, Emre and Sa\c{c}ak\c{c}\i{}, Metehan and T\"{u}z\"{u}n, Eray},
title = {An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643673},
doi = {10.1145/3643673},
abstract = {GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates’ extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18&nbsp;days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates’ positive impact on large-scale open-source projects, offering recommendations for improved effectiveness.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {117},
numpages = {28},
keywords = {Issue templates, issue forms, issue tracking, GitHub issues, bug tracking, empirical study}
}

@inproceedings{10.1145/3183519.3183540,
author = {Urli, Simon and Yu, Zhongxing and Seinturier, Lionel and Monperrus, Martin},
title = {How to design a program repair bot? insights from the repairnator project},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183540},
doi = {10.1145/3183519.3183540},
abstract = {Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a "program repair bot" and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 523 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 15 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {95–104},
numpages = {10},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1145/3350546.3352519,
author = {Horawalavithana, Sameera and Bhattacharjee, Abhishek and Liu, Renhao and Choudhury, Nazim and O. Hall, Lawrence and Iamnitchi, Adriana},
title = {Mentions of Security Vulnerabilities on Reddit, Twitter and GitHub},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352519},
doi = {10.1145/3350546.3352519},
abstract = {Activity on social media is seen as a relevant sensor for different aspects of the society. In a heavily digitized society, security vulnerabilities pose a significant threat that is publicly discussed on social media. This study presents a comparison of user-generated content related to security vulnerabilities on three digital platforms: two social media conversation channels (Reddit and Twitter) and a collaborative software development platform (GitHub). Our data analysis shows that while more security vulnerabilities are discussed on Twitter, relevant conversations go viral earlier on Reddit. We show that the two social media platforms can be used to accurately predict activity on GitHub.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {200–207},
numpages = {8},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1145/3543507.3583214,
author = {Hays, Chris and Schutzman, Zachary and Raghavan, Manish and Walk, Erin and Zimmer, Philipp},
title = {Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583214},
doi = {10.1145/3543507.3583214},
abstract = {Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near-perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules — shallow decision trees trained on a small number of features — achieve near-state-of-the-art performance on most available datasets and that bot detection datasets, even when combined together, do not generalize well to out-of-sample datasets. Our findings reveal that predictions are highly dependent on each dataset’s collection and labeling procedures rather than fundamental differences between bots and humans. These results have important implications for both transparency in sampling and labeling procedures and potential biases in research using existing bot detection tools for pre-processing.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3660–3669},
numpages = {10},
keywords = {Social media, bot detection},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3334480.3382998,
author = {Liu, Dongyu and Smith, Micah J. and Veeramachaneni, Kalyan},
title = {Understanding User-Bot Interactions for Small-Scale Automation in Open-Source Development},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382998},
doi = {10.1145/3334480.3382998},
abstract = {Small-scale automation tools, or "bots," have been widely deployed in open-source software development to support manual project maintenance tasks. Though interactions between these bots and human developers can have significant effects on user experience, previous research has instead mostly focused on project outcomes. We reviewed existing small-scale bots in wide use on GitHub. After an in-depth qualitative and quantitative evaluation, we compiled several important design principles for human-bot interaction in this context. Following the requirements, we further propose a workflow to support bot developers.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {HCI design and evaluation methods, human-centered computing, software and its engineering, software creation and management},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/3639476.3639777,
author = {Mishra, Shyamal and Chatterjee, Preetha},
title = {Exploring ChatGPT for Toxicity Detection in GitHub},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639777},
doi = {10.1145/3639476.3639777},
abstract = {Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses significant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models effectively, we need large software engineering-specific toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training effective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {6–10},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3643991.3644915,
author = {Bernardo, Jo\~{a}o Helis and Da Costa, Daniel Alencar and Medeiros, S\'{e}rgio Queiroz de and Kulesza, Uir\'{a}},
title = {How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644915},
doi = {10.1145/3643991.3644915},
abstract = {Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build duration, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {665–676},
numpages = {12},
keywords = {continuous integration, machine learning, GitHub actions, mining software repositories},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3196398.3196429,
author = {Middleton, Justin and Murphy-Hill, Emerson and Green, Demetrius and Meade, Adam and Mayer, Roger and White, David and McDonald, Steve},
title = {Which contributions predict whether developers are accepted into github teams},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196429},
doi = {10.1145/3196398.3196429},
abstract = {Open-source software (OSS) often evolves from volunteer contributions, so OSS development teams must cooperate with their communities to attract new developers. However, in view of the myriad ways that developers interact over platforms for OSS development, observers of these communities may have trouble discerning, and thus learning from, the successful patterns of developer-to-team interactions that lead to eventual team acceptance. In this work, we study project communities on GitHub to discover which forms of software contribution characterize developers who begin as development team outsiders and eventually join the team, in contrast to developers who remain team outsiders. From this, we identify and compare the forms of contribution, such as pull requests and several forms of discussion comments, that influence whether new developers join OSS teams, and we discuss the implications that these behavioral patterns have for the focus of designers and educators.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {403–413},
numpages = {11},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3545258.3545284,
author = {Wu, Xiaojun and Gao, Anze and Zhang, Yang and Wang, Tao and Tang, Yi},
title = {A Preliminary Study of Bots Usage in Open Source Community},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545284},
doi = {10.1145/3545258.3545284},
abstract = {Bots are seen as a promising approach in software development, which help to deal with the ever-increasing complexity of modern software engineering and development. The number of bots in open source community, such as GitHub, has expanded substantially over the last three years. Due to its increasing popularity, it is essential to characterize the current usage of bots in practices. In this paper, we present an empirical study of bots usage in GitHub community. By analyzing 7,399 projects from GitHub, we find that 4,148 (56%) projects have used bots. Through automatic identification and manual detection, we collect a total of 196 bots. We then analyze and classify them into 4 categories and 14 topics. Finally, we discuss some raised implications for bots in current GitHub community.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {175–180},
numpages = {6},
keywords = {Bots, Empirical study, GitHub, Open source},
location = {Hohhot, China},
series = {Internetware '22}
}

@inproceedings{10.1145/3475716.3475775,
author = {van Mil, Frenk C.J. and Rastogi, Ayushi and Zaidman, Andy},
title = {Promises and Perils of Inferring Personality on GitHub},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475775},
doi = {10.1145/3475716.3475775},
abstract = {Background: Personality plays a pivotal role in our understanding of human actions and behavior. Today, the applications of personality are widespread, built on the solutions from psychology to infer personality. Aim: In software engineering, for instance, one widely used solution to infer personality uses textual communication data. As studies on personality in software engineering continue to grow, it is imperative to understand the performance of these solutions. Method: This paper compares the inferential ability of three widely studied text-based personality tests against each other and the ground truth on GitHub. We explore the challenges and potential solutions to improve the inferential ability of personality tests. Results: Our study shows that solutions for inferring personality are far from being perfect. Software engineering communications data can infer individual developer personality with an average error rate of 41%. In the best case, the error rate can be reduced up to 36% by following our recommendations1.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {17},
numpages = {11},
keywords = {LIWC, Mining Software Repositories, Personality, Personality Insights, Software Developer},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3487351.3488278,
author = {Moradi-Jamei, Behnaz and Kramer, Brandon L. and Calder\'{o}n, J. Bayo\'{a}n Santiago and Korkmaz, Gizem},
title = {Community formation and detection on GitHub collaboration networks},
year = {2022},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3488278},
doi = {10.1145/3487351.3488278},
abstract = {This paper studies community formation in OSS collaboration networks. While most current work examines the emergence of small-scale OSS projects, our approach draws on a large-scale historical dataset of 1.8 million GitHub users and their repository contributions. OSS collaborations are characterized by small groups of users that work closely together, leading to the presence of communities defined by short cycles in the underlying network structure. To understand the impact of this phenomenon, we apply a pre-processing step that accounts for the cyclic network structure by using Renewal-Nonbacktracking Random Walks (RNBRW) and the strength of pairwise collaborations before implementing the Louvain method to identify communities within the network. Equipping Louvain with RNBRW and the contribution strength provides a more assertive approach for detecting small-scale teams and reveals nontrivial differences in community detection such as users' tendencies toward preferential attachment to more established collaboration communities. Using this method, we also identify key factors that affect community formation, including the effect of users' location and primary programming language, which was determined using a comparative method of contribution activities. Overall, this paper offers several promising methodological insights for both open-source software experts and network scholars interested in studying team formation.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {244–251},
numpages = {8},
keywords = {community detection, community formation, open source software},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/2950290.2983989,
author = {Storey, Margaret-Anne and Zagalsky, Alexey},
title = {Disrupting developer productivity one bot at a time},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983989},
doi = {10.1145/2950290.2983989},
abstract = {Bots are used to support different software development activities, from automating repetitive tasks to bridging knowledge and communication gaps in software teams. We anticipate the use of Bots will increase and lead to improvements in software quality and developer and team productivity, but what if the disruptive effect is not what we expect?  Our goal in this paper is to provoke and inspire researchers to study the impact (positive and negative) of Bots on software development. We outline the modern Bot landscape and use examples to describe the common roles Bots occupy in software teams. We propose a preliminary cognitive support framework that can be used to understand these roles and to reflect on the impact of Bots in software development on productivity. Finally, we consider challenges that Bots may bring and propose some directions for future research.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {928–931},
numpages = {4},
keywords = {Human computer interaction, computer supported collaborative work, productivity, software engineering},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3510003.3510196,
author = {Xiao, Wenxin and He, Hao and Xu, Weiwei and Tan, Xin and Dong, Jinhao and Zhou, Minghui},
title = {Recommending good first issues in GitHub OSS projects},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510196},
doi = {10.1145/3510003.3510196},
abstract = {Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for newcomers to locate suitable development tasks, while existing "Good First Issues" (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RecGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RecGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RecGFI, we collect 53,510 resolved issues among 100 GitHub projects and carefully restore their historical states to build ground truth datasets. Our evaluation shows that RecGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals interesting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1830–1842},
numpages = {13},
keywords = {good first issues, onboarding, open-source software},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3524842.3527941,
author = {Ait, Adem and Izquierdo, Javier Luis C\'{a}novas and Cabot, Jordi},
title = {An empirical study on the survival rate of GitHub projects},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527941},
doi = {10.1145/3524842.3527941},
abstract = {The number of Open Source projects hosted in social coding platforms such as GitHub is constantly growing. However, many of these projects are not regularly maintained and some are even abandoned shortly after they were created. In this paper we analyze early project development dynamics in software projects hosted on GitHub, including their survival rate. To this aim, we collected all 1,127 GitHub repositories from four different ecosystems (i.e., NPM packages, R packages, WordPress plugins and Laravel packages) created in 2016. We stored their activity in a time series database and analyzed their activity evolution along their lifespan, from 2016 to now. Our results reveal that the prototypical development process consists of intensive coding-driven active periods followed by long periods of inactivity. More importantly, we have found that a significant number of projects die in the first year of existence with the survival rate decreasing year after year. In fact, the probability of surviving longer than five years is less than 50% though some types of projects have better chances of survival.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {365–375},
numpages = {11},
keywords = {empirical study, mining software repositories, open source analysis, survival analysis},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3328529.3328547,
author = {Wirth, Kurt and Menchen-Trevino, Ericka and Moore, Ryan T.},
title = {Bots By Topic: Exploring Differences in Bot Activity by Conversation Topic},
year = {2019},
isbn = {9781450366519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328529.3328547},
doi = {10.1145/3328529.3328547},
abstract = {This study introduces a new tool to compare bot levels in real-time across conversation topics or hashtags. With the data collected, we measured higher levels of bot activity in some topics of conversation as compared to others and propose a novel application of bot detection analysis to advance research in this fast-changing field.},
booktitle = {Proceedings of the 10th International Conference on Social Media and Society},
pages = {77–82},
numpages = {6},
keywords = {BotOMeter, Twitter, Twitter bots, bots, botscan, computational propaganda, disinformation, misinformation, social bots},
location = {Toronto, ON, Canada},
series = {SMSociety '19}
}

@inproceedings{10.1145/3593434.3593468,
author = {Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fahmideh, Mahdi and Aktar, Mst Shamima and Mikkonen, Tommi},
title = {Towards Human-Bot Collaborative Software Architecting with ChatGPT},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593468},
doi = {10.1145/3593434.3593468},
abstract = {Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders’ perspectives, designers’ intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects’ knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects’ productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {279–285},
numpages = {7},
keywords = {ChatGPT, DevBots, Large Language Models, Software Architecture},
location = {Oulu, Finland},
series = {EASE '23}
}

@article{10.1145/3409116,
author = {Cresci, Stefano},
title = {A decade of social bot detection},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3409116},
doi = {10.1145/3409116},
abstract = {Bots increasingly tamper with political elections and economic discussions. Tracing trends in detection strategies and key suggestions on how to win the fight.},
journal = {Commun. ACM},
month = {sep},
pages = {72–83},
numpages = {12}
}

@article{10.1145/3449093,
author = {Li, Renee and Pandurangan, Pavitthra and Frluckaj, Hana and Dabbish, Laura},
title = {Code of Conduct Conversations in Open Source Software Projects on Github},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449093},
doi = {10.1145/3449093},
abstract = {The rapid growth of open source software necessitates a deeper understanding of moderation and governance methods currently used within these projects. The code of conduct, a set of rules articulating standard behavior and responsibilities for participation within a community, is becoming an increasingly common policy document in open source software projects for setting project norms of behavior and discouraging negative or harassing comments and conversation. This study describes the conversations around adopting and crafting a code of conduct as well as those utilizing code of conduct for community governance. We conduct a qualitative analysis of a random sample of GitHub issues that involve the code of conduct. We find that codes of conduct are used both proactively and reactively to govern community behavior in project issues. Oftentimes, the initial addition of a code of conduct does not involve much community participation and input. However, a controversial moderation act is capable of inciting mass community feedback and backlash. Project maintainers balance the tension between disciplining potentially offensive forms of speech and encouraging broad and inclusive participation. These results have implications for the design of inclusive and effective governance practices for open source software communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {19},
numpages = {31},
keywords = {collaboration, open source software}
}

@inproceedings{10.1145/3639478.3640025,
author = {Todd, Liam and Grundy, John and Treude, Christoph},
title = {GitHubInclusifier: Finding and fixing non-inclusive language in GitHub Repositories},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640025},
doi = {10.1145/3639478.3640025},
abstract = {Non-inclusive language in software artefacts has been recognised as a serious problem. We describe a tool to find and fix non-inclusive language in a variety of GitHub repository artefacts. These include various README files, PDFs, code comments, and code. A wide variety of non-inclusive language including racist, ageist, ableist, violent and others are located and issues created, tagging the artefacts for checking. Suggested fixes can be generated using third-party LLM APIs, and approved changes made to documents, including code refactorings, and committed to the repository.The tool and evaluation data are available from: https://github.com/LiamTodd/github-inclusifierThe demo video is available at: https://www.youtube.com/watch?v=1z1QKdQg-nM},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {89–93},
numpages = {5},
keywords = {inclusive language, refactoring, biased language, inappropriate language, software documentation, software maintenance tools},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3623476.3623524,
author = {Ait, Adem and C\'{a}novas Izquierdo, Javier Luis and Cabot, Jordi},
title = {A Tool for the Definition and Deployment of Platform-Independent Bots on Open Source Projects},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623524},
doi = {10.1145/3623476.3623524},
abstract = {The development of Open Source Software (OSS) projects is a collaborative process that heavily relies on active contributions by passionate developers. Creating, retaining and nurturing an active community of developers is a challenging task; and finding the appropriate expertise to drive the development process is not always easy. To alleviate this situation, many OSS projects try to use bots to automate some development tasks, thus helping community developers to cope with the daily workload of their projects. However, the techniques and support for developing bots is specific to the code hosting platform where the project is being developed (e.g., GitHub or GitLab). Furthermore, there is no support for orchestrating bots deployed in different platforms nor for building bots that go beyond pure development activities. In this paper, we propose a tool to define and deploy bots for OSS projects, which besides automation tasks they offer a more social facet, improving community interactions. The tool includes a Domain-Specific Language (DSL) which allows defining bots that can be deployed on top of several platforms and that can be triggered by different events (e.g., creation of a new issue or a pull request). We describe the design and the implementation of the tool, and illustrate its use with examples.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {214–219},
numpages = {6},
keywords = {Bot, Domain-Specific Language, Open Source},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@article{10.1145/3522587,
author = {Rombaut, Benjamin and Cogo, Filipe R. and Adams, Bram and Hassan, Ahmed E.},
title = {There’s no Such Thing as a Free Lunch: Lessons Learned from Exploring the Overhead Introduced by the Greenkeeper Dependency Bot in Npm},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3522587},
doi = {10.1145/3522587},
abstract = {Dependency management bots are increasingly being used to support the software development process, for example, to automatically update a dependency when a new version is available. Yet, human intervention is often required to either accept or reject any action or recommendation the bot creates. In this article, our objective is to study the extent to which dependency management bots create additional, and sometimes unnecessary, work for their users. To accomplish this, we analyze 93,196 issue reports opened by Greenkeeper, a popular dependency management bot used in open source software projects in the npm ecosystem. We find that Greenkeeper is responsible for half of all issues reported in client projects, inducing a significant amount of overhead that must be addressed by clients, since many of these issues were created as a result of Greenkeeper taking incorrect action on a dependency update (i.e., false alarms). Reverting a broken dependency update to an older version, which is a potential solution that requires the least overhead and is automatically attempted by Greenkeeper, turns out to not be an effective mechanism. Finally, we observe that 56% of the commits referenced by Greenkeeper issue reports only change the client’s dependency specification file to resolve the issue. Based on our findings, we argue that dependency management bots should (i) be configurable to allow clients to reduce the amount of generated activity by the bots, (ii) take into consideration more sources of information than only the pass/fail status of the client’s build pipeline to help eliminate false alarms, and (iii) provide more effective incentives to encourage clients to resolve dependency issues.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {11},
numpages = {40},
keywords = {Dependency management, software bots, mining software repositories, greenkeeper, overhead}
}

@inproceedings{10.1145/3524842.3527959,
author = {Abdellatif, Ahmad and Wessel, Mairieli and Steinmacher, Igor and Gerosa, Marco A. and Shihab, Emad},
title = {BotHunter: an approach to detect software bots in GitHub},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527959},
doi = {10.1145/3524842.3527959},
abstract = {Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {6–17},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3611643.3613077,
author = {Ehsani, Ramtin and Rezapour, Rezvaneh and Chatterjee, Preetha},
title = {Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613077},
doi = {10.1145/3611643.3613077},
abstract = {To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2092–2096},
numpages = {5},
keywords = {moral principles, open source, textual analysis, toxicity},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3217804.3217895,
author = {Celi\'{n}ska, Dorota},
title = {Coding together in a social network: collaboration among GitHub users},
year = {2018},
isbn = {9781450363341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3217804.3217895},
doi = {10.1145/3217804.3217895},
abstract = {In this article we investigate developers involved in the creation of Open Source software to identify which characteristics favor innovation in the Open Source community. The results of the analysis show that higher reputation in the community improves the probability of gaining collaborators to a certain degree, but developers are also driven by reciprocity. This is consistent with the concept of gift economy. A significant network effect exists and emerges from standardization, showing that developers using the most popular programming languages in the service are likely to have more collaborators. Providing additional information (valid URL to developer's homepage) improves the chances of finding coworkers. The results can be generalized for the population of mature users of GitHub.},
booktitle = {Proceedings of the 9th International Conference on Social Media and Society},
pages = {31–40},
numpages = {10},
keywords = {GitHub, Open Source, collaboration, forking, gift economy, innovations, network externality, reciprocity, reputation},
location = {Copenhagen, Denmark},
series = {SMSociety '18}
}

@inproceedings{10.1109/BotSE.2019.00018,
author = {Wessel, Mairieli and Steinmacher, Igor and Wiese, Igor and Gerosa, Marco A.},
title = {Should I stale or should I close? an analysis of a bot that closes abandoned issues and pull requests},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00018},
doi = {10.1109/BotSE.2019.00018},
abstract = {On GitHub, projects use bots to automate predefined and repetitive tasks related to issues and pull requests. Our research investigates the adoption of the stale bot, which helps maintainers triaging abandoned issues and pull requests. We analyzed the bots' configuration settings and their modifications over time. These settings define the time for tagging issues and pull request as stale and closing them. We collected data from 765 OSS projects hosted on GitHub. Our results indicate that most of the studied projects made no more than three modifications in the configurations file, issues tagged as bug reports are exempt from being considered stale, while the same occurs with pull requests that need some input to be processed.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {38–42},
numpages = {5},
keywords = {abandoned issues, bots, open source software},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1109/ICSE43902.2021.00058,
author = {Moldon, Lukas and Strohmaier, Markus and Wachs, Johannes},
title = {How Gamification Affects Software Developers: Cautionary Evidence from a Natural Experiment on GitHub},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00058},
doi = {10.1109/ICSE43902.2021.00058},
abstract = {We examine how the behavior of software developers changes in response to removing gamification elements from GitHub, an online platform for collaborative programming and software development. We find that the unannounced removal of daily activity streak counters from the user interface (from user profile pages) was followed by significant changes in behavior. Long-running streaks of activity were abandoned and became less common. Weekend activity decreased and days in which developers made a single contribution became less common. Synchronization of streaking behavior in the platform's social network also decreased, suggesting that gamification is a powerful channel for social influence. Focusing on a set of software developers that were publicly pursuing a goal to make contributions for 100 days in a row, we find that some of these developers abandon this quest following the removal of the public streak counter. Our findings provide evidence for the significant impact of gamification on the behavior of developers on large collaborative programming and software development platforms. They urge caution: gamification can steer the behavior of software developers in unexpected and unwanted directions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {549–561},
numpages = {13},
keywords = {GitHub, behavior, gamification, natural experiment, software engineering},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3368089.3418539,
author = {Wessel, Mairieli},
title = {Enhancing developers’ support on pull requests activities with software bots},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418539},
doi = {10.1145/3368089.3418539},
abstract = {Software bots are employed to support developers' activities, serving as conduits between developers and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save development cost, time, and effort, the bots' presence can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers enhance existing bots. Toward this end, we are interviewing maintainers, contributors, and bot developers to understand the problems in the human-bot interaction and how they affect the collaboration in a project. Afterward, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1674–1677},
numpages = {4},
keywords = {GitHub Bots, Open-source Software, Software Bots},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1109/ASE.2019.00081,
author = {Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem},
title = {RefBot: intelligent software refactoring bot},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00081},
doi = {10.1109/ASE.2019.00081},
abstract = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost.In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {823–834},
numpages = {12},
keywords = {Software bot, quality, refactoring},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3627050.3627054,
author = {Kassem, Khaled and Pavlova, Galya and Schlund, Sebastian and Michahelles, Florian},
title = {Build-A-Bot: Developing A Software Platform For A Modular Mobile Robot},
year = {2024},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627050.3627054},
doi = {10.1145/3627050.3627054},
abstract = {This work aims to create a software platform that can manage dynamic changes in the configuration and functionalities of a modular robot that is currently in development, simulate its final appearance and behavior in augmented reality, and evaluate its usability through a user study. We developed a software platform for a modular collaborative robot where users can add, remove, and swap modules without code changes. The software platform displays possible functions based on the current configuration of modules. To enable 3D interaction with the robot’s digital twin in real-time, we utilized an AR environment with the HoloLens. We conducted a user study with 28 participants without prior knowledge of robotics to evaluate the software’s usability and user experience for non-experts. The study results indicate that the software platform was well-received and user-friendly, with the digital twin in an AR environment providing a realistic robot simulation. Participants’ subjective feedback on usability, user experience, and cognitive workload of different software components was collected, and the analysis showed that the platform is suitable for non-experts. The results showed that our design of the platform and its UI is well-accepted and easy to use, shown by a SUS score of 74.29. We show how we built our software platform, as well as the simulation in AR. Moreover, we propose the practice of using AR simulation to design the software platform before future physical prototype development, in order to test different scenarios, and possibly inform the physical design. Finally, we recommend that future work explores usability with a more diverse set of non-expert users, as well as different tasks.},
booktitle = {Proceedings of the 13th International Conference on the Internet of Things},
pages = {74–81},
numpages = {8},
location = {Nagoya, Japan},
series = {IoT '23}
}

@inproceedings{10.1145/3406865.3418368,
author = {Wessel, Mairieli},
title = {Leveraging Software Bots to Enhance Developers' Collaboration in Online Programming Communities},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418368},
doi = {10.1145/3406865.3418368},
abstract = {Software bots are applications that are integrated into human communication channels, serving as an interface between users and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save developers' costs, time, and effort, the interaction of these bots can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers to enhance existing bots, thereby improving the partnership with contributors and maintainers. Toward this end, we are interviewing developers to understand what are the problems on the human-bot interaction and how they affect human collaboration. Afterwards, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.},
booktitle = {Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {183–188},
numpages = {6},
keywords = {github bots, open source software, software bots, software engineering},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3510003.3512765,
author = {Wessel, Mairieli and Abdellatif, Ahmad and Wiese, Igor and Conte, Tayana and Shihab, Emad and Gerosa, Marco A. and Steinmacher, Igor},
title = {Bots for pull requests: the good, the bad, and the promising},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3512765},
doi = {10.1145/3510003.3512765},
abstract = {Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort ("the good"). However, their interactions can be disruptive and noisy and lead to information overload ("the bad"). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface ("the promising"). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {274–286},
numpages = {13},
keywords = {GitHub bots, automation, collaborative development, design fiction, human-bot interaction, open source software, software bots},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3633287,
author = {Richards, Mike and Waugh, Kevin and Slaymaker, Mark and Petre, Marian and Woodthorpe, John and Gooch, Daniel},
title = {Bob or Bot: Exploring ChatGPT's Answers to University Computer Science Assessment},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
url = {https://doi.org/10.1145/3633287},
doi = {10.1145/3633287},
abstract = {Cheating has been a long-standing issue in university assessments. However, the release of ChatGPT and other free-to-use generative AI tools has provided a new and distinct method for cheating. Students can run many assessment questions through the tool and generate a superficially compelling answer, which may or may not be accurate.&nbsp;We ran a dual-anonymous “quality assurance” marking exercise across four end-of-module assessments across a distance university computer science (CS) curriculum. Each marker received five ChatGPT-generated scripts alongside 10 student scripts. A total of 90 scripts were marked; every ChatGPT-generated script for the undergraduate modules received at least a passing grade (&gt;40%), with all of the introductory module CS1 scripts receiving a distinction (&gt;85%). None of the ChatGPT-taught postgraduate scripts received a passing grade (&gt;50%). We also present the results of interviewing the markers and of running our sample scripts through a GPT-2 detector and the TurnItIn AI detector, which both identified every ChatGPT-generated script but differed in the number of false positives. As such, we contribute a baseline understanding of how the public release of generative AI is likely to significantly impact quality assurance processes. Our analysis demonstrates that in most cases, across a range of question formats, topics, and study levels, ChatGPT is at least capable of producing adequate answers for undergraduate assessment.},
journal = {ACM Trans. Comput. Educ.},
month = {jan},
articleno = {5},
numpages = {32},
keywords = {ChatGPT, generative AI, cheating, quality assurance, university assessment’}
}

@article{10.1145/3476042,
author = {Wessel, Mairieli and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco Aurelio},
title = {Don't Disturb Me: Challenges of Interacting with Software Bots on Open Source Software Projects},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476042},
doi = {10.1145/3476042},
abstract = {Software bots are used to streamline tasks in Open Source Software (OSS) projects' pull requests, saving development cost, time, and effort. However, their presence can be disruptive to the community. We identified several challenges caused by bots in pull request interactions by interviewing 21 practitioners, including project maintainers, contributors, and bot developers. In particular, our findings indicate noise as a recurrent and central problem. Noise affects both human communication and development workflow by overwhelming and distracting developers. Our main contribution is a theory of how human developers perceive annoying bot behaviors as noise on social coding platforms. This contribution may help practitioners understand the effects of adopting a bot, and researchers and tool designers may leverage our results to better support human-bot interaction on social coding platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {301},
numpages = {21},
keywords = {collaborative development, github bots, human-bot interaction, open source software, software bots, software engineering}
}

@article{10.1145/3274451,
author = {Wessel, Mairieli and de Souza, Bruno Mendes and Steinmacher, Igor and Wiese, Igor S. and Polato, Ivanilton and Chaves, Ana Paula and Gerosa, Marco A.},
title = {The Power of Bots: Characterizing and Understanding Bots in OSS Projects},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274451},
doi = {10.1145/3274451},
abstract = {Leveraging the pull request model of social coding platforms, Open Source Software (OSS) integrators review developers' contributions, checking aspects like license, code quality, and testability. Some projects use bots to automate predefined, sometimes repetitive tasks, thereby assisting integrators' and contributors' work. Our research investigates the usage and impact of such bots. We sampled 351 popular projects from GitHub and found that 93 (26%) use bots. We classified the bots, collected metrics from before and after bot adoption, and surveyed 228 developers and integrators. Our results indicate that bots perform numerous tasks. Although integrators reported that bots are useful for maintenance tasks, we did not find a consistent, statistically significant difference between before and after bot adoption across the analyzed projects in terms of number of comments, commits, changed files, and time to close pull requests. Our survey respondents deem the current bots as not smart enough and provided insights into the bots' relevance for specific tasks, challenges, and potential new features. We discuss some of the raised suggestions and challenges in light of the literature in order to help GitHub bot designers reuse and test ideas and technologies already investigated in other contexts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {182},
numpages = {19},
keywords = {automated agents, bots, chatbots, open source software, pull request, pull-based model}
}

@inproceedings{10.1145/3524842.3528533,
author = {Wessel, Mairieli and Gerosa, Marco A. and Shihab, Emad},
title = {Software bots in software engineering: benefits and challenges},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528533},
doi = {10.1145/3524842.3528533},
abstract = {Software bots are becoming increasingly popular in software engineering (SE). In this tutorial, we define what a bot is and present several examples. We also discuss the many benefits bots provide to the SE community, including helping in development tasks (such as pull request review and integration) and onboarding newcomers to a project. Finally, we discuss the challenges related to interacting with and developing software bots.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {724–725},
numpages = {2},
keywords = {GitHub bots, automation, chatbots, collaborative development, human-bot interaction, open source software, software bots},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3387940.3391504,
author = {Wessel, Mairieli and Steinmacher, Igor},
title = {The Inconvenient Side of Software Bots on Pull Requests},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391504},
doi = {10.1145/3387940.3391504},
abstract = {Software bots are applications that integrate their work with humans' tasks, serving as conduits between users and other tools. Due to their ability to automate tasks, bots have been widely adopted by Open Source Software (OSS) projects hosted on GitHub. Commonly, OSS projects use bots to automate a variety of routine tasks to save time from maintainers and contributors. Although bots can be useful for supporting maintainers' work, sometimes their comments are seen as spams, and are quickly ignored by contributors. In fact, the way that these bots interact on pull requests can be disruptive and perceived as unwelcoming. In this paper, we propose the concept of a meta-bot to deal with current problems on the human-bot interaction on pull requests. Besides providing additional value to this interaction, meta-bot will reduce interruptions and help maintainers and contributors stay aware of important information.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {51–55},
numpages = {5},
keywords = {bots, meta-bot, open source software, pull-based model, software bots},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3379597.3387478,
author = {Dey, Tapajit and Mousavi, Sara and Ponce, Eduardo and Fry, Tanner and Vasilescu, Bogdan and Filippova, Anna and Mockus, Audris},
title = {Detecting and Characterizing Bots that Commit Code},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387478},
doi = {10.1145/3379597.3387478},
abstract = {Background: Some developer activity traditionally performed manually, such as making code commits, opening, managing, or closing issues is increasingly subject to automation in many OSS projects. Specifically, such activity is often performed by tools that react to events or run at specific times. We refer to such automation tools as bots and, in many software mining scenarios related to developer productivity or code quality, it is desirable to identify bots in order to separate their actions from actions of individuals. Aim: Find an automated way of identifying bots and code committed by these bots, and to characterize the types of bots based on their activity patterns. Method and Result: We propose BIMAN, a systematic approach to detect bots using author names, commit messages, files modified by the commit, and projects associated with the commits. For our test data, the value for AUC-ROC was 0.9. We also characterized these bots based on the time patterns of their code commits and the types of files modified, and found that they primarily work with documentation files and web pages, and these files are most prevalent in HTML and JavaScript ecosystems. We have compiled a shareable dataset containing detailed information about 461 bots we found (all of which have more than 1000 commits) and 13,762,430 commits they created.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {209–219},
numpages = {11},
keywords = {automated commits, bots, ensemble model, random forest, social coding platforms, software engineering},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/BotSE.2019.00008,
author = {Lebeuf, Carlene and Zagalsky, Alexey and Foucault, Matthieu and Storey, Margaret-Anne},
title = {Defining and classifying software bots: a faceted taxonomy},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00008},
doi = {10.1109/BotSE.2019.00008},
abstract = {While bots have been around for many decades, recent technological advancements and the increasing adoption of language-based communication platforms have led to a surge of new software bots, which have become increasingly pervasive in our everyday lives. Although many novel bots are being designed and deployed, the terms used to describe them and their properties are vast, diverse, and often inconsistent. Even the concept of what is or is not a bot is unclear. This hinders our ability to study, understand, design, and classify bots.In this paper, we present a taxonomy of software bots, which focuses on the observable properties and behaviours of software bots, as well as the environments where bots are deployed and designed. We see this taxonomy as a focal point for a discussion in our community so that together we can deeply consider how to evaluate and understand existing bots, as well as how we may design more innovative and productive bots.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {classification, software bots, software engineering, taxonomy},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@article{10.1145/3539814.3539823,
author = {Wagner, Stefan and Gerosa, Marco A. and Wessel, Mairieli},
title = {Summary of the Third International Workshop on Bots in Software Engineering (BotSE 2021)},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3539814.3539823},
doi = {10.1145/3539814.3539823},
abstract = {Bots automate tasks in software engineering projects and interact with software developers. Bots have been proposed, for example, for testing, maintenance, and automating bug fixes. The research community has been discussing these bots in the International Workshop on Bots in Software Engineering (BotSE), collocated with ICSE (the International Conference on Software Engineering). The workshop participants share experiences and challenges, discuss new usages of bots, and map out future directions. In this paper, we present a summary of the 3rd edition of the workshop, which comprised nine papers, one journal-first presentation, and two keynotes, followed by extensive discussion. More details can be found at http://botse.org/},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {25–27},
numpages = {3},
keywords = {automation, collaborative development, github bots, chatbots, human-bot interaction, open source software, software bots}
}

@inproceedings{10.1145/2851581.2892311,
author = {Murgia, Alessandro and Janssens, Daan and Demeyer, Serge and Vasilescu, Bogdan},
title = {Among the Machines: Human-Bot Interaction on Social Q&amp;A Websites},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892311},
doi = {10.1145/2851581.2892311},
abstract = {With the rise of social media and advancements in AI technology, human-bot interaction will soon be commonplace. In this paper we explore human-bot interaction in STACK OVERFLOW, a question and answer website for developers. For this purpose, we built a bot emulating an ordinary user answering questions concerning the resolution of git error messages. In a first run this bot impersonated a human, while in a second run the same bot revealed its machine identity. Despite being functionally identical, the two bot variants elicited quite different reactions.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1272–1279},
numpages = {8},
keywords = {social bot, stack overflow, turing test},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3306446.3340833,
author = {Farda-Sarbas, Mariam and Zhu, Hong and Nest, Marisa Frizzi and M\"{u}ller-Birn, Claudia},
title = {Approving automation: analyzing requests for permissions of bots in wikidata},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340833},
doi = {10.1145/3306446.3340833},
abstract = {Wikidata, initially developed to serve as a central structured knowledge base for Wikipedia, is now a melting point for structured data for companies, research projects and other peer production communities. Wikidata's community consists of humans and bots, and most edits in Wikidata come from these bots. Prior research has raised concerns regarding the challenges for editors to ensure the quality of bot-generated data, such as the lack of quality control and knowledge diversity. In this research work, we provide one way of tackling these challenges by taking a closer look at the approval process of bot activity on Wikidata. We collected all bot requests, i.e. requests for permissions (RfP) from October 2012 to July 2018. We analyzed these 683 bot requests by classifying them regarding activity focus, activity type, and source mentioned. Our results show that the majority of task requests deal with data additions to Wikidata from internal sources, especially from Wikipedia. However, we can also show the existing diversity of external sources used so far. Furthermore, we examined the reasons which caused the unsuccessful closing of RfPs. In some cases, the Wikidata community is reluctant to implement specific bots, even if they are urgently needed because there is still no agreement in the community regarding the technical implementation. This study can serve as a foundation for studies that connect the approved tasks with the editing behavior of bots on Wikidata to understand the role of bots better for quality control and knowledge diversity.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {15},
numpages = {10},
keywords = {Wikidata, bot, task approval},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1109/CHASE.2019.00021,
author = {Pinheiro, Andr\'{e} M. and Rabello, Caio S. and Furtado, Leonardo B. and Pinto, Gustavo and de Souza, Cleidson R. B.},
title = {Expecting the unexpected: distilling bot development, challenges, and motivations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2019.00021},
doi = {10.1109/CHASE.2019.00021},
abstract = {Software bots are becoming an increasingly popular tool in the software development landscape, which is particularly due to their potential of use in several different contexts. More importantly, software developers interested in transitioning to bot development may have to face challenges intrinsic related to bot software development. However, so far, it is still unclear what is the profile of bot developers, what motivate them, or what challenges do they face when dealing with bot development. To shed an initial light on this direction, we conducted a survey with 43 Github users who have been involved (showing their interest or actively contributing to) in bot software projects.},
booktitle = {Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {51–52},
numpages = {2},
location = {Montreal, Quebec, Canada},
series = {CHASE '19}
}

@inproceedings{10.1109/BotSE.2019.00009,
author = {Erlenhov, Linda and de Oliveira Neto, Francisco Gomes and Scandariato, Riccardo and Leitner, Philipp},
title = {Current and future bots in software development},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00009},
doi = {10.1109/BotSE.2019.00009},
abstract = {Bots that support software development ("DevBots") are seen as a promising approach to deal with the ever-increasing complexity of modern software engineering and development. Existing DevBots are already able to relieve developers from routine tasks such as building project images or keeping dependencies up-to-date. However, advances in machine learning and artificial intelligence hold the promise of future, significantly more advanced, DevBots. In this paper, we introduce the terminology of contemporary and ideal DevBots. Contemporary DevBots represent the current state of practice, which we characterise using a facet-based taxonomy. We exemplify this taxonomy using 11 existing, industrial-strength bots. We further provide a vision and definition of future (ideal) DevBots, which are not only autonomous, but also adaptive, as well as technically and socially competent. These properties may allow ideal DevBots to act more akin to artificial team mates than simple development tools.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {7–11},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3368089.3409680,
author = {Erlenhov, Linda and Neto, Francisco Gomes de Oliveira and Leitner, Philipp},
title = {An empirical study of bots in software development: characteristics and challenges from a practitioner’s perspective},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409680},
doi = {10.1145/3368089.3409680},
abstract = {Software engineering bots – automated tools that handle tedious tasks – are increasingly used by industrial and open source projects to improve developer productivity. Current research in this area is held back by a lack of consensus of what software engineering bots (DevBots) actually are, what characteristics distinguish them from other tools, and what benefits and challenges are associated with DevBot usage. In this paper we report on a mixed-method empirical study of DevBot usage in industrial practice. We report on findings from interviewing 21 and surveying a total of 111 developers. We identify three different personas among DevBot users (focusing on autonomy, chat interfaces, and “smartness”), each with different definitions of what a DevBot is, why developers use them, and what they struggle with.We conclude that future DevBot research should situate their work within our framework, to clearly identify what type of bot the work targets, and what advantages practitioners can expect. Further, we find that there currently is a lack of general purpose “smart” bots that go beyond simple automation tools or chat interfaces. This is problematic, as we have seen that such bots, if available, can have a transformative effect on the projects that use them.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {445–455},
numpages = {11},
keywords = {Empirical study, Software bot, Software engineering},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1109/BotSE.2019.00011,
author = {Subramanian, Venkatesh and Ramachandra, Nisha and Dubash, Neville},
title = {TutorBot: contextual learning guide for software engineers},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00011},
doi = {10.1109/BotSE.2019.00011},
abstract = {This document is poster submission on using conversational chat bot to guide a software engineer in their learning journey and keeping pace with the technology changes. We describe the motivation, technical approach, and experience of building, piloting such a Bot in a controlled setting and capturing the user feedback. The document also discusses future opportunities to extend and enhance the functionality.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {16–17},
numpages = {2},
keywords = {ASR, LMS, ML, MOOCs, NLP, bot, collaborative filtering, personalization, recommender},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3543873.3587612,
author = {Cernera, Federico and La Morgia, Massimo and Mei, Alessandro and Mongardini, Alberto Maria and Sassi, Francesco},
title = {Ready, Aim, Snipe! Analysis of Sniper Bots and their Impact on the DeFi Ecosystem},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587612},
doi = {10.1145/3543873.3587612},
abstract = {In the world of cryptocurrencies, public listing of a new token often generates significant hype, in many cases causing its price to skyrocket in a few seconds. In this scenario, timing is crucial to determine the success or failure of an investment opportunity. In this work, we present an in-depth analysis of sniper bots, automated tools designed to buy tokens as soon as they are listed on the market. We leverage GitHub open-source repositories of sniper bots to analyze their features and how they are implemented. Then, we build a dataset of Ethereum and BNB Smart Chain (BSC) liquidity pools to identify addresses that serially take advantage of sniper bots. Our findings reveal 14,029 sniping operations on Ethereum and 1,395,042 in BSC that bought tokens for a total of $10,144,808 dollars and $18,720,447, respectively. We find that Ethereum operations have a higher success rate but require a larger investment. Finally, we analyze token smart contracts to identify mechanisms that can hinder sniper bots.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1093–1102},
numpages = {10},
keywords = {AMM, BNB Smart Chain, Ethereum, Trading bots},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3419369,
author = {Mendoza, Marcelo and Tesconi, Maurizio and Cresci, Stefano},
title = {Bots in Social and Interaction Networks: Detection and Impact Estimation},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3419369},
doi = {10.1145/3419369},
abstract = {The rise of bots and their influence on social networks is a hot topic that has aroused the interest of many researchers. Despite the efforts to detect social bots, it is still difficult to distinguish them from legitimate users. Here, we propose a simple yet effective semi-supervised method that allows distinguishing between bots and legitimate users with high accuracy. The method learns a joint representation of social connections and interactions between users by leveraging graph-based representation learning. Then, on the proximity graph derived from user embeddings, a sample of bots is used as seeds for a label propagation algorithm. We demonstrate that when the label propagation is done according to pairwise account proximity, our method achieves F1 = 0.93, whereas other state-of-the-art techniques achieve F1 ≤ 0.87. By applying our method to a large dataset of retweets, we uncover the presence of different clusters of bots in the network of Twitter interactions. Interestingly, such clusters feature different degrees of integration with legitimate users. By analyzing the interactions produced by the different clusters of bots, our results suggest that a significant group of users was systematically exposed to content produced by bots and to interactions with bots, indicating the presence of a selective exposure phenomenon.},
journal = {ACM Trans. Inf. Syst.},
month = {oct},
articleno = {5},
numpages = {32},
keywords = {Social bots, disinformation, label propagation, selective exposure, semi-supervised bot detection, user embeddings}
}

@inproceedings{10.1145/3110025.3110163,
author = {Minnich, Amanda and Chavoshi, Nikan and Koutra, Danai and Mueen, Abdullah},
title = {BotWalk: Efficient Adaptive Exploration of Twitter Bot Networks},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110163},
doi = {10.1145/3110025.3110163},
abstract = {We propose BotWalk, a near-real time adaptive Twitter exploration algorithm to identify bots exhibiting novel behavior. Due to suspension pressure, Twitter bots are constantly changing their behavior to evade detection. Traditional supervised approaches to bot detection are non-adaptive and thus cannot identify novel bot behaviors. We therefore devise an unsupervised approach, which allows us to identify bots as they evolve. We characterize users with a behavioral feature vector which consists of (well-studied in isolation) metadata-, content-, temporal-, and network-based features. We identify a random bot from our seed bank, populated initially by previously-labeled bots, gather this user's followers' features from Twitter in real time, and employ an unsupervised ensemble anomaly detection method in the multi-dimensional behavioral space. These potential bots are folded into the seed bank and the process is then repeated, with the new seeds' features allowing us to adaptively identify novel bot behavior. BotWalk allows for the identification of on average 6,000 potential bots a day. Our method allowed us to detect 7,995 previously undiscovered bots from a sample of 15 seed bots with a precision of 90%.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {467–474},
numpages = {8},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@article{10.1145/3359317,
author = {Zheng, Lei (Nico) and Albano, Christopher M. and Vora, Neev M. and Mai, Feng and Nickerson, Jeffrey V.},
title = {The Roles Bots Play in Wikipedia},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359317},
doi = {10.1145/3359317},
abstract = {Bots are playing an increasingly important role in the creation of knowledge in Wikipedia. In many cases, editors and bots form tightly knit teams. Humans develop bots, argue for their approval, and maintain them, performing tasks such as monitoring activity, merging similar bots, splitting complex bots, and turning off malfunctioning bots. Yet this is not the entire picture. Bots are designed to perform certain functions and can acquire new functionality over time. They play particular roles in the editing process. Understanding these roles is an important step towards understanding the ecosystem, and designing better bots and interfaces between bots and humans. This is important for understanding Wikipedia along with other kinds of work in which autonomous machines affect tasks performed by humans. In this study, we use unsupervised learning to build a nine category taxonomy of bots based on their functions in English Wikipedia. We then build a multi-class classifier to classify 1,601 bots based on labeled data. We discuss different bot activities, including their edit frequency, their working spaces, and their software evolution. We use a model to investigate how bots playing certain roles will have differential effects on human editors. In particular, we build on previous research on newcomers by studying the relationship between the roles bots play, the interactions they have with newcomers, and the ensuing survival rate of the newcomers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {215},
numpages = {20},
keywords = {bots, governance, online communities, roles, taxonomy, wikipedia}
}

@inproceedings{10.1145/3589335.3651959,
author = {Niedermayer, Thomas and Saggese, Pietro and Haslhofer, Bernhard},
title = {Detecting Financial Bots on the Ethereum Blockchain},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651959},
doi = {10.1145/3589335.3651959},
abstract = {The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%. Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2024},
pages = {1742–1751},
numpages = {10},
keywords = {blockchain, bots, defi, ethereum, machine learning},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3665326,
author = {van Berkel, Niels and Pohl, Henning},
title = {Collaborating with Bots and Automation on OpenStreetMap},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1073-0516},
url = {https://doi.org/10.1145/3665326},
doi = {10.1145/3665326},
abstract = {OpenStreetMap (OSM) is a large online community where users collaborate to map the world. In addition to manual edits, the OSM mapping database is regularly modified by bots and automated edits. In this paper, we seek to better understand how people and bots interact and conflict with each other. We start by analysing over 15 years of mailing list discussions related to bots and automated edits. From this data, we uncover five themes, including how automation results in power differentials between users and how community ideals of consensus clash with the realities of bot use. Subsequently, we surveyed OSM contributors on their experiences with bots and automated edits. We present findings about the current escalation and review mechanisms, as well as the lack of appropriate tools for evaluating and discussing bots. We discuss how OSM and similar communities could use these findings to better support collaboration between humans and bots.},
note = {Just Accepted},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {may},
keywords = {OpenStreetMap, Bots, Automated edits, Automation, Collaboration}
}

@inproceedings{10.1145/3324884.3415295,
author = {Khanan, Chaiyakarn and Luewichana, Worawit and Pruktharathikoon, Krissakorn and Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee},
title = {JITBot: an explainable just-in-time defect prediction bot},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415295},
doi = {10.1145/3324884.3415295},
abstract = {Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explainability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1336–1339},
numpages = {4},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3579375.3579400,
author = {Javed, Muhammad and Dimaguila, Gerardo Luis and Habibabadi, Sedigh Khademi and Palmer, Chris and Buttery, Jim},
title = {Learning from Machines? Social Bots Influence on COVID-19 Vaccination-Related Discussions: 2021 in Review},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579400},
doi = {10.1145/3579375.3579400},
abstract = {The World Health Organization defines vaccine hesitancy as a delay in acceptance or refusal of vaccination despite the availability of vaccination services. Vaccine hesitancy contributes to lower rates of vaccination in a population and delayed vaccine coverage. A large number of COVID-19 vaccines have been administered worldwide against COVID-19. Due to concerns people have about COVID-19 vaccine adverse events, a significant proportion of people exhibit hesitancy towards the vaccines. These are often prompted by information and misinformation spread through social media conversation, which is not driven exclusively by genuine human-run accounts. Social bots have been shown to be very active during the pandemic participating in discussions about vaccines, including the spread of conflicting and misleading information.Using a novel ensemble technique, we sought to identify and describe the involvement of social bots in COVID-19 vaccination-related discussions on Twitter and how this could have influenced sentiments and hesitancies about COVID-19 vaccines. We included tweets from January to December 2021 to present a whole year's analysis in relation to the vaccines. Unique usernames from these posts were passed to Botometer and Tweetbotornot, programs that review Twitter accounts, to detect a broad range of social bots using a scoring system. A domain-oriented transfer learning technique is applied by finetuning the CT-BERT V2 model to detect the influence of social bots on COVID-19 vaccine sentiments. We computed the ratio of sentiment transmission from bots-to-human, human-to-human, human-to-bots, and bots-to-bots. BERTopic was used to extract the topics of discussion to identify the amplified or transferred hesitancies.Social bots’ participation in online discussions noticeably influenced human sentiments and hesitancies about COVID-19 vaccination. A major portion of sentiments transferred from bot to human during the period of study appeared to amplify or transfer hesitancies regarding COVID-19 vaccination.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {190–197},
numpages = {8},
keywords = {COVID-19 vaccine, Social Bots, Social Media, Transfer Learning, Vaccine Hesitancy},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@inproceedings{10.1145/3634737.3644998,
author = {Akhtar, Mohammad Majid and Masood, Rahat and Ikram, Muhammad and Kanhere, Salil S},
title = {SoK: False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3644998},
doi = {10.1145/3634737.3644998},
abstract = {The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics concerning false information detection have been overlooked in previous research.To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM element. Through a systematic investigation of prior research, we identify commonalities, highlight existing gaps, and extract valuable insights in the field.Our findings underscore the urgent need for interdisciplinary research to effectively combat social media manipulations, and our systematization can guide future research efforts and assist OSN providers in ensuring the safety and integrity of their platforms.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1784–1800},
numpages = {17},
keywords = {social media manipulation, false information, bots, user psychology, online social networks (OSN), OSN safety},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3611643.3616248,
author = {Asthana, Sumit and Sajnani, Hitesh and Voyloshnikova, Elena and Acharya, Birendra and Herzig, Kim},
title = {A Case Study of Developer Bots: Motivations, Perceptions, and Challenges},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616248},
doi = {10.1145/3611643.3616248},
abstract = {Continuous integration and deployment (CI/CD) is now a widely adopted development model in practice as it reduces the time from ideas to customers. This adoption has also revived the idea of "shifting left" during software development -- a practice intended to find and prevent defects early in the software delivery process. To assist with that, engineering systems integrate developer bots in the development workflow to improve developer productivity and help them identify issues early in the software delivery process.  

In this paper, we present a case study of developer bots in Microsoft. We identify and analyze 23 developer bots that are deployed across 13,000 repositories and assist about 6,000 developers daily in their CI/CD software development workflows. We classify these bots across five major categories: Config Violation, Security, Data-privacy, Developer Productivity, and Code Quality. By conducting interviews and surveys with bot developers and bot users and by analyzing about half a million historical bot actions spanning over one and a half years, we present software workflows that motivate bot instrumentation, factors impacting their usefulness as perceived by bot users, and challenges associated with their use. Our findings echo existing issues with bots, such as noise, and illustrate new benefits (e.g., cross-team communication) and challenges (e.g., too many bots) for large software teams.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1268–1280},
numpages = {13},
keywords = {devbots, developer productivity, recommendations, software maintenance, software quality},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/BotSE.2019.00020,
author = {Kumar, Rahul and Bansal, Chetan and Maddila, Chandra and Sharma, Nitin and Martelock, Shawn and Bhargava, Ravi},
title = {Building sankie: an AI platform for DevOps},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00020},
doi = {10.1109/BotSE.2019.00020},
abstract = {There has been a fundamental shift amongst software developers and engineers in the past few years. The software development life cycle (SDLC) for a developer has increased in complexity and scale. Changes that were developed and deployed over a matter of days or weeks are now deployed in a matter of hours. Due to greater availability of compute, storage, better tooling, and the necessity to react, developers are constantly looking to increase their velocity and throughput of developing and deploying changes. Consequently, there is a great need for more intelligent and context sensitive DevOps tools and services that help developers increase their efficiency while developing and debugging. Given the vast amounts of heterogeneous data available from the SDLC, such intelligent tools and services can now be built and deployed at a large scale to help developers achieve their goals and be more productive. In this paper, we present Sankie, a scalable and general service that has been developed to assist and impact all stages of the modern SDLC. Sankie provides all the necessary infrastructure (back-end and front-end bots) to ingest data from repositories and services, train models based on the data, and eventually perform decorations or provide information to engineers to help increase the velocity and throughput of changes, bug fixes etc. This paper discusses the architecture as well as some of the key observations we have made from wide scale deployment of Sankie within Microsoft.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {48–53},
numpages = {6},
keywords = {Azure, DevOps, bot, empirical software engineering, infrastructure, machine learning, pull request, scale, software development life cycle},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3586102.3586104,
author = {Barkworth, Ashley and Tabassum, Rehnuma and Habibi Lashkari, Arash},
title = {Detecting IMAP Credential Stuffing Bots Using Behavioural Biometrics},
year = {2023},
isbn = {9781450397520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586102.3586104},
doi = {10.1145/3586102.3586104},
abstract = {Credential stuffing has seen a great uptick in use and is now one of the most common types of cyberattacks. Legacy email protocols like the Internet Mail Access Protocol (IMAP) are particularly vulnerable to this kind of attack as they do not support multi-factor authentication (MFA). We propose a supervised learning system that detects credential stuffing bots using two kinds of behavioural biometrics: mouse and keystroke dynamics. The system records a user’s mouse and keystroke events while they complete three tasks in a graphical user interface (GUI) application. We also developed two types of bots: a simple bot and an advanced bot, the latter of which uses techniques to simulate human-like mouse and keyboard motions. We evaluated our system using the Random Forest (RF), decision tree (DT), support vector machine (SVM), and k-nearest neighbors (KNN) algorithms and compared them against two data sets: Simple containing human and simple bot data, and Advanced containing human and advanced bot data. The highest accuracy against the Simple and Advanced data sets were both 96.95% achieved by the KNN and RF classifiers, respectively. The RF classifier showed the best overall performance, achieving the highest precision and mean AUC against the Simple data set and the highest scores across all metrics against the Advanced data set.},
booktitle = {Proceedings of the 2022 12th International Conference on Communication and Network Security},
pages = {7–15},
numpages = {9},
keywords = {behavioural analytics, email security, intrusion prevention systems},
location = {Beijing, China},
series = {ICCNS '22}
}

@inproceedings{10.1109/BotSE.2019.00021,
author = {Brown, Chris and Parnin, Chris},
title = {Sorry to bother you: designing bots for effective recommendations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00021},
doi = {10.1109/BotSE.2019.00021},
abstract = {Bots have been proposed as a way to encourage developer actions and support software development activities. Many bots make recommendations to users, however humans may find these recommendations ineffective or problematic. In this paper, we argue that while bots can help automate many tasks, ultimately bots still need to find ways to interact with humans and handle all of the associated social and cognitive problems entailed. To illustrate this problem, we performed a small study where we generated 52 pull requests making tool recommendation to developers. As a result, we only convinced two developers to accept the pull request, while receiving several forms of feedback on why the pull request was ineffective. We summarize this feedback and suggest design principles for bot recommendations, including how psychology frameworks, such as nudge theory, can be used to improve human-bot interactions.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {54–58},
numpages = {5},
keywords = {developer actions, digital nudge, software engineering, tool adoption},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3377811.3380412,
author = {Zhou, Shurui and Vasilescu, Bogdan and K\"{a}stner, Christian},
title = {How has forking changed in the last 20 years? a study of hard forks on GitHub},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380412},
doi = {10.1145/3377811.3380412},
abstract = {The notion of forking has changed with the rise of distributed version control systems and social coding environments, like GitHub. Traditionally forking refers to splitting off an independent development branch (which we call hard forks); research on hard forks, conducted mostly in pre-GitHub days showed that hard forks were often seen critical as they may fragment a community Today, in social coding environments, open-source developers are encouraged to fork a project in order to contribute to the community (which we call social forks), which may have also influenced perceptions and practices around hard forks. To revisit hard forks, we identify, study, and classify 15,306 hard forks on GitHub and interview 18 owners of hard forks or forked repositories. We find that, among others, hard forks often evolve out of social forks rather than being planned deliberately and that perception about hard forks have indeed changed dramatically, seeing them often as a positive noncompetitive alternative to the original project.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {445–456},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/3382225.3382263,
author = {Chavoshi, Nikan and Mueen, Abdullah},
title = {Model bots, not humans on social media},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {The Posting schedule reveals characteristic patterns of users on social media. Motivated by this knowledge, several researchers have modeled posting schedules and argued that deviation from the model indicates bot or spammer characteristics. It is true that circadian rhythms induce regularity in human posting behavior; however, in this paper, we show that this regularity is an individual trait and insufficient to develop a generic model. More surprisingly, we show that bots are more structured in their posting behaviors compared to humans by using a Convolutional Neural Network (CNN).More precisely, we demonstrate using Class Activation Maps that bots contain less entropy than humans. Thus, we conclude that bots are more amenable to generic models than humans. We evaluate the hypothesis on more than 32 million posts from 12 thousand Twitter users with 97% accuracy.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {178–185},
numpages = {8},
keywords = {CNN, automated accounts, classification, social media, temporal analysis},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3340531.3412690,
author = {Yao, Tianjun and Li, Qing and Liang, Shangsong and Zhu, Yadong},
title = {BotSpot: A Hybrid Learning Framework to Uncover Bot Install Fraud in Mobile Advertising},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412690},
doi = {10.1145/3340531.3412690},
abstract = {Mobile advertising has become inarguably one of the fastest growing industries all over the world. The influx of capital attracts increasing fraudsters to defraud money from advertisers. There are many tricks a fraudster can leverage, among which bot install fraud is undoubtedly the most insidious one due to its ability to implement sophisticated behavioral patterns and emulate normal users, so as to evade from detection rules defined by human experts. In this work, we propose an anti-fraud method based on heterogeneous graph that incorporates both local context and global context via graph neural networks (GNN) and gradient boosting classifier to detect bot fraud installs at Mobvista, a leading global mobile advertising company. Offline evaluations in two datasets show the proposed method outperforms all the competitive baseline methods by at least 2.2% in the first dataset and 5.75% in the second dataset given the evaluation metric Recall@90% Precision. Furthermore, we deploy our method to tackle million-scale data daily at Mobvista. The online performance also shows that the proposed methods consistently detect more bots than other baseline methods.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2901–2908},
numpages = {8},
keywords = {ad fraud detection, deep ensemble learning, deep learning, graph neural networks, mobile advertising},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1109/ICSE-NIER.2017.17,
author = {Beschastnikh, Ivan and Lungu, Mircea F. and Zhuang, Yanyan},
title = {Accelerating software engineering research adoption with analysis bots},
year = {2017},
isbn = {9781538626757},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2017.17},
doi = {10.1109/ICSE-NIER.2017.17},
abstract = {An important part of software engineering (SE) research is to develop new analysis techniques and to integrate these techniques into software development practice. However, since access to developers is non-trivial and research tool adoption is slow, new analyses are typically evaluated as follows: a prototype tool that embeds the analysis is implemented, a set of projects is identified, their revisions are selected, and the tool is run in a controlled environment, rarely involving the developers of the software. As a result, research artifacts are brittle and it is unclear if an analysis tool would actually be adopted.In this paper, we envision harnessing the rich interfaces provided by popular social coding platforms for automated deployment and evaluation of SE research analysis. We propose that SE analyses can be deployed as analysis bots. We focus on two specific benefits of such an approach: (1) analysis bots can help evaluate analysis techniques in a less controlled, and more realistic context, and (2) analysis bots provide an interface for developers to "subscribe" to new research techniques without needing to trust the implementation, the developer of the new tool, or to install the analysis tool locally. We outline basic requirements for an analysis bots platform, and present research challenges that would need to be resolved for bots to flourish.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track},
pages = {35–38},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {ICSE-NIER '17}
}

@article{10.1145/3437479.3437484,
author = {Shihab, Emad and Wagner, Stefan and Aur\'{e}lio Gerosa, Marco},
title = {Summary of the 2nd International Workshop on Bots in Software Engineering (BotSE 2020)},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3437479.3437484},
doi = {10.1145/3437479.3437484},
abstract = {Bots automate many tasks in software engineering projects often in the form of chatbots. Bots have been proposed, for example, for testing, maintenance, or automating bug fixes. Following the success of the first BotSE workshop, we organized this second edition collocated with ICSE 2020 to bring together the research community that investigates bots for software engineering. Specifically, the workshop's goal was to share experiences and challenges, discuss new types of bots, and map out future directions. The workshop program comprised the presentation of 8 papers and 2 keynotes, followed by extensive discussion. Overall, the community matured by discussing how to design, build, and evaluate bots. The community aims to organise a 3rd edition of the workshop. Website: http://botse.org/},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {20–22},
numpages = {3}
}

@article{10.1145/3610092,
author = {Hsieh, Jane and Kim, Joselyn and Dabbish, Laura and Zhu, Haiyi},
title = {"Nip it in the Bud": Moderation Strategies in Open Source Software Projects and the Role of Bots},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610092},
doi = {10.1145/3610092},
abstract = {Much of our modern digital infrastructure relies critically upon open sourced software. The communities responsible for building this cyberinfrastructure require maintenance and moderation, which is often supported by volunteer efforts. Moderation, as a non-technical form of labor, is a necessary but often overlooked task that maintainers undertake to sustain the community around an OSS project. This study examines the various structures and norms that support community moderation, describes the strategies moderators use to mitigate conflicts, and assesses how bots can play a role in assisting these processes. We interviewed 14 practitioners to uncover existing moderation practices and ways that automation can provide assistance. Our main contributions include a characterization of moderated content in OSS projects, moderation techniques, as well as perceptions of and recommendations for improving the automation of moderation tasks. We hope that these findings will inform the implementation of more effective moderation practices in open source communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {301},
numpages = {29},
keywords = {automation, coordination, moderation, open source}
}

@article{10.1145/3592981,
author = {Denning, Peter J.},
title = {Can Generative AI Bots Be Trusted?},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3592981},
doi = {10.1145/3592981},
abstract = {It will be a long road to learning how to use generative AI wisely.},
journal = {Commun. ACM},
month = {may},
pages = {24–27},
numpages = {4}
}

@inproceedings{10.1109/ICSE.2019.00060,
author = {Kavaler, David and Trockman, Asher and Vasilescu, Bogdan and Filkov, Vladimir},
title = {Tool choice matters: JavaScript quality assurance tools and usage outcomes in GitHub projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00060},
doi = {10.1109/ICSE.2019.00060},
abstract = {Quality assurance automation is essential in modern software development. In practice, this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context. Data and analytics of the pros and cons can inform these decisions. Yet, in most cases, there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices.We propose a general methodology to model the time-dependent effect of automation tool choice on four outcomes of interest: prevalence of issues, code churn, number of pull requests, and number of contributors, all with a multitude of controls. On a large data set of npm JavaScript projects, we extract the adoption events for popular tools in three task classes: linters, dependency managers, and coverage reporters. Using mixed methods approaches, we study the reasons for the adoptions and compare the adoption effects within each class, and sequential tool adoptions across classes. We find that some tools within each group are associated with more beneficial outcomes than others, providing an empirical perspective for the benefits of each. We also find that the order in which some tools are implemented is associated with varying outcomes.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {476–487},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3134684,
author = {Geiger, R. Stuart and Halfaker, Aaron},
title = {Operationalizing Conflict and Cooperation between Automated Software Agents in Wikipedia: A Replication and Expansion of 'Even Good Bots Fight'},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134684},
doi = {10.1145/3134684},
abstract = {This paper replicates, extends, and refutes conclusions made in a study published in PLoS ONE ("Even Good Bots Fight"), which claimed to identify substantial levels of conflict between automated software agents (or bots) in Wikipedia using purely quantitative methods. By applying an integrative mixed-methods approach drawing on trace ethnography, we place these alleged cases of bot-bot conflict into context and arrive at a better understanding of these interactions. We found that overwhelmingly, the interactions previously characterized as problematic instances of conflict are typically better characterized as routine, productive, even collaborative work. These results challenge past work and show the importance of qualitative/quantitative collaboration. In our paper, we present quantitative metrics and qualitative heuristics for operationalizing bot-bot conflict. We give thick descriptions of kinds of events that present as bot-bot reverts, helping distinguish conflict from non-conflict. We computationally classify these kinds of events through patterns in edit summaries. By interpreting found/trace data in the socio-technical contexts in which people give that data meaning, we gain more from quantitative measurements, drawing deeper understandings about the governance of algorithmic systems in Wikipedia. We have also released our data collection, processing, and analysis pipeline, to facilitate computational reproducibility of our findings and to help other researchers interested in conducting similar mixed-method scholarship in other platforms and contexts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {dec},
articleno = {49},
numpages = {33},
keywords = {algorithms, automation, bots, conflict, critical data studies, mixed methods, open science, peer production, replication, reproducibility, trace ethnography, wikipedia, wikis}
}

@article{10.1145/3629132,
author = {Jain, Vivek and Alam, S M Maksudul and Krishnamurthy, Srikanth V. and Faloutsos, Michalis},
title = {C2Store: C2 Server Profiles at Your Fingertips},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CoNEXT3},
url = {https://doi.org/10.1145/3629132},
doi = {10.1145/3629132},
abstract = {How can we build a definitive capability for tracking C2 servers? Having a large-scale continuously updating capability would be essential for understanding the spatiotemporal behaviors of C2 servers and, ultimately, for helping contain botnet activities. Unfortunately, existing information from threat intelligence feeds and previous works is often limited to a specific set of botnet families or short-term data collections. Responding to this need, we present C2Store, an initiative to provide the most comprehensive information on C2 servers. Our work makes the following contributions: (a) we develop techniques to collect, verify, and combine C2 server addresses from five types of sources, including uncommon platforms, such as GitHub and Twitter; (b) we create an open-access annotated database of 335,967 C2 servers across 133 malware families, which supports semantically-rich and smart queries; (c) we identify surprising behaviors of C2 servers with respect to their spatiotemporal patterns and behaviors. First, we successfully mine Twitter and GitHub and identify C2 servers with a precision of 97% and 94%, respectively. Furthermore, we find that the threat feeds identify only 24% of the servers in our database, with Twitter and GitHub providing 32%. A surprising observation is the identification of 250 IP addresses, each of which hosts more than 5 C2 servers for different botnet families at the same time. Overall, we envision C2Store as an ongoing effort that will facilitate research by providing timely, historical, and comprehensive C2 server information by critically combining multiple sources of information.},
journal = {Proc. ACM Netw.},
month = {nov},
articleno = {10},
numpages = {21},
keywords = {C2 servers, GitHub, Twitter, botnet}
}

@inproceedings{10.1145/3475716.3475778,
author = {Klotzman, Vanessa and Farmahinifarahani, Farima and Lopes, Cristina},
title = {Public Software Development Activity During the Pandemic},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475778},
doi = {10.1145/3475716.3475778},
abstract = {Background The emergence of the COVID-19 pandemic has impacted all human activity, including software development. Early reports seem to indicate that the pandemic may have had a negative effect on software developers, socially and personally, but that their software development productivity may not have been negatively impacted. Aims: Early reports about the effects of the pandemic on software development focused on software developers' well-being and on their productivity as employees. We are interested in a different aspect of software development: the developers' public contributions, as seen in GitHub and Stack Overflow activities. Did the pandemic affect the developers' public contributions and, of so, in what way? Method: Considering the data from between 2017 and till 2020, we study the trends within GitHub's push, create, pull request, and release events, and within Stack Overflow's new users, posts, votes, and comments. We performed linear regressions, correlation analyses, outlier analyses, hypothesis testing, and we also contacted individual developers in order to gather qualitative insights about their unusual public contributions. Results: Our study shows that within GitHub and Stack Overflow, the onset of the pandemic (March/April 2020) is reflected in a set of outliers in developers' contributions that point to an increase in activity. The distributions of contributions during the entire year of 2020 were, in some aspects, different, but, in other aspects, similar from the recent past. Additionally, we found one noticeably disrupted pattern of contribution in Stack Overflow, namely the ratio Questions/Answers, which was much higher in 2020 than before. Testimonials from the developers we contacted were mixed: while some developers reported that their increase in activity was due to the pandemic, others reported that it was not. Conclusion: In Github, there was a noticeable increase in public software development activity in 2020, as well as more abrupt changes in daily activities; in Stack Overflow, there was a noticeable increase in new users and new questions at the onset of the pandemic, and in the ratio of Questions/Answers during 2020. The results may be attributed to the pandemic, but other factors could have come into play.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {18},
numpages = {12},
keywords = {Developer activity, GitHub, Pandemic, Stack Overflow},
location = {Bari, Italy},
series = {ESEM '21}
}

@article{10.1145/3359224,
author = {Qiu, Huilian Sophie and Li, Yucen Lily and Padala, Susmita and Sarma, Anita and Vasilescu, Bogdan},
title = {The Signals that Potential Contributors Look for When Choosing Open-source Projects},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359224},
doi = {10.1145/3359224},
abstract = {While open-source software has become ubiquitous, its sustainability is in question: without a constant supply of contributor effort, open-source projects are at risk. While prior work has extensively studied the motivations of open-source contributors in general, relatively little is known about how people choose which project to contribute to, beyond personal interest. This question is especially relevant in transparent social coding environments like GitHub, where visible cues on personal profile and repository pages, known as signals, are known to impact impression formation and decision making. In this paper, we report on a mixed-methods empirical study of the signals that influence the contributors' decision to join a GitHub project. We first interviewed 15 GitHub contributors about their project evaluation processes and identified the important signals they used, including the structure of the README and the amount of recent activity. Then, we proceeded quantitatively to test out the impact of each signal based on the data of 9,977 GitHub projects. We reveal that many important pieces of information lack easily observable signals, and that some signals may be both attractive and unattractive. Our findings have direct implications for open-source maintainers and the design of social coding environments, e.g., features to be added to facilitate better project searching experience.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {122},
numpages = {29},
keywords = {github, open-source software, signaling theory}
}

@article{10.1145/3359176,
author = {Muri\'{c}, Goran and Abeliuk, Andres and Lerman, Kristina and Ferrara, Emilio},
title = {Collaboration Drives Individual Productivity},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359176},
doi = {10.1145/3359176},
abstract = {How does the number of collaborators affect individual productivity? Results of prior research have been conflicting, with some studies reporting an increase in individual productivity as the number of collaborators grows, while other studies showing that the free-rider effect skews the effort invested by individuals, making larger groups less productive. The difference between these schools of thought is substantial: if a super-scaling effect exists, as suggested by former studies, then as groups grow, their productivity will increase even faster than their size, super-linearly improving their efficiency. We address this question by studying two planetary-scale collaborative systems: GitHub and Wikipedia. By analyzing the activity of over 2 million users on these platforms, we discover that the interplay between group size and productivity exhibits complex, previously-unobserved dynamics: the productivity of smaller groups scales super-linearly with group size, but saturates at larger sizes. This effect is not an artifact of the heterogeneity of productivity: the relation between group size and productivity holds at the individual level. People tend to do more when collaborating with more people. We propose a generative model of individual productivity that captures the non-linearity in collaboration effort. The proposed model is able to explain and predict group work dynamics in GitHub and Wikipedia by capturing their maximally informative behavioral features, and it paves the way for a principled, data-driven science of collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {74},
numpages = {24},
keywords = {collaboration, github, productivity, software development, teamwork, wikipedia}
}

@inproceedings{10.1145/3524610.3527920,
author = {Chakroborti, Debasish and Schneider, Kevin A. and Roy, Chanchal K.},
title = {Backports: change types, challenges and strategies},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527920},
doi = {10.1145/3524610.3527920},
abstract = {Source code repositories allow developers to manage multiple versions (or branches) of a software system. Pull-requests are used to modify a branch, and backporting is a regular activity used to port changes from a current development branch to other versions. In open-source software, backports are common and often need to be adapted by hand, which motivates us to explore backports and backporting challenges and strategies. In our exploration of 68,424 backports from 10 GitHub projects, we found that bug, test, document, and feature changes are commonly backported. We identified a number of backporting challenges, including that backports were inconsistently linked to their original pull-request (49%), that backports had incompatible code (13%), that backports failed to be accepted (10%), and that there were backporting delays (16 days to create, 5 days to merge). We identified some general strategies for addressing backporting issues. We also noted that backporting strategies depend on the project type and that further investigation is needed to determine their suitability. Furthermore, we created the first-ever backports dataset that can be used by other researchers and practitioners for investigating backports and backporting.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {636–647},
numpages = {12},
keywords = {GitHub, backport, branches, port, pull-request},
location = {Virtual Event},
series = {ICPC '22}
}

@inproceedings{10.1145/3433174.3433587,
author = {Chousein, Zeynep and Tetik, Haci Yakup and Sa\u{g}lam, Rahime Belen and B\"{u}lb\"{u}l, Abdullah and Li, Shujun},
title = {Tension between GDPR and Public Blockchains: A Data-Driven Analysis of Online Discussions},
year = {2021},
isbn = {9781450387514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433174.3433587},
doi = {10.1145/3433174.3433587},
abstract = {Since coming into effect in May 2018, the EU General Data Protection Regulation (GDPR) has raised serious concerns among users of public (permissionless) blockchain systems. Such concerns are triggered by a tension between some unique characteristics of public blockchain systems and some new data subject rights introduced in the GDPR, e.g., the data immutability and the “right to erasure” (a.k.a.&nbsp;“the right to be forgotten”). The aim of this work is to understand how service providers and developers behind public blockchain systems have communicated about such GDPR-related challenges to their users and how the users have perceived such GDPR-related issues. To this end, for 50 public blockchain systems whose corresponding cryptocurrency had a capital market size over $150 million, we analyzed relevant communications and discussions on the following three online channels: blog and forums posts, GitHub repositories, and discussions on Twitter. Our results show that service providers and developers of the selected public blockchain systems did not play an active role in GDPR-related online discussions on Twitter. They also did not communicate with their users about GDPR on their forums and blogs frequently, where we could identify only 56 posts out of 17,821 posts for the period we studied. Our study also reveals that only an extreme minority of the studied systems (4) mentioned GDPR in their GitHub repositories. Our work adds new evidence on the lack of transparency and active communications of the public blockchain sector on the challenging GDPR compliance issue of public blockchain systems.},
booktitle = {13th International Conference on Security of Information and Networks},
articleno = {17},
numpages = {8},
keywords = {GDPR, GitHub, NLP, Twitter, blockchain, blog, classification, data protection, distributed ledger, law, machine learning, natural language processing, online forum, permissionless, privacy, topic modeling},
location = {Merkez, Turkey},
series = {SIN 2020}
}

@inproceedings{10.1145/3387940.3391506,
author = {Brown, Chris and Parnin, Chris},
title = {Sorry to Bother You Again: Developer Recommendation Choice Architectures for Designing Effective Bots},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391506},
doi = {10.1145/3387940.3391506},
abstract = {Software robots, or bots, are useful for automating a wide variety of programming and software development tasks. Despite the advantages of using bots throughout the software engineering process, research shows that developers often face challenges interacting with these systems. To improve automated developer recommendations from bots, this work introduces developer recommendation choice architectures. Choice architecture is a behavioral science concept that suggests the presentation of options impacts the decisions humans make. To evaluate the impact of framing recommendations for software engineers, we examine the impact of one choice architecture, actionability, for improving the design of bot recommendations. We present the results of a preliminary study evaluating this choice architecture in a bot and provide implications for integrating choice architecture into the design of future software engineering bots.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {56–60},
numpages = {5},
keywords = {choice architecture, developer behavior, recommendations, software engineering},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3131704.3131718,
author = {Yang, Cheng and Zhang, Xunhui and Zeng, Lingbin and Fan, Qiang and Yin, Gang and Wang, Huaimin},
title = {An Empirical Study of Reviewer Recommendation in Pull-based Development Model},
year = {2017},
isbn = {9781450353137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131704.3131718},
doi = {10.1145/3131704.3131718},
abstract = {Code review is an important process to reduce code defects and improve software quality. However, in social coding communities using the pull-based model, everyone can submit code changes, which increases the required code review efforts. Therefore, there is a great need of knowing the process of code review and analyzing the pre-existing reviewer recommendation algorithms. In this paper, we do an empirical study about the PRs and their reviewers in Rails project. Moreover, we reproduce a popular and effective IR-based code reviewer recommendation algorithm and validate it on our dataset which contains 16,049 PRs. We find that the inactive reviewers are very important to code reviewing process, however, the pre-existing method's recommendation result strongly depends on the activeness of reviewers.},
booktitle = {Proceedings of the 9th Asia-Pacific Symposium on Internetware},
articleno = {14},
numpages = {6},
keywords = {GitHub, code reviewer recommendation, pull request},
location = {Shanghai, China},
series = {Internetware '17}
}

@article{10.1145/3579639,
author = {Rahman, Akond and Shamim, Shazibul Islam and Bose, Dibyendu Brinto and Pandita, Rahul},
title = {Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3579639},
doi = {10.1145/3579639},
abstract = {Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters.Objective: The goal of this paper is to help practitioners secure their Kubernetes clusters by identifying security misconfigurations that occur in Kubernetes manifests.Methodology: We conduct an empirical study with 2,039 Kubernetes manifests mined from 92 open-source software repositories to systematically characterize security misconfigurations in Kubernetes manifests. We also construct a static analysis tool called Security Linter for Kubernetes Manifests (SLI-KUBE) to quantify the frequency of the identified security misconfigurations.Results: In all, we identify 11 categories of security misconfigurations, such as absent resource limit, absent securityContext, and activation of hostIPC. Specifically, we identify 1,051 security misconfigurations in 2,039 manifests. We also observe the identified security misconfigurations affect entities that perform mesh-related load balancing, as well as provision pods and stateful applications. Furthermore, practitioners agreed to fix 60% of 10 misconfigurations reported by us.Conclusion: Our empirical study shows Kubernetes manifests to include security misconfigurations, which necessitates security-focused code reviews and application of static analysis when Kubernetes manifests are developed.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {99},
numpages = {36},
keywords = {Configuration, container orchestration, devops, devsecops, empirical study, Kubernetes, misconfiguration, security}
}

@inproceedings{10.1145/3528228.3528404,
author = {Park, Doje and Cho, Heetae and Lee, Seonah},
title = {Classifying issues into custom labels in GitBot},
year = {2022},
isbn = {9781450393331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528228.3528404},
doi = {10.1145/3528228.3528404},
abstract = {GitBots are bots in Git repositories to automate repetitive tasks that occur in software development, testing and maintenance. GitBots are expected to perform the repetitive tasks that are normally done by humans, such as feedback on issue reports and answers to questions. However, studies on GitBots for labeling issue reports fall short of replacing developers' labeling tasks. Developers still manually attach labels to issues. In this paper, we introduce an issue labeling bot classifying issue reports into custom labels that developers define by themselves so that our bot could attach labels in a similar way to human behavior.},
booktitle = {Proceedings of the Fourth International Workshop on Bots in Software Engineering},
pages = {28–32},
numpages = {5},
keywords = {FastText, GitBot, classification, custom label, issue report},
location = {Pittsburgh, Pennsylvania},
series = {BotSE '22}
}

@inproceedings{10.1145/3125433.3125468,
author = {Squire, Megan},
title = {The Lives and Deaths of Open Source Code Forges},
year = {2017},
isbn = {9781450351874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125433.3125468},
doi = {10.1145/3125433.3125468},
abstract = {Code forges are third party software repositories that also provide various tools and facilities for distributed software development teams to use, including source code control systems, mailing lists and communication forums, bug tracking systems, web hosting space, and so on. The main contributions of this paper are to present some new data sets relating to the technology adoption lifecycles of a group of six free, libre, and open source software (FLOSS) code forges, and to compare the lifecycles of the forges to each other and to the model presented by classical Diffusion of Innovation (DoI) theory. We find that the observed adoption patterns of code forges rarely follow the DoI model, especially as larger code forges are beset by spam and abuse. The only forge exhibiting a DoI-like lifecycle was a smaller, community-managed, special-purpose forge whose demise was planned in advance. The results of this study will be useful in explaining adoption trajectories, both to practitioners building collaborative FLOSS ecosystems and to researchers who study the evolution and adoption of socio-technical systems.},
booktitle = {Proceedings of the 13th International Symposium on Open Collaboration},
articleno = {15},
numpages = {8},
keywords = {CodePlex, FLOSS, GitHub, Google Code, ObjectWeb, Open source, RubyForge, SourceForge, code forge, diffusion of innovations, free software, software evolution, technology adoption},
location = {Galway, Ireland},
series = {OpenSym '17}
}

@inproceedings{10.1145/3623762.3633499,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {The Robots Are Here: Navigating the Generative AI Revolution in Computing Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633499},
doi = {10.1145/3623762.3633499},
abstract = {Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {108–159},
numpages = {52},
keywords = {ai, artificial intelligence, chatgpt, code generation, codex, computer programming, copilot, cs1, curriculum, generative ai, github, gpt, gpt-3, gpt-4, large language models, llm, llms, novice programming, openai, pedagogical practices, programming},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

@inproceedings{10.1145/3540250.3558950,
author = {Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu},
title = {Understanding automated code review process and developer experience in industry},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558950},
doi = {10.1145/3540250.3558950},
abstract = {Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1398–1407},
numpages = {10},
keywords = {code review, code review automation, review bot, static analysis},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3274321,
author = {Morgan, Jonathan T. and Filippova, Anna},
title = { 'Welcome' Changes? Descriptive and Injunctive Norms in a Wikipedia Sub-Community},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274321},
doi = {10.1145/3274321},
abstract = {Open online communities rely on social norms for behavior regulation, group cohesion, and sustainability. Research on the role of social norms online has mainly focused on one source of influence at a time, making it difficult to separate different normative influences and understand their interactions. In this study, we use the Focus Theory to examine interactions between several sources of normative influence in a Wikipedia sub-community: local descriptive norms, local injunctive norms, and norms imported from similar sub-communities. We find that exposure to injunctive norms has a stronger effect than descriptive norms, that the likelihood of performing a behavior is higher when both injunctive and descriptive norms are congruent, and that conflicting social norms may negatively impact pro-normative behavior. We contextualize these findings through member interviews, and discuss their implications for both future research on normative influence in online groups and the design of systems that support open collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {52},
numpages = {26},
keywords = {intra-group processes, online communities, open collaboration, social norms, socialization, virtual teams, wikipedia}
}

@inproceedings{10.1145/3510457.3513062,
author = {Kim, Hyungjin and Kwon, Yonghwi and Kwon, Hyukin and Ryou, Yeonhee and Joh, Sangwoo and Kim, Taeksu and Kim, Chul-Joo},
title = {A unified code review automation for large-scale industry with diverse development environments},
year = {2022},
isbn = {9781450392266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510457.3513062},
doi = {10.1145/3510457.3513062},
abstract = {Code Review is an essential activity to ensure the quality of the software in the development process. Code Review Automation with various analyses can reduce human efforts of code review activities. However, it is a challenge to automate the code review process for large-scale companies such as Samsung Electronics due to their complex development environments: many kinds of products, various sizes of software, different version control systems, and diverse code review systems. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments. Our findings provide practical evidence that our system motivates developers in Samsung Electronics to improve code quality.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
pages = {23–24},
numpages = {2},
keywords = {code review, code review automation, review bot, static analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIP '22}
}

@article{10.1145/3392155,
author = {Huang, Yuheng and Wang, Haoyu and Wu, Lei and Tyson, Gareth and Luo, Xiapu and Zhang, Run and Liu, Xuanzhe and Huang, Gang and Jiang, Xuxian},
title = {Understanding (Mis)Behavior on the EOSIO Blockchain},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3392155},
doi = {10.1145/3392155},
abstract = {EOSIO has become one of the most popular blockchain platforms since its mainnet launch in June 2018. In contrast to the traditional PoW-based systems (e.g., Bitcoin and Ethereum), which are limited by low throughput, EOSIO is the first high throughput Delegated Proof of Stake system that has been widely adopted by many decentralized applications. Although EOSIO has millions of accounts and billions of transactions, little is known about its ecosystem, especially related to security and fraud. In this paper, we perform a large-scale measurement study of the EOSIO blockchain and its associated DApps. We gather a large-scale dataset of EOSIO and characterize activities including money transfers, account creation and contract invocation. Using our insights, we then develop techniques to automatically detect bots and fraudulent activity. We discover thousands of bot accounts (over 30% of the accounts in the platform) and a number of real-world attacks (301 attack accounts). By the time of our study, 80 attack accounts we identified have been confirmed by DApp teams, causing 828,824 EOS tokens losses (roughly $2.6 million) in total.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {jun},
articleno = {37},
numpages = {28},
keywords = {attack detection, blockchain, bot account, dapp, eosio}
}

@inproceedings{10.1145/3578837.3578876,
author = {Lo, Wen-Chi},
title = {A Fintech Workshop Course Applies Project-Based Learning in the Intelligent Query of the Money Laundering Control Act},
year = {2023},
isbn = {9781450398428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578837.3578876},
doi = {10.1145/3578837.3578876},
abstract = {This article considers the application of project-based learning in the intelligent query of the money laundering control act through the course implementation method. This course is designed and implemented to provide practical applications of financial technology for college students in the departments and classes of the Financial Technology College. The teaching goal of this workshop course is to develop students' knowledge and skills and apply them to practical cases, such as using LINE BOT as the most popular human-machine interface and underlying technologies such as Django website building tools SQLite3 database, and Python Crawler technology. The workshop involves completing tasks as a class utilizing students' critical thinking, communication skills, teamwork, and entrepreneurial spirit. From the perspective of the economic environment, people focus on the practical significance of teamwork teaching methods. The evaluation results show that project-based learning has performed well in teaching financial technology workshops. The experience of this course and student reviews show that students like the practical application aspect of the system and are interested in learning. We believe that other business schools that offer similar Fintech workshop courses will also benefit from this approach.},
booktitle = {Proceedings of the 2022 6th International Conference on Education and E-Learning},
pages = {267–274},
numpages = {8},
keywords = {Chatbot LINE BOT, Fintech Workshop, Intelligent Query of the Money Laundering Control Act, Project-Based Learning},
location = {Yamanashi, Japan},
series = {ICEEL '22}
}

@inproceedings{10.1145/3510455.3512783,
author = {Ochoa, Lina and Degueule, Thomas and Falleri, Jean-R\'{e}my},
title = {BreakBot: analyzing the impact of breaking changes to assist library evolution},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512783},
doi = {10.1145/3510455.3512783},
abstract = {"If we make this change to our code, how will it impact our clients?" It is difficult for library maintainers to answer this simple---yet essential!---question when evolving their libraries. Library maintainers are constantly balancing between two opposing positions: make changes at the risk of breaking some of their clients, or avoid changes and maintain compatibility at the cost of immobility and growing technical debt. We argue that the lack of objective usage data and tool support leaves maintainers with their own subjective perception of their community to make these decisions.We introduce BreakBot, a bot that analyses the pull requests of Java libraries on GitHub to identify the breaking changes they introduce and their impact on client projects. Through static analysis of libraries and clients, it extracts and summarizes objective data that enrich the code review process by providing maintainers with the appropriate information to decide whether---and how---changes should be accepted, directly in the pull requests.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {26–30},
numpages = {5},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/3644032.3644444,
author = {Sterk, Alexander and Wessel, Mairieli and Hooten, Eli and Zaidman, Andy},
title = {Running a Red Light: An Investigation into Why Software Engineers (Occasionally) Ignore Coverage Checks},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644444},
doi = {10.1145/3644032.3644444},
abstract = {Many modern code coverage tools track and report code coverage data generated from running tests during continuous integration. They report code coverage data through a variety of channels, including email, Slack, Mattermost, or through the web interface of social coding platforms such as GitHub. In fact, this ensemble of tools can be configured in such a way that the software engineer gets a failing status check when code coverage drops below a certain threshold. In this study, we broadly investigate the opinions and experience with code coverage tools through a survey among 279 software engineers whose projects use the Codecov coverage tool and bot. In particular, we are investigating why software engineers would ignore a failing status check caused by drop in code coverage. We observe that &gt;80% of software engineers --- at least sometimes --- ignore these failing status checks, and we get insights into the main reasons why software engineers ignore these checks.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {12–22},
numpages = {11},
keywords = {software testing, code coverage, coverage checks},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3524842.3528441,
author = {Koshchenko, Ekaterina and Klimov, Egor and Kovalenko, Vladimir},
title = {Multimodal recommendation of messenger channels},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528441},
doi = {10.1145/3524842.3528441},
abstract = {Collaboration platforms, such as GitHub and Slack, are a vital instrument in the day-to-day routine of software engineering teams. The data stored in these platforms has a significant value for data-driven methods that assist with decision-making and help improve software quality. However, the distribution of this data across different platforms leads to the fact that combining it is a very time-consuming process. Most existing algorithms for socio-technical assistance, such as recommendation systems, are based only on data directly related to the purpose of the algorithms, often originating from a single system.In this work, we explore the capabilities of a multimodal recommendation system in the context of software engineering. Using records of interaction between employees in a software company in messenger channels and repositories, as well as the organizational structure, we build several channel recommendation models for a software engineering collaboration platform, and compare them on historical data. In addition, we implement a channel recommendation bot and assess the quality of recommendations from the best models with a user study.We find that the multimodal recommender yields better recommendations than unimodal baselines, allows to mitigate the overfitting problem, and helps to deal with cold start. Our findings suggest that the multimodal approach is promising for other recommendation problems in software engineering.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {495–505},
numpages = {11},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3377811.3380335,
author = {Mirsaeedi, Ehsan and Rigby, Peter C.},
title = {Mitigating turnover with code review recommendation: balancing expertise, workload, and knowledge distribution},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380335},
doi = {10.1145/3377811.3380335},
abstract = {Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover with minimal impacton the development process. We evaluate review recommenders in the context of ensuring expertise during review, Expertise, reducing the review workload of the core team, CoreWorkload, and reducing the Files at Risk to turnover, FaR. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing risk of knowledge loss from turnover by up to 65%. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover by -29% but they unacceptably reduce the overall expertise during reviews by -26%. We develop the Sofia recommender that suggests experts when none of the files under review are hoarded by developers, but distributes knowledge when files are at risk. In this way, we are able to simultaneously increase expertise during review with a ΔExpertise of 6%, with a negligible impact on workload of ΔCoreWorkload of 0.09%, and reduce the files at risk by ΔFaR -28%. Sofia is integrated into GitHub pull requests allowing developers to select an appropriate expert or "learner" based on the context of the review. We release the Sofia bot as well as the code and data for replication purposes.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1183–1195},
numpages = {13},
keywords = {code review, knowledge distribution, recommenders, tool support, turnover},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3629479.3629481,
author = {Silva, Geovana Ramos Sousa and Canedo, Edna Dias},
title = {Unveiling Quality in Chatbot Conversations: Quantitative Analysis of Chatbot Requirements},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629481},
doi = {10.1145/3629479.3629481},
abstract = {As conversational assistants and natural language interfaces proliferate, the demand for a precise understanding of quality software requirements for chatbots becomes increasingly critical. In this work, we adopted a quantitative methodology, scrutinizing a dataset composed of conversational requirements from a diverse range of agile projects for chatbot development, and identified meaningful patterns in the language and structure utilized. Our investigation led to significant findings, revealing the importance of structured documentation, conversation flow, and user interaction in the development of chatbots, with the most desired quality attributes being capability, naturalness, straightforwardness, and clarity. In addition, a significant emphasis was placed on feature development and meeting acceptance criteria. The research also illuminated the iterative nature of chatbot development, with a recurrent presence of verbs related to improvement or refactoring. While less pronounced, the roles of documentation and testing in ensuring chatbot quality and effectiveness were also noted. This work provides valuable insights into chatbot requirements management and the significance of quality attributes in chatbot development.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {148–157},
numpages = {10},
keywords = {chatbot, github issues, open source, quality attributes, software requirements},
location = {Bras\'{\i}lia, Brazil},
series = {SBQS '23}
}

@inproceedings{10.1145/3589335.3641256,
author = {Zhang, Yizhou and Sharma, Karishma and Du, Lun and Liu, Yan},
title = {Toward Mitigating Misinformation and Social Media Manipulation in LLM Era},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641256},
doi = {10.1145/3589335.3641256},
abstract = {The pervasive abuse of misinformation to influence public opinion on social media has become increasingly evident in various domains, encompassing politics, as seen in presidential elections, and healthcare, most notably during the recent COVID-19 pandemic. This threat has grown in severity as the development of Large Language Models (LLMs) empowers manipulators to generate highly convincing deceptive content with greater efficiency. Furthermore, the recent strides in chatbots integrated with LLMs, such as ChatGPT, have enabled the creation of human-like interactive social bots, posing a significant challenge to both human users and the social-bot-detection systems of social media platforms.These challenges motivate researchers to develop algorithms to mitigate misinformation and social media manipulations. This tutorial introduces the advanced machine learning researches that are helpful for this goal, including (1) detection of social manipulators, (2) learning causal models of misinformation and social manipulation, and (3) LLM-generated misinformation detection. In addition, we also present possible future directions.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2024},
pages = {1302–1305},
numpages = {4},
keywords = {causal effect estimation, coordinated campaigns, disinformation, fake news, large language models, multi-modal fake news detection, social bots, social media},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3196709.3196784,
author = {Kocielnik, Rafal and Avrahami, Daniel and Marlow, Jennifer and Lu, Di and Hsieh, Gary},
title = {Designing for Workplace Reflection: A Chat and Voice-Based Conversational Agent},
year = {2018},
isbn = {9781450351980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196709.3196784},
doi = {10.1145/3196709.3196784},
abstract = {Conversational agents stand to play an important role in supporting behavior change and well-being in many domains. With users able to interact with conversational agents through both text and voice, understanding how designing for these channels supports behavior change is important. To begin answering this question, we designed a conversational agent for the workplace that supports workers' activity journaling and self-learning through reflection. Our agent, named Robota, combines chat-based communication as a Slack Bot and voice interaction through a personal device using a custom Amazon Alexa Skill. Through a 3-week controlled deployment, we examine how voice-based and chat-based interaction affect workers' reflection and support self-learning. We demonstrate that, while many current technical limitations exist, adding dedicated mobile voice interaction separate from the already busy chat modality may further enable users to step back and reflect on their work. We conclude with discussion of the implications of our findings to design of workplace self-tracking systems specifically and to behavior-change systems in general.},
booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
pages = {881–894},
numpages = {14},
keywords = {bots, conversational agents, modalities, reflection, workspace activity reporting},
location = {Hong Kong, China},
series = {DIS '18}
}

@inproceedings{10.1145/2818052.2869117,
author = {Lin, Bin and Zagalsky, Alexey and Storey, Margaret-Anne and Serebrenik, Alexander},
title = {Why Developers Are Slacking Off: Understanding How Software Teams Use Slack},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869117},
doi = {10.1145/2818052.2869117},
abstract = {Slack is a modern communication platform for teams that is seeing wide and rapid adoption by software develop-ment teams. Slack not only facilitates team messaging and archiving, but it also supports a wide plethora of inte-grations to external services and bots. We have found that Slack and its integrations (i.e., bots) are playing an increas-ingly significant role in software development, replacing email in some cases and disrupting software development processes. To understand how Slack impacts development team dynamics, we designed an exploratory study to inves-tigate how developers use Slack and how they benefit from it. We find that developers use Slack for personal, team-wide and community-wide purposes. Our research also reveals that developers use and create diverse integrations (called bots) to support their work. This study serves as the first step towards understanding the role of Slack in sup-porting software engineering.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {333–336},
numpages = {4},
keywords = {Bots, Collaboration, Slack, Social Media, Software Development},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1145/3613904.3642787,
author = {Cai, Jie and Lin, Ya-Fang and Zhang, He and Carroll, John M.},
title = {Third-Party Developers and Tool Development For Community Management on Live Streaming Platform Twitch},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642787},
doi = {10.1145/3613904.3642787},
abstract = {Community management is critical for stakeholders to collaboratively build and sustain communities with socio-technical support. However, most of the existing research has mainly focused on the community members and the platform, with little attention given to the developers who act as intermediaries between the platform and community members and develop tools to support community management. This study focuses on third-party developers (TPDs) for the live streaming platform Twitch and explores their tool development practices. Using a mixed method with in-depth qualitative analysis, we found that TPDs maintain complex relationships with different stakeholders (streamers, viewers, platform, professional developers), and the multi-layered policy restricts their agency regarding idea innovation and tool development. We argue that HCI research should shift its focus from tool users to tool developers with regard to community management. We propose designs to support closer collaboration between TPDS and the platform and professional developers and streamline TPDs’ development process with unified toolkits and policy documentation.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {926},
numpages = {18},
keywords = {Community Management, Community moderation, Discord, Extension and Bot Development, Live Streaming, Moderation Tools, Platform Governance, Third-Party Developers, Twitch},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613905.3651051,
author = {Moore, Nicole and Amith, Muhammad and Neumann, Ana and Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui},
title = {Translating motivational interviewing for the HPV vaccine into a computable ontology model for automated AI conversational interaction},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651051},
doi = {10.1145/3613905.3651051},
abstract = {Human papillomavirus (HPV) vaccinations are lower than expected. To protect the onset of head and neck cancers, innovative strategies to improve the rates are needed. Artificial intelligence may offer some solutions, specifically conversational agents to perform counseling methods. We present our efforts in developing a dialogue model for automating motivational interviewing (MI) to encourage HPV vaccination. We developed a formalized dialogue model for MI using an existing ontology-based framework to manifest a computable representation using OWL2. New utterance classifications were identified along with the ontology that encodes the dialogue model. Our work is available on GitHub under the GPL v.3. We discuss how an ontology-based model of MI can help standardize/formalize MI counseling for HPV vaccine uptake. Our future steps will involve assessing MI fidelity of the ontology model, operationalization, and testing the dialogue model in a simulation with live participants.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {341},
numpages = {12},
keywords = {cancer, chat bots, conversational agents, dialogue systems, human papillomavirus, ontology, oral health, patient-provider communication},
location = {
},
series = {CHI EA '24}
}

@article{10.1145/3359146,
author = {Kiene, Charles and Jiang, Jialun Aaron and Hill, Benjamin Mako},
title = {Technological Frames and User Innovation: Exploring Technological Change in Community Moderation Teams},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359146},
doi = {10.1145/3359146},
abstract = {Management of technological change in organizations is one of the most enduring topics in the literature on computer-supported cooperative work. The successful navigation of technological change is both more challenging and more critical in online communities that are entirely mediated by technology than it is in traditional organizations. This paper presents an analysis of 14 in-depth interviews with moderators of subcommunities of one technological platform (Reddit) that added communities on a new technological platform (Discord). Moderation teams experienced several problems related to moderating content at scale as well as a disconnect between the affordances of Discord and their assumptions based on their experiences on Reddit. We found that moderation teams used Discord's API to create scripts and bots that augmented Discord to make the platform work more like tools on Reddit. These tools were particularly important in communities struggling with scale. Our findings suggest that increasingly widespread end user programming allow users of social computing systems to innovate and deploy solutions to unanticipated design problems by transforming new technological platforms to align with their past expectations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {44},
numpages = {23},
keywords = {API, bots, chat, computer-mediated communication, discord, moderation, online communities, reddit, social computing, technological change}
}

@inproceedings{10.1145/3587102.3588792,
author = {Savelka, Jaromir and Agarwal, Arav and Bogart, Christopher and Song, Yifan and Sakr, Majd},
title = {Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588792},
doi = {10.1145/3587102.3588792},
abstract = {We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (&lt;70% on even entry-level modules). Yet, it is clear that a straightforward application of these easily accessible models could enable a learner to obtain a non-trivial portion of the overall available score (&gt;55%) in introductory and intermediate courses alike. While the models exhibit remarkable capabilities, including correcting solutions based on auto-grader's feedback, some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps). These findings can be leveraged by instructors wishing to adapt their assessments so that GPT becomes a valuable assistant for a learner as opposed to an end-to-end solution.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {117–123},
numpages = {7},
keywords = {AI code generation, GPT, GitHub copilot, alphacode, codex, generative pre-trained transformers, introductory and intermediate programming, programming knowledge assessment, python course},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3568813.3600142,
author = {Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
title = {Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600142},
doi = {10.1145/3568813.3600142},
abstract = {This paper studies recent developments in large language models’ (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class’ assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4’s handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {78–92},
numpages = {15},
keywords = {AI code generation, AlphaCode, ChatGPT, Codex, GPT, GitHub Copilot, MCQ, Multiple-choice question answering, Python course, coding exercises, generative pre-trained transformers, introductory and intermediate programming, programming knowledge assessment},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3560835.3564557,
author = {Hejderup, Joseph},
title = {On the Use of Tests for Software Supply Chain Threats},
year = {2022},
isbn = {9781450398855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560835.3564557},
doi = {10.1145/3560835.3564557},
abstract = {Development teams are increasingly investing in automating the updating of third-party libraries to limit the patch time of zero-day exploits such as the Equifax breach. GitHub bots such as Dependabot and Renovate build such functionality by leveraging existing test infrastructure in repositories to test and evaluate new library updates. However, two recent studies suggest that test suites in projects lack effectiveness and coverage to reliably find regressions in third-party libraries. Adequate test coverage and effectiveness are critical in discovering new vulnerabilities and weaknesses from third-party libraries. The recent Log4Shell incident exemplifies this, as projects will likely not have adequate tests for logging libraries. This position paper discusses the weaknesses and challenges of current testing practices and techniques from a supply chain security perspective. We highlight two key challenges that researchers and practitioners need to address: (1) the lack of resources and best practices for testing the uses of third-party libraries and (2) enhancing the reliability of automating library updates.},
booktitle = {Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {47–49},
numpages = {3},
keywords = {dependency updates, package repositories, software supply chain threats, testing practices},
location = {Los Angeles, CA, USA},
series = {SCORED'22}
}

@article{10.1145/3349589,
author = {Monperrus, Martin and Urli, Simon and Durieux, Thomas and Martinez, Matias and Baudry, Benoit and Seinturier, Lionel},
title = {Repairnator patches programs automatically},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2019},
number = {July},
url = {https://doi.org/10.1145/3349589},
doi = {10.1145/3349589},
abstract = {Repairnator is a bot. It constantly monitors software bugs discovered during continuous integration of open-source software and tries to fix them automatically. If it succeeds in synthesizing a valid patch, Repairnator proposes the patch to the human developers, disguised under a fake human identity. To date, Repairnator has been able to produce patches that were accepted by the human developers and permanently merged into the code base. This is a milestone for human-competitiveness in software engineering research on automatic program repair.},
journal = {Ubiquity},
month = {jul},
articleno = {2},
numpages = {12}
}

@inproceedings{10.1145/3341105.3374041,
author = {Calisir, Emre and Brambilla, Marco},
title = {Wide-spectrum characterization of long-running political phenomena on social media: the brexit case},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374041},
doi = {10.1145/3341105.3374041},
abstract = {In this study, we propose a wide-spectrum analysis of long-running political events on social media, with reference to an interesting real-world international case: the so-called Brexit, the process through which the United Kingdom activated the option of leaving the European Union. In this study, we model the users participating in 33 months of Twitter debate, covering their behaviour and demographics. By using publicly shared tweets, we developed a stance classification model to evaluate the change of stance over time. We also extracted the key topics of the long-running debate, studying which political side have discussed them most and what is the general sentiment on each. We also revealed the participation of bot accounts, and we found that the higher the bot score, the more likely the account is in a pro-Leave position. We conclude our study with a temporal and comparative analysis of politicians' social media accounts.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1869–1876},
numpages = {8},
keywords = {automated political accounts, brexit referendum, political stance classification, topic discovery},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3611643.3616314,
author = {Win, Hsu Myat and Wang, Haibo and Tan, Shin Hwei},
title = {Towards Automated Detection of Unethical Behavior in Open-Source Software Projects},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616314},
doi = {10.1145/3611643.3616314},
abstract = {Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholders’ perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8% average true positive rate (up to 100% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {644–656},
numpages = {13},
keywords = {Ethics in Software Engineering, Open-source software projects},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3387904.3389270,
author = {Caulo, Maria and Lin, Bin and Bavota, Gabriele and Scanniello, Giuseppe and Lanza, Michele},
title = {Knowledge Transfer in Modern Code Review},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389270},
doi = {10.1145/3387904.3389270},
abstract = {Knowledge transfer is one of the main goals of modern code review, as shown by several studies that surveyed and interviewed developers. While knowledge transfer is a clear expectation of the code review process, there are no analytical studies using data mined from software repositories to assess the effectiveness of code review in "training" developers and improve their skills over time. We present a mining-based study investigating how and whether the code review process helps developers to improve their contributions to open source projects over time. We analyze 32,062 peer-reviewed pull requests (PRs) made across 4,981 GitHub repositories by 728 developers who created their GitHub account in 2015. We assume that PRs performed in the past by a developer D that have been subject to a code review process have "transferred knowledge" to D. Then, we verify if over time (i.e., when more and more reviewed PRs are made by D), the quality of the contributions made by D to open source projects increases (as assessed by proxies we defined, such as the acceptance of PRs, or the polarity of the sentiment in the review comments left for the submitted PRs). With the above measures, we were unable to capture the positive impact played by the code review process on the quality of developers' contributions. This might be due to several factors, including the choices we made in our experimental design.Additional investigations are needed to confirm or contradict such a negative result.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {230–240},
numpages = {11},
keywords = {code review, knowledge transfer, mining software repositories},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3510003.3510121,
author = {Fang, Hongbo and Lamba, Hemank and Herbsleb, James and Vasilescu, Bogdan},
title = {"This is damn slick!": estimating the impact of tweets on open source project popularity and new contributors},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510121},
doi = {10.1145/3510003.3510121},
abstract = {Twitter is widely used by software developers. But how effective are tweets at promoting open source projects? How could one use Twitter to increase a project's popularity or attract new contributors? In this paper we report on a mixed-methods empirical study of 44,544 tweets containing links to 2,370 open-source GitHub repositories, looking for evidence of causal effects of these tweets on the projects attracting new GitHub stars and contributors, as well as characterizing the high-impact tweets, the people likely being attracted by them, and how they differ from contributors attracted otherwise. Among others, we find that tweets have a statistically significant and practically sizable effect on obtaining new stars and a small average effect on attracting new contributors. The popularity, content of the tweet, as well as the identity of tweet authors all affect the scale of the attraction effect. In addition, our qualitative analysis suggests that forming an active Twitter community for an open source project plays an important role in attracting new committers via tweets. We also report that developers who are new to GitHub or have a long history of Twitter usage but few tweets posted are most likely to be attracted as contributors to the repositories mentioned by tweets. Our work contributes to the literature on open source sustainability.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2116–2129},
numpages = {14},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3611643.3613085,
author = {Orrei, Vincenzo and Raglianti, Marco and Nagy, Csaba and Lanza, Michele},
title = {Contribution-Based Firing of Developers?},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613085},
doi = {10.1145/3611643.3613085},
abstract = {There has been some recent clamor about the developer layoff and turnover policies enacted by high-profile corporate executives. Precisely defining the contributions in software development has always been a thorny issue, as it is difficult to establish a developer’s “performance” without recurring to guesswork, due to how software development works and how Git persists history.  
Taking inspiration from a seemingly informal notion, the pony factor, we present an approach to identify the key developers in a software project. We present an analysis of 1,011 GitHub repositories, providing fact-based reflections on development contributions.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2062–2066},
numpages = {5},
keywords = {Contribution, developer performance, mining, pony factor},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3468264.3473110,
author = {Heum\"{u}ller, Robert and Nielebock, Sebastian and Ortmeier, Frank},
title = {Exploit those code reviews! bigger data for deeper learning},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473110},
doi = {10.1145/3468264.3473110},
abstract = {Modern code review (MCR) processes are prevalent in most organizations that develop software due to benefits in quality assurance and knowledge transfer. With the rise of collaborative software development platforms like GitHub and Bitbucket, today, millions of projects share not only their code but also their review data. Although researchers have tried to exploit this data for more than a decade, most of that knowledge remains a buried treasure. A crucial catalyst for many advances in deep learning, however, is the accessibility of large-scale standard datasets for different learning tasks. This paper presents the ETCR (Exploit Those Code Reviews!) infrastructure for mining MCR datasets from any GitHub project practicing pull-request-based development. We demonstrate its effectiveness with ETCR-Elasticsearch, a dataset of &gt;231𝑘 review comments for &gt;47𝑘 Java file revisions in &gt;40𝑘 pull-requests from the Elasticsearch project. ETCR is designed with the challenge of deep learning in mind. Compared to previous datasets, ETCR datasets include all information for linking review comments to nodes in the respective program’s Abstract Syntax Tree.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1505–1509},
numpages = {5},
keywords = {code review, datasets, deep learning, source code},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3021460.3021477,
author = {Agrawall, Akash and Chaitanya, Krishna and Agrawal, Arnav Kumar and Choppella, Venkatesh},
title = {Mitigating Browser-based DDoS Attacks using CORP},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021477},
doi = {10.1145/3021460.3021477},
abstract = {On March 27, 2015, Github witnessed a massive DDoS attack, the largest in Github's history till date. In this incident, browsers and users were used as vectors to launch the attack. In this paper, we analyse such browser-based DDoS attacks and simulate them in a lab environment. Existing browser security policies like Same Origin Policy (SOP), Content Security Policy (CSP) do not mitigate these attacks by design. In this paper we observe that CORP (Cross Origin Request Policy), a browser security policy, can be used to mitigate these attacks. CORP enables a server to control cross-origin interactions initiated by a browser. The browser intercepts the cross-origin requests and blocks unwanted requests by the server. This takes the load off the server to mitigate the attack.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {137–146},
numpages = {10},
keywords = {Browser, Browser-based DDoS, Cross-origin requests, DDoS, Javascript, MITM (Man in the middle)},
location = {Jaipur, India},
series = {ISEC '17}
}

@inproceedings{10.1145/3580305.3599832,
author = {Zhang, Jing and Zhang, Xiaokang and Zhang-Li, Daniel and Yu, Jifan and Yao, Zijun and Ma, Zeyao and Xu, Yiqi and Wang, Haohua and Zhang, Xiaohan and Lin, Nianyi and Lu, Sunrui and Li, Juanzi and Tang, Jie},
title = {GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599832},
doi = {10.1145/3580305.3599832},
abstract = {We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics. Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. All the source code is available on Github.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5564–5575},
numpages = {12},
keywords = {dialogue evaluation, dialogue system, large language model},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3183440.3190332,
author = {Beller, Moritz},
title = {Toward an empirical theory of feedback-driven development},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190332},
doi = {10.1145/3183440.3190332},
abstract = {Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are pre- rather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial framework on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {503–505},
numpages = {3},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3614419.3644026,
author = {Kang, Inwon and Mridul, Maruf Ahmed and Sanders, Abraham and Ma, Yao and Munasinghe, Thilanka and Gupta, Aparna and Seneviratne, Oshani},
title = {Deciphering Crypto Twitter},
year = {2024},
isbn = {9798400703348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614419.3644026},
doi = {10.1145/3614419.3644026},
abstract = {Cryptocurrency is a fast-moving space, with a continuous influx of new projects every year. However, an increasing number of incidents in the space, such as hacks and security breaches, threaten the growth of the community and the development of technology. This dynamic and often tumultuous landscape is vividly mirrored and shaped by discussions within “Crypto Twitter,” a key digital arena where investors, enthusiasts, and skeptics converge, revealing real-time sentiments and trends through social media interactions. We present our analysis on a Twitter dataset collected during a formative period of the cryptocurrency landscape. We collected 40 million tweets using keywords related to cryptocurrency and performed a nuanced analysis that involved grouping the tweets by semantic similarity and constructing a tweet and user network. We used sentence-level embeddings and autoencoders to create K-means clusters of tweets. We identified six groups of tweets and their topics to examine different cryptocurrency-related interests and the change in sentiment over time. For example, we identified different groups of tweets demonstrating coordinated behavior in the market or expressing distrust in centralized cryptocurrency exchanges. Moreover, we discovered sentiment indicators that point to real-life incidents in the crypto world, such as the FTX incident of November 2022. We also constructed and analyzed different networks of tweets and users in our dataset by considering the reply and quote relationships and analyzed the largest components of each network. Our networks reveal a structure of bot activity in Crypto Twitter and suggest that they can be detected and handled using a network-based approach. Our work sheds light on the potential of social media signals to detect and understand crypto events, benefiting investors, regulators, and curious observers alike, as well as the potential for bot detection in Crypto Twitter using a network-based approach.},
booktitle = {Proceedings of the 16th ACM Web Science Conference},
pages = {331–342},
numpages = {12},
keywords = {Blockchain, Cryptocurrency, NLP, Social Networks, Twitter},
location = {Stuttgart, Germany},
series = {WEBSCI '24}
}

@inproceedings{10.1145/3167132.3167362,
author = {Pereira, Juanan and D\'{\i}az, Oscar},
title = {A quality analysis of facebook messenger's most popular chatbots},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167362},
doi = {10.1145/3167132.3167362},
abstract = {This work introduces a set of quality attributes for chatbots. The selection is grounded on scholarly but also reputed blog references from 2016 and 2017. In addition, attributes should be amenable to be extracted (semi) automatically. On these premises, we consider four attributes: "support of a minimal set of common commands", "foresee language variations in both inputs and ouput", "human-assistance provision" and "timeliness". These attributes are worked out for the 100 most popular chatbots in Facebook Messager. The aim is to look for correlations between these attributes and chatbot popularity in terms of number of "likes". Results show that there is no significance correlation with any of the attributes. However, the experiment come up with two main insights. First, the lack of common communication paterns that would permit users to move their experiences and expectations from one chatbot to another. Second, the existence of many programming errors that reflect that bot programming is still a nascent area.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2144–2150},
numpages = {7},
keywords = {chatbots, conversational agents, messaging, mobile UI},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3463274.3463805,
author = {Cheriyan, Jithin and Savarimuthu, Bastin Tony Roy and Cranefield, Stephen},
title = {Towards offensive language detection and reduction in four Software Engineering communities},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463805},
doi = {10.1145/3463274.3463805},
abstract = {Software Engineering (SE) communities such as Stack Overflow have become unwelcoming, particularly through members’ use of offensive language. Research has shown that offensive language drives users away from active engagement within these platforms. This work aims to explore this issue more broadly by investigating the nature of offensive language in comments posted by users in four prominent SE platforms – GitHub, Gitter, Slack and Stack Overflow (SO). It proposes an approach to detect and classify offensive language in SE communities by adopting natural language processing and deep learning techniques. Further, a Conflict Reduction System (CRS), which identifies offence and then suggests what changes could be made to minimize offence has been proposed. Beyond showing the prevalence of offensive language in over 1 million comments from four different communities which ranges from 0.07% to 0.43%, our results show promise in successful detection and classification of such language. The CRS system has the potential to drastically reduce manual moderation efforts to detect and reduce offence in SE communities.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {254–259},
numpages = {6},
keywords = {Conflict reduction, Offensive language detection, SE platforms},
location = {Trondheim, Norway},
series = {EASE '21}
}

@inproceedings{10.1145/3530019.3535307,
author = {Tony, Catherine and Balasubramanian, Mohana and D\'{\i}az Ferreyra, Nicol\'{a}s E. and Scandariato, Riccardo},
title = {Conversational DevBots for Secure Programming: An Empirical Study on SKF Chatbot},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3535307},
doi = {10.1145/3530019.3535307},
abstract = {Conversational agents or chatbots are widely investigated and used across different fields including healthcare, education, and marketing. Still, the development of chatbots for assisting secure coding practices is in its infancy. In this paper, we present the results of an empirical study on SKF chatbot, a software-development bot (DevBot) designed to answer queries about software security. To the best of our knowledge, SKF chatbot is one of the very few of its kind, thus a representative instance of conversational DevBots aiding secure software development. In this study, we collect and analyse empirical evidence on the effectiveness of SKF chatbot, while assessing the needs and expectations of its users (i.e., software developers). Furthermore, we explore the factors that may hinder the elaboration of more sophisticated conversational security DevBots and identify features for improving the efficiency of state-of-the-art solutions. All in all, our findings provide valuable insights pointing towards the design of more context-aware and personalized conversational DevBots for security engineering.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {276–281},
numpages = {6},
keywords = {DevBot, Empirical study, Secure programming, Software Chatbot},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3540250.3549177,
author = {Fregnan, Enrico and Braz, Larissa and D'Ambros, Marco and \c{C}al\i{}kl\i{}, G\"{u}l and Bacchelli, Alberto},
title = {First come first served: the impact of file position on code review},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549177},
doi = {10.1145/3540250.3549177},
abstract = {The most popular code review tools (e.g., Gerrit and GitHub) present the files to review sorted in alphabetical order. Could this choice or, more generally, the relative position in which a file is presented bias the outcome of code reviews? We investigate this hypothesis by triangulating complementary evidence in a two-step study.  
First, we observe developers’ code review activity. We analyze the review comments pertaining to 219,476 Pull Requests (PRs) from 138 popular Java projects on GitHub. We found files shown earlier in a PR to receive more comments than files shown later, also when controlling for possible confounding factors: e.g., the presence of discussion threads or the lines added in a file. Second, we measure the impact of file position on defect finding in code review. Recruit- ing 106 participants, we conduct an online controlled experiment in which we measure participants’ performance in detecting two unrelated defects seeded into two different files. Participants are assigned to one of two treatments in which the position of the defective files is switched. For one type of defect, participants are not affected by its file’s position; for the other, they have 64% lower odds to identify it when its file is last as opposed to first. Overall, our findings provide evidence that the relative position in which files are presented has an impact on code reviews’ outcome; we discuss these results and implications for tool design and code review.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {483–494},
numpages = {12},
keywords = {Code Review, Cognitive Bias, Controlled Experiment},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3196398.3196430,
author = {Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan},
title = {SOTorrent: reconstructing and analyzing the evolution of stack overflow posts},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196430},
doi = {10.1145/3196398.3196430},
abstract = {Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {319–330},
numpages = {12},
keywords = {code snippets, open dataset, software evolution, stack overflow},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3357384.3357971,
author = {Gong, Qingyuan and Zhang, Jiayun and Chen, Yang and Li, Qi and Xiao, Yu and Wang, Xin and Hui, Pan},
title = {Detecting Malicious Accounts in Online Developer Communities Using Deep Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357971},
doi = {10.1145/3357384.3357971},
abstract = {Online developer communities like GitHub provide services such as distributed version control and task management, which allow a massive number of developers to collaborate online. However, the openness of the communities makes themselves vulnerable to different types of malicious attacks, since the attackers can easily join and interact with legitimate users. In this work, we formulate the malicious account detection problem in online developer communities, and propose GitSec, a deep learning-based solution to detect malicious accounts. GitSec distinguishes malicious accounts from legitimate ones based on the account profiles as well as dynamic activity characteristics. On one hand, GitSec makes use of users' descriptive features from the profiles. On the other hand, GitSec processes users' dynamic behavioral data by constructing two user activity sequences and applying a parallel neural network design to deal with each of them, respectively. An attention mechanism is used to integrate the information generated by the parallel neural networks. The final judgement is made by a decision maker implemented by a supervised machine learning-based classifier. Based on the real-world data of GitHub users, our extensive evaluations show that GitSec is an accurate detection system, with an F1-score of 0.922 and an AUC value of 0.940.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1251–1260},
numpages = {10},
keywords = {deep learning, malicious account detection, online developer community, social networks},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1109/ICSE.2019.00099,
author = {Sarker, Farhana and Vasilescu, Bogdan and Blincoe, Kelly and Filkov, Vladimir},
title = {Socio-technical work-rate increase associates with changes in work patterns in online projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00099},
doi = {10.1109/ICSE.2019.00099},
abstract = {Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance.Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {936–947},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3377813.3381364,
author = {Fischer-Nielsen, Anders and Fu, Zhoulai and Su, Ting and W\k{a}sowski, Andrzej},
title = {The forgotten case of the dependency bugs: on the example of the robot operating system},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381364},
doi = {10.1145/3377813.3381364},
abstract = {A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.The paper presents a definition and a taxonomy of dependency bugs extracted from data. It describes multiple facets of these bugs and estimates that as many as 15% (!) of all reported bugs are dependency bugs. We show that lightweight tools can find dependency bugs efficiently, although it is challenging to decide which tools to build and difficult to build general tools. We present the research problem to the community, and posit that it should be feasible to eradicate it from software development practice.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {21–30},
numpages = {10},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/2950290.2950364,
author = {Zhu, Jiaxin and Zhou, Minghui and Mockus, Audris},
title = {Effectiveness of code contribution: from patch-based to pull-request-based tools},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950364},
doi = {10.1145/2950290.2950364},
abstract = {Code contributions in Free/Libre and Open Source Software projects are controlled to maintain high-quality of software. Alternatives to patch-based code contribution tools such as mailing lists and issue trackers have been developed with the pull request systems being the most visible and widely available on GitHub. Is the code contribution process more effective with pull request systems? To answer that, we quantify the effectiveness via the rates contributions are accepted and ignored, via the time until the first response and final resolution and via the numbers of contributions. To control for the latent variables, our study includes a project that migrated from an issue tracker to the GitHub pull request system and a comparison between projects using mailing lists and pull request systems. Our results show pull request systems to be associated with reduced review times and larger numbers of contributions. However, not all the comparisons indicate substantially better accept or ignore rates in pull request systems. These variations may be most simply explained by the differences in contribution practices the projects employ and may be less affected by the type of tool. Our results clarify the importance of understanding the role of tools in effective management of the broad network of potential contributors and may lead to strategies and practices making the code contribution more satisfying and efficient from both contributors' and maintainers' perspectives.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {871–882},
numpages = {12},
keywords = {Code contribution, FLOSS, effectiveness, issue tracker, mailing list, pull request},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@article{10.1145/3479529,
author = {Chopra, Ashish and Mo, Morgan and Dodson, Samuel and Beschastnikh, Ivan and Fels, Sidney S. and Yoon, Dongwook},
title = {"@alex, this fixes #9": Analysis of Referencing Patterns in Pull Request Discussions},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479529},
doi = {10.1145/3479529},
abstract = {Pull Requests (PRs) are a frequently used method for proposing changes to source code repositories. When discussing proposed changes in a PR discussion, stakeholders often reference a wide variety of information objects for establishing shared awareness and common ground. Previous work has not considered how the referential behavior impacts collaborative software development via PRs. This knowledge gap is the major barrier in evaluating the current support for referencing in PRs and improving them. We conducted an explorative analysis of textasciitilde7K references, collected from 450 public PRs on GitHub, and constructed taxonomies of referent types and expressions. Using our annotated dataset, we identified several patterns in the use of references. Referencing source code elements was prevalent but the authoring interface lacks support for it. Three classes of contextual factors influence referencing behaviors: referent type, discussion thread, and project attributes. Referencing patterns may indicate PR outcomes (e.g., merged PRs frequently reference issues, users, and tests). We conclude with design implications to support more effective referencing in PR discussion interfaces.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {385},
numpages = {25},
keywords = {code review, pull request, qualitative content analysis, reference, software development, taxonomy}
}

@inproceedings{10.1145/3202185.3210791,
author = {Sabuncuo\u{g}lu, Alpay and Erkaya, Merve and Buruk, O\u{g}uz Turan and G\"{o}ksun, Tilbe},
title = {Code notes: designing a low-cost tangible coding tool for/with children},
year = {2018},
isbn = {9781450351522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3202185.3210791},
doi = {10.1145/3202185.3210791},
abstract = {Programming has become an essential subject for today's education curriculum and as a result, the importance of creating the right environments to teach is increasing. For such environments, featuring tangible tools enhances creativity and collaboration. However, due to their high prices, current tangible tools are not reachable by most of the students. We developed Code Notes as a low-cost, attainable and tangible tool aimed to motivate children to support programming education. Code Notes is comprised of an Android app and code-cardboards to teach the basic concepts in programming. We continue to develop the platform with insights gained from children. This paper shares the design phases of Code Notes and observations from our two-month programming project. We also presented some future concepts of Code Notes that offer an active and embodied interaction with the teaching material.},
booktitle = {Proceedings of the 17th ACM Conference on Interaction Design and Children},
pages = {644–649},
numpages = {6},
keywords = {affordable systems for education, collaborative learning environments, mobile learning, tangible blocks},
location = {Trondheim, Norway},
series = {IDC '18}
}

@inproceedings{10.1109/ICSE-Companion.2019.00036,
author = {Moreno, David and Due\~{n}as, Santiago and Cosentino, Valerio and Fernandez, Miguel Angel and Zerouali, Ahmed and Robles, Gregorio and Gonzalez-Barahona, Jesus M.},
title = {SortingHat: wizardry on software project members},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00036},
doi = {10.1109/ICSE-Companion.2019.00036},
abstract = {Nowadays, software projects and in particular open source ones heavily rely on a plethora of tools (e.g., Git, GitHub) to support and coordinate development activities. Despite their paramount value, they foster to fragment members' contribution, since members can access them with different identities (e.g., email, username). Thus, researchers and practitioners willing to evaluate individual members contributions are often forced to develop ad-hoc scripts or perform manual work to merge identities. This comes at the risk of obtaining wrong results and hindering replication of their work. In this demo we present SortingHat, which helps to track unique identities of project members and their related information such as gender, country and organization enrollments. It allows to manipulate identities interactively as well as to load bulks of identities via batch files (useful for projects with large communities). SortingHat is a component of GrimoireLab, an industry strong free platform developed by Bitergia, which offers commercial software analytics and is part of the CHAOSS project of the Linux Foundation. A video showing SortingHat is available at https://youtu.be/724I1XcQV6c.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {51–54},
numpages = {4},
keywords = {empirical software engineering, identity merging, open source software, software development, software mining},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3387940.3391481,
author = {Brown, Chris and Parnin, Chris},
title = {Comparing Different Developer Behavior Recommendation Styles},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391481},
doi = {10.1145/3387940.3391481},
abstract = {Research shows that one of the most effective ways software engineers discover useful developer behaviors, or tools and practices designed to help developers complete programming tasks, is through human-to-human recommendations from coworkers during work activities. However, due to the increasingly distributed nature of the software industry and development teams, opportunities for these peer interactions are in decline. To overcome the deprecation of peer interactions in software engineering, we explore the impact of several system-to-human recommendation systems, including the recently introduced suggested changes feature on GitHub which allows users to propose code changes to developers on contributions to repositories, to discover their impact on developer recommendations. In this work, we aim to study the effectiveness of suggested changes for recommending developer behaviors by performing a user study with professional software developers to compare static analysis tool recommendations from emails, pull requests, issues, and suggested changes. Our results provide insight into creating systems for recommendations between developers and design implications for improving automated recommendations to software engineers.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {78–85},
numpages = {8},
keywords = {developer behavior, developer recommendations, software engineering, tool adoption},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3543873.3587351,
author = {Nettersheim, Florian and Arlt, Stephan and Rademacher, Michael and Dehling, Florian},
title = {Katti: An Extensive and Scalable Tool for Website Analyses},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587351},
doi = {10.1145/3543873.3587351},
abstract = {Research on web security and privacy frequently relies on tools that analyze a set of websites. One major obstacle to the judicious analysis is the employment of a rock-solid and feature-rich web crawler. For example, the automated analysis of ad-malware campaigns on websites requests crawling a vast set of domains on multiple real web browsers, while simultaneously mitigating bot detections and applying user interactions on websites. Further, the ability to attach various threat analysis frameworks lacks current tooling efforts in web crawling and analyses. In this paper we introduce Katti, which overcomes several of today’s technical hurdles in web crawling. Our tool employs a distributed task queue that efficiently and reliably handles both large crawling and threat analyses requests. Katti&nbsp; extensively collects all available web data through an integrated person-in-the-middle proxy. Moreover, Katti&nbsp; is not limited to a specific use case, allowing users to easily customize our tool to their individual research intends.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {217–220},
numpages = {4},
keywords = {analyses, analysis, crawling, web, website},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3097983.3098101,
author = {Maurus, Samuel and Plant, Claudia},
title = {Let's See Your Digits: Anomalous-State Detection using Benford's Law},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098101},
doi = {10.1145/3097983.3098101},
abstract = {Benford's Law explains a curious phenomenon in which the leading digits of "naturally-occurring" numerical data are distributed in a precise fashion. In this paper we begin by showing that system metrics generated by many modern information systems like Twitter, Wikipedia, YouTube and GitHub obey this law. We then propose a novel unsupervised approach called BenFound that exploits this property to detect anomalous system events. BenFound tracks the "Benfordness" of key system metrics, like the follower counts of tweeting Twitter users or the change deltas in Wikipedia page edits. It then applies a novel Benford-conformity test in real-time to identify "non-Benford events". We investigate a variety of such events, showing that they correspond to unnatural and often undesirable system interactions like spamming, hashtag-hijacking and denial-of-service attacks. The result is a technically-uncomplicated and effective "red flagging" technique that can be used to complement existing anomaly-detection approaches. Although not without its limitations, it is highly efficient and requires neither obscure parameters, nor text streams, nor natural-language processing.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {977–986},
numpages = {10},
keywords = {anomaly detection, benford's law, data streams, nonparametric statistical tests, time series data},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/2872518.2891112,
author = {Steiner, Thomas},
title = {Wikipedia Tools for Google Spreadsheets},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891112},
doi = {10.1145/2872518.2891112},
abstract = {In this paper, we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free, Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online, while collaborating with other users in realtime. Wikipedia is a free-access, free-content Internet encyclopedia, whose content and data is available, among other means, through an API. With the Wikipedia Tools for Google Spreadsheets, we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released under the permissive Apache 2.0 license.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {997–1000},
numpages = {4},
keywords = {google sheets, google spreadsheets, wikidata, wikipedia},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/3544902.3546637,
author = {Karmakar, Shubhashis and Codabux, Zadia and Vidoni, Melina},
title = {An Experience Report on Technical Debt in Pull Requests: Challenges and Lessons Learned},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546637},
doi = {10.1145/3544902.3546637},
abstract = {Background: GitHub is a collaborative platform for global software development, where Pull Requests (PRs) are essential to bridge code changes with version control. However, developers often trade software quality for faster implementation, incurring Technical Debt (TD). When developers undertake reviewers’ roles and evaluate PRs, they can often detect TD instances, leading to either PR rejection or discussions. Aims: We investigated whether Pull Request Comments (PRCs) indicate TD by assessing three large-scale repositories: Spark, Kafka, and React. Method: We combined manual classification with automated detection using machine learning and deep learning models. Results: We classified two datasets and found that 37.7 and 38.7% of PRCs indicate TD, respectively. Our best model achieved F1 = 0.85 when classifying TD during the validation phase. Conclusions: We faced several challenges during this process, which may hint that TD in PRCs is discussed differently from other software artifacts (e.g., code comments, commits, issues, or discussion forums). Thus, we present challenges and lessons learned to assist researchers in pursuing this area of research.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {295–300},
numpages = {6},
keywords = {Mining Software Repositories, Pull Request Comments, Technical Debt},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1145/3661167.3661224,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Motivation Research Using Labeling Functions},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661224},
doi = {10.1145/3661167.3661224},
abstract = {Motivation is an important factor in software development. However, it is a subjective concept that is hard to quantify and study empirically. In order to use the wealth of data available about real software development projects in GitHub, we represent the motivation of developers using labeling functions. These are validated heuristics that need only be better than a guess, computable on a dataset. We define four labeling functions for motivation based on behavioral cues like working in diverse hours of the day. We validated the functions by agreement with respect to a developers survey, per person behavior, and temporal changes. We then apply them to 150 thousand developers working on GitHub projects. Using the identification of motivated developers, we measure developer performance gaps. We show that motivated developers have up to 70% longer activity period, produce up to 300% more commits, and invest up to 44% more time per commit.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {222–231},
numpages = {10},
keywords = {methodology, motivation, software engineering, weak supervision},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3368089.3409705,
author = {Lamba, Hemank and Trockman, Asher and Armanios, Daniel and K\"{a}stner, Christian and Miller, Heather and Vasilescu, Bogdan},
title = {Heard it through the Gitvine: an empirical study of tool diffusion across the npm ecosystem},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409705},
doi = {10.1145/3368089.3409705},
abstract = {Automation tools like continuous integration services, code coverage reporters, style checkers, dependency managers, etc. are all known to provide significant improvements in developer productivity and software quality. Some of these tools are widespread, others are not. How do these automation "best practices" spread? And how might we facilitate the diffusion process for those that have seen slower adoption? In this paper, we rely on a recent innovation in transparency on code hosting platforms like GitHub---the use of repository badges---to track how automation tools spread in open-source ecosystems through different social and technical mechanisms over time. Using a large longitudinal data set, multivariate network science techniques, and survival analysis, we study which socio-technical factors can best explain the observed diffusion process of a number of popular automation tools. Our results show that factors such as social exposure, competition, and observability affect the adoption of tools significantly, and they provide a roadmap for software engineers and researchers seeking to propagate best practices and tools.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {505–517},
numpages = {13},
keywords = {diffusion, innovations, open source ecosystem, software tools},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3597503.3639184,
author = {Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael},
title = {PyTy: Repairing Static Type Errors in Python},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639184},
doi = {10.1145/3597503.3639184},
abstract = {Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {87},
numpages = {13},
keywords = {automatic program repair, type annotation, transfer learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639477.3639756,
author = {Williams, David and Callan, James and Kirbas, Serkan and Mechtaev, Sergey and Petke, Justyna and Prideaux-Ghee, Thomas and Sarro, Federica},
title = {User-Centric Deployment of Automated Program Repair at Bloomberg},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639756},
doi = {10.1145/3639477.3639756},
abstract = {Automated program repair (APR) tools have unlocked the potential for the rapid rectification of codebase issues. However, to encourage wider adoption of program repair in practice, it is necessary to address the usability concerns related to generating irrelevant or out-of-context patches. When software engineers are presented with patches they deem uninteresting or unhelpful, they are burdened with more "noise" in their workflows and become less likely to engage with APR tools in future. This paper presents a novel approach to optimally time, target, and present auto-generated patches to software engineers. To achieve this, we designed, developed, and deployed a new tool dubbed B-Assist, which leverages GitHub's Suggested Changes interface to seamlessly integrate automated suggestions into active pull requests (PRs), as opposed to creating new, potentially distracting PRs. This strategy ensures that suggestions are not only timely, but also contextually relevant and delivered to engineers most familiar with the affected code. Evaluation among Bloomberg software engineers demonstrated their preference for this approach. From our user study, B-Assist's efficacy is evident, with the acceptance rate of patch suggestions being as high as 74.56%; engineers also found the suggestions valuable, giving usefulness ratings of at least 4 out of 5 in 78.2% of cases. Further, this paper sheds light on persisting usability challenges in APR and lays the groundwork for enhancing the user experience in future APR tools.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {81–91},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639477.3643648,
author = {Davila, Nicole and Wiese, Igor and Steinmacher, Igor and Lucio da Silva, Lucas and Kawamoto, Andre and Favaro, Gilson Jose Peres and Nunes, Ingrid},
title = {An Industry Case Study on Adoption of AI-based Programming Assistants},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3643648},
doi = {10.1145/3639477.3643648},
abstract = {Programming assistants based on artificial intelligence (AI), such as ChatGPT and GitHub Copilot, have gained worldwide popularity recently. Studies in software development have explored the adoption of these tools, investigating their characteristics and impacts and how practitioners interact and perceive them. To contribute to this growing body of knowledge, in this study, we aim to explore the adoption of AI-based programming assistants in the Brazilian industry. More specifically, we aim to understand how practitioners of a particular Brazilian agroindustry-related company perceive and use AI-based tools to develop software. Using an online survey, we collected and analyzed 72 responses from employees of the studied company. Our findings suggest that practitioners mainly adopt ChatGPT and GitHub Copilot, interacting with these tools to accelerate online searching, typing, and syntax recall. A recurrent difficulty is the lack of context in the suggestions provided by these tools, but participants work on detailed descriptions to contextualize and cope with this challenge. Among the reasons for not using AI-based tools, the most influential is that participants use a commercial programming language, i.e., Uniface, which these tools lack examples. Our results provide insights into the state of the practice related to AI-based programming assistants and discuss implications for practitioners and researchers.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {92–102},
numpages = {11},
keywords = {artificial intelligence, generative AI, ChatGPT, industry case study, software development},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3630106.3659019,
author = {Choksi, Madiha Zahrah and Mandel, Ilan and Widder, David and Shvartzshnaider, Yan},
title = {The Emerging Artifacts of Centralized Open-Code},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659019},
doi = {10.1145/3630106.3659019},
abstract = {In 2022, generative model based coding assistants became widely available with the public release of GitHub Copilot. Approaches to generative coding are often critiqued within the context of advances in machine learning. We argue that tools such as Copilot are better understood when contextualized against technologies derived from the same communities and datasets. Our work traces the historical and ideological origins of free and open source code and characterizes the process of centralization. We examine three case studies —Dependabot, Crater, and Copilot— to compare the engineering, social, and legal qualities of technical artifacts derived from shared community-based labor. Our analysis focuses on the implications these artifacts create for infrastructural dependencies, community adoption, and intellectual property. Reframing generative coding assistants through a set of peer technologies broadens considerations for academics and policymakers beyond machine learning, to include the ways technical artifacts are derived from communities.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1971–1983},
numpages = {13},
keywords = {Artificial Intelligence, Commons, Ethics, Free Software, Governance, Licenses, Political Economy},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1109/CHASE.2019.00011,
author = {Cheng, Jinghui and Guo, Jin L. C.},
title = {Activity-based analysis of open source software contributors: roles and dynamics},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2019.00011},
doi = {10.1109/CHASE.2019.00011},
abstract = {Contributors to open source software (OSS) communities assume diverse roles to take different responsibilities. One major limitation of the current OSS tools and platforms is that they provide a uniform user interface regardless of the activities performed by the various types of contributors. This paper serves as a non-trivial first step towards resolving this challenge by demonstrating a methodology and establishing knowledge to understand how the contributors' roles and their dynamics, reflected in the activities contributors perform, are exhibited in OSS communities. Based on an analysis of user action data from 29 GitHub projects, we extracted six activities that distinguished four Active roles and five Supporting roles of OSS contributors, as well as patterns in role changes. Through the lens of the Activity Theory, these findings provided rich design guidelines for OSS tools to support diverse contributor roles.},
booktitle = {Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {activity-based analysis, contributor roles, open source community, open source software},
location = {Montreal, Quebec, Canada},
series = {CHASE '19}
}

@article{10.1145/3530785,
author = {Khatoonabadi, Sayedhassan and Costa, Diego Elias and Abdalkareem, Rabe and Shihab, Emad},
title = {On Wasted Contributions: Understanding the Dynamics of Contributor-Abandoned Pull Requests–A Mixed-Methods Study of 10 Large Open-Source Projects},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3530785},
doi = {10.1145/3530785},
abstract = {Pull-based development has enabled numerous volunteers to contribute to open-source projects with fewer barriers. Nevertheless, a considerable amount of pull requests (PRs) with valid contributions are abandoned by their contributors, wasting the effort and time put in by both the contributors and maintainers. To better understand the underlying dynamics of contributor-abandoned PRs, we conduct a mixed-methods study using both quantitative and qualitative methods. We curate a dataset consisting of 265,325 PRs including 4,450 abandoned ones from ten popular and mature GitHub projects and measure 16 features characterizing PRs, contributors, review processes, and projects. Using statistical and machine learning techniques, we find that complex PRs, novice contributors, and lengthy reviews have a higher probability of abandonment and the rate of PR abandonment fluctuates alongside the projects’ maturity or workload. To identify why contributors abandon their PRs, we also manually examine a random sample of 354 abandoned PRs. We observe that the most frequent abandonment reasons are related to the obstacles faced by contributors, followed by the hurdles imposed by maintainers during the review process. Finally, we survey the top core maintainers of the studied projects to understand their perspectives on dealing with PR abandonment and on our findings.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {15},
numpages = {39},
keywords = {Socio-technical factors, pull-based development, modern code review, social coding platforms, open-source software, mixed-methods research}
}

@inproceedings{10.1145/3629527.3652268,
author = {Vasilevskii, Aleksei and Kachur, Oleksandr},
title = {Self-Service Performance Testing Platform for Autonomous Development Teams},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3652268},
doi = {10.1145/3629527.3652268},
abstract = {In the modern fast paced and highly autonomous software development teams, it's crucial to maintain a sustainable approach to all performance engineering activites, including performance testing. The high degree of autonomy often results in teams building their own frameworks that are not used consistently and may be abandoned due to lack of support or integration with existing infrastructure, processes and tools.To address these challenges, we present a self-service performance testing platform based on open-source software, that supports distributed load generation, historical results storage and a notification system to trigger alerts in Slack messenger. In addition, it integrates with GitHub Actions to enable developers running load tests as part of their CI/CD pipelines.We'd like to share some of the technical solutions and the details of the decision-making process behind the performance testing platform in a scale-up environment, our experience in building this platform and, most importantly, rolling it out to autonomous development teams and onboarding them into the continuous performance improvement process.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {242–248},
numpages = {7},
keywords = {autonomous teams, continuous integration, microservices, performance testing},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@inproceedings{10.1145/3643991.3644886,
author = {Ni, Chao and Shen, Liyu and Yang, Xiaohu and Zhu, Yan and Wang, Shaohua},
title = {MegaVul: A C/C++ Vulnerability Dataset with Comprehensive Code Representations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644886},
doi = {10.1145/3643991.3644886},
abstract = {We constructed a newly large-scale and comprehensive C/C++ vulnerability dataset named MegaVul by crawling the Common Vulnerabilities and Exposures (CVE) database and CVE-related open-source projects. Specifically, we collected all crawlable descriptive information of the vulnerabilities from the CVE database and extracted all vulnerability-related code changes from 28 Git-based websites. We adopt advanced tools to ensure the extracted code integrality and enrich the code with four different transformed representations. Totally, MegaVul contains 17,380 vulnerabilities collected from 992 open-source repositories spanning 169 different vulnerability types disclosed from January 2006 to October 2023. Thus, MegaVul can be used for a variety of software security-related tasks including detecting vulnerabilities and assessing vulnerability severity. All information is stored in the JSON format for easy usage. MegaVul is publicly available on GitHub and will be continuously updated. It can be easily extended to other programming languages.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {738–742},
numpages = {5},
keywords = {common vulnerabilities and exposures, C/C++ code, code representation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3653691,
author = {Liang, Kai-Hui and Shi, Weiyan and Oh, Yoo Jung and Wang, Hao-Chuan and Zhang, Jingwen and Yu, Zhou},
title = {Dialoging Resonance in Human-Chatbot Conversation: How Users Perceive and Reciprocate Recommendation Chatbot's Self-Disclosure Strategy},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3653691},
doi = {10.1145/3653691},
abstract = {Using chatbots to make recommendations is increasingly popular. The design of recommendation chatbots has mainly been taking an information-centric approach by focusing on the recommended content per se. Limited attention is on how social connection and relational strategies, such as self-disclosure from a chatbot, may influence users' perception and acceptance of the recommendation. In this work, we designed, implemented, and evaluated a social chatbot capable of performing three different levels of self-disclosure: factual information (low), cognitive opinions (medium), and emotions (high). In the evaluation, we recruited 372 participants to converse with the chatbot on two topics: movies and COVID-19 experiences. In each topic, the chatbot conducted small talks and made relevant recommendations to the topic. Participants were randomly assigned to four experimental conditions where the chatbot used factual, cognitive, emotional, and adaptive strategies to perform self-disclosures. By training a text classifier to identify users' level of self-disclosure in real-time, the adaptive chatbot can dynamically match its self-disclosure language to the level of disclosure exhibited by the users. Our results show that users reciprocate with higher-level self-disclosure when a recommendation chatbot displays emotions throughout the conversation. The utilization of emotional disclosure by the chatbot resulted in enhanced enjoyment during interactions and a more favorable perception of the bot. This, in turn, led to greater effectiveness in making recommendations, including a higher likelihood of accepting the recommendation. We discuss the understandings obtained and implications to future design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {200},
numpages = {28},
keywords = {affective computing, ai, chatbot, conversational agent, conversational design, dialogue system, nlp, persuasion strategies, recommendation system, relational chatbot, self-disclosure}
}

@inproceedings{10.1145/3382494.3410685,
author = {Dey, Tapajit and Mockus, Audris},
title = {Effect of Technical and Social Factors on Pull Request Quality for the NPM Ecosystem},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410685},
doi = {10.1145/3382494.3410685},
abstract = {Background: Pull request (PR) based development, which is a norm for the social coding platforms, entails the challenge of evaluating the contributions of, often unfamiliar, developers from across the open source ecosystem and, conversely, submitting a contribution to a project with unfamiliar maintainers. Previous studies suggest that the decision of accepting or rejecting a PR may be influenced by a diverging set of technical and social factors, but often focus on relatively few projects, do not consider ecosystem-wide measures, or the possible non-monotonic relationships between the predictors and PR acceptance probability. Aim: We aim to shed light on this important decision making process by testing which measures significantly affect the probability of PR acceptance on a significant fraction of a large ecosystem, rank them by their relative importance in predicting PR acceptance, and determine the shape of the functions that map each predictor to PR acceptance. Method: We proposed seven hypotheses regarding which technical and social factors might affect PR acceptance and created 17 measures based on them. Our dataset consisted of 470,925 PRs from 3349 popular NPM packages and 79,128 GitHub users who created those. We tested which of the measures affect PR acceptance and ranked the significant measures by their importance in a predictive model. Results: Our predictive model had and AUC of 0.94, and 15 of the 17 measures were found to matter, including five novel ecosystem-wide measures. Measures describing the number of PRs submitted to a repository and what fraction of those get accepted, and signals about the PR review phase were most significant. We also discovered that only four predictors have a linear influence on the PR acceptance probability while others showed a more complicated response. Conclusion: Our findings should be helpful for PR creators, integrators, as well as tool designers to focus on the important factors affecting PR acceptance.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {11},
numpages = {11},
keywords = {NPM Packages, Predictive Model, Pull Request, Social Factors},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/2742647.2742676,
author = {Li, Wenhao and Li, Haibo and Chen, Haibo and Xia, Yubin},
title = {AdAttester: Secure Online Mobile Advertisement Attestation Using TrustZone},
year = {2015},
isbn = {9781450334945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742647.2742676},
doi = {10.1145/2742647.2742676},
abstract = {Mobile advertisement (ad for short) is a major financial pillar for developers to provide free mobile apps. However, it is frequently thwarted by ad fraud, where rogue code tricks ad providers by forging ad display or user clicks, or both. With the mobile ad market growing drastically (e.g., from $8.76 billion in 2012 to $17.96 billion in 2013), it is vitally important to provide a verifiable mobile ad framework to detect and prevent ad frauds. Unfortunately, this is notoriously hard as mobile ads usually run in an execution environment with a huge TCB.This paper proposes a verifiable mobile ad framework called AdAttester, based on ARM?s TrustZone technology. AdAttester provides two novel security primitives, namely unforgeable clicks and verifiable display. The two primitives attest that ad-related operations (e.g., user clicks) are initiated by the end user (instead of a bot) and that the ad is displayed intact and timely. AdAttester leverages the secure world of TrustZone to implement these two primitives to collect proofs, which are piggybacked on ad requests to ad providers for attestation. AdAttester is non-intrusive to mobile users and can be incrementally deployed in existing ad ecosystem. A prototype of AdAttester is implemented for Android running on a Samsung Exynos 4412 board. Evaluation using 182 typical mobile apps with ad frauds shows that AdAttester can accurately distinguish ad fraud from legitimate ad operations, yet incurs small performance overhead and little impact on user experience.},
booktitle = {Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {75–88},
numpages = {14},
location = {Florence, Italy},
series = {MobiSys '15}
}

@inproceedings{10.1145/3661167.3661189,
author = {Coutinho, Daniel and Cito, Luisa and Lima, Maria Vit\'{o}ria and Arantes, Beatriz and Alves Pereira, Juliana and Arriel, Johny and Godinho, Jo\~{a}o and Martins, Vinicius and Lib\'{o}rio, Paulo V\'{\i}tor C. F. and Leite, Leonardo and Garcia, Alessandro and Assun\c{c}\~{a}o, Wesley K. G. and Steinmacher, Igor and Baffa, Augusto and Fonseca, Baldoino},
title = {"Looks Good To Me ;-)": Assessing Sentiment Analysis Tools for Pull Request Discussions},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661189},
doi = {10.1145/3661167.3661189},
abstract = {Modern software development relies on cloud-based collaborative platforms (e.g., GitHub and GitLab). In these platforms, developers often employ a pull-based development approach, proposing changes via pull requests and engaging in communication via asynchronous message exchanges. Since communication is key for software development, studies have linked different types of sentiments embedded in the communication to their effects on software projects, such as bug-inducing commits or the non-acceptance of pull requests. In this context, sentiment analysis tools are paramount to detect the sentiment of developers’ messages and prevent potentially harmful impact. Unfortunately, existing state-of-the-art tools vary in terms of the nature of their data collection and labeling processes. Yet, there is no comprehensive study comparing the performance and generalizability of existing tools utilizing a dataset that was designed and systematically curated to this end, and in this specific context. Therefore, in this study, we design a methodology to assess the effectiveness of existing sentiment analysis tools in the context of pull request discussions. For that, we created a dataset that contains ≈ 1.8K manually labeled messages from 36 software projects. The messages were labeled by 19 experts (neuroscientists and software engineers), using a novel and systematic manual classification process designed to reduce subjectivity. By applying these existing tools to the dataset, we observed that while some tools ]perform acceptably, their performance is far from ideal, especially when classifying negative messages. This is interesting since negative sentiment is often related to a critical or unfavorable opinion. We also observed that some messages have characteristics that can make them harder to classify, causing disagreements between the experts and possible misclassifications by the tools, requiring more attention from researchers. Our contributions include valuable resources to pave the way to develop robust and mature sentiment analysis tools that capture/anticipate potential problems during software development.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {211–221},
numpages = {11},
keywords = {human aspects, repository mining, sentiment analysis},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3524842.3528479,
author = {Warrick, Melanie and Rosenblatt, Samuel F. and Young, Jean-Gabriel and Casari, Amanda and H\'{e}bert-Dufresne, Laurent and Bagrow, James},
title = {The OCEAN mailing list data set: network analysis spanning mailing lists and code repositories},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528479},
doi = {10.1145/3524842.3528479},
abstract = {Communication surrounding the development of an open source project largely occurs outside the software repository itself. Historically, large communities often used a collection of mailing lists to discuss the different aspects of their projects. Multimodal tool use, with software development and communication happening on different channels, complicates the study of open source projects as a sociotechnical system. Here, we combine and standardize mailing lists of the Python community, resulting in 954,287 messages from 1995 to the present. We share all scraping and cleaning code to facilitate reproduction of this work, as well as smaller datasets for the Golang (122,721 messages), Angular (20,041 messages) and Node.js (12,514 messages) communities. To showcase the usefulness of these data, we focus on the CPython repository and merge the technical layer (which GitHub account works on what file and with whom) with the social layer (messages from unique email addresses) by identifying 33% of GitHub contributors in the mailing list data. We then explore correlations between the valence of social messaging and the structure of the collaboration network. We discuss how these data provide a laboratory to test theories from standard organizational science in large open source projects.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {338–342},
numpages = {5},
keywords = {datasets, network analysis, sociotechnical systems, text tagging},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@article{10.1145/3664811,
author = {Wang, Haoye and Gao, Zhipeng and Bi, Tingting and Grundy, John and Wang, Xinyu and Wu, Minghui and Yang, Xiaohu},
title = {What Makes a Good TODO Comment?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664811},
doi = {10.1145/3664811},
abstract = {Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. To assist developers, we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {165},
numpages = {30},
keywords = {Documentation, comment quality, comment lifecycle}
}

@inproceedings{10.1145/3571473.3571508,
author = {Ferreira, M\'{\i}vian and Gon\c{c}alves, Diego and Bigonha, Mariza and Ferreira, Kecia},
title = {Characterizing Commits in Open-Source Software},
year = {2023},
isbn = {9781450399999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571473.3571508},
doi = {10.1145/3571473.3571508},
abstract = {Mining software repositories has been the basis of many studies on software engineering. Many of these works rely on commits’ data extracted since commit is the basic unit of information about activities performed on the projects. However, not knowing the characteristics of commits may introduce biases and threats in studies that consider commits’ data. This work presents an empirical study to characterize commits in terms of four aspects: the size of commits in the total number of files; the size of commits in the number of source-code files, the size of commits by category; and the time interval of commits performed by contributors. We analyzed 1M commits from the 24 most popular and active Java-based projects hosted on GitHub. The main findings of this work show that: the size of commits follows a heavy-tailed distribution; most commits involve one to 10 files; most commits affect one to four source-code files; the commits involving hundreds of files not only refer to merge or management activities; the distribution of the time intervals is approximately a Normal distribution, i.e., the distribution tends to be symmetric, and the mean is representative; in the average, a developer proceed a commit every eight hours. The results of this study should be considered by researchers in empirical works to avoid biases when analyzing commits’ data. Besides, the results provide information that practitioners may apply to improve the management and the planning of software activities.},
booktitle = {Proceedings of the XXI Brazilian Symposium on Software Quality},
articleno = {7},
numpages = {10},
keywords = {Java, commit, empirical study, mining software repositories, open-source},
location = {Curitiba, Brazil},
series = {SBQS '22}
}

@article{10.1145/3341717,
author = {Miraldo, Victor Cacciari and Swierstra, Wouter},
title = {An efficient algorithm for type-safe structural diffing},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {ICFP},
url = {https://doi.org/10.1145/3341717},
doi = {10.1145/3341717},
abstract = {Effectively computing the difference between two version of a source file has become an indispensable part of software development. The de facto standard tool used by most version control systems is the UNIX diff utility, that compares two files on a line-by-line basis without any regard for the structure of the data stored in these files. This paper presents an alternative datatype generic algorithm for computing the difference between two values of any algebraic datatype. This algorithm maximizes sharing between the source and target trees, while still running in linear time. Finally, this paper demonstrates that by instantiating this algorithm to the Lua abstract syntax tree and mining the commit history of repositories found on GitHub, the resulting patches can often be merged automatically, even when existing technology has failed.},
journal = {Proc. ACM Program. Lang.},
month = {jul},
articleno = {113},
numpages = {29},
keywords = {Generic Programming, Haskell, Version Control, diff}
}

@inproceedings{10.1145/3643916.3644424,
author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva},
title = {Understanding Regular Expression Denial of Service (ReDoS): Insights from LLM-Generated Regexes and Developer Forums},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644424},
doi = {10.1145/3643916.3644424},
abstract = {Regular expression Denial of Service (ReDoS) represents an algorithmic complexity attack that exploits the processing of regular expressions (regexes) to produce a denial-of-service attack. This attack occurs when a regex's evaluation time scales polynomially or exponentially with input length, posing significant challenges for software developers. The advent of Large Language Models (LLMs) has revolutionized the generation of regexes from natural language prompts, but not without its risks. Prior works showed that LLMs can generate code with vulnerabilities and security smells. In this paper, we examined the correctness and security of regexes generated by LLMs as well as the characteristics of LLM-generated vulnerable regexes. Our study also examined ReDoS patterns in actual software projects, aligning them with corresponding regex equivalence classes and algorithmic complexity. Moreover, we analyzed developer discussions on GitHub and StackOverflow, constructing a taxonomy to investigate their experiences and perspectives on ReDoS. In this study, we found that GPT-3.5 was the best LLM to generate regexes that are both correct and secure. We also observed that LLM-generated regexes mainly have polynomial ReDoS vulnerability patterns, and it is consistent with vulnerable regexes found in open source projects. We also found that developers' main discussions around insecure regexes is related to mitigation strategies to remove vulnerable regexes.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {190–201},
numpages = {12},
keywords = {ReDoS, DoS attack, large language models, regex generation},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@article{10.1145/3387111,
author = {Wang, Zhendong and Feng, Yang and Wang, Yi and Jones, James A. and Redmiles, David},
title = {Unveiling Elite Developers’ Activities in Open Source Projects},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3387111},
doi = {10.1145/3387111},
abstract = {Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB. We investigate elite developers’ contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {16},
numpages = {35},
keywords = {Elite developers, developers’ activity, open source software (OSS), productivity, project outcomes, software quality}
}

@inproceedings{10.1145/3661167.3661216,
author = {Siddiq, Mohammed Latif and Da Silva Santos, Joanna Cecilia and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Al Rifat, Fahmid and Carvalho Lopes, Vin\'{\i}cius},
title = {Using Large Language Models to Generate JUnit Tests: An Empirical Study},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661216},
doi = {10.1145/3661167.3661216},
abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {313–322},
numpages = {10},
keywords = {junit, large language models, test generation, test smells, unit testing},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3469127,
author = {Pereira, Juanan and D\'{\i}az, \'{O}scar},
title = {Struggling to Keep Tabs on Capstone Projects: A Chatbot to Tackle Student Procrastination},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
url = {https://doi.org/10.1145/3469127},
doi = {10.1145/3469127},
abstract = {Capstone projects usually represent the most significant academic endeavor with which students have been involved. Time management tends to be one of the hurdles. On top, University students are prone to procrastinatory behavior. Inexperience and procrastination team up for students failing to meet deadlines. Supervisors strive to help. Yet heavy workloads frequently prevent tutors from continuous involvement. This article looks into the extent to which conversational agents (a.k.a. chatbots) can tackle procrastination in single-student capstone projects. Specifically, chatbot enablers put in play include (1) alerts, (2) advice, (3) automatic rescheduling, (4) motivational messages, and (5) reference to previous capstone projects. Informed by Cognitive Behavioural Theory, these enablers are framed within the three phases involved in self-regulation misalignment: pre-actional, actional, and post-actional. To motivate this research, we first analyzed 77 capstone-project reports. We found that students’ Gantt charts (1) fail to acknowledge review meetings (70%) and milestones (100%) and (2) suffer deviations from the initial planned effort (16.28%). On these grounds, we develop GanttBot, a Telegram chatbot that is configured from the student’s Gantt diagram. GanttBot reminds students about close landmarks, it informs tutors when intervention might be required, and it learns from previous projects about common pitfalls, advising students accordingly. For evaluation purposes, course 17/18 acts as the control group ( N=28 ) while course 18/19 acts as the treatment group ( N=25  students). Using “overdue days” as the proxy for procrastination, results indicate that course 17/18 accounted for an average of 19 days of delay (SD = 5), whereas these days go down to 10 for the intervention group in course 18/19 (SD = 4). GanttBot is available for public usage as a Telegram chatbot.},
journal = {ACM Trans. Comput. Educ.},
month = {oct},
articleno = {4},
numpages = {22},
keywords = {Conversational agents, chatbots, procrastination, project management}
}

@inproceedings{10.1145/3377811.3380410,
author = {Overney, Cassandra and Meinicke, Jens and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {How to not get rich: an empirical study of donations in open source},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380410},
doi = {10.1145/3377811.3380410},
abstract = {Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1209–1221},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/3398761.3398866,
author = {Muri\'{c}, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\'{e}s and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio},
title = {Massive Cross-Platform Simulations of Online Social Networks},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {895–903},
numpages = {9},
keywords = {agent based simulation, ai agents, collaborative platforms, massive scale simulations, online social networks},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.1145/3617593,
author = {Weiss, Michael and Tonella, Paolo},
title = {Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617593},
doi = {10.1145/3617593},
abstract = {Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remote-only model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {28},
numpages = {29},
keywords = {Datasets, neural networks, gaze detection, text tagging}
}

@inproceedings{10.1145/3576915.3623140,
author = {Rack, Jeremy and Staicu, Cristian-Alexandru},
title = {Jack-in-the-box: An Empirical Study of JavaScript Bundling on the Web and its Security Implications},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623140},
doi = {10.1145/3576915.3623140},
abstract = {In recent years, we have seen an increased interest in studying the software supply chain of user-facing applications to uncover problematic third-party dependencies. Prior work shows that web applications often rely on outdated or vulnerable third-party code. Moreover, real-world supply chain attacks show that dependencies can also be used to deliver malicious code, e.g., for carrying cryptomining operations. Nonetheless, existing measurement studies in this domain neglect an important software engineering practice: developers often merge together third-party code into a single file called bundle, which they then deliver from their own servers, making it appear as first-party code. Bundlers like Webpack or Rollup are popular open-source projects with tens of thousand of GitHub stars, suggesting that this technology is widely-used by developers. Ignoring bundling may result in underestimating the complexity of modern software supply chains.In this work, we aim to address these methodological shortcomings of prior work. To this end, we propose a novel methodology for automatically detecting bundles, and partially reverse engineer them. Using this methodology, we conduct the first large-scale empirical study of bundled code on the web and examine its security implications. We provide evidence about the high prevalence of bundles, which are contained in 40% of all websites, and the average website includes more than one bundle. Following our methodology, we reidentify 1 051 vulnerabilities originating from 33 vulnerable npm packages, included in bundled code. Among the vulnerabilities, we find 17 critical and 59 high severity ones, which might enable malicious actors to execute attacks such as arbitrary code execution. Analyzing the low-rated libraries included in bundles, we discover 10 security holding packages, which suggest that supply-chain attacks affecting bundles are not only possible, but they are already happening.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3198–3212},
numpages = {15},
keywords = {bundles, dependencies, javascript, software supply chain security},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@article{10.1145/3582697,
author = {Momotaz, Farhani and Ehtesham-Ul-Haque, Md and Billah, Syed Masum},
title = {Understanding the Usages, Lifecycle, and Opportunities of Screen Readers’ Plugins},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3582697},
doi = {10.1145/3582697},
abstract = {Screen reader plugins are small pieces of code that blind users can download and install to enhance the capabilities of their screen readers. This article aims to understand why blind users use these plugins, as well as how these plugins are developed, deployed, and maintained. To this end, we conducted an interview study with 14 blind users to gain individual perspectives and analyzed 2,000 online posts scraped from three plugin-related forums to gain the community perspective. Our study revealed that screen reader users rely on plugins for various reasons, such as to improve the usability of screen readers and application software, to make partially accessible applications accessible, and to receive custom auditory feedback. Furthermore, installing plugins is easy; uninstalling them is unlikely; and finding them online is ad hoc, challenging, and sometimes poses security threats. In addition, developing screen reader plugins is technically demanding; only a handful of people develop plugins. Unfortunately, most plugins do not receive updates once distributed and become obsolete. The lack of financial incentives plays in the slow growth of the plugin ecosystem. Further, we outlined the complex, tripartite collaboration among individual blind users, their online communities, and developer communities in creating a plugin. Additionally, we reported several phenomena within and between these communities that are likely to influence a plugin’s development. Based on our findings, we recommend creating a community-driven repository for all plugins hosted on a peer-to-peer infrastructure, engaging third-party developers, and raising general awareness about the benefits and dangers of plugins. We believe our findings will inspire HCI researchers to embrace the plugin-based distribution model as an effective way to combat accessibility and usability problems in non-visual interaction and to investigate potential ways to improve the collaboration between blind users and developer communities.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {17},
numpages = {35},
keywords = {NVDA, screen reader, assistive technology, visual impairments, plugin, extension, scripts}
}

@inproceedings{10.1145/3498366.3505789,
author = {Volokhin, Sergey and Collins, Marcus and Rokhlenko, Oleg and Agichtein, Eugene},
title = {Generating and Validating Contextually Relevant Justifications for Conversational Recommendation},
year = {2022},
isbn = {9781450391863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498366.3505789},
doi = {10.1145/3498366.3505789},
abstract = {Providing a justification or explanation for a recommendation has been shown to improve the users’ experience with recommender systems, in particular by increasing confidence in the recommendations. However, in order to be effective in a conversational setting, the justifications have to be appropriate for the conversation so far. Previous approaches rely on a user history of reviews and ratings of related items to personalize the recommendation, but this information is not generally available when conversing with a new user, and as such a cold-start problem imposes a challenge in generating suitable justifications. To address this problem, we propose and validate a new method, CONJURE (CONversational JUstificatons for REcommendations) to generate contextually relevant justifications for conversational recommendations. Specifically, we investigate whether the conversation itself can be used effectively to model the user, identify relevant review content from other users, and generate a justification that boosts the user’s confidence in and understanding of the recommendation. To implement CONJURE, we test several novel extensions to prior algorithms, by exploiting an auxiliary corpus of movie reviews to construct the justifications from extracted pieces of those reviews. In particular, we explore different conversation representations and ranking approaches. To evaluate CONJURE, we developed a pairwise crowd task to compare justifications. Our results show large, significant improvements in Efficiency and Transparency metrics over the previous non-contextualized template-based methods. We plan to release our code and an augmented conversation corpus on Github.},
booktitle = {Proceedings of the 2022 Conference on Human Information Interaction and Retrieval},
pages = {284–289},
numpages = {6},
keywords = {conversational recommendations, explainable recommendations},
location = {Regensburg, Germany},
series = {CHIIR '22}
}

@inproceedings{10.1145/3611643.3613084,
author = {Sajadi, Amirali and Damevski, Kostadin and Chatterjee, Preetha},
title = {Towards Understanding Emotions in Informal Developer Interactions: A Gitter Chat Study},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613084},
doi = {10.1145/3611643.3613084},
abstract = {Emotions play a significant role in teamwork and collaborative activities like software development. While researchers have analyzed developer emotions in various software artifacts (e.g., issues, pull requests), few studies have focused on understanding the broad spectrum of emotions expressed in chats. As one of the most widely used means of communication, chats contain valuable information in the form of informal conversations, such as negative perspectives about adopting a tool. In this paper, we present a dataset of developer chat messages manually annotated with a wide range of emotion labels (and sub-labels), and analyze the type of information present in those messages. We also investigate the unique signals of emotions specific to chats and distinguish them from other forms of software communication. Our findings suggest that chats have fewer expressions of Approval and Fear but more expressions of Curiosity compared to GitHub comments. We also notice that Confusion is frequently observed when discussing programming-related information such as unexpected software behavior. Overall, our study highlights the potential of mining emotions in developer chats for supporting software maintenance and evolution tools.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2097–2101},
numpages = {5},
keywords = {emotion analysis, software developer chats},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/2568225.2568305,
author = {Singer, Leif and Figueira Filho, Fernando and Storey, Margaret-Anne},
title = {Software engineering at the speed of light: how developers stay current using twitter},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568305},
doi = {10.1145/2568225.2568305},
abstract = {The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {211–221},
numpages = {11},
keywords = {Awareness, Learning, Microblogging, Social Media, Twitter},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3597503.3608142,
author = {Fang, Hongbo and Herbsleb, James and Vasilescu, Bogdan},
title = {Novelty Begets Popularity, But Curbs Participation - A Macroscopic View of the Python Open-Source Ecosystem},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608142},
doi = {10.1145/3597503.3608142},
abstract = {Who creates the most innovative open-source software projects? And what fate do these projects tend to have? Building on a long history of research to understand innovation in business and other domains, as well as recent advances towards modeling innovation in scientific research from the science of science field, in this paper we adopt the analogy of innovation as emerging from the novel recombination of existing bits of knowledge. As such, we consider as innovative the software projects that recombine existing software libraries in novel ways, i.e., those built on top of atypical combinations of packages as extracted from import statements. We then report on a large-scale quantitative study of innovation in the Python open-source software ecosystem. Our results show that higher levels of innovativeness are statistically associated with higher GitHub star counts, i.e., novelty begets popularity. At the same time, we find that controlling for project size, the more innovative projects tend to involve smaller teams of contributors, as well as be at higher risk of becoming abandoned in the long term. We conclude that innovation and open source sustainability are closely related and, to some extent, antagonistic.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {55},
numpages = {11},
keywords = {open-source software, innovation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ASE.2019.00064,
author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Goues, Claire Le and Neubig, Graham and Vasilescu, Bogdan},
title = {DIRE: a neural approach to decompiled identifier naming},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00064},
doi = {10.1109/ASE.2019.00064},
abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {628–639},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/3611668,
author = {Nourry, Olivier and Kashiwa, Yutaro and Lin, Bin and Bavota, Gabriele and Lanza, Michele and Kamei, Yasutaka},
title = {The Human Side of Fuzzing: Challenges Faced by Developers during Fuzzing Activities},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611668},
doi = {10.1145/3611668},
abstract = {Fuzz testing, also known as fuzzing, is a software testing technique aimed at identifying software vulnerabilities. In recent decades, fuzzing has gained increasing popularity in the research community. However, existing studies led by fuzzing experts mainly focus on improving the coverage and performance of fuzzing techniques. That is, there is still a gap in empirical knowledge regarding fuzzing, especially about the challenges developers face when they adopt fuzzing. Understanding these challenges can provide valuable insights to both practitioners and researchers on how to further improve fuzzing processes and techniques.We conducted a study to understand the challenges encountered by developers during fuzzing. More specifically, we first manually analyzed 829 randomly sampled fuzzing-related GitHub issues and constructed a taxonomy consisting of 39 types of challenges (22 related to the fuzzing process itself, 17 related to using external fuzzing providers). We then surveyed 106 fuzzing practitioners to verify the validity of our taxonomy and collected feedback on how the fuzzing process can be improved. Our taxonomy, accompanied with representative examples and highlighted implications, can serve as a reference point on how to better adopt fuzzing techniques for practitioners, and indicates potential directions researchers can work on toward better fuzzing approaches and practices.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {14},
numpages = {26},
keywords = {Fuzzing, software testing, empirical software engineering}
}

@inproceedings{10.1145/3540250.3558908,
author = {Cassee, Nathan},
title = {Sentiment in software engineering: detection and application},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558908},
doi = {10.1145/3540250.3558908},
abstract = {In software engineering the role of human aspects is an important one, especially as developers indicate that they experience a wide range of emotions while developing software. Within software engineering researchers have sought to understand the role emotions and sentiment play in the  
development of software by studying issues, pull-requests and commit messages.  
To detect sentiment, automated tools are used, and in this doctoral thesis we plan to study the use of these sentiment analysis tools, their applications, best practices for their usage and the effect of non-natural language on their performance. In addition to studying the application of sentiment analysis tools, we also aim to study self-admitted technical debt and bots in software engineering, to understand why developers express sentiment and what they signal when they express sentiment. Through studying both the application of sentiment analysis tools and the role of sentiment in software engineering, we hope to provide practical recommendations for both researchers and developers.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1800–1804},
numpages = {5},
keywords = {human aspects, sentiment, sentiment analysis, software engineering},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3475716.3475785,
author = {Coelho, Fl\'{a}via and Tsantalis, Nikolaos and Massoni, Tiago and Alves, Everton L. G.},
title = {An Empirical Study on Refactoring-Inducing Pull Requests},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475785},
doi = {10.1145/3475716.3475785},
abstract = {Background: Pull-based development has shaped the practice of Modern Code Review (MCR), in which reviewers can contribute code improvements, such as refactorings, through comments and commits in Pull Requests (PRs). Past MCR studies uniformly treat all PRs, regardless of whether they induce refactoring or not. We define a PR as refactoring-inducing, when refactoring edits are performed after the initial commit(s), as either a result of discussion among reviewers or spontaneous actions carried out by the PR developer. Aims: This mixed study (quantitative and qualitative) explores code reviewing-related aspects intending to characterize refactoring-inducing PRs. Method: We hypothesize that refactoring-inducing PRs have distinct characteristics than non-refactoring-inducing ones and thus deserve special attention and treatment from researchers, practitioners, and tool builders. To investigate our hypothesis, we mined a sample of 1,845 Apache's merged PRs from GitHub, mined refactoring edits in these PRs, and ran a comparative study between refactoring-inducing and non-refactoring-inducing PRs. We also manually examined 2,096 review comments and 1,891 detected refactorings from 228 refactoring-inducing PRs. Results: We found 30.2% of refactoring-inducing PRs in our sample and that they significantly differ from non-refactoring-inducing ones in terms of number of commits, code churn, number of file changes, number of review comments, length of discussion, and time to merge. However, we found no statistical evidence that the number of reviewers is related to refactoring-inducement. Our qualitative analysis revealed that at least one refactoring edit was induced by review in 133 (58.3%) of the refactoring-inducing PRs examined. Conclusions: Our findings suggest directions for researchers, practitioners, and tool builders to improve practices around pull-based code review.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {9},
numpages = {12},
keywords = {code review mining, empirical study, refactoring-inducing pull request},
location = {Bari, Italy},
series = {ESEM '21}
}

@article{10.1145/3641005,
author = {Bhimdiwala, Ayesha and Adavi, Krishna Akhil Kumar and Arif, Ahmer},
title = {Fighting for Their Voice: Understanding Indian Muslim Women's Responses to Networked Harassment},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3641005},
doi = {10.1145/3641005},
abstract = {Social computing researchers have described how online harassment is taking on more sophisticated forms where communities rather than lone individuals carry out attacks. At the same time, those who are targeted are coming together with members of the online crowd to confront this networked harassment. However, we currently lack empirical studies of how such self-organizing efforts take place, especially in non-western contexts like India. To address this gap, this paper examines two case studies of gendered Islamophobia, Sulli Deals and Bulli Bai where a group of Indian Muslim women were targeted via fake auctions hosted on GitHub, that were subsequently amplified over social media. We conducted 12 in-depth interviews with the women targeted in these incidents to understand how these women collectively made sense of these incidents, and the social and spiritual resources they drew upon as they constructed a response together. Our findings describe how our participants understood these incidents (e.g. with families and each other); the work they did to combat the harassment (e.g. coordinating a unified media response); and how religious beliefs and spiritual practices played into this (e.g. values around courage, patience, flexibility, and humility). We conclude by discussing how these findings can refresh our understanding of designing community-driven responses to networked harassment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {166},
numpages = {24},
keywords = {India, Islam, Muslim, networked harassment, online harassment, religion and HCI, women}
}

@inproceedings{10.1145/3626252.3630938,
author = {Liu, Rongxin and Zenke, Carter and Liu, Charlie and Holmes, Andrew and Thornton, Patrick and Malan, David J.},
title = {Teaching CS50 with AI: Leveraging Generative Artificial Intelligence in Computer Science Education},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630938},
doi = {10.1145/3626252.3630938},
abstract = {In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had "a personal tutor.'' Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {750–756},
numpages = {7},
keywords = {ai, artificial intelligence, generative ai, large language models, llms},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00138,
author = {Daniel, Gwendal and Cabot, Jordi},
title = {The software challenges of building smart chatbots},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00138},
doi = {10.1109/ICSE-Companion52605.2021.00138},
abstract = {Chatbots are becoming complex software artifacts that require a high-level of expertise in a variety of technical domains. This technical briefing will cover the software engineering challenges of developing high-quality chatbots. Attendees will be able to create their own bots leveraging the open source chatbot development platform Xatkit.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {324–325},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1109/TCBB.2021.3120937,
author = {Pereira, Getulio and Ghosh, Preetam and Santos, Anderson},
title = {A Bridging Centrality Plugin for GEPHI and a Case Study for &lt;italic&gt;Mycobacterium Tuberculosis&lt;/italic&gt; H37Rv},
year = {2021},
issue_date = {Nov.-Dec. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3120937},
doi = {10.1109/TCBB.2021.3120937},
abstract = {Bridging Centrality (BriCe) is a popular measure that combines the Betweenness centrality and Bridging coefficient metrics to characterize nodes acting as a bridge among clusters. However, there were no implementations of the BriCe plugin that can be readily used in the GEPHI software or any other software dedicated to graph-based studies. In this paper, we present the BriCe plugin for GEPHI. It is available as a third-party functionality from the native GEPHI interface as a handy plugin to add; hence, no additional download and installation process is necessary. The BriCe plugin for GEPHI is open-source, and one can access the code through the GEPHI GitHub repository. As a use case of the BriCe plugin, we analyzed the genome of Mycobacterium tuberculosis H37Rv to identify biological explanations on &lt;italic&gt;why some proteins were ranked with top BriCe values?&lt;/italic&gt; For instance, we were able to formulate a new hypothesis combining the predicted sub cellular localization and high BriCe values concerning lipopolysaccharides (LPS) exportation. Our hypothesis provides a possible link among proteins of a glycosyltransferase group and the type VII Secretion System. The Bridging Centrality plugin for GEPHI is an easy to use tool for analyzing complex graphs and draw novel insights from graphical data.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {oct},
pages = {2741–2746},
numpages = {6}
}

@inproceedings{10.1145/3611643.3613093,
author = {Cabra-Acela, Laura and Mojica-Hanke, Anamaria and Linares-V\'{a}squez, Mario and Herbold, Steffen},
title = {On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613093},
doi = {10.1145/3611643.3613093},
abstract = {Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support users’ daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&amp;A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user’s context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at  GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2142–2146},
numpages = {5},
keywords = {Good practices, Information retrieval, Large language models, Machine learning},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3484828,
author = {Amaral, Gabriel and Piscopo, Alessandro and Kaffee, Lucie-aim\'{e}e and Rodrigues, Odinaldo and Simperl, Elena},
title = {Assessing the Quality of Sources in Wikidata Across Languages: A Hybrid Approach},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3484828},
doi = {10.1145/3484828},
abstract = {Wikidata is one of the most important sources of structured data on the web, built by a worldwide community of volunteers. As a secondary source, its contents must be backed by credible references; this is particularly important, as Wikidata explicitly encourages editors to add claims for which there is no broad consensus, as long as they are corroborated by references. Nevertheless, despite this essential link between content and references, Wikidata's ability to systematically assess and assure the quality of its references remains limited. To this end, we carry out a mixed-methods study to determine the relevance, ease of access, and authoritativeness of Wikidata references, at scale and in different languages, using online crowdsourcing, descriptive statistics, and machine learning. Building on previous work of ours, we run a series of microtasks experiments to evaluate a large corpus of references, sampled from Wikidata triples with labels in several languages. We use a consolidated, curated version of the crowdsourced assessments to train several machine learning models to scale up the analysis to the whole of Wikidata. The findings help us ascertain the quality of references in Wikidata and identify common challenges in defining and capturing the quality of user-generated multilingual structured data on the web. We also discuss ongoing editorial practices, which could encourage the use of higher-quality references in a more immediate way. All data and code used in the study are available on GitHub for feedback and further improvement and deployment by the research community.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {23},
numpages = {35},
keywords = {Wikidata, crowdsourcing, verifiability, data quality, knowledge graphs}
}

@inproceedings{10.1145/3487351.3492721,
author = {Osterritter, Luke and Carley, Kathleen M.},
title = {Conversations around organizational risk and insider threat},
year = {2022},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3492721},
doi = {10.1145/3487351.3492721},
abstract = {Organizational risk and resilience as well as insider threat have been studied through the lenses of socio-psychological studies and information and computer sciences. As with all disciplines, it is an area in which practitioners, enthusiasts, and experts discuss the theory, issues, and solutions of the field in various online public forums. Such conversations, despite their public nature, can be difficult to understand and to study, even by those deeply involved in the communities themselves. Who are the key actors? How can we understand and characterize the culture around such communities, the problems they face, and the solutions favored by the experts in the field? Which narratives are being created and propagated, and by whom - and are these actors truly people, or are they autonomous agents, or "bots"?In this paper, we demonstrate the value in applying dynamic network analysis and social network analysis to gain situational awareness of the public conversation around insider threat, nation-state espionage, and industrial espionage. Characterizing public discourse around a topic can reveal individuals and organizations attempting to push or shape narratives in ways that might not be obvious to casual observation. Such techniques have been used to great effect in the study of elections, the COVID-19 pandemic, and the study of misinformation and disinformation, and we hope to show that their use in this area is a powerful way to build a foundation of understanding around the conversations in the online public forum, provide data and analysis for use in further research, and equip counter insider threat practitioners with new insights.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {613–621},
numpages = {9},
keywords = {dynamic network analysis, insider threat, online social networks, organizational risk, social cybersecurity},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/3578503.3583604,
author = {B\"{a}r, Dominik and Calderon, Fausto and Lawlor, Michael and Licklederer, Sophia and Totzauer, Manuel and Feuerriegel, Stefan},
title = {Analyzing Social Media Activities at Bellingcat},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578503.3583604},
doi = {10.1145/3578503.3583604},
abstract = {Open-source journalism emerged as a new phenomenon in the media ecosystem, which uses crowdsourcing to fact-check and generate investigative reports for world events using open sources (e.g., social media). A particularly prominent example is Bellingcat. Bellingcat is known for its investigations on the illegal use of chemical weapons during the Syrian war, the Russian responsibility for downing flight MH17, the identification of the perpetrators in the attempted murder of Alexei Navalny, and war crimes in the Russo-Ukraine war. Crucial for this is social media in order to disseminate findings and crowdsource fact-checks. In this work, we characterize the social media activities at Bellingcat on Twitter. For this, we built a comprehensive dataset of all N = &nbsp;24,682 tweets posted by Bellingcat on Twitter since its inception in July 2014. Our analysis is three-fold: (1)&nbsp;We analyze how Bellingcat uses Twitter to disseminate information and collect information from its follower base. Here, we find a steady increase in both posts and replies over time, particularly during the Russo-Ukrainian war, which is in line with the growing importance of Bellingcat for the traditional media ecosystem. (2)&nbsp;We identify characteristics of posts that are successful in eliciting user engagement. User engagement is particularly large for posts embedding additional media items and with a more negative sentiment. (3)&nbsp;We examine how the follower base has responded to the Russian invasion of Ukraine. Here, we find that the sentiment has become more polarized and negative. We attribute this to a ∼ 13-fold increase in bots interacting with the Bellingcat account. Overall, our findings provide recommendations for how open-source journalism such as Bellingcat can successfully operate on social media.},
booktitle = {Proceedings of the 15th ACM Web Science Conference 2023},
pages = {163–173},
numpages = {11},
keywords = {Bellingcat, Open Source Journalism, Russian Invasion, Social Media, Twitter},
location = {Austin, TX, USA},
series = {WebSci '23}
}

@inproceedings{10.1145/3634737.3637637,
author = {Lepipas, Anastasios and Borovykh, Anastasia and Demetriou, Soteris},
title = {Username Squatting on Online Social Networks: A Study on X},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637637},
doi = {10.1145/3634737.3637637},
abstract = {Adversaries have been targeting unique identifiers to launch typo-squatting, mobile app squatting and even voice squatting attacks. Anecdotal evidence suggest that online social networks (OSNs) are also plagued with accounts that use similar usernames. This can be confusing to users but can also be exploited by adversaries. However, to date no study characterizes this problem on OSNs. In this work, we define the username squatting problem and design the first multi-faceted measurement study to characterize it on X. We develop a username generation tool (UsernameCrazy) to help us analyze hundreds of thousands of username variants derived from celebrity accounts. Our study reveals that thousands of squatted usernames have been suspended by X, while tens of thousands that still exist on the network are likely bots. Out of these, a large number share similar profile pictures and profile names to the original account signalling impersonation attempts. We found that squatted accounts are being mentioned by mistake in tweets hundreds of thousands of times and are even being prioritized in searches by the network's search recommendation algorithm exacerbating the negative impact squatted accounts can have in OSNs. We use our insights and take the first step to address this issue by designing a framework (SQUAD) that combines UsernameCrazy with a new classifier to efficiently detect suspicious squatted accounts. Our evaluation of SQUAD's prototype implementation shows that it can achieve 94% F1-score when trained on a small dataset.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {621–637},
numpages = {17},
keywords = {username squatting, social networks, impersonation, typo-mentions},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@article{10.1145/3565973,
author = {Dong, Guimin and Tang, Mingyue and Wang, Zhiyuan and Gao, Jiechao and Guo, Sikun and Cai, Lihua and Gutierrez, Robert and Campbel, Bradford and Barnes, Laura E. and Boukhechba, Mehdi},
title = {Graph Neural Networks in IoT: A Survey},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3565973},
doi = {10.1145/3565973},
abstract = {The Internet of Things (IoT) boom has revolutionized almost every corner of people’s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.},
journal = {ACM Trans. Sen. Netw.},
month = {apr},
articleno = {47},
numpages = {50},
keywords = {Graph neural network, Internet of Things, sensor network, survey}
}

@article{10.1145/3576042,
author = {Zhang, Ting and Han, Donggyun and Vinayakarao, Venkatesh and Irsan, Ivana Clairine and Xu, Bowen and Thung, Ferdian and Lo, David and Jiang, Lingxiao},
title = {Duplicate Bug Report Detection: How Far Are We?},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3576042},
doi = {10.1145/3576042},
abstract = {Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in the research literature. The industry uses some other techniques. Unfortunately, there is insufficient comparison among them, and it is unclear how far we have been. This work fills this gap by comparing the aforementioned techniques. To compare them, we first need a benchmark that can estimate how a tool would perform if applied in a realistic setting today. Thus, we first investigated potential biases that affect the fair comparison of the accuracy of DBRD techniques. Our experiments suggest that data age and issue tracking system (ITS) choice cause a significant difference. Based on these findings, we prepared a new benchmark. We then used it to evaluate DBRD techniques to estimate better how far we have been. Surprisingly, a simpler technique outperforms recently proposed sophisticated techniques on most projects in our benchmark. In addition, we compared the DBRD techniques proposed in research with those used in Mozilla and VSCode. Surprisingly, we observe that a simple technique already adopted in practice can achieve comparable results as a recently proposed research tool. Our study gives reflections on the current state of DBRD, and we share our insights to benefit future DBRD research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {97},
numpages = {32},
keywords = {Bug reports, duplicate bug report detection, deep learning, empirical study}
}

@inproceedings{10.1145/3148055.3148071,
author = {Torres, Johnny and Vaca, Carmen and Abad, Cristina L.},
title = {What Ignites a Reply? Characterizing Conversations in Microblogs},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148071},
doi = {10.1145/3148055.3148071},
abstract = {Nowadays, microblog platforms provide a medium to share content and interact with other users. With the large-scale data generated on these platforms, the origin and reasons of users engagement in conversations has attracted the attention of the research community. In this paper, we analyze the factors that might spark conversations in Twitter, for the English and Spanish languages. Using a corpus of 2.7 million tweets, we reconstruct existing conversations, then extract several contextual and content features. Based on the features extracted, we train and evaluate several predictive models to identify tweets that will spark a conversation. Our findings show that conversations are more likely to be initiated by users with high activity level and popularity. For less popular users, the type of content generated is a more important factor. Experimental results shows that the best predictive model is able obtain an average score $F1=0.80$. We made available the dataset scripts and code used in this paper to the research community via Github.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {149–156},
numpages = {8},
keywords = {big data, machine learning, social computing},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3631085.3631337,
author = {Lemos, Marcelo Luiz Harry Diniz and Vieira, Ronaldo E Silva and Tavares, Anderson Rocha and Marcolino, Leandro Soriano and Chaimowicz, Luiz},
title = {Scale-Invariant Reinforcement Learning in Real-Time Strategy Games},
year = {2024},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631085.3631337},
doi = {10.1145/3631085.3631337},
abstract = {Real-time strategy games present a significant challenge for artificial game-playing agents by combining several fundamental AI problems. Despite the difficulties, attempts to create autonomous agents using Deep Reinforcement Learning have been successful, with bots like AlphaStar beating even expert human players. Many RTS games include several distinct world maps with different dimensions, which may affect the agent’s observation and the representation of game states. However, most current architectures suffer from fixed input sizes or require extensive and complex training. In this paper, we overcome these limitations by combining Grid-Wise Control with Spatial Pyramid Pooling (SPP). Specifically, we employ the encoder-decoder framework provided by the GridNet architecture and enhance the critic component of PPO by adding an SPP layer to it. The new layer generates a standardized representation of any game state regardless of the initial observation dimensions, allowing the agent to act on any map. Our evaluation demonstrates that our proposed method improves the models’ flexibility and provides a more effective and efficient solution for training autonomous agents in multiple RTS game scenarios.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
pages = {11–19},
numpages = {9},
keywords = {RTS games, game-playing AI, reinforcement learning},
location = {Rio Grande (RS), Brazil},
series = {SBGames '23}
}

@article{10.1145/3588686,
author = {Hu, Sihao and Zhang, Zhen and Lu, Shengliang and He, Bingsheng and Li, Zhao},
title = {Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588686},
doi = {10.1145/3588686},
abstract = {With the proliferation of pump-and-dump schemes (P&amp;Ds) in the cryptocurrency market, it becomes imperative to detect such fraudulent activities in advance to alert potentially susceptible investors. In this paper, we focus on predicting the pump probability of all coins listed in the target exchange before a scheduled pump time, which we refer to as the target coin prediction task. Firstly, we conduct a comprehensive study of the latest 709 P&amp;D events organized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis reveals some interesting patterns of P&amp;Ds, such as that pumped coins exhibit intra-channel homogeneity and inter-channel heterogeneity. Here channel refers a form of group in Telegram that is frequently used to coordinate P&amp;D events. This observation inspires us to develop a novel sequence-based neural network, dubbed SNN, which encodes a channel's P&amp;D event history into a sequence representation via the positional attention mechanism to enhance the prediction accuracy. Positional attention helps to extract useful information and alleviates noise, especially when the sequence length is long. Extensive experiments verify the effectiveness and generalizability of proposed methods. Additionally, we release the code and P&amp;D dataset on GitHub https://github.com/Bayi-Hu/Pump-and-Dump-Detection-on-Cryptocurrency, and regularly update the dataset.},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {6},
numpages = {19},
keywords = {blockchain security, datasets, time-series forecasting}
}

@inproceedings{10.1145/3589334.3645708,
author = {Choi, Yoonjung and Lee, Woonghee and Hur, Junbeom},
title = {PhishinWebView: Analysis of Anti-Phishing Entities in Mobile Apps with WebView Targeted Phishing},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645708},
doi = {10.1145/3589334.3645708},
abstract = {Despite the relentless efforts on developing anti-phishing techniques, phishing attacks continue to proliferate, often incorporating evasion techniques to bypass detection. While recent studies have continuously enhanced our understanding of their evasion techniques in desktop environments, few studies have been conducted to explore how the phishing attack is being handled in mobile environments, specifically WebView.In this study, we systematically evaluate the blocking processes of anti-phishing entities in individual apps in the real world by designing the phishing attack tailored to WebView. Specifically, we select eight well-known apps using WebView, and report 80 typical phishing sites (without evasion techniques) and 130 user-agent-specific phishing sites (accessible exclusively via each app's WebView). For scalable analysis, we develop an autonomous evaluation framework and investigate accessibility of both apps and Safe Browsing entities. As a result, we find that user-agent-specific (UA-specific) phishing sites successfully evade blocking across all of the eight Android apps. We also investigate accessing strategies of anti-phishing crawlers of both the apps and Safe Browsing entities; and find that only two apps' crawlers can access UA-specific phishing sites without any subsequent actions such as blocking the link. Based on our experiment results, we present security recommendations to take proactive phishing cautions using link preview bots. To the best of our knowledge, this is the first study that explores how the WebView environments handle phishing attacks and disclose their limitation in the real world.},
booktitle = {Proceedings of the ACM on Web Conference 2024},
pages = {1923–1932},
numpages = {10},
keywords = {evasion technique, phishing, webview security},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3038912.3052587,
author = {Simeonovski, Milivoj and Pellegrino, Giancarlo and Rossow, Christian and Backes, Michael},
title = {Who Controls the Internet? Analyzing Global Threats using Property Graph Traversals},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052587},
doi = {10.1145/3038912.3052587},
abstract = {The Internet is built on top of intertwined network services, e.g., email, DNS, and content distribution networks operated by private or governmental organizations. Recent events have shown that these organizations may, knowingly or unknowingly, be part of global-scale security incidents including state-sponsored mass surveillance programs and large-scale DDoS attacks. For example, in March 2015 the Great Cannon attack has shown that an Internet service provider can weaponize millions of Web browsers and turn them into DDoS bots by injecting malicious JavaScript code into transiting TCP connections.While attack techniques and root cause vulnerabilities are routinely studied, we still lack models and algorithms to study the intricate dependencies between services and providers, reason on their abuse, and assess the attack impact. To close this gap, we present a technique that models services, providers, and dependencies as a property graph. Moreover, we present a taint-style propagation-based technique to query the model, and present an evaluation of our framework on the top 100k Alexa domains.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {647–656},
numpages = {10},
keywords = {(dos) denial of service attacks, cyber-attacks, property graph traversals},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3524842.3528018,
author = {Subash, Keerthana Muthu and Kumar, Lakshmi Prasanna and Vadlamani, Sri Lakshmi and Chatterjee, Preetha and Baysal, Olga},
title = {DISCO: a dataset of discord chat conversations for software engineering research},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528018},
doi = {10.1145/3524842.3528018},
abstract = {Today, software developers work on complex and fast-moving projects that often require instant assistance from other domain and subject matter experts. Chat servers such as Discord facilitate live communication and collaboration among developers all over the world. With numerous topics discussed in parallel, mining and analyzing the chat data of these platforms would offer researchers and tool makers opportunities to develop software tools and services such as automated virtual assistants, chat bots, chat summarization techniques, Q&amp;A thesaurus, and more.In this paper, we propose a dataset called DISCO consisting of the one-year public DIScord chat COnversations of four software development communities. We have collected the chat data of the channels containing general programming Q&amp;A discussions from the four Discord servers, applied a disentanglement technique [13] to extract conversations from the chat transcripts, and performed a manual validation of conversations on a random sample (500 conversations). Our dataset consists of 28, 712 conversations, 1, 508, 093 messages posted by 323, 562 users. As a case study on the dataset, we applied a topic modelling technique for extracting the top five general topics that are most discussed in each Discord channel.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {227–231},
numpages = {5},
keywords = {chat conversations, conversation disentanglement, discord, online communities, software developers},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1109/ICSE-Companion.2019.00082,
author = {Brown, Chris},
title = {Digital nudges for encouraging developer actions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00082},
doi = {10.1109/ICSE-Companion.2019.00082},
abstract = {Researchers have examined a wide variety of practices to help software engineers complete different programming tasks. Despite the fact that studies show software engineering practices and tools created to improve the software development process are useful for preventing bugs, decreasing debugging costs, reducing debugging time, and providing additional benefits, software engineers rarely use them in practice. To persuade humans to alter and adopt new behaviors, psychologists have studied the concept of nudges. My research aims to investigate how digital nudges, or the process of using technology to automatically create nudges, can be beneficial in helping software developers and teams adopt software engineering activities and integrate them into their normal workflow.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {202–205},
numpages = {4},
keywords = {developer actions, nudge theory, software engineering, tool adoption},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3569949,
author = {Joblin, Mitchell and Eckl, Barbara and Bock, Thomas and Schmid, Angelika and Siegmund, Janet and Apel, Sven},
title = {Hierarchical and Hybrid Organizational Structures in Open-source Software Projects: A Longitudinal Study},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3569949},
doi = {10.1145/3569949},
abstract = {Despite the absence of a formal process and a central command-and-control structure, developer organization in open-source software (OSS) projects are far from being a purely random process. Prior work indicates that, over time, highly successful OSS projects develop a hybrid organizational structure that comprises a hierarchical part and a non-hierarchical part. This suggests that hierarchical organization is not necessarily a global organizing principle and that a fundamentally different principle is at play below the lowest positions in the hierarchy. Given the vast proportion of developers are in the non-hierarchical part, we seek to understand the interplay between these two fundamentally differently organized groups, how this hybrid structure evolves, and the trajectory individual developers take through these structures over the course of their participation. We conducted a longitudinal study of the full histories of 20&nbsp;popular OSS projects, modeling their organizational structures as networks of developers connected by communication ties and characterizing developers’ positions in terms of hierarchical (sub)structures in these networks. We observed a number of notable trends and patterns in the subject projects: (1)&nbsp;hierarchy is a pervasive structural feature of developer networks of OSS projects; (2)&nbsp;OSS projects tend to form hybrid organizational structures, consisting of a hierarchical and a non-hierarchical part; and (3)&nbsp;the positional trajectory of a developer starts loosely connected in the non-hierarchical part and then tightly integrate into the hierarchical part, which is associated with the acquisition of experience (tenure), in addition to coordination and coding activities. Our study (a)&nbsp;provides a methodological basis for further investigations of hierarchy formation, (b)&nbsp;suggests a number of hypotheses on prevalent organizational patterns and trends in OSS projects to be addressed in further work, and (c)&nbsp;may ultimately guide the governance of organizational structures.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {86},
numpages = {29},
keywords = {Open-source software projects, developer networks, organizational structure, hierarchy}
}

@inproceedings{10.1145/3345629.3345637,
author = {S\"{u}l\"{u}n, Emre and T\"{u}z\"{u}n, Eray and Do\u{g}rus\"{o}z, U\u{g}ur},
title = {Reviewer Recommendation using Software Artifact Traceability Graphs},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345637},
doi = {10.1145/3345629.3345637},
abstract = {Various types of artifacts (requirements, source code, test cases, documents, etc.) are produced throughout the lifecycle of a software. These artifacts are often related with each other via traceability links that are stored in modern application lifecycle management repositories. Throughout the lifecycle of a software, various types of changes can arise in any one of these artifacts. It is important to review such changes to minimize their potential negative impacts. To maximize benefits of the review process, the reviewer(s) should be chosen appropriately.In this study, we reformulate the reviewer suggestion problem using software artifact traceability graphs. We introduce a novel approach, named RSTrace, to automatically recommend reviewers that are best suited based on their familiarity with a given artifact. The proposed approach, in theory, could be applied to all types of artifacts. For the purpose of this study, we focused on the source code artifact and conducted an experiment on finding the appropriate code reviewer(s). We initially tested RSTrace on an open source project and achieved top-3 recall of 0.85 with an MRR (mean reciprocal ranking) of 0.73. In a further empirical evaluation of 37 open source projects, we confirmed that the proposed reviewer recommendation approach yields promising top-k and MRR scores on the average compared to the existing reviewer recommendation approaches.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {66–75},
numpages = {10},
keywords = {code review, modern code review, pull-request review, reviewer recommendation, software traceability, suggesting reviewers},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@article{10.1145/3654659,
author = {Brzezinski, Dariusz and Stachowiak, Julia and Stefanowski, Jerzy and Szczech, Izabela and Susmaga, Robert and Aksenyuk, Sofya and Ivashka, Uladzimir and Yasinskyi, Oleksandr},
title = {Properties of Fairness Measures in the Context of Varying Class Imbalance and Protected Group Ratios},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3654659},
doi = {10.1145/3654659},
abstract = {Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jun},
articleno = {170},
numpages = {18},
keywords = {Group fairness, class imbalance, protected group imbalance}
}

@inproceedings{10.1145/3544902.3546639,
author = {Rahman, Shadikur and Koana, Umme Ayman and Nayebi, Maleknaz},
title = {Example Driven Code Review Explanation},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546639},
doi = {10.1145/3544902.3546639},
abstract = {Background: Code reviewing is an essential part of software development to ensure software quality. However, the abundance of review tasks and the intensity of the workload for reviewers negatively impact the quality of the reviews. The short review text is often unactionable. Aims: We propose the Example Driven Review Explanation (EDRE) method to facilitate the code review process by adding additional explanations through examples. EDRE recommends similar code reviews as examples to further explain a review and help a developer to understand the received reviews with less communication overhead. Method: Through an empirical study in an industrial setting and by analyzing 3,722 Code reviews across three open-source projects, we compared five methods of data retrieval, text classification, and text recommendation. Results: EDRE using TF-IDF word embedding along with an SVM classifier can provide practical examples for each code review with 92% F-score and 90% Accuracy. Conclusions: The example-based explanation is an established method for assisting experts in explaining decisions. EDRE can accurately provide a set of context-specific examples to facilitate the code review process in software teams.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {307–312},
numpages = {6},
keywords = {Code Review, Decision Explanation, Natural Language Processing (NLP), Software Engineering},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1109/ASE.2019.00144,
author = {Zhou, Shurui},
title = {Improving collaboration efficiency in fork-based development},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00144},
doi = {10.1109/ASE.2019.00144},
abstract = {Fork-based development is a lightweight mechanism that allows developers to collaborate with or without explicit coordination. Although it is easy to use and popular, when developers each create their own fork and develop independently, their contributions are usually not easily visible to others. When the number of forks grows, it becomes very difficult to maintain an overview of what happens in individual forks, which would lead to additional problems and inefficient practices: lost contributions, redundant development, fragmented communities, and so on. Facing the problems mentioned above, we developed two complementary strategies: (1) Identifying existing best practices and suggesting evidence-based interventions for projects that are inefficient; (2) designing new interventions that could improve the awareness of a community using fork-based development, and help developers to detect redundant development to reduce unnecessary effort.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1218–1221},
numpages = {4},
keywords = {awareness of collaboration, distributed collaboration, fork-based development, open-source community},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3517745.3561433,
author = {Edu, Jide and Mulligan, Cliona and Pierazzi, Fabio and Polakis, Jason and Suarez-Tangil, Guillermo and Such, Jose},
title = {Exploring the security and privacy risks of chatbots in messaging services},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561433},
doi = {10.1145/3517745.3561433},
abstract = {The unprecedented adoption of messaging platforms for work and recreation has made it an attractive target for malicious actors. In this context, third-party apps (so-called chatbots) offer a variety of attractive functionalities that support the experience in large channels. Unfortunately, under the current permission and deployment models, chatbots in messaging systems could steal information from channels without the victim's awareness. In this paper, we propose a methodology that incorporates static and dynamic analysis for automatically assessing security and privacy issues in messaging platform chatbots. We also provide preliminary findings from the popular Discord platform that highlight the risks that chatbots pose to users. Unlike other popular platforms like Slack or MS Teams, Discord does not implement user-permission checks---a task entrusted to third-party developers. Among others, we find that 55% of chatbots from a leading Discord repository request the "administrator" permission, and only 4.35% of chatbots with permissions actually provide a privacy policy.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {581–588},
numpages = {8},
keywords = {chatbots, discord, messaging platorms, security and privacy},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3597926.3598092,
author = {Liu, Hao and Wang, Yanlin and Wei, Zhao and Xu, Yong and Wang, Juhong and Li, Hui and Ji, Rongrong},
title = {RefBERT: A Two-Stage Pre-trained Framework for Automatic Rename Refactoring},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598092},
doi = {10.1145/3597926.3598092},
abstract = {Refactoring is an indispensable practice of improving the quality and maintainability of source code in software evolution. Rename refactoring is the most frequently performed refactoring that suggests a new name for an identifier to enhance readability when the identifier is poorly named. However, most existing works only identify renaming activities between two versions of source code, while few works express concern about how to suggest a new name. In this paper, we study automatic rename refactoring on variable names, which is considered more challenging than other rename refactoring activities. We first point out the connections between rename refactoring and various prevalent learning paradigms and the difference between rename refactoring and general text generation in natural language processing. Based on our observations, we propose RefBERT, a two-stage pre-trained framework for rename refactoring on variable names. RefBERT first predicts the number of sub-tokens in the new name and then generates sub-tokens accordingly. Several techniques, including constrained masked language modeling, contrastive learning, and the bag-of-tokens loss, are incorporated into RefBERT to tailor it for automatic rename refactoring on variable names. Through extensive experiments on our constructed refactoring datasets, we show that the generated variable names of RefBERT are more accurate and meaningful than those produced by the existing method. Our implementation and data are available at https://github.com/KDEGroup/RefBERT.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {740–752},
numpages = {13},
keywords = {bag-of-tokens loss, contrastive learning, language modeling, rename refactoring},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3238147.3238190,
author = {Liu, Zhongxin and Xia, Xin and Hassan, Ahmed E. and Lo, David and Xing, Zhenchang and Wang, Xinyu},
title = {Neural-machine-translation-based commit message generation: how far are we?},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238190},
doi = {10.1145/3238147.3238190},
abstract = {Commit messages can be regarded as the documentation of software changes. These messages describe the content and purposes of changes, hence are useful for program comprehension and software maintenance. However, due to the lack of time and direct motivation, commit messages sometimes are neglected by developers. To address this problem, Jiang et al. proposed an approach (we refer to it as NMT), which leverages a neural machine translation algorithm to automatically generate short commit messages from code. The reported performance of their approach is promising, however, they did not explore why their approach performs well. Thus, in this paper, we first perform an in-depth analysis of their experimental results. We find that (1) Most of the test &lt;pre&gt;diffs&lt;/pre&gt; from which NMT can generate high-quality messages are similar to one or more training &lt;pre&gt;diffs&lt;/pre&gt; at the token level. (2) About 16% of the commit messages in Jiang et al.’s dataset are noisy due to being automatically generated or due to them describing repetitive trivial changes. (3) The performance of NMT declines by a large amount after removing such noisy commit messages. In addition, NMT is complicated and time-consuming. Inspired by our first finding, we proposed a simpler and faster approach, named NNGen (Nearest Neighbor Generator), to generate concise commit messages using the nearest neighbor algorithm. Our experimental results show that NNGen is over 2,600 times faster than NMT, and outperforms NMT in terms of BLEU (an accuracy measure that is widely used to evaluate machine translation systems) by 21%. Finally, we also discuss some observations for the road ahead for automated commit message generation to inspire other researchers.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {373–384},
numpages = {12},
keywords = {Commit message generation, Nearest neighbor algorithm, Neural machine translation},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3551349.3559526,
author = {Jarukitpipat, Vipawan and Chhun, Klinton and Wanprasert, Wachirayana and Ragkhitwetsagul, Chaiyong and Choetkiertikul, Morakot and Sunetnanta, Thanwadee and Kula, Raula Gaikovina and Chinthanet, Bodin and Ishio, Takashi and Matsumoto, Kenichi},
title = {V-Achilles: An Interactive Visualization of Transitive Security Vulnerabilities},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559526},
doi = {10.1145/3551349.3559526},
abstract = {A key threat to the usage of third-party dependencies has been the threat of security vulnerabilities, which risks unwanted access to a user application. As part of an ecosystem of dependencies, users of a library are prone to both the direct and transitive dependencies adopted into their applications. Recent work involves tool supports for vulnerable dependency updates, rarely showing the complexity of the transitive updates. In this paper, we introduce our solution to support vulnerability updating in npm. V-Achilles is a prototype that shows a visualization (i.e., using dependency graphs) affected by vulnerability attacks. In addition to the tool overview, we highlight three use cases to demonstrate the usefulness and application of our prototype with real-world npm packages. The prototype is available at https://github.com/MUICT-SERU/V-Achilles, with an accompanying video demonstration at https://www.youtube.com/watch?v=tspiZfhMNcs.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {169},
numpages = {4},
keywords = {fixing known vulnerabilities, software libraries},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3644815.3644981,
author = {Rahman, Md Tajmilur and Singh, Rahul and Sultan, Mir Yousuf},
title = {Automating Patch Set Generation from Code Reviews Using Large Language Models},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644981},
doi = {10.1145/3644815.3644981},
abstract = {The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs' feedback, thereby evaluating their readiness to support developers in responding to code-review comments. Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {273–274},
numpages = {2},
keywords = {large language models, automated code review, software engineering, pull requests, code quality},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@article{10.1145/3512941,
author = {Wang, Dakuo and Wang, Haoyu and Yu, Mo and Ashktorab, Zahra and Tan, Ming},
title = {Group Chat Ecology in Enterprise Instant Messaging: How Employees Collaborate Through Multi-User Chat Channels on Slack},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512941},
doi = {10.1145/3512941},
abstract = {Despite the long history of studying instant messaging usage, we know very little about how today's people participate in group chat channels and interact with others inside a real-world organization. In this short paper, we aim to update the existing knowledge on how group chat is used in the context of today's organizations. The knowledge is particularly important for the new norm of remote works under the COVID-19 pandemic. We have the privilege of collecting two valuable datasets: a total of 4,300 group chat channels in Slack from an R&amp;D department in a multinational IT company; and a total of 117 groups' performance data. Through qualitative coding of 100 randomly sampled group channels from the 4,300 channels dataset, we identified and reported 9 categories such as Project channels, IT-Support channels, and Event channels. We further defined a feature metric with 21 meta-features (and their derived features) without looking at the message content to depict the group communication style for these group chat channels, with which we successfully trained a machine learning model that can automatically classify a given group channel into one of the 9 categories. In addition to the descriptive data analysis, we illustrated how these communication metrics can be used to analyze team performance. We cross-referenced 117 project teams and their team-based Slack channels and identified 57 teams that appeared in both datasets, then we built a regression model to reveal the relationship between these group communication styles and the project team performance. This work contributes an updated empirical understanding of human-human communication practices within the enterprise setting, and suggests design opportunities for the future of human-AI communication experience.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {94},
numpages = {14},
keywords = {channel, communication style, conversation, enterprise, group chat, instant messaging, organizational behavior, remote work, slack, sms, team performance}
}

@article{10.1145/3568392,
author = {Srba, Ivan and Moro, Robert and Tomlein, Matus and Pecher, Branislav and Simko, Jakub and Stefancova, Elena and Kompan, Michal and Hrckova, Andrea and Podrouzek, Juraj and Gavornik, Adrian and Bielikova, Maria},
title = {Auditing YouTube’s Recommendation Algorithm&nbsp;for Misinformation Filter Bubbles},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3568392},
doi = {10.1145/3568392},
abstract = {In this article, we present results of an auditing study performed over YouTube aimed at investigating how fast a user can get into a misinformation filter bubble, but also what it takes to “burst the bubble,” i.e., revert the bubble enclosure. We employ a sock puppet audit methodology, in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation-promoting content. Then they try to burst the bubbles and reach more balanced recommendations by watching misinformation-debunking content. We record search results, home page results, and recommendations for the watched videos. Overall, we recorded 17,405 unique videos, out of which we manually annotated 2,914 for the presence of misinformation. The labeled data was used to train a machine learning model classifying videos into three classes (promoting, debunking, neutral) with the accuracy of 0.82. We use the trained model to classify the remaining videos that would not be feasible to annotate manually.Using both the manually and automatically annotated data, we observe the misinformation bubble dynamics for a range of audited topics. Our key finding is that even though filter bubbles do not appear in some situations, when they do, it is possible to burst them by watching misinformation-debunking content (albeit it manifests differently from topic to topic). We also observe a sudden decrease of misinformation filter bubble effect when misinformation-debunking videos are watched after misinformation-promoting videos, suggesting a strong contextuality of recommendations. Finally, when comparing our results with a previous similar study, we do not observe significant improvements in the overall quantity of recommended misinformation content.},
journal = {ACM Trans. Recomm. Syst.},
month = {jan},
articleno = {6},
numpages = {33},
keywords = {Audit, recommender systems, filter bubble, misinformation, personalization, automatic labeling, ethics, YouTube}
}

@inproceedings{10.1145/3477314.3507255,
author = {Ca\~{n}izares, Pablo C. and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Automating the measurement of heterogeneous chatbot designs},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507255},
doi = {10.1145/3477314.3507255},
abstract = {Chatbots are being increasingly used to provide a natural language interface to all kinds of software services. However, while there are many platforms and tools for chatbot development, they typically lack support to statically measure properties of the designed chatbots, as indicators of their size, complexity, quality or usability, and facilitating comparison.To attack this problem, in this paper we propose a suite of 20 metrics for chatbot designs. The metrics are defined on a neutral chatbot design language, becoming independent of the implementation platform. We have developed a tool, called Asymob, which supports the translation of chatbots defined in several platforms into this neutral format to perform the measurements. As a proof-of-concept, we evaluate the metrics over a collection of Dialogflow and Rasa chatbots from several sources and open-source repositories. Our metrics helped detecting quality issues statically, and served as a basis for comparing chatbots from different origins and built using different technologies.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1491–1498},
numpages = {8},
keywords = {chatbot design, metrics, quality assurance},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3618257.3624841,
author = {Ardi, Calvin and Calder, Matt},
title = {The Prevalence of Single Sign-On on the Web: Towards the Next Generation of Web Content Measurement},
year = {2023},
isbn = {9798400703829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618257.3624841},
doi = {10.1145/3618257.3624841},
abstract = {Much of the content and structure of the Web remains inaccessible to evaluate at scale because it is gated by user authentication. This limitation restricts researchers to examining only a superficial layer of a website: the landing page or public, search-indexable pages. Since it is infeasible to create individual accounts across thousands of webpages, we examine the prevalence of Single Sign-On (SSO) on the web to explore the feasibility of using a few accounts to authenticate to many sites. We find that 58% of the top 10K websites with logins are accessible with popular 3rd-party SSO providers, such as Google, Facebook, and Apple, indicating that leveraging SSO offers a scalable solution to access a large volume of user-gated content.},
booktitle = {Proceedings of the 2023 ACM on Internet Measurement Conference},
pages = {124–130},
numpages = {7},
keywords = {single sign-on, top lists, web authentication, web measurement},
location = {Montreal QC, Canada},
series = {IMC '23}
}

@inproceedings{10.1109/ASE.2019.00052,
author = {Horton, Eric and Parnin, Chris},
title = {V2: fast detection of configuration drift in Python},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00052},
doi = {10.1109/ASE.2019.00052},
abstract = {Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions.We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {477–488},
numpages = {12},
keywords = {configuration drift, configuration management, configuration repair, dependencies, environment inference},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/3479575,
author = {Smith, Micah J. and Cito, J\"{u}rgen and Lu, Kelvin and Veeramachaneni, Kalyan},
title = {Enabling Collaborative Data Science Development with the Ballet Framework},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479575},
doi = {10.1145/3479575},
abstract = {While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {431},
numpages = {39},
keywords = {collaborative framework, data science, feature definition, feature engineering, feature validation, machine learning, mutual information, streaming feature selection}
}

@inproceedings{10.1145/3613905.3650896,
author = {Omidvar Tehrani, Behrooz and M, Ishaani and Anubhai, Anmol},
title = {Evaluating Human-AI Partnership for LLM-based Code Migration},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650896},
doi = {10.1145/3613905.3650896},
abstract = {The potential of Generative AI, especially Large Language Models (LLMs), to transform software development is remarkable. In this paper, we focus on one area in software development called “code migration”. We define code migration as the process of transitioning the language version of a code repository by converting both the source code and its dependencies. Carefully designing an effective human-AI partnership is essential for boosting developer productivity and faster migrations when performing code migrations. Though human-AI partnerships have been generally explored in the literature, their application to code migrations remains largely unexamined. In this work, we leverage an LLM-based code migration tool called Amazon Q Code Transformation to conduct semi-structured interviews with 11 participants undertaking code migrations. We discuss human’s role in the human-AI partnership (human as a director and a reviewer) and define a trust framework based on various model outcomes to earn trust with LLMs. The guidelines presented in this paper offer a vital starting point for designing human-AI partnerships that effectively augment and complement human capabilities in software development with Generative AI.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {133},
numpages = {8},
keywords = {Application Modernization, Code Migration, Human-AI Partnership, Human-in-the-Loop Techniques, Trust Framework},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3643991.3644910,
author = {Lin, Hong Yi and Thongtanunam, Patanamon and Treude, Christoph and Charoenwet, Wachiraphan},
title = {Improving Automated Code Reviews: Learning From Experience},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644910},
doi = {10.1145/3643991.3644910},
abstract = {Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {278–283},
numpages = {6},
keywords = {code review, review comments, neural machine translation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3468264.3468589,
author = {Soto-Valero, C\'{e}sar and Durieux, Thomas and Baudry, Benoit},
title = {A longitudinal analysis of bloated Java dependencies},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468589},
doi = {10.1145/3468264.3468589},
abstract = {We study the evolution and impact of bloated dependencies in a single software ecosystem: Java/Maven. Bloated dependencies are third-party libraries that are packaged in the application binary but are not needed to run the application. We analyze the history of 435 Java projects. This historical data includes 48,469 distinct dependencies, which we study across a total of 31,515 versions of Maven dependency trees. Bloated dependencies steadily increase over time, and 89.2% of the direct dependencies that are bloated remain bloated in all subsequent versions of the studied projects. This empirical evidence suggests that developers can safely remove a bloated dependency. We further report novel insights regarding the unnecessary maintenance efforts induced by bloat. We find that 22% of dependency updates performed by developers are made on bloated dependencies, and that Dependabot suggests a similar ratio of updates on bloated dependencies.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1021–1031},
numpages = {11},
keywords = {dependencies, java, software bloat},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3388440.3412413,
author = {Oniani, David and Wang, Yanshan},
title = {A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412413},
doi = {10.1145/3388440.3412413},
abstract = {COVID-19 (2019 Novel Coronavirus) has resulted in an ongoing pandemic and as of 26 July 2020, has caused more than 15.7 million cases and over 640,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf (Term Frequency - Inverse Document Frequency), Bidirectional Encoder Representations from Transformers (BERT), Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT), and Universal Sentence Encoder (USE) to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online and made its source code available free of charge to anyone interested in running it locally, online, or just for experimental purposes. Overall, our work has yielded significant results in both designing a chatbot that produces high-quality responses to COVID-19-related questions and comparing several embedding generation techniques.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {33},
numpages = {9},
keywords = {ai, bert, biobert, cord-19, covid-19, dataset, gpt-2, nlp, semantic similarity, tf-idf, use},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3524842.3528432,
author = {Kudrjavets, Gunnar and Kumar, Aditya and Nagappan, Nachiappan and Rastogi, Ayushi},
title = {Mining code review data to understand waiting times between acceptance and merging: an empirical analysis},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528432},
doi = {10.1145/3524842.3528432},
abstract = {Increasing code velocity (or the speed with which code changes are reviewed and merged) is integral to speeding up development and contributes to the work satisfaction of engineers. While factors affecting code change acceptance have been investigated in the past, solutions to decrease the code review lifetime are less understood. This study investigates the code review process to quantify delays and investigate opportunities to potentially increase code velocity. We study the temporal characteristics of half a million code reviews hosted on Gerrit and Phabricator, starting from the first response, to a decision to accept or reject the changes, and until the changes are merged into a target branch. We identified two types of time delays: (a) the wait time from the proposal of code changes until first response, and (b) the wait time between acceptance and merging. Our study indicates that reducing the time between acceptance and merging has the potential to speed up Phabricator code reviews by 29--63%. Small code changes and changes made by authors with a large number of previously accepted code reviews have a higher chance of being immediately accepted, without code review iterations. Our analysis suggests that switching from manual to automatic merges can help increase code velocity.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {579–590},
numpages = {12},
keywords = {code review, code velocity, developer productivity, non-productive time},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3448734.3450911,
author = {Zhao, Hao and Shu, Hui and Xing, Ying},
title = {A Review on IoT Botnet},
year = {2021},
isbn = {9781450389570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448734.3450911},
doi = {10.1145/3448734.3450911},
abstract = {The rapid development of the Internet of Things (IoT) also generates security threats that cannot be ignored. This paper lists the development history of botnets, introduces representative IoT botnets, and reveals their new characteristics. The botnet mechanism and characteristics of IoT botnets are analyzed in depth from three aspects: composition structure, life cycle, and attack behavior, and are compared with traditional botnets. Networks for comparison. The analysis discusses the details of IoT botnet technologies such as scanning discovery, covert communication and survival residency. Summarizes the research status of IoT botnet detection technologies and gives future directions. Finally, the importance of preventing IoT botnets and solving IoT security problems is discussed.},
booktitle = {The 2nd International Conference on Computing and Data Science},
articleno = {181},
numpages = {7},
keywords = {IoT, botnet, cyber security, summary},
location = {Stanford, CA, USA},
series = {CONF-CDS 2021}
}

@inproceedings{10.1145/3575879.3576012,
author = {Dagkoulis, Ioannis and Moussiades, Lefteris},
title = {A Comparative Evaluation of Chatbot Development Platforms},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3576012},
doi = {10.1145/3575879.3576012},
abstract = {Chatbots and virtual assistants have become part of people's everyday life. The need for mass production of these services rapidly and efficiently has created an explosion of software-related services focused on developing chatbots. Big companies like Google, Microsoft, Amazon, and IBM offer complete Chatbot Development Platforms and compete with each other. Our effort is to help people interested in using these platforms decide which is the best CDP for their case. Similar attempts have happened but are now outdated as CDPs have introduced breaking changes. We study each CDP, define criteria and calculate scores based on requirement assumptions. In parallel, we observe how innovations in NLP are presented in the market through CDPs.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {322–328},
numpages = {7},
location = {Athens, Greece},
series = {PCI '22}
}

@inproceedings{10.5555/3091125.3091387,
author = {Medeiros, Lenin and Bosse, Tibor},
title = {An Empathic Agent that Alleviates Stress by Providing Support via Social Media},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper describes the development of an 'artificial friend', i.e., an intelligent agent that provides support via text messages in social media in order to alleviate the stress that users experience as a result of everyday problems. The agent consists of three main components: 1) a module that processes text messages based on text mining and classifies them into categories of problems, 2) a module that selects appropriate support strategies based on a validated psychological model of emotion regulation, and 3) a module that generates appropriate responses based on the output of the first two modules. The application is able to interact with users via the social network Telegram.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1634–1636},
numpages = {3},
keywords = {chatbots, emotion regulation, empathic agents, experimentation, social media, text mining},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/3643991.3644918,
author = {Tufano, Rosalia and Mastropaolo, Antonio and Pepe, Federica and Dabic, Ozren and Di Penta, Massimiliano and Bavota, Gabriele},
title = {Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644918},
doi = {10.1145/3643991.3644918},
abstract = {Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {571–583},
numpages = {13},
keywords = {ChatGPT, empirical study},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3597021,
author = {Chen, Yaxing and Zheng, Qinghua and Yan, Zheng},
title = {Efficient Bi-objective SQL Optimization for Enclaved Cloud Databases with Differentially Private Padding},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3597021},
doi = {10.1145/3597021},
abstract = {Hardware-enabled enclaves have been applied to efficiently enforce data security and privacy protection in cloud database services. Such enclaved systems, however, are reported to suffer from I/O-size (also referred to as communication-volume)-based side-channel attacks. Albeit differentially private padding has been exploited to defend against these attacks as a principle method, it introduces a challenging bi-objective parametric query optimization (BPQO) problem and current solutions are still not satisfactory. Concretely, the goal in BPQO is to find a Pareto-optimal plan that makes a tradeoff between query performance and privacy loss; existing solutions are subjected to poor computational efficiency and high cloud resource waste. In this article, we propose a two-phase optimization algorithm called TPOA to solve the BPQO problem. TPOA incorporates two novel ideas: divide-and-conquer to separately handle parameters according to their types in optimization for dimensionality reduction; on-demand-optimization to progressively build a set of necessary Pareto-optimal plans instead of seeking a complete set for saving resources. Besides, we introduce an acceleration mechanism in TPOA to improve its efficiency, which prunes the non-optimal candidate plans in advance. We theoretically prove the correctness of TPOA, numerically analyze its complexity, and formally give an end-to-end privacy analysis. Through a comprehensive evaluation on its efficiency by running baseline algorithms over synthetic and test-bed benchmarks, we can conclude that TPOA outperforms all benchmarked methods with an overall efficiency improvement of roughly two orders of magnitude; moreover, the acceleration mechanism speeds up TPOA by 10-200\texttimes{}.},
journal = {ACM Trans. Database Syst.},
month = {jun},
articleno = {6},
numpages = {40},
keywords = {Enclaved database, SGX, SQL queries, differential privacy, bi-objective parametric query optimization, tradeoff between performance and privacy loss}
}

@inproceedings{10.1145/3544548.3580780,
author = {Ebert, Nico and Scheppler, Bj\"{o}rn and Ackermann, Kurt Alexander and Geppert, Tim},
title = {QButterfly: Lightweight Survey Extension for Online User Interaction Studies for Non-Tech-Savvy Researchers},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580780},
doi = {10.1145/3544548.3580780},
abstract = {We provide a user-friendly, flexible, and lightweight open-source HCI toolkit (github.com/QButterfly) that allows non-tech-savvy researchers to conduct online user interaction studies using the widespread Qualtrics and LimeSurvey platforms. These platforms already provide rich functionality (e.g., for experiments or usability tests) and therefore lend themselves to an extension to display stimulus web pages and record clickstreams. The toolkit consists of a survey template with embedded JavaScript, a JavaScript library embedded in the HTML web pages, and scripts to analyze the collected data. No special programming skills are required to set up a study or match survey data and user interaction data after data collection. We empirically validated the software in a laboratory and a field study. We conclude that this extension, even in its preliminary version, has the potential to make online user interaction studies (e.g., with crowdsourced participants) accessible to a broader range of researchers.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {161},
numpages = {8},
keywords = {HCI toolkit, LimeSurvey, Qualtrics, online experiments, online user interaction studies, open source},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3644385,
author = {Chapman, Adriane and Lauro, Luca and Missier, Paolo and Torlone, Riccardo},
title = {Supporting Better Insights of Data Science Pipelines with Fine-grained Provenance},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3644385},
doi = {10.1145/3644385},
abstract = {Successful data-driven science requires complex data engineering pipelines to clean, transform, and alter data in preparation for machine learning, and robust results can only be achieved when each step in the pipeline can be justified, and its effect on the data explained. In this framework, we aim at providing data scientists with facilities to gain an in-depth understanding of how each step in the pipeline affects the data, from the raw input to training sets ready to be used for learning. Starting from an extensible set of data preparation operators commonly used within a data science setting, in this work we present a provenance management infrastructure for generating, storing, and querying very granular accounts of data transformations, at the level of individual elements within datasets whenever possible. Then, from the formal definition of a core set of data science preprocessing operators, we derive a provenance semantics embodied by a collection of templates expressed in PROV, a standard model for data provenance. Using those templates as a reference, our provenance generation algorithm generalises to any operator with observable input/output pairs. We provide a prototype implementation of an application-level provenance capture library to produce, in a semi-automatic way, complete provenance documents that account for the entire pipeline. We report on the ability of that reference implementation to capture provenance in real ML benchmark pipelines and over TCP-DI synthetic data. We finally show how the collected provenance can be used to answer a suite of provenance benchmark queries that underpin some common pipeline inspection questions, as expressed on the Data Science Stack Exchange.},
journal = {ACM Trans. Database Syst.},
month = {apr},
articleno = {6},
numpages = {42},
keywords = {Provenance, data science, data preparation, preprocessing}
}

@inproceedings{10.1109/ICSE-SEIS58686.2023.00025,
author = {Qiu, Huilian Sophie and Zhao, Zihe H and Yu, Tielin Katy and Wang, Justin and Ma, Alexander and Fang, Hongbo and Dabbish, Laura and Vasilescu, Bogdan},
title = {Gender Representation Among Contributors to Open-Source Infrastructure: An Analysis of 20 Package Manager Ecosystems},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS58686.2023.00025},
doi = {10.1109/ICSE-SEIS58686.2023.00025},
abstract = {While the severe underrepresentation of women and non-binary people in open source is widely recognized, there is little empirical data on how the situation has changed over time and which subcommunities have been more effectively reducing the gender imbalance. To obtain a clearer image of gender representation in open source, we compiled and synthesized existing empirical data from the literature, and computed historical trends in the representation of women across 20 open source ecosystems. While inherently limited by the ability of automatic name-based gender inference to capture true gender identities at an individual level, our census still provides valuable populationlevel insights. Across all and in most ecosystems, we observed a promising upward trend in the percentage of women among code contributors over time, but also high variation in the percentage of women contributors across ecosystems. We also found that, in most ecosystems, women withdraw earlier from open-source participation than men.The representation of women and non-binary people has been extremely low in the open-source software community. Most of the statistics reported by prior studies are below 10%. However, the majority of the prior works were based on subsamples instead of the entire population. Our work started with a review of the gender distributions reported in the literature. Then we provided an overview of the gender distribution in 20 of the largest open-source ecosystem, i.e., grouped by package managers such as npm and PyPI, and investigated its change over time. Moreover, we analyzed the turnover rate between men and women contributors. Across all and in most ecosystems, we observed a promising upward trend in the percentage of women among code contributors over time, but also high variation in the percentage of women contributors across ecosystems. We also found that, in most ecosystems, women withdraw earlier from open-source participation than men.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
pages = {180–187},
numpages = {8},
keywords = {open-source software, gender diversity},
location = {Melbourne, Australia},
series = {ICSE-SEIS '23}
}

@article{10.1145/3637228,
author = {Ca\~{n}izares, Pablo C. and L\'{o}pez-Morales, Jose Mar\'{\i}a and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Measuring and Clustering Heterogeneous Chatbot Designs},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637228},
doi = {10.1145/3637228},
abstract = {Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies.To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {90},
numpages = {43},
keywords = {Chatbot design, metrics, clustering, quality assurance, model-driven engineering}
}

@inproceedings{10.1145/3440084.3441213,
author = {Khan, Shahnawaz and Rabbani, Mustafa Raza},
title = {Chatbot as Islamic Finance Expert (CaIFE): When Finance Meets Artificial Intelligence},
year = {2021},
isbn = {9781450388894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440084.3441213},
doi = {10.1145/3440084.3441213},
abstract = {Artificial intelligence (AI) is the key technology in the new disruptive technological innovation and industrial transformation. AI has very wide application in finance and banking. The financial institutions not only answer the queries of the customers, but they should also clarify the complaints the customer face and provide the solution. For this purpose, many banks and financial institutions are using Chatbot to provide solution to customer complaints and queries. Chatbots are very efficient in providing solution to customers queries and are available 24 hours to give solution to customer's complaints. Finally, we propose an artificial Intelligence based interactive Chatbot called 'Chatbot as Islamic Finance Expert' (CaIFE). Our interactive Chatbot CaIFE receives automatic robot support related to Islamic finance and banking by having users communicate with a robot having knowledge accumulated by machine learning. It answers any query related to Islamic finance and banking on real time basis. It then presents a case study of CaIFE and explains its characteristics and limitations.},
booktitle = {Proceedings of the 2020 4th International Symposium on Computer Science and Intelligent Control},
articleno = {37},
numpages = {5},
keywords = {Artificial Intelligence, Chatbot, Financial Expert, Islamic finance, NLP, Sharia, machine learning},
location = {Newcastle upon Tyne, United Kingdom},
series = {ISCSIC 2020}
}

@inproceedings{10.1145/3510003.3510205,
author = {Tian, Yingchen and Zhang, Yuxia and Stol, Klaas-Jan and Jiang, Lin and Liu, Hui},
title = {What makes a good commit message?},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510205},
doi = {10.1145/3510003.3510205},
abstract = {A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an "audit trail" by which developers can understand how the source code of a project has changed---and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a "good" commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether "good" commit messages can be automatically identified; such automation could prompt developers to write better commit messages.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2389–2401},
numpages = {13},
keywords = {commit message quality, commit-based software development, open collaboration},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3638247,
author = {Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua},
title = {Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638247},
doi = {10.1145/3638247},
abstract = {The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {124},
numpages = {24},
keywords = {AI chain engineering, visual programming, large language models, No/Low code, SE for AI}
}

@inproceedings{10.1145/3643991.3644924,
author = {Kola-Olawuyi, Ajiromola and Weeraddana, Nimmi Rashinika and Nagappan, Meiyappan},
title = {The Impact of Code Ownership of DevOps Artefacts on the Outcome of DevOps CI Builds},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644924},
doi = {10.1145/3643991.3644924},
abstract = {DevOps is a key element in sustaining the quality and efficiency of software development. Yet, the effectiveness of DevOps methodologies extends beyond just technological expertise. It is greatly affected by the manner in which teams handle and engage with DevOps artefacts. Grasping the intricacies of code ownership and contribution patterns within DevOps artefacts is vital for refining strategies and ensuring they deliver their full potential.There are two main strategies to manage DevOps artefacts as suggested in prior work: (1) all project developers need to contribute to DevOps artefacts, and (2) a dedicated group of developers needs to be authoring DevOps artefacts. To analyze which strategy works best for Open-Source Software (OSS) projects, we conduct an empirical analysis on a dataset of 892,193 CircleCI builds spanning 1,689 OSS projects. We employ a two-pronged approach to our study. First, we investigate the impact of chronological code ownership of DevOps artefacts on the outcome of a CI build on a build level. Second, we study the impact of the Skewness of DevOps contributions on the success rate of CI builds at the project level.Our findings reveal that, in general, larger chronological ownership and higher Skewness values of DevOps contributions are related to more successful build outcomes and higher rates of successful build outcomes, respectively. We further find that projects with low Skewness values could have high build success rates when the number of developers in the project is relatively small. Thus, our results suggest that while larger software organizations are better off having dedicated DevOps developers, smaller organizations would benefit from having all developers involved in DevOps.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {543–555},
numpages = {13},
keywords = {DevOps, code ownership, continuous integrations, empirical study},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3517931,
author = {Dutta, Hridoy Sankar and Chakraborty, Tanmoy},
title = {Blackmarket-Driven Collusion on Online Media: A Survey},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3517931},
doi = {10.1145/3517931},
abstract = {Online media platforms have enabled users to connect with individuals and organizations, and share their thoughts. Other than connectivity, these platforms also serve multiple purposes, such as education, promotion, updates, and awareness. Increasing, the reputation of individuals in online media (aka social reputation) is thus essential these days, particularly for business owners and event managers who are looking to improve their publicity and sales. The natural way of gaining social reputation is a tedious task, which leads to the creation of unfair ways to boost the reputation of individuals artificially. Several online blackmarket services have developed a thriving ecosystem with lucrative offers to attract content promoters for publicizing their content online. These services are operated in such a way that most of their inorganic activities are going unnoticed by the media authorities, and the customers of the blackmarket services are less likely to be spotted. We refer to such unfair ways of bolstering social reputation in online media as collusion. This survey is the first attempt to provide readers a comprehensive outline of the latest studies dealing with the identification and analysis of blackmarket-driven collusion in online media. We present a broad overview of the problem, definitions of the related problems and concepts, the taxonomy of the proposed approaches, and a description of the publicly available datasets and online tools, and we discuss the outstanding issues. We believe that collusive entity detection is a newly emerging topic in anomaly detection and cyber-security research in general, and the current survey will provide readers with an easy-to-access and comprehensive list of methods, tools, and resources proposed so far for detecting and analyzing collusive entities on online media.},
journal = {ACM/IMS Trans. Data Sci.},
month = {may},
articleno = {43},
numpages = {37},
keywords = {Collusion, blackmarket, Twitter, YouTube, social media analysis}
}

@inproceedings{10.1145/3600211.3604698,
author = {Gilbert, Thomas Krendl and Lambert, Nathan and Dean, Sarah and Zick, Tom and Snoswell, Aaron and Mehta, Soham},
title = {Reward Reports for Reinforcement Learning},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604698},
doi = {10.1145/3600211.3604698},
abstract = {Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta’s BlenderBot 3 chatbot. Several others for game-playing (DeepMind’s MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {84–130},
numpages = {47},
keywords = {Reward function, disaggregated evaluation, documentation, ethical considerations, reporting},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3623341,
author = {Lill, Alexander and Meyer, Andr\'{e} N. and Fritz, Thomas},
title = {On the Helpfulness of Answering Developer Questions on Discord with Similar Conversations and Posts from the Past},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623341},
doi = {10.1145/3597503.3623341},
abstract = {A big part of software developers' time is spent finding answers to their coding-task-related questions. To answer their questions, developers usually perform web searches, ask questions on Q&amp;A websites, or, more recently, in chat communities. Yet, many of these questions have frequently already been answered in previous chat conversations or other online communities. Automatically identifying and then suggesting these previous answers to the askers could, thus, save time and effort. In an empirical analysis, we first explored the frequency of repeating questions on the Discord chat platform and assessed our approach to identify them automatically. The approach was then evaluated with real-world developers in a field experiment, through which we received 142 ratings on the helpfulness of the suggestions we provided to help answer 277 questions that developers posted in four Discord communities. We further collected qualitative feedback through 53 surveys and 10 follow-up interviews. We found that the suggestions were considered helpful in 40% of the cases, that suggesting Stack Overflow posts is more often considered helpful than past Discord conversations, and that developers have difficulties describing their problems as search queries and, thus, prefer describing them as natural language questions in online communities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {58},
numpages = {13},
keywords = {developer questions, chat community, semantic similarity},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3468264.3468562,
author = {Shi, Lin and Chen, Xiao and Yang, Ye and Jiang, Hanzhi and Jiang, Ziyou and Niu, Nan and Wang, Qing},
title = {A first look at developers’ live chat on Gitter},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468562},
doi = {10.1145/3468264.3468562},
abstract = {Modern communication platforms such as Gitter and Slack play an increasingly critical role in supporting software teamwork, especially in open source development.Conversations on such platforms often contain intensive, valuable information that may be used for better understanding OSS developer communication and collaboration. However, little work has been done in this regard. To bridge the gap, this paper reports a first comprehensive empirical study on developers' live chat, investigating when they interact, what community structures look like, which topics are discussed, and how they interact. We manually analyze 749 dialogs in the first phase, followed by an automated analysis of over 173K dialogs in the second phase. We find that developers tend to converse more often on weekdays, especially on Wednesdays and Thursdays (UTC), that there are three common community structures observed, that developers tend to discuss topics such as API usages and errors, and that six dialog interaction patterns are identified in the live chat communities. Based on the findings, we provide recommendations for individual developers and OSS communities, highlight desired features for platform vendors, and shed light on future research directions. We believe that the findings and insights will enable a better understanding of developers' live chat, pave the way for other researchers, as well as a better utilization and mining of knowledge embedded in the massive chat history.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {391–403},
numpages = {13},
keywords = {Empirical Study, Live chat, Open source, Team communication},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.5555/2675327.2675345,
author = {Hannah, Kyle and Gianvecchio, Steven},
title = {Zeuslite: a tool for botnet analysis in the classroom},
year = {2015},
issue_date = {January 2015},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {30},
number = {3},
issn = {1937-4771},
abstract = {The security of information systems depends not only on technology, but also on the knowledge of practitioners. Botnets, which are networks of compromised computers, are among the foremost threats to computer security. As a major security problem, it is important for current and future practitioners to understand the threat. However, it is difficult to gain practical experience with botnets due to the arduous process of setting up and analyzing them. In this paper, we introduce ZeusLite, a lightweight and user-friendly tool for setting up and analyzing Zeus, one of the Internet's largest and most notorious botnets. ZeusLite intentionally limits Zeus's functionality, making it useful for study but not malicious purposes. The tool includes two main parts: (1) a modified version of the original Zeus binary with select features removed, and (2) a reverse-engineered version of its command and control (C&amp;C) server. ZeusLite can be set up in only a few minutes, allowing students to easily gain hands on experience responding to malware incident. Based on ZeusLite, we also outline a set of possible lab exercises that would teach students how to recognize and remove a Zeus infection.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {109–116},
numpages = {8}
}

@inproceedings{10.1145/3640794.3665577,
author = {Gomez-Vazquez, Marcos and Cabot, Jordi and Claris\'{o}, Robert},
title = {Automatic Generation of Conversational Interfaces for Tabular Data Analysis},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665577},
doi = {10.1145/3640794.3665577},
abstract = {Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources, including support for data analytics questions that are responded via charts rendered by the chatbot. Moreover, our chatbots are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns matched to the chatbot intents and entities.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {33},
numpages = {6},
keywords = {Chatbot, Code generation, Data analysis, NLP, No-code},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@article{10.1145/3544791,
author = {Maddila, Chandra and Upadrasta, Sai Surya and Bansal, Chetan and Nagappan, Nachiappan and Gousios, Georgios and van Deursen, Arie},
title = {Nudge: Accelerating Overdue Pull Requests toward Completion},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3544791},
doi = {10.1145/3544791},
abstract = {Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge’s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {35},
numpages = {30},
keywords = {Pull-based software development, pull request, merge conflict, distributed software development}
}

@inproceedings{10.1145/3290605.3300793,
author = {Das, Maitraye and Hecht, Brent and Gergle, Darren},
title = {The Gendered Geography of Contributions to OpenStreetMap: Complexities in Self-Focus Bias},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300793},
doi = {10.1145/3290605.3300793},
abstract = {Millions of people worldwide contribute content to peer production repositories that serve human information needs and provide vital world knowledge to prominent artificial intelligence systems. Yet, extreme gender participation disparities exist in which men significantly outnumber women. A central concern has been that due to self-focus bias, these disparities can lead to corresponding gender content disparities, in which content of interest to men is better represented than content of interest to women. This paper investigates the relationship between participation and content disparities in OpenStreetMap. We replicate findings that women are dramatically under-represented as OSM contributors, and observe that men and women contribute different types of content and do so about different places. However, the character of these differences confound simple narratives about self-focus bias: we find that on a proportional basis, men produced a higher proportion of contributions in feminized spaces compared to women, while women produced a higher proportion of contributions in masculinized spaces compared to men.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {gender, openstreetmap, peer production, rural, self-focus bias, urban},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3430665.3456352,
author = {Chow, Scott P. and Komarlu, Tanay and Conrad, Phillip T.},
title = {Teaching Testing with Modern Technology Stacks in Undergraduate Software Engineering Courses},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456352},
doi = {10.1145/3430665.3456352},
abstract = {Students' experience with software testing in undergraduate computing courses is often relatively shallow, as compared to the importance of the topic. This experience report describes introducing industrial-strength testing into CMPSC 156, an upper division course in software engineering at UC Santa Barbara. We describe our efforts to modify our software engineering course to introduce rigorous test-coverage requirements into full-stack web development projects, requirements similar to those the authors had experienced in a professional software development setting. We present student feedback on the course and coverage metrics for the projects. We reflect on what about these changes worked (or didn't), and provide suggestions for other instructors that would like to give their students a deeper experience with software testing in their software engineering courses.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {241–247},
numpages = {7},
keywords = {computer science education, continuous integration, integration testing, software engineering education, test coverage, testing, unit testing, web applications},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@inproceedings{10.1145/3611643.3616282,
author = {Fang, Hongbo and Herbsleb, James and Vasilescu, Bogdan},
title = {Matching Skills, Past Collaboration, and Limited Competition: Modeling When Open-Source Projects Attract Contributors},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616282},
doi = {10.1145/3611643.3616282},
abstract = {Attracting and retaining new developers is often at the heart of open-source project sustainability and success.  
 Previous research found many intrinsic (or endogenous) project characteristics associated with the attractiveness of projects to new developers, but the impact of factors external to the project itself have largely been overlooked.  
 In this work, we focus on one such external factor, a project's labor pool, which is defined as the set of contributors active in the overall open-source ecosystem that the project could plausibly attempt to recruit from at a given time. How are the size and characteristics of the labor pool associated with a project's attractiveness to new contributors?  
 Through an empirical study of over 516,893 Python projects, we found that the size of the project's labor pool, the technical skill match, and the social connection between the project's labor pool and members of the focal project all significantly influence the number of new developers that the focal project attracts, with the competition between projects with overlapping labor pools also playing a role.  
 Overall, the labor pool factors add considerable explanatory power compared to models with only project-level characteristics.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {42–54},
numpages = {13},
keywords = {labor pool, open source, sustainability},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3639477.3639719,
author = {Di, Peng and Li, Jianguo and Yu, Hang and Jiang, Wei and Cai, Wenting and Cao, Yang and Chen, Chaoyu and Chen, Dajun and Chen, Hongwei and Chen, Liang and Fan, Gang and Gong, Jie and Gong, Zi and Hu, Wen and Guo, Tingting and Lei, Zhichao and Li, Ting and Li, Zheng and Liang, Ming and Liao, Cong and Liu, Bingchang and Liu, Jiachen and Liu, Zhiwei and Lu, Shaojun and Shen, Min and Wang, Guangpei and Wang, Huan and Wang, Zhi and Xu, Zhaogui and Yang, Jiawei and Ye, Qing and Zhang, Gehao and Zhang, Yu and Zhao, Zelin and Zheng, Xunjin and Zhou, Hailian and Zhu, Lifu and Zhu, Xianying},
title = {CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639719},
doi = {10.1145/3639477.3639719},
abstract = {Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {418–429},
numpages = {12},
keywords = {code large language models, multi-lingual, chinese prompts},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3313831.3376151,
author = {Seering, Joseph and Hammer, Jessica and Kaufman, Geoff and Yang, Diyi},
title = {Proximate Social Factors in First-Time Contribution to Online Communities},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376151},
doi = {10.1145/3313831.3376151},
abstract = {In the course of every member's integration into an online community, a decision must be made to participate for the first time. The challenges of effective recruitment, management, and retention of new users have been extensively explored in social computing research. However, little work has looked at in-the-moment factors that lead users to decide to participate instead of "lurk", conditions which can be shaped to draw new users in at crucial moments. In this work we analyze 183 million messages scraped from chatrooms on the livestreaming platform Twitch in order to understand differences between first-time participants' and regulars' behaviors and to identify conditions that encourage first-time participation. We find that presence of diverse types of users increases likelihood of new participation, with effects depending on the size of the community. We also find that information-seeking behaviors in first-time participation are negatively associated with retention in the short and medium term.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {newcomers, online communities, participation, retention, social roles, twitch},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3180155.3180238,
author = {Bradley, Nick C. and Fritz, Thomas and Holmes, Reid},
title = {Context-aware conversational developer assistants},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180238},
doi = {10.1145/3180155.3180238},
abstract = {Building and maintaining modern software systems requires developers to perform a variety of tasks that span various tools and information sources. The crosscutting nature of these development tasks requires developers to maintain complex mental models and forces them (a) to manually split their high-level tasks into low-level commands that are supported by the various tools, and (b) to (re)establish their current context in each tool. In this paper we present Devy, a Conversational Developer Assistant (CDA) that enables developers to focus on their high-level development tasks. Devy reduces the number of manual, often complex, low-level commands that developers need to perform, freeing them to focus on their high-level tasks. Specifically, Devy infers high-level intent from developer's voice commands and combines this with an automatically-generated context model to determine appropriate workflows for invoking low-level tool actions; where needed, Devy can also prompt the developer for additional information. Through a mixed methods evaluation with 21 industrial developers, we found that Devy provided an intuitive interface that was able to support many development tasks while helping developers stay focused within their development environment. While industrial developers were largely supportive of the automation Devy enabled, they also provided insights into several other tasks and workflows CDAs could support to enable them to better focus on the important parts of their development tasks.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {993–1003},
numpages = {11},
keywords = {conversational development assistants, natural user interfaces},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3510003.3510068,
author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yifan and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Automated testing of software that uses machine learning APIs},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510068},
doi = {10.1145/3510003.3510068},
abstract = {An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {212–224},
numpages = {13},
keywords = {machine learning, machine learning API, software testing},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3474374.3486916,
author = {Tjiam, Kevin and Wang, Rui and Chen, Huanhuan and Liang, Kaitai},
title = {Your Smart Contracts Are Not Secure: Investigating Arbitrageurs and Oracle Manipulators in Ethereum},
year = {2021},
isbn = {9781450386616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474374.3486916},
doi = {10.1145/3474374.3486916},
abstract = {Smart contracts on Ethereum enable billions of dollars to be transacted in a decentralized, transparent and trustless environment. However, adversaries lie await in the Dark Forest, waiting to exploit any and all smart contract vulnerabilities in order to extract profits from unsuspecting victims in this new financial system. As the blockchain space moves at a breakneck pace, exploits on smart contract vulnerabilities rapidly evolve, and existing research quickly becomes obsolete. It is imperative that smart contract developers stay up to date on the current most damaging vulnerabilities and countermeasures to ensure the security of users' funds, and to collectively ensure the future of Ethereum as a financial settlement layer. This research work focuses on two smart contract vulnerabilities: transaction-ordering dependency and oracle manipulation. Combined, these two vulnerabilities have been exploited to extract hundreds of millions of dollars from smart contracts in the past year (2020-2021). For each of them, this paper presents: (1) a literary survey from recent (as of 2021) formal and informal sources; (2) a reproducible experiment as code demonstrating the vulnerability and, where applicable, countermeasures to mitigate the vulnerability; and (3) analysis and discussion on proposed countermeasures. To conclude, strengths, weaknesses and trade-offs of these countermeasures are summarised, inspiring directions for future research.},
booktitle = {Proceedings of the 3rd Workshop on Cyber-Security Arms Race},
pages = {25–35},
numpages = {11},
keywords = {arbitrageurs, ethereum, oracle manipulator, security, smart contract, vulnerability},
location = {Virtual Event, Republic of Korea},
series = {CYSARM '21}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00030,
author = {Babii, Hlib and Prenner, Julian Aron and Stricker, Laurin and Karmakar, Anjan and Janes, Andrea and Robbes, Romain},
title = {Mining software repositories with a collaborative heuristic repository},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00030},
doi = {10.1109/ICSE-NIER52604.2021.00030},
abstract = {Many software engineering studies or tasks rely on categorizing software engineering artifacts. In practice, this is done either by defining simple but often imprecise heuristics, or by manual labelling of the artifacts. Unfortunately, errors in these categorizations impact the tasks that rely on them. To improve the precision of these categorizations, we propose to gather heuristics in a collaborative heuristic repository, to which researchers can contribute a large amount of diverse heuristics for a variety of tasks on a variety of SE artifacts. These heuristics are then leveraged by state-of-the-art weak supervision techniques to train high-quality classifiers, thus improving the categorizations. We present an initial version of the heuristic repository, which we applied to the concrete task of commit classification.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {106–110},
numpages = {5},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/3644032.3644456,
author = {Canizares, Pablo C. and \'{A}vila, Daniel and Perez-Soler, Sara and Guerra, Esther and De Lara, Juan},
title = {Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644456},
doi = {10.1145/3644032.3644456},
abstract = {Conversational agents - or chatbots - are increasingly used as the user interface to many software services. While open-domain chatbots like ChatGPT excel in their ability to chat about any topic, task-oriented conversational agents are designed to perform goal-oriented tasks (e.g., booking or shopping) guided by a dialogue-based user interaction, which is explicitly designed. Like any kind of software system, task-oriented conversational agents need to be properly tested to ensure their quality. For this purpose, some tools permit defining and executing conversation test cases. However, there are currently no established means to assess the coverage of the design of a task-oriented agent by a test suite, or mechanisms to automate quality test case generation ensuring the agent coverage.To attack this problem, we propose test coverage criteria for task-oriented conversational agents, and define coverage-based strategies to synthesise test scenarios, some oriented to test case reduction. We provide an implementation of the criteria and the strategies that is independent of the agent development platform. Finally, we report on their evaluation on open-source Dialogflow and Rasa agents, and a comparison against a state-of-the-art testing tool. The experiment shows benefits in terms of test generation correctness, increased coverage and reduced testing time.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {23–33},
numpages = {11},
keywords = {testing, test suite generation, task-oriented conversational agents},
location = {Lisbon, Portugal},
series = {AST '24}
}

@article{10.1145/3276483,
author = {Zappa Nardelli, Francesco and Belyakova, Julia and Pelenitsyn, Artem and Chung, Benjamin and Bezanson, Jeff and Vitek, Jan},
title = {Julia subtyping: a rational reconstruction},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276483},
doi = {10.1145/3276483},
abstract = {Programming languages that support multiple dispatch rely on an expressive notion of subtyping to specify method applicability. In these languages, type annotations on method declarations are used to select, out of a potentially large set of methods, the one that is most appropriate for a particular tuple of arguments. Julia is a language for scientific computing built around multiple dispatch and an expressive subtyping relation. This paper provides the first formal definition of Julia's subtype relation and motivates its design. We validate our specification empirically with an implementation of our definition that we compare against the existing Julia implementation on a collection of real-world programs. Our subtype implementation differs on 122 subtype tests out of 6,014,476. The first 120 differences are due to a bug in Julia that was fixed once reported; the remaining 2 are under discussion.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {113},
numpages = {27},
keywords = {Multiple Dispatch, Subtyping}
}

@inproceedings{10.1145/3475716.3475769,
author = {Imtiaz, Nasif and Thorn, Seaver and Williams, Laurie},
title = {A comparative study of vulnerability reporting by software composition analysis tools},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475769},
doi = {10.1145/3475716.3475769},
abstract = {Background: Modern software uses many third-party libraries and frameworks as dependencies. Known vulnerabilities in these dependencies are a potential security risk. Software composition analysis (SCA) tools, therefore, are being increasingly adopted by practitioners to keep track of vulnerable dependencies. Aim: The goal of this study is to understand the difference in vulnerability reporting by various SCA tools. Understanding if and how existing SCA tools differ in their analysis may help security practitioners to choose the right tooling and identify future research needs. Method: We present an in-depth case study by comparing the analysis reports of 9 industry-leading SCA tools on a large web application, OpenMRS, composed of Maven (Java) and npm (JavaScript) projects. Results: We find that the tools vary in their vulnerability reporting. The count of reported vulnerable dependencies ranges from 17 to 332 for Maven and from 32 to 239 for npm projects across the studied tools. Similarly, the count of unique known vulnerabilities reported by the tools ranges from 36 to 313 for Maven and from 45 to 234 for npm projects. Our manual analysis of the tools' results suggest that accuracy of the vulnerability database is a key differentiator for SCA tools. Conclusion: We recommend that practitioners should not rely on any single tool at the present, as that can result in missing known vulnerabilities. We point out two research directions in the SCA space: i) establishing frameworks and metrics to identify false positives for dependency vulnerabilities; and ii) building automation technologies for continuous monitoring of vulnerability data from open source package ecosystems.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {5},
numpages = {11},
keywords = {case study, dependency, security tools, software composition analysis, supply chain security, vulnerability},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/2970276.2970306,
author = {Hannebauer, Christoph and Patalas, Michael and St\"{u}nkel, Sebastian and Gruhn, Volker},
title = {Automatically recommending code reviewers based on their expertise: an empirical comparison},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970306},
doi = {10.1145/2970276.2970306},
abstract = {Code reviews are an essential part of quality assurance in Free, Libre, and Open Source (FLOSS) projects. However, finding a suitable reviewer can be difficult, and delayed or forgotten reviews are the consequence. Automating reviewer selection with suitable algorithms can mitigate this problem. We compare empirically six algorithms based on modification expertise and two algorithms based on review expertise on four major FLOSS projects. Our results indicate that the algorithms based on review expertise yield better recommendations than those based on modification expertise. The algorithm Weighted Review Count (WRC) recommends at least one out of five reviewers correctly in 69 % to 75 % of all cases, which is one of the best results achieved in the comparison.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {99–110},
numpages = {12},
keywords = {Code reviewer recommendation, code reviews, expertise metrics, issue tracker, open source, patches, recommendation system},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3233391.3233966,
author = {Zimmermann, Th\'{e}o},
title = {Challenges in the collaborative development of a complex mathematical software and its ecosystem},
year = {2018},
isbn = {9781450359368},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233391.3233966},
doi = {10.1145/3233391.3233966},
abstract = {This is a contribution to the OpenSym 2018 Doctoral Symposium. This paper describes my PhD objectives. As an insider in the Coq development team, I've worked at making the release process of the Coq proof assistant smoother and more automated, at opening the development to external contributions, and at shaping the ecosystem around Coq. I'm intending to evaluate how well-known software engineering techniques and results about open source software communities apply in the specific case of the proof assistant I'm studying.},
booktitle = {Proceedings of the 14th International Symposium on Open Collaboration},
articleno = {22},
numpages = {3},
keywords = {Coq, Open source software, mathematical software, proof assistant, release management},
location = {Paris, France},
series = {OpenSym '18}
}

@inproceedings{10.1145/3643991.3644934,
author = {Begoug, Mahi and Chouchen, Moataz and Ouni, Ali and Abdullah Alomar, Eman and Mkaouer, Mohamed Wiem},
title = {Fine-Grained Just-In-Time Defect Prediction at the Block Level in Infrastructure-as-Code (IaC)},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644934},
doi = {10.1145/3643991.3644934},
abstract = {Infrastructure-as-Code (IaC) is an emerging software engineering practice that leverages source code to facilitate automated configuration of software systems' infrastructure. IaC files are typically complex, containing hundreds of lines of code and dependencies, making them prone to defects, which can result in breaking online services at scale. To help developers early identify and fix IaC defects, research efforts have introduced IaC defect prediction models at the file level. However, the granularity of the proposed approaches remains coarse-grained, requiring developers to inspect hundreds of lines of code in a file, while only a small fragment of code is defective. To alleviate this issue, we introduce a machine-learning-based approach to predict IaC defects at a fine-grained level, focusing on IaC blocks, i.e., small code units that encapsulate specific behaviours within an IaC file. We trained various machine learning algorithms based on a mixture of code, process, and change-level metrics. We evaluated our approach on 19 open-source projects that use Terraform, a widely used IaC tool. The results indicated that there is no single algorithm that consistently outperforms the others in 19 projects. Overall, among the six algorithms, we observed that the LightGBM model achieved a higher average of 0.21 in terms of MCC and 0.71 in terms of AUC. Models analysis reveals that the developer's experience and the relative number of added lines tend to be the most important features. Additionally, we found that blocks belonging to the most frequent types are more prone to defects. Our defect prediction models have also shown sensitivity to concept drift, indicating that IaC practitioners should regularly retrain their models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {100–112},
numpages = {13},
keywords = {defect prediction, infrastructure-as-code, IaC, terraform},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@proceedings{10.1145/3545258,
title = {Internetware '22: Proceedings of the 13th Asia-Pacific Symposium on Internetware},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hohhot, China}
}

@inproceedings{10.1145/3372787.3390430,
author = {Ruane, Elayne and Smith, Ross and Bean, Dan and Tjalve, Michael and Ventresque, Anthony},
title = {Developing a conversational agent with a globally distributed team: an experience report},
year = {2020},
isbn = {9781450370936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372787.3390430},
doi = {10.1145/3372787.3390430},
abstract = {In this experience report, we discuss the development of a solution that enables conflict-affected youth to discover and access relevant learning content. A team of individuals from a not-for-profit, a large multi-national technology company, and an academic institution, collaborated to develop that solution as a conversational agent named Hakeem. We provide a brief motivation and product description before outlining our design and development process including forming a distributed virtual team, engaging in user-centred design with conflict-affected youth in Lebanon, and using a minimum viable product approach while adapting Scrum for distributed development. We end this report with a reflection on the lessons learned thus far.},
booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
pages = {122–126},
numpages = {5},
keywords = {chatbot, global software engineering, software development},
location = {Seoul, Republic of Korea},
series = {ICGSE '20}
}

@inproceedings{10.1145/3319619.3326814,
author = {Alghamdi, Mahfouth and Treude, Christoph and Wagner, Markus},
title = {Toward human-like summaries generated from heterogeneous software artefacts},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326814},
doi = {10.1145/3319619.3326814},
abstract = {Automatic text summarisation has drawn considerable interest in the field of software engineering. It can improve the efficiency of software developers, enhance the quality of products, and ensure timely delivery. In this paper, we present our initial work towards automatically generating human-like multi-document summaries from heterogeneous software artefacts. Our analysis of the text properties of 545 human-written summaries from 15 software engineering projects will ultimately guide heuristics searches in the automatic generation of human-like summaries.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1701–1702},
numpages = {2},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1109/MSR.2017.54,
author = {Rausch, Thomas and Hummer, Waldemar and Leitner, Philipp and Schulte, Stefan},
title = {An empirical analysis of build failures in the continuous integration workflows of Java-based open-source software},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.54},
doi = {10.1109/MSR.2017.54},
abstract = {Continuous Integration (CI) has become a common practice in both industrial and open-source software development. While CI has evidently improved aspects of the software development process, errors during CI builds pose a threat to development efficiency. As an increasing amount of time goes into fixing such errors, failing builds can significantly impair the development process and become very costly. We perform an in-depth analysis of build failures in CI environments. Our approach links repository commits to data of corresponding CI builds. Using data from 14 open-source Java projects, we first identify 14 common error categories. Besides test failures, which are by far the most common error category (up to &gt;80% per project), we also identify noisy build data, e.g., induced by transient Git interaction errors, or general infrastructure flakiness. Second, we analyze which factors impact the build results, taking into account general process and specific CI metrics. Our results indicate that process metrics have a significant impact on the build outcome in 8 of the 14 projects on average, but the strongest influencing factor across all projects is overall stability in the recent build history. For 10 projects, more than 50% (up to 80%) of all failed builds follow a previous build failure. Moreover, the fail ratio of the last k=10 builds has a significant impact on build results for all projects in our dataset.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {345–355},
numpages = {11},
keywords = {build errors, continuous integration, correlation analysis, mining software repositories},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3318464.3384684,
author = {Bonifati, Angela and Martens, Wim and Timm, Thomas},
title = {SHARQL: Shape Analysis of Recursive SPARQL Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384684},
doi = {10.1145/3318464.3384684},
abstract = {We showcase SHARQL, a system that allows to navigate SPARQL query logs, can inspect complex queries by visualizing their shape, and can serve as a back-end to flexibly produce statistics about the logs. Even though SPARQL query logs are increasingly available and have become public recently, their navigation and analysis is hampered by the lack of appropriate tools. SPARQL queries are sometimes hard to understand and their inherent properties, such as their shape, their hypertree properties, and their property paths are even more difficult to be identified and properly rendered. In SHARQL, we show how the analysis and exploration of several hundred million queries is possible. We offer edge rendering which works with complex hyperedges, regular edges, and property paths of SPARQL queries. The underlying database stores more than one hundred attributes per query and is therefore extremely flexible for exploring the query logs and as a back-end to compute and display analytical properties of the entire logs or parts thereof.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2701–2704},
numpages = {4},
keywords = {SPARQL, database queries, query logs},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3524842.3528512,
author = {Bugayenko, Yegor and Bakare, Ayomide and Cheverda, Arina and Farina, Mirko and Kruglov, Artem and Plaksin, Yaroslav and Succi, Giancarlo and Pedrycz, Witold},
title = {Automatically prioritizing and assigning tasks from code repositories in puzzle driven development},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528512},
doi = {10.1145/3524842.3528512},
abstract = {Automatically prioritizing software development tasks extracted from codes could provide significant technical and organizational advantages. Tools exist for the automatic extraction of tasks, but they still lack the ability to capture their mutual dependencies; hence, the capability to prioritize them. Solving this important puzzle is the goal of the presented industrial challenge.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {722–723},
numpages = {2},
keywords = {software development, task prioritization, text tagging},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3625007.3627729,
author = {Shahrour, Gheida and Quincey, Ed De and Lal, Sangeeta},
title = {The Content Quality of Crowdsourced Knowledge on Stack Overflow- A Systematic Mapping Study},
year = {2024},
isbn = {9798400704093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625007.3627729},
doi = {10.1145/3625007.3627729},
abstract = {Community Question Answering (CQA) forums such as Stack Overflow (SO) are a form of crowdsourced knowledge for software engineers who seek solutions to development and programming challenges. While such a forum provides valuable support to engineers, it often contains low quality content that impacts users' experience and the longevity of new users. Past research shows that most of the low-quality content comes from violating general Netiquette Rules (NRs). In the past, several researchers have worked on analysing the content of SO and suggested approaches to increase its quality. However, to the best of our knowledge, there is no previous work that has reviewed the scale of scientific attention that is given to this cause and the recommendations that have been made. We have conducted a Systematic Mapping Study (SMS) using five relevant databases, reviewing 1,489 papers and selecting 18 that are relevant to help to address this gap. We have found that SO has attracted increasing research interest on reducing NRs violations to improve the quality of communication on SO. Interestingly, the majority of papers used manual qualitative and quantitative analysis approaches to investigate this area. We have found that further research is required to identify more violation features, generalisable sources of data and that the use of computational analysis approaches are still needed in this area.},
booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {330–334},
numpages = {5},
keywords = {stack overflow, crowdsourcing, content quality, netiquette rules violations, systematic mapping study},
location = {Kusadasi, Turkiye},
series = {ASONAM '23}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00030,
author = {P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Creating and migrating chatbots with conga},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00030},
doi = {10.1109/ICSE-Companion52605.2021.00030},
abstract = {Chatbots are agents that enable the interaction of users and software by means of written or spoken natural language conversation. Their use is growing, and many companies are starting to offer their services via chatbots, e.g., for booking, shopping or customer support. For this reason, many chatbot development tools have emerged, which makes choosing the most appropriate tool difficult. Moreover, there is hardly any support for migrating chatbots between tools.To alleviate these issues, we propose a model-driven engineering solution that includes: (i) a domain-specific language to model chatbots independently of the development tool; (ii) a recommender that suggests the most suitable development tool for the given chatbot requirements and model; (iii) code generators that synthesize the chatbot code for the selected tool; and (iv) parsers to extract chatbot models out of existing chatbot implementations. Our solution is supported by a web IDE called Conga that can be used for both chatbot creation and migration. A demo video is available at https://youtu.be/3sw1FDdZ7XY.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {37–40},
numpages = {4},
keywords = {chatbots, domain-specific languages, migration, model-driven engineering},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3361149.3364227,
author = {Abidi, Mouna and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {Anti-patterns for multi-language systems},
year = {2019},
isbn = {9781450362061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361149.3364227},
doi = {10.1145/3361149.3364227},
abstract = {Multi-language systems are common nowadays because most of the systems are developed using components written in different programming languages. These systems could arise from three different reasons: (1) to leverage the strengths and take benefits of each language, (2) to reduce the cost by reusing code written in other languages, (3) to include and accommodate legacy code. However, they also introduce additional challenges, including the increase in the complexity and the need for proper interfaces and interactions between the different languages. To address these challenges, the software-engineering research community, as well as the industry, should describe and provide common guidelines, idioms, and patterns to support the development, maintenance, and evolution of these systems. These patterns are an effective means of improving the quality of multi-language systems. They capture good practices to adopt and bad practices to avoid. In order to help to improve the quality of multi-language systems, we analysed open-source systems, developers' documentation, bug reports, and programming language specifications to extract bad practices of multi-language systems usage. We encoded and cataloged these practices in the form of design anti-patterns. We report here six anti-patterns. These results could help not only researchers but also professional developers considering the use of more than one programming language.},
booktitle = {Proceedings of the 24th European Conference on Pattern Languages of Programs},
articleno = {42},
numpages = {14},
keywords = {anti-patterns, code analysis, multi-language systems, software quality},
location = {Irsee, Germany},
series = {EuroPLop '19}
}

@inproceedings{10.1145/3544549.3583940,
author = {Chen, Eason},
title = {Which Factors Predict the Chat Experience of a Natural Language Generation Dialogue Service?},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583940},
doi = {10.1145/3544549.3583940},
abstract = {In this paper, we proposed a conceptual model to predict the chat experience in a natural language generation dialog system. We evaluated the model with 120 participants with Partial Least Squares Structural Equation Modeling (PLS-SEM) and obtained an R-square (R2) with 0.541. The model considers various factors, including the prompts used for generation; coherence, sentiment, and similarity in the conversation; and users’ perceived dialog agents’ favorability. We then further explore the effectiveness of the subset of our proposed model. The results showed that users’ favorability and coherence, sentiment, and similarity in the dialogue are positive predictors of users’ chat experience. Moreover, we found users may prefer dialog agents with characteristics of Extroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism. Through our research, an adaptive dialog system might use collected data to infer factors in our model, predict the chat experience for users through these factors, and optimize it by adjusting prompts.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {574},
numpages = {6},
keywords = {Big Five Personality, Chatbot, Coherence, Dialogue System, Partial Least Squares Structural Equation Modeling, User Experience},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3658271.3658337,
author = {Santos, Patricia de Oliveira and Figueiredo, Allan Chamon and Nuno Moura, Pedro and Diirr, Bruna and Alvim, Adriana C. F. and Santos, Rodrigo Pereira Dos},
title = {Impacts of the Usage of Generative Artificial Intelligence on Software Development Process},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658271.3658337},
doi = {10.1145/3658271.3658337},
abstract = {Context: Over the years, tools have been created to improve the execution of development process activities. The emergence of generative Artificial Intelligence (AI) and, more recently, the launch and dissemination of Copilot, ChatGPT-3 and other generative tools, have broadened the discussion about the possibility of using conversational generative AI tools in diverse development tasks. Problem: There is still a lack of secondary studies to map the literature about how software development process activities can be affected by the usage of generative AI tools. Solution: This study aims to identify in which activities of the software development process Natural Language (NL) generative AI tools have been used and how they can impact requirements specification, design/architecture, development and testing activities. IS Theory: The study was developed under the aegis of the Task Technology Fit theory. Method: This work presents the results of a Systematic Mapping Review (SMR) carried out to collect research results that investigate the application of generative AI tools in the software development process. Results: Results indicate that the main activities affected are development and testing and that, although there are still some issues to be addressed, there are benefits in using AI generative tools compared to using more traditional methods like human-human pair programming and code testing made by software engineering professionals. Contribution: It was possible to collect studies to identify in which activities of the software development process generative AI tools can be applied and what are the impacts of using this technology.},
booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems},
articleno = {65},
numpages = {9},
keywords = {ChatGPT, Copilot, Generative AI, Software Engineering, Software Process},
location = {Juiz de Fora, Brazil},
series = {SBSI '24}
}

@inproceedings{10.1145/3231104.3231106,
author = {Meiklejohn, Christopher S.},
title = {Partisan: Enabling Real-World Protocol Evaluation},
year = {2018},
isbn = {9781450357753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231104.3231106},
doi = {10.1145/3231104.3231106},
abstract = {We present the design and implementation of Partisan, an Erlang library for enabling real-world experiments of dis- tributed protocols and applications. Partisan is a batteries- included"library facilitating internode communication in Er- lang, runtime selection of cluster topology, pluggable layers that provide additional functionality such as causal delivery and reliable message delivery, and a mechanism for perform- ing deterministic fault injection. Partisan has been used in the evaluation of one research prototype, two real-world ap- plications, and has seen industry adoption in the Erlang and Elixir communities.},
booktitle = {Proceedings of the 2018 Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems},
pages = {45–48},
numpages = {4},
keywords = {cloud computing, distributed systems, erlang},
location = {Egham, United Kingdom},
series = {ApPLIED '18}
}

@inproceedings{10.1145/3564533.3564576,
author = {Lombeyda, Santiago and Djorgovski, S. George and Tran, An and Liu, Joy},
title = {An Open, Multi-Platform Software Architecture for Online Education in the Metaverse},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564533.3564576},
doi = {10.1145/3564533.3564576},
abstract = {Use of online platforms for education is a vibrant and growing arena, incorporating a variety of software platforms and technologies, including various modalities of extended reality. We present our Enhanced Reality Teaching Concierge, an open networking hub architected to enable efficient and easy connectivity between a wide variety of services or applications to a wide variety of clients, designed to showcase 3D for academic purposes across web technologies, virtual reality, and even virtual worlds. The agnostic nature of the system, paired with efficient architecture, and simple and open protocols furnishes an ecosystem that can easily be tailored to maximize the innate characteristics of each 3D display environment while sharing common data and control systems with the ultimate goal of a seamless, expandable, nimble education metaverse.},
booktitle = {Proceedings of the 27th International Conference on 3D Web Technology},
articleno = {12},
numpages = {4},
location = {Evry-Courcouronnes, France},
series = {Web3D '22}
}

@inproceedings{10.1145/3387263.3387272,
author = {Sadman, Nafiz and Gupta, Kishor Datta and Haque, Ariful and Poudyal, Subash and Sen, Sajib},
title = {Detect Review Manipulation by Leveraging Reviewer Historical Stylometrics in Amazon, Yelp, Facebook and Google Reviews},
year = {2020},
isbn = {9781450377355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387263.3387272},
doi = {10.1145/3387263.3387272},
abstract = {Consumers now check reviews and recommendations before consuming any services or products. But traders try to shape reviews and ratings of their merchandise to gain more consumers. Seldom they attempt to manage their competitor's review and recommendation. These manipulations are hard to detect by standard lookup from an everyday consumer, but by thoroughly examining, customers can identify these manipulations. In this paper, we try to mimic how a specialist will look to detect review manipulation and came up with algorithms that are compatible with significant and well known online services. We provide a historical stylometry based methodology to detect review manipulations and supported that with results from Amazon, Yelp, Google, and Facebook.},
booktitle = {Proceedings of the 2020 The 6th International Conference on E-Business and Applications},
pages = {42–47},
numpages = {6},
keywords = {Fake review detection, HCI, Jaccard similarity, Natural language processing, Review manipulation, Sentient analysis, Stylometry},
location = {Kuala Lumpur, Malaysia},
series = {ICEBA 2020}
}

@inproceedings{10.1145/3524842.3528471,
author = {Rossi, Davide and Zacchiroli, Stefano},
title = {Geographic diversity in public code contributions: an exploratory large-scale study over 50 years},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528471},
doi = {10.1145/3524842.3528471},
abstract = {We conduct an exploratory, large-scale, longitudinal study of 50 years of commits to publicly available version control system repositories, in order to characterize the geographic diversity of contributors to public code and its evolution over time. We analyze in total 2.2 billion commits collected by Software Heritage from 160 million projects and authored by 43 million authors during the 1971--2021 time period. We geolocate developers to 12 world regions derived from the United Nation geoscheme, using as signals email top-level domains, author names compared with names distributions around the world, and UTC offsets mined from commit metadata.We find evidence of the early dominance of North America in open source software, later joined by Europe. After that period, the geographic diversity in public code has been constantly increasing. We also identify relevant historical shifts related to the UNIX wars, the increase of coding literacy in Central and South Asia, and broader phenomena like colonialism and people movement across countries (immigration/emigration).},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {80–85},
numpages = {6},
keywords = {commit, diversity, geography, open source, social coding, software heritage, version control systems},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3306446.3340823,
author = {de Lacerda, Arthur R. T. and Aguiar, Carla S. R.},
title = {FLOSS FAQ chatbot project reuse: how to allow nonexperts to develop a chatbot},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340823},
doi = {10.1145/3306446.3340823},
abstract = {FAQ chatbots possess the capability to provide answers to frequently asked questions of a particular service, platform, or system. Currently, FAQ chatbot is the most popular domain of use of dialog assistants. However, developing a chatbot project requires a full-stack team formed by numerous specialists, such as dialog designer, data scientist, software engineer, DevOps, business strategist and experts from the domain, which can be both time and resources consuming. Language processing can be particularly challenging in languages other than English due to the scarcity of training datasets.Most of the requirements of FAQ chatbots are similar, domain-specific, and projects could profit from Open Source Software (OSS) reuse. In this paper, we examine how OSS FAQ chatbot projects can benefit from reuse at the project level (black-box reuse). We present an experience report of a FLOSS FAQ chatbot project developed in Portuguese to an e-government service in Brazil. It comprises of the chatbot distribution service, as well as for analytics tool integrated and deployed on-premises. We identified assets that could be reused as a black-box and the assets that should be customized for a particular application. We categorized these assets in architecture, corpus, dialog flows, machine learning models, and documentation. This paper discusses how automation, pre-configuration, and templates can aid newcomers to develop chatbots in Portuguese without the need for specialized skills required from tools in chatbot architecture. Our main contribution is to highlight the issues non-English FAQ chatbots projects will likely face and the assets that can be reused. It allows non-chatbot experts to develop a quality-assured OSS FAQ chatbot in a shorter project cycle.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {3},
numpages = {8},
keywords = {FLOSS, FLOSS FAQ chatbot, OSS, black-box reuse, conversational agents, e-government, experience report, open source, portuguese chatbot},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1145/3313831.3376708,
author = {Seering, Joseph and Luria, Michal and Ye, Connie and Kaufman, Geoff and Hammer, Jessica},
title = {It Takes a Village: Integrating an Adaptive Chatbot into an Online Gaming Community},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376708},
doi = {10.1145/3313831.3376708},
abstract = {While the majority of research in chatbot design has focused on creating chatbots that engage with users one-on-one, less work has focused on the design of conversational agents for online communities. In this paper we present results from a three week test of a social chatbot in an established online community. During this study, the chatbot "grew up" from "birth" through its teenage years, engaging with community members and "learning" vocabulary from their conversations. We discuss the design of this chatbot, how users' interactions with it evolved over the course of the study, and how it impacted the community as a whole. We discuss how we addressed challenges in developing a chatbot whose vocabulary could be shaped by users, and conclude with implications for the role of machine learning in social interactions in online communities and potential future directions for design of community-based chatbots.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {AI, babybot, chatbot, community interaction, interaction design, long-term study, machine learning, twitch},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3641822.3641875,
author = {Frluckaj, Hana and Qiu, Huilian Sophie and Vasilescu, Bogdan and Dabbish, Laura},
title = {From the Inside Out: Organizational Impact on Open-Source Communities and Women's Representation},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641875},
doi = {10.1145/3641822.3641875},
abstract = {The involvement of companies and public institutions in open-source software (OSS) has become widespread. While studies have explored the business models of for-profit organizations and their impact on software quality, little is known about their influence on OSS communities, especially in terms of diversity and inclusion. This knowledge gap is significant, considering that many organizations have the resources to enhance diversity and inclusion internally, but whether these efforts extend to OSS remains uncertain. To address this gap, we conducted interviews with maintainers of community-owned and organization-owned OSS projects, revealing tensions between organizations and their projects and identifying the impact of internal policies on OSS communities. Our findings reveal that, on the one hand, organization-owned projects often restrict external contributions due to stringent operating procedures and segmented communication, leading to limited external engagement. On the other hand, these organizations positively influence diversity and inclusion, notably in the representation and roles of women and the implementation of mentorship programs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {36–50},
numpages = {15},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.5555/2820518.2820540,
author = {Thongtanunam, Patanamon and McIntosh, Shane and Hassan, Ahmed E. and Iida, Hajimu},
title = {Investigating code review practices in defective files: an empirical study of the Qt system},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. To evaluate the impact that characteristics of MCR practices have on software quality, this paper comparatively studies MCR practices in defective and clean source code files. We investigate defective files along two perspectives: 1) files that will eventually have defects (i.e., future-defective files) and 2) files that have historically been defective (i.e., risky files). Through an empirical study of 11,736 reviews of changes to 24,486 files from the Qt open source project, we find that both future-defective files and risky files tend to be reviewed less rigorously than their clean counterparts. We also find that the concerns addressed during the code reviews of both defective and clean files tend to enhance evolvability, i.e., ease future maintenance (like documentation), rather than focus on functional issues (like incorrect program logic). Our findings suggest that although functionality concerns are rarely addressed during code review, the rigor of the reviewing process that is applied to a source code file throughout a development cycle shares a link with its defect proneness.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {168–179},
numpages = {12},
keywords = {code review, software quality},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3613904.3642769,
author = {Atcheson, Alex and Koshy, Vinay and Karahalios, Karrie},
title = {Not What it Used to Be: Characterizing Content and User-base Changes in Newly Created Online Communities},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642769},
doi = {10.1145/3613904.3642769},
abstract = {Attracting new members is vital to the health of many online communities. Yet, prior qualitative work suggests that newcomers to online communities can be disruptive – either due to a lack of awareness around existing community norms or to differing expectations around how the community should operate. Consequently, communities may have to navigate a trade-off between growth and development of community identity. We evaluate the presence of this trade-off through a longitudinal analysis of two years of commenting data for each of 1,620 Reddit communities. We find that, on average, communities become less linguistically distinctive as they grow. These changes appear to be driven almost equally by newcomers and returning users. Surprisingly, neither heavily moderated communities nor communities undergoing major user-base diversification are any more or less likely to maintaining distinctiveness. Taken together, our results complicate the assumption that growth is inherently beneficial for online communities.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {738},
numpages = {12},
keywords = {Computational Social Science, Content Moderation, Growth, Online communities},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3055116.3055118,
author = {Shooster, Forrest Z. and Slattery, Chase and Meier, Luna and Wengert, Rebecca and Campito, Michael and Woodward, Alex and Laroussini, Marc},
title = {The Big Wave: An Accessible Parallel Gameplay Information Gathering Puzzle Game made for the Global Game Jam},
year = {2017},
isbn = {9781450347976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055116.3055118},
doi = {10.1145/3055116.3055118},
abstract = {Presented here is the design of a game with minimal required interactions of that player and minimal programming needed to implement. This game was designed for the 2017 Global Game Jam and supports accessibility for the colorblind and deaf. The Big Wave is an information gathering puzzle game which utilizes simultaneously playing audio tracks to hide information being conveyed by various channels on a HAM radio. Various hints throughout the story, which may be directly connected to the story or may just be coincidentally similar, are presented to clue the player into what needs to be done to proceed. This paper describes the design of this game and our process methodology in producing it over the course of under 48 hours. Also covered are our plans if more time was available to have worked on the project.},
booktitle = {Proceedings of the Second International Conference on Game Jams, Hackathons, and Game Creation Events},
pages = {40–43},
numpages = {4},
keywords = {accessibility, ham radio, information gathering, parallel gameplay, puzzle game, waves},
location = {San Francisco, California, USA},
series = {ICGJ '17}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00082,
author = {Zhao, Zihe H},
title = {The Distribution and Disengagement of Women Contributors in Open-Source: 2008--2021},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00082},
doi = {10.1109/ICSE-Companion58688.2023.00082},
abstract = {The underrepresentation of women contributors in the open-source software (OSS) community has been a widely recognized problem. Past research has found that, in OSS collaboration, a gender-diverse team can enhance productivity and lower community smell [1]--[3]. However, these benefits will be hindered when a team lacks gender diversity. To better address this gender imbalance, we need to first understand the overall gender representation.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {305–307},
numpages = {3},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3624062.3624135,
author = {Pearce, Olga and Scott, Alec and Becker, Gregory and Haque, Riyaz and Hanford, Nathan and Brink, Stephanie and Jacobsen, Doug and Poxon, Heidi and Domke, Jens and Gamblin, Todd},
title = {Towards Collaborative Continuous Benchmarking for HPC},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624135},
doi = {10.1145/3624062.3624135},
abstract = {Benchmarking is integral to procurement of HPC systems, communicating HPC center workloads to HPC vendors, and verifying performance of the delivered HPC systems. Currently, HPC benchmarking is manual and challenging at every step, posing a high barrier to entry, and hampering reproducibility of the benchmarks across different HPC systems. In this paper, we propose collaborative continuous benchmarking to enable functional reproducibility, automation, and community collaboration in HPC benchmarking. Recent progress in HPC automation allows us to consider previously unimaginable large-scale improvements to the HPC ecosystem. We define the minimal requirements for collaborative continuous benchmarking and develop a common language to streamline the interactions between HPC centers, vendors, and researchers. We demonstrate the initial implementation of collaborative continuous benchmarking, and introduce an open source continuous benchmarking repository, Benchpark, for community collaboration. We believe collaborative continuous benchmarking will help overcome the human bottleneck in HPC benchmarking, enabling better evaluation of our systems and enabling a more productive collaboration within the HPC community.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {627–635},
numpages = {9},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3639474.3640067,
author = {Moster, Makayla and Kokinda, Ella and Boyer, D. Matthew and Rodeghero, Paige},
title = {Experiences with Summer Camp Communication via Discord},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640067},
doi = {10.1145/3639474.3640067},
abstract = {Teamwork and communication skills are essential for those entering the workforce, especially for software development positions. For remote development positions, the ability to work with a team and communicate remotely through a communication tool are important skills that are generally not taught in standard university courses. In this experience report, we discuss our experience using Discord for communication and collaboration during our virtual summer camp focused on teaching teamwork and game design to 27 autistic high school students. Overall, we found using Discord beneficial in many ways that we did not anticipate, including quicker instructor coordination, improved socialization, and more. Additionally, we provide recommendations for those who may want to use Discord in a similar virtual environment.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {56–65},
numpages = {10},
keywords = {remote, autism, game coding camp, discord},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3596454.3597179,
author = {S\'{a}enz, Juan Pablo and De Russis, Luigi},
title = {Listen Veronica! Can You Give Me a Hand With This Bug?},
year = {2023},
isbn = {9798400702068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3596454.3597179},
doi = {10.1145/3596454.3597179},
abstract = {Developing software implies looking for documentation, following tutorials, making implementation decisions, encountering errors, and overcoming them. Behind each aspect is the developer’s reasoning that, if not collected, is lost after the implementation. Conversely, if captured and linked to the code, the developers’ reasoning and motivations for each step they accomplish can become a valuable asset, meaningful for them and other developers. Looking for a mechanism to capture such knowledge seamlessly, we present Veronica. It is a conversational agent integrated directly into Visual Studio Code that, based on the developers’ self-explanatory reasoning, records memos and links them with the code they are writing. Furthermore, Veronica can interact with the web browser to automatically gather the sources consulted by the developer and attach them to the code. We validated our approach by conducting a usability study with eight participants that positively assessed the tool’s usefulness and suggested improvements in the graphical interface.},
booktitle = {Companion Proceedings of the 2023 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {24–30},
numpages = {7},
keywords = {conversational agent, development environment, documentation, software},
location = {Swansea, United Kingdom},
series = {EICS '23 Companion}
}

@inproceedings{10.1145/3446871.3469770,
author = {A. Gonzalez, Luis},
title = {Investigating the Benefits of Applying Artificial Intelligence Techniques to Enhance Learning Experiences in Capstone Courses},
year = {2021},
isbn = {9781450383264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446871.3469770},
doi = {10.1145/3446871.3469770},
abstract = {This research seeks to improve the learning experiences in Software Engineering Programs using Virtual Assistants based on Artificial Intelligence (AI) models. Students of Software Engineering Capstone Courses face real world situations and challenges that grant them valuable experiences for their professional preparation. However, since this knowledge is acquired through real-life exposure projects, it is difficult to transmit it among different generations of students. In consequence, all the gained knowledge, experiences, and computer codes developed are lost and cannot be reused outside the project context when they finish their assignment at the end of the semester. To address this challenge, this thesis considers the development of AI based virtual assistants applied in higher education, in a form of a lesson learned system, a recommender system integrated with a chatbot, to help students , solve problems similar to those they face in the different stages of their software project development by recommending previous lessons learned. The innovative contribution lies in the implementation of the described techniques from the state-of-art artificial intelligence field in an educational platform with the goal to leverage the experience gained during years of the teaching a Capstone Course in Software Engineering to new student generations who might benefit from this universal knowledge gained previously, in order to assist software engineering students to enhance their learning experience.},
booktitle = {Proceedings of the 17th ACM Conference on International Computing Education Research},
pages = {398–400},
numpages = {3},
keywords = {Software Engineering teaching, collective knowledge, lessons learned, recommender systems},
location = {Virtual Event, USA},
series = {ICER 2021}
}

@article{10.1145/3430360,
author = {Samtani, Sagar and Kantarcioglu, Murat and Chen, Hsinchun},
title = {Trailblazing the Artificial Intelligence for Cybersecurity Discipline: A Multi-Disciplinary Research Roadmap},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3430360},
doi = {10.1145/3430360},
abstract = {Cybersecurity has rapidly emerged as a grand societal challenge of the 21st century. Innovative solutions to proactively tackle emerging cybersecurity challenges are essential to ensuring a safe and secure society. Artificial Intelligence (AI) has rapidly emerged as a viable approach for sifting through terabytes of heterogeneous cybersecurity data to execute fundamental cybersecurity tasks, such as asset prioritization, control allocation, vulnerability management, and threat detection, with unprecedented efficiency and effectiveness. Despite its initial promise, AI and cybersecurity have been traditionally siloed disciplines that relied on disparate knowledge and methodologies. Consequently, the AI for Cybersecurity discipline is in its nascency. In this article, we aim to provide an important step to progress the AI for Cybersecurity discipline. We first provide an overview of prevailing cybersecurity data, summarize extant AI for Cybersecurity application areas, and identify key limitations in the prevailing landscape. Based on these key issues, we offer a multi-disciplinary AI for Cybersecurity roadmap that centers on major themes such as cybersecurity applications and data, advanced AI methodologies for cybersecurity, and AI-enabled decision making. To help scholars and practitioners make significant headway in tackling these grand AI for Cybersecurity issues, we summarize promising funding mechanisms from the National Science Foundation (NSF) that can support long-term, systematic research programs. We conclude this article with an introduction of the articles included in this special issue.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {dec},
articleno = {17},
numpages = {19},
keywords = {Cybersecurity, adversarial machine learning, analytics, artificial intelligence, cyber threat intelligence, disinformation, security operations centers}
}

@inproceedings{10.1109/ICSE43902.2021.00094,
author = {Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {Representation of Developer Expertise in Open Source Software},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00094},
doi = {10.1109/ICSE43902.2021.00094},
abstract = {Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {995–1007},
numpages = {13},
keywords = {API, API embedding, Developer Expertise, Developer embedding, Doc2Vec, Expertise, Machine Learning, Open Source, Project embedding, Skill Space, Vector Embedding, World of Code},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3589335.3651442,
author = {Lin, ChungYi and Tung, Shen-Lung and Su, Hung-Ting and Hsu, Winston H.},
title = {Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651442},
doi = {10.1145/3589335.3651442},
abstract = {Predicting vehicle flow is crucial for traffic management but is often limited by the scope of sensors. In contrast, extensive mobile network coverage enables us to utilize counts of mobile users' network activities (cellular traffic) on roadways as a proxy for vehicle flow. However, cellular traffic counts, which encompass various user types, may not directly align with vehicle flow. To address this issue, we present a new task: utilizing cellular traffic to predict vehicle flow in camera-free areas. This is supported by our Tel2Veh dataset, which comprises extensive cellular traffic and sparse vehicle flows. To tackle this task, we propose a two-stage framework. It first independently extracts features from multimodal data, and then integrates them using a graph neural network (GNN)-based fusion to generate predictions of vehicle flow in camera-free areas. We pioneer the fusion of telecom and vision-based data, paving the way for future expansions in web-based applications and systems.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2024},
pages = {1083–1086},
numpages = {4},
keywords = {cellular traffic, intelligent transportation, multi-source fusion},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3545945.3569790,
author = {Hundhausen, Christopher and Conrad, Phill and Adesope, Olusola and Tariq, Ahsun and Sbai, Samir and Lu, Andrew},
title = {Investigating Reflection in Undergraduate Software Development Teams: An Analysis of Online Chat Transcripts},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569790},
doi = {10.1145/3545945.3569790},
abstract = {Metacognition is widely acknowledged as a key soft skill in collaborative software development. The ability to plan, monitor, and reflect on cognitive and team processes is crucial to the efficient and effective functioning of a software team. To explore students' use of reflection--one aspect of metacognition--in undergraduate team software projects, we analyzed the online chat channels of teams participating in agile software development projects in two undergraduate courses that took place exclusively online (n = 23 teams, 117 students, and 4,915 chat messages). Teams' online chats were dominated by discussions of work completed and to be done; just two percent of all chat messages showed evidence of reflection. A follow-up analysis of chat vignettes centered around reflection messages (n = 63) indicates that three-fourths of the those messages were prompted by a course requirement; just 14% arose organically within the context of teams' ongoing project work. Based on our findings, we identify opportunities for computing educators to increase, through pedagogical and technological interventions, teams' use of reflection in team software projects.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {743–749},
numpages = {7},
keywords = {agile, content analysis, metacognition, reflection, software engineering, software engineering education, team software projects},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3614419.3643995,
author = {Voggenreiter, Angelina and Brandt, Sophie and Putterer, Fabian and Frings, Andreas and Pfeffer, Juergen},
title = {The Role of Likes: How Online Feedback Impacts Users' Mental Health},
year = {2024},
isbn = {9798400703348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614419.3643995},
doi = {10.1145/3614419.3643995},
abstract = {Social media usage has been shown to have both positive and negative consequences for users’ mental health. Several studies indicated that peer feedback plays an important role in the relationship between social media use and mental health. In this research, we analyse the impact of receiving online feedback on users’ emotional experience, social connectedness and self-esteem. In an experimental study, we let users interact with others on a Facebook-like system over the course of a week while controlling for the amount of positive reactions they receive from their peers. We find that experiencing little to no reaction from others does not only elicit negative emotions and stress amongst users, but also induces low levels of self-esteem. In contrast, receiving much positive online feedback, evokes feelings of social connectedness and reduces overall loneliness. On a societal level, our study can help to better understand the mechanisms through which social media use impacts mental health in a positive or negative way. On a methodological level, we provide a new open-source tool for designing and conducting social media experiments.},
booktitle = {Proceedings of the 16th ACM Web Science Conference},
pages = {302–310},
numpages = {9},
keywords = {Facebook, exclusion, likes, ostracism, rejection, self-esteem, social media, social status},
location = {Stuttgart, Germany},
series = {WEBSCI '24}
}

@inproceedings{10.1145/3340531.3417433,
author = {Habib, Javeria and Zhang, Shuo and Balog, Krisztian},
title = {IAI MovieBot: A Conversational Movie Recommender System},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3417433},
doi = {10.1145/3340531.3417433},
abstract = {Conversational recommender systems support users in accomplishing recommendation-related goals via multi-turn conversations. To better model dynamically changing user preferences and provide the community with a reusable development framework, we introduce IAI MovieBot, a conversational recommender system for movies. It features a task-specific dialogue flow, a multi-modal chat interface, and an effective way to deal with dynamically changing user preferences. The system is made available open source and is operated as a channel on Telegram.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3405–3408},
numpages = {4},
keywords = {conversational information access, conversational recommender systems, dialogue systems, user preference modeling},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/2993274.2993277,
author = {Rayana, Rayene Ben and Killian, Sylvain and Trangez, Nicolas and Calmettes, Arnaud},
title = {GitWaterFlow: a successful branching model and tooling, for achieving continuous delivery with multiple version branches},
year = {2016},
isbn = {9781450343992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993274.2993277},
doi = {10.1145/2993274.2993277},
abstract = {Collaborative software development presents organizations with a near-constant flow of day-to-day challenges, and there is no available off-the-shelf solution that covers all needs. This paper provides insight into the hurdles that Scality's Engineering team faced in developing and extending a sophisticated storage solution, while coping with ever-growing development teams, challenging - and regularly shifting - business requirements, and non-trivial new feature development. The authors present a novel combination of a Git-based Version Control and Branching model with a set of innovative tools dubbed GitWaterFlow to cope with the issues encountered, including the need to both support old product versions and to provide time-critical delivery of bug fixes. In the spirit of Continuous Delivery, Scality Release Engineering aims to ensure high quality and stability, to present short and predictable release cycles, and to minimize development disruption. The team's experience with the GitWaterFlow model suggests that the approach has been effective in meeting these goals in the given setting, with room for unceasing fine-tuning and improvement of processes and tools.},
booktitle = {Proceedings of the 4th International Workshop on Release Engineering},
pages = {17–20},
numpages = {4},
keywords = {branching model, concurrent release cycles, continuous integration, gatekeeper, version control, workflow automation},
location = {Seattle, WA, USA},
series = {RELENG 2016}
}

@inproceedings{10.5555/3615924.3615927,
author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
title = {Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {24–33},
numpages = {10},
keywords = {Software Engineering, AI-based solutions, Performance Evaluation, ChatGPT, Machine Learning},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3573428.3573729,
author = {Xu, Yingyu},
title = {Deep reinforcement learning and imitation learning based on VizDoom},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573729},
doi = {10.1145/3573428.3573729},
abstract = {Reinforcement learning is a field of machine learning that focuses on intelligent agents, primarily the concept of what actions an intelligent agent takes in the environment to maximize cumulative reward. In environments where rewards are scarce, a manual approach is necessary. However, manually designing the reward function to meet the desired behavior can be very complicated. A very useful solution is Imitation Learning (IL). This paper proposes two reinforcement learning algorithms for the basic scene of the VizDoom video game, and uses IL to improve the performance of one of the models.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1700–1706},
numpages = {7},
keywords = {Imitation learning, Reinforcement learning},
location = {Xiamen, China},
series = {EITCE '22}
}

@article{10.1145/3583562,
author = {Sarker, Jaydeb and Turzo, Asif Kamal and Dong, Ming and Bosu, Amiangshu},
title = {Automated Identification of Toxic Code Reviews Using ToxiCR},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583562},
doi = {10.1145/3583562},
abstract = {Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {118},
numpages = {32},
keywords = {Toxicity, code review, sentiment analysis, Natural Language Processing, tool development}
}

@inproceedings{10.1145/3613904.3642012,
author = {Wang, Leijie and Vincent, Nicholas and Rukanskaitundefined, Julija and Zhang, Amy Xian},
title = {Pika: Empowering Non-Programmers to Author Executable Governance Policies in Online Communities},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642012},
doi = {10.1145/3613904.3642012},
abstract = {Internet users have formed a wide array of online communities with diverse community goals and nuanced norms. However, most online platforms only offer a limited set of governance models in their software infrastructure and leave little room for customization. Consequently, technical proficiency becomes a prerequisite for online communities to build governance policies in code, excluding non-programmers from participation in designing community governance. In this paper, we present Pika, a system that empowers non-programmers to author a wide range of executable governance policies. At its core, Pika incorporates a declarative language that decomposes governance policies into modular components, thereby facilitating expressive policy authoring through a user-friendly, form-based web interface. Our user studies with 10 non-programmers and 7 programmers show that Pika can empower non-programmers to author policies approximately 2.5 times faster than programmers who author in code. We also provide insights about Pika’s expressivity in supporting diverse policies online communities want.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {925},
numpages = {18},
keywords = {Community Governance, Declarative Language, End-user Programming, Online Communities},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3579856.3582811,
author = {von Arx, Theo and Paterson, Kenneth G.},
title = {On the Cryptographic Fragility of the Telegram Ecosystem},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3582811},
doi = {10.1145/3579856.3582811},
abstract = {Telegram is a popular messenger with more than 550 million active users per month and with a large ecosystem of different clients. The wide adoption of Telegram by protestors relying on private and secure messaging provides motivation for developing a profound understanding of its cryptographic design and how this influences its security properties. Telegram has its own bespoke transport layer security protocol, MTProto&nbsp;2.0. This protocol was recently subjected to a detailed study by Albrecht et al. (IEEE S&amp;P 2022). They gave attacks on the protocol and its implementations, along with a security proof for a modified version of the protocol. We complement that study by analysing a range of third-party client implementations of MTProto&nbsp;2.0. We report practical replay attacks for the Pyrogram, Telethon and GramJS clients, and a more theoretical timing attack against the MadelineProto client. We show how vulnerable third-party clients can affect the security of the entire ecosystem, including official clients. Our analysis reveals that many third-party clients fail to securely implement MTProto&nbsp;2.0. We discuss the reasons for these failures, focussing on complications in the design of MTProto&nbsp;2.0 that lead developers to omit security-critical features or to implement the protocol in an insecure manner. We also discuss changes that could be made to MTProto&nbsp;2.0 to remedy this situation. Overall, our work highlights the cryptographic fragility of the Telegram ecosystem.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {328–341},
numpages = {14},
keywords = {encrypt-and-MAC, reply attack, telegram, timing side-channel},
location = {Melbourne, VIC, Australia},
series = {ASIA CCS '23}
}

@inproceedings{10.1145/3368308.3415377,
author = {Hajek, Jeremy and Rashid, Mudassir and Sevil, Mert and Cinar, Ali and Alvarez Fernandez, Pablo Angel and Jain, Dhiraj},
title = {The Necessity of Interdisciplinary Software Development for Building Viable Research Platforms: Case Study in Automated Drug Delivery in Diabetes},
year = {2020},
isbn = {9781450370455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368308.3415377},
doi = {10.1145/3368308.3415377},
abstract = {Developing viable and robust software is an inescapable artifact of graduate research. The challenges lie in the complexities of developing, deploying, and securing software to support the research objectives. Combined with the transitive nature of students, the management of the software development and launch process is an arduous task. A standardized framework for developing and launching complex software is required. Within a university, individual departments do not typically possess the expertise, resources, software and infrastructure to translate research results to a viable product or tool. Extending upon the research of Hilton et al., [5] we designed a software development pipeline in an integrated multi-disciplinary research context. The integrated and collaborative software pipeline formulated from the onset of the project streamlines the development phase and provides an iterative feedback and testing environment. This approach is applied to the development of automated insulin delivery systems, with the synergistic efforts of interdisciplinary teams yielding a mobile application and server software solutions, and a framework for the iterative advancement of the software capabilities into the future.},
booktitle = {Proceedings of the 21st Annual Conference on Information Technology Education},
pages = {390–396},
numpages = {7},
keywords = {CI/CD, agile, app development, containers, full stack, interdisciplinary, opensource, research, rest API},
location = {Virtual Event, USA},
series = {SIGITE '20}
}

@inproceedings{10.1145/3535511.3535555,
author = {Monteiro, Lucas Henrique de Assis and Rodrigues, Cleyton M\'{a}rio de Oliveira and Sousa, A\^{e}da Monalliza Cunha de},
title = {An Information System for Law Integrating Ontological Bases with a Legal Reasoner Chatbot},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535555},
doi = {10.1145/3535511.3535555},
abstract = {Context: The Semantic Web aims to assign meanings to resources available on the internet so that humans and computers can understand them. It can be used in the most diverse contexts, facilitating the development of systems where expert knowledge is formalized through logical-mathematical resources, mitigating potential inconsistencies, and promoting more human-friendly interaction services. Problem: The existence of semantic anomalies (use of rhetorical language, polysemy and inaccuracies) in the Brazilian Legal Domain enables the use of Semantic Web standards and technologies to mitigate these problems. Solution: This work deals with the development of an Information System that uses resources from the Semantic Web for the formal representation and the realization of legal inferences about Crimes Against Property. SI Theory: The Behavioral Decision Theory was approached, mainly in the incorporation of real patterns of decision making. Method: Bibliographic and documentary research methods were used to list the main concepts related to the Criminal Types investigated. The research is prescriptive and has a quali-quantitative approach. Summary of Results: A prototype system is presented, integrating ontologies of Brazilian Law with a chatbot that enables interaction with users in natural language, as well as performing reasoning tasks based on the knowledge formalized in these ontologies. Contributions and Impact in the IS area: The research will contribute to the automation of decision-making processes involving crimes against property, serving as an aid for professionals or law students and for legal simulations by ordinary people. Furthermore, it will serve as a reference for the development of other information systems with similar objectives in other contexts.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Information Systems},
articleno = {44},
numpages = {8},
keywords = {Chatbot, Law, Ontology},
location = {Curitiba, Brazil},
series = {SBSI '22}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@inproceedings{10.1145/3411763.3457782,
author = {Mathur, Arunesh},
title = {Identifying and Measuring Manipulative User Interfaces at Scale on the Web},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3457782},
doi = {10.1145/3411763.3457782},
abstract = {In this dissertation, I present measurement methods to automatically identify manipulative user interfaces—colloquially known as “dark patterns”—at scale on the web. Using these methods, I quantify the prevalence of dark patterns in three studies and show how dark patterns are rampant on the web, thus a pressing concern for society. First, I examine whether social media content creators, or “influencers,” disclose their relationships with advertisers to their audience. Analyzing over 500K YouTube videos and 2.1M Pinterest pins, I find that only about 10% of all advertising content is disclosed to users. Second, I examine various types of dark patterns in shopping websites. Analyzing data from 11K shopping websites, I discover over 1,800 dark patterns on over 1,200 websites that mislead users into making more purchases or disclosing more information than they would otherwise. Third, I examine dark patterns in political emails from the 2020 U.S. election cycle. Through an analysis of over 100K emails, I find that over 40% of emails from the median sender contain dark patterns that nudge recipients to open emails or make donations they might otherwise not make. I further outlay the conceptual foundation of dark patterns and articulate a set of normative perspectives for analyzing the effects of dark patterns. I conclude with how the lessons learned from the studies can be used to build technical defenses and to lay out policy recommendations to mitigate the spread of these interfaces.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {7},
numpages = {5},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3524842.3528049,
author = {de Almeida, Carlos D. A. and Feij\'{o}, Diego N. and Rocha, Lincoln S.},
title = {Studying the impact of continuous delivery adoption on bug-fixing time in apache's open-source projects},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528049},
doi = {10.1145/3524842.3528049},
abstract = {Buggy software impacts people's lives and businesses. Nowadays, a huge portion of a software project's cost is spent on debugging (finding and fixing bugs). Therefore, reducing the time needed to release new software versions free from bugs becomes crucial. Continuous delivery (CD) arises as an alternative to traditional software release engineering by providing the capability to faster and continuously release software to customers through automated pipelines. Previous studies claim that CD adoption leads to a reduction in the software release cycle time, including the time lag to fix reported bugs (bug-fixing time) and apply correction patches in the affected versions. However, there is a lack of empirical evidence supporting (or not) this claim. To fulfill this gap, we conducted an empirical study to evaluate the impact of CD adoption in the bug-fixing time. We study 25 open-source projects comparing the bug-fixing time before and after adopting CD. Our results show that bug-fixing time after CD adoption becomes shorter (with statistical significance) than the bug-fixing time before CD adoption.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {132–136},
numpages = {5},
keywords = {bug-fixing time, continuous delivery, mining software repositories},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3312614.3312615,
author = {Falco, Gregory and Li, Caleb and Fedorov, Pavel and Caldera, Carlos and Arora, Rahul and Jackson, Kelly},
title = {NeuroMesh: IoT Security Enabled by a Blockchain Powered Botnet Vaccine},
year = {2019},
isbn = {9781450366403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312614.3312615},
doi = {10.1145/3312614.3312615},
abstract = {Internet-of-Things (IoT) devices are ubiquitous and growing rapidly in number. However, IoT manufacturers have focused on the functionality and features of the devices and made security an afterthought. Since IoT devices have small memory capacities and low-power processors, many security firms have not been able to develop anti-malware software for these devices. Current IoT security solutions are heavy and unreliable. We have developed a lightweight IoT security solution that uses hacker tools against the hackers -- in essence, a vaccine for IoT. Our software provides managed security and intelligence to IoT devices using a "friendly" botnet operated through a proven, existing communication infrastructure for distributed systems -- the Bitcoin blockchain.},
booktitle = {Proceedings of the International Conference on Omni-Layer Intelligent Systems},
pages = {1–6},
numpages = {6},
keywords = {Bitcoin, Blockchain, Botnet, Embedded System Security, IoT Device Management, IoT Security, Machine Learning, Mirai, Security Architecture, Software Vaccine},
location = {Crete, Greece},
series = {COINS '19}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3404835.3463011,
author = {Zhu, Yutao and Nie, Jian-Yun and Zhou, Kun and Du, Pan and Jiang, Hao and Dou, Zhicheng},
title = {Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463011},
doi = {10.1145/3404835.3463011},
abstract = {A proactive dialogue system has the ability to proactively lead the conversation. Different from the general chatbots which only react to the user, proactive dialogue systems can be used to achieve some goals, e.g., to recommend some items to the user. Background knowledge is essential to enable smooth and natural transitions in dialogue. In this paper, we propose a new multi-task learning framework for retrieval-based knowledge-grounded proactive dialogue. To determine the relevant knowledge to be used, we frame knowledge prediction as a complementary task and use explicit signals to supervise its learning. The final response is selected according to the predicted knowledge, the goal to achieve, and the context. Experimental results show that explicit modeling of knowledge prediction and goal selection can greatly improve the final response selection. Our code is available at https://github.com/DaoD/KPN/.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2000–2004},
numpages = {5},
keywords = {multi-task learning, proactive dialogue, retrieval-based chatbot},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3581783.3613460,
author = {He, Lingxiao and Liao, Xingyu and Liu, Wu and Liu, Xinchen and Cheng, Peng and Mei, Tao},
title = {FastReID: A Pytorch Toolbox for General Instance Re-identification},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613460},
doi = {10.1145/3581783.3613460},
abstract = {General Instance Re-identification is a very important task in computer vision, which can be widely used in many practical applications, such as person/vehicle re-identification, face recognition, wildlife protection, commodity tracing, snapshots, and so on. To meet the increasing application demand for general instance re-identification, we present FastReID as a widely used software system. In FastReID, the highly modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art projects, including person re-id, partial re-id, cross-domain re-id, and vehicle re-id. Moreover, we plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most general and high-performance toolbox that supports single and multiple GPU servers, it can reproduce our project results very easily. The source codes and models have been released at https://github.com/JDAI-CV/fast-reid.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9664–9667},
numpages = {4},
keywords = {fastreid, person re-identification, pytorch toolbox, vehicle re-identification},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3628034.3628039,
author = {Serbout, Souhaila and El Malki, Amine and Pautasso, Cesare and Zdun, Uwe},
title = {API Rate Limit Adoption -- A pattern collection},
year = {2024},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628034.3628039},
doi = {10.1145/3628034.3628039},
abstract = {The API Rate Limit pattern controls the rate at which clients make API requests by counting the number of requests in a specified time interval and reacting against abusive clients, in order to protect the limited resources of the API from exhaustion and denial of service attacks. This practice helps service providers to prevent abuse and ensure fair resource allocation, maintain system stability, monitor and control service availability, protect against DDoS attacks In this research paper, we have identified patterns covering the API Rate Limit pattern adoption starting from its documentation to its implementation. Our objective is to elucidate the trade-offs associated with different identified patterns and offer guidance to developers in making informed decisions when choosing the most suitable Rate Limit method, scope, and granularity for their service. By providing a comprehensive overview of how to adopt the Rate Limit pattern, this paper aims to enhance the understanding of how APIs can be designed to facilitate high scalability, security, reliability, and service availability. Furthermore, we present each pattern along with known uses observed in real-world APIs and technologies.},
booktitle = {Proceedings of the 28th European Conference on Pattern Languages of Programs},
articleno = {5},
numpages = {20},
keywords = {Application Programming Interfaces, Pattern Language, Rate Limit},
location = {Irsee, Germany},
series = {EuroPLoP '23}
}

@inproceedings{10.1145/3132465.3132466,
author = {Hagemeister, Philipp and Mauve, Martin},
title = {Enabling distributed revision control systems in delay-tolerant networks},
year = {2017},
isbn = {9781450355278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132465.3132466},
doi = {10.1145/3132465.3132466},
abstract = {A distributed revision control system (dRCS) such as git or mercurial allows users to track changes1 to a common document.2 When multiple users commit, a primary challenge of a dRCS becomes to provide one view of the current repository state.},
booktitle = {Proceedings of the Fifth ACM/IEEE Workshop on Hot Topics in Web Systems and Technologies},
articleno = {14},
numpages = {6},
location = {San Jose, California},
series = {HotWeb '17}
}

@inproceedings{10.1145/3581971.3581990,
author = {Kurbatov, Dmitry and Rybnikova, Marianna and Madhwal, Yash and Yanovich, Yury and Zotov, Gleb},
title = {OnlyTips: Blockchain-Driven Tips Service},
year = {2023},
isbn = {9781450397575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581971.3581990},
doi = {10.1145/3581971.3581990},
abstract = {Gratuity (tips) plays a valuable role in the service sector. Tips exist in many countries more and more in a digital format. But digital tips lack transparency: givers have no proof of delivery, and recipients have no proof of assessing all the transactions and the amount’s immutability. A tips service provider is a single point of failure and should be trusted to keep the system working. Blockchain allows running a system in a trustless environment and can be a clue to the reliable decentralized digital gratuity. In the paper, we analyze a technology and business potential of a blockchain-based tips service and propose a full-stack prototype.},
booktitle = {Proceedings of the 2022 5th International Conference on Blockchain Technology and Applications},
pages = {129–139},
numpages = {11},
keywords = {availability service, blockchain, reputation, smart contracts, testing},
location = {Xi'an, China},
series = {ICBTA '22}
}

@inproceedings{10.1145/3469213.3470245,
author = {Wang, Nan and Sun, Qingyu and Jiao, Qingju},
title = {Abnormal user identification in online social networks based on user behavior},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470245},
doi = {10.1145/3469213.3470245},
abstract = {Recently, in online social networks zombie fans have been shown a more complex and humanizing form. The existing methods based on basic features such as the number of followee, follower scale, user name and information content are not very efficient, which may lead to many misrecognitions and missed detection. Behavior pattern is the most fundamental feature of online users and abnormal users certainly have particular actions which are different from the normal users. Therefore, in this paper, a zombie fans identification method has been proposed based on the behavior characteristics like retweet, comment and the corresponding regularity. Furthermore, with user behaviors, invalid user identification is also researched. The experimental results showed that the abnormal user recognition method proposed in this paper had high identification accuracy.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {45},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3488932.3517414,
author = {Musch, Marius and Kirchner, Robin and Boll, Max and Johns, Martin},
title = {Server-Side Browsers: Exploring the Web's Hidden Attack Surface},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3517414},
doi = {10.1145/3488932.3517414},
abstract = {As websites grow ever more dynamic and load more of their content on the fly, automatically interacting with them via simple tools like curl is getting less of an option. Instead, headless browsers with JavaScript support, such as PhantomJS and Puppeteer, have gained traction on the Web over the last few years. For various use cases like messengers and social networks that display link previews, these browsers visit arbitrary, user-controlled URLs. To avoid compromise through known vulnerabilities, these browsers need to be diligently kept up-to-date. In this paper, we investigate the phenomenon of what we coin server-side browsers at scale and find that many websites are running severely outdated browsers on the server-side. Remarkably, the majority of them had not been updated for more than 6 months and over 60% of the discovered implementations were found to be vulnerable to publicly available proof-of-concept exploits.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {1168–1181},
numpages = {14},
keywords = {browser, fingerprinting, instrumentation, server-side requests, web security},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3387940.3391488,
author = {Etemadi, Khashayar and Monperrus, Martin},
title = {On the Relevance of Cross-project Learning with Nearest Neighbours for Commit Message Generation},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391488},
doi = {10.1145/3387940.3391488},
abstract = {Commit messages play an important role in software maintenance and evolution. Nonetheless, developers often do not produce high-quality messages. A number of commit message generation methods have been proposed in recent years to address this problem. Some of these methods are based on neural machine translation (NMT) techniques. Studies show that the nearest neighbor algorithm (NNGen) outperforms existing NMT-based methods, although NNGen is simpler and faster than NMT. In this paper, we show that NNGen does not take advantage of cross-project learning in the majority of the cases. We also show that there is an even simpler and faster variation of the existing NNGen method which outperforms it in terms of the BLEU_4 score without using cross-project learning.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {470–475},
numpages = {6},
keywords = {commit message generation, nearest neighbor algorithm, neural machine translation},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1145/3657303,
author = {Pessoa, Larissa and Martins, Lia and Hsu, Meng and Freitas, Rosiane de},
title = {ZoAM GameBot: a Journey to the Lost Computational World in the Amazonia},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3657303},
doi = {10.1145/3657303},
abstract = {The search for alternative teaching-learning processes that attract more interest and involvement of young people, has inspired the development of a game with a chatbot architecture based on interactive storytelling and multiple learning paths. Thus, we introduce in this article the GameBot ZoAm, developed for the Discord instant messaging and social platform. ZoAm offers a unique learning experience centered around storytelling, focusing on fundamental computing concepts and logical challenges that enhance computational thinking skills. Furthermore, the game also promotes an appreciation for Amazonian culture and folklore, with decision-making with human values. An action research study was conducted involving students from the last years of the end of elementary school. The research utilized a heuristic analysis based on the Gameplay Heuristics (PLAY) by Desurvire and Wiberg (ANO), and the evaluation model proposed by Korhonen and Koivisto (ANO) for mobile devices. The analysis employed a reduced and merged set of heuristics from these models, suited for the gamebot’s context, focusing on I) Usability, II) Gameplay and Immersion, and III) Mobility. Regarding the reliability coefficient used to evaluate the survey applied to students after playing the gamebot, Cronbach’s Alpha and Guttman Lambda-6 (G6(smc)) coefficients were applied. These metrics were chosen to ensure the internal consistency and reliability of survey items, reflecting on how effectively the questions measured the focuses proposed by the heuristic analysis. The findings indicate that the game has the potential to facilitate the assimilation of the integrated concepts and sustain student interest throughout gameplay.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = {may},
keywords = {conversation simulation applications, educational digital games, narratives and storytelling, computational thinking}
}

@inproceedings{10.1145/3294109.3295638,
author = {Strohmeier, Paul and H\r{a}kansson, Victor and Honnet, Cedric and Ashbrook, Daniel and Hornb\ae{}k, Kasper},
title = {Optimizing Pressure Matrices: Interdigitation and Interpolation Methods for Continuous Position Input},
year = {2019},
isbn = {9781450361965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3294109.3295638},
doi = {10.1145/3294109.3295638},
abstract = {This paper provides resources and design recommendations for optimizing position input for pressure sensor matrices, a sensor design often used in eTextiles. Currently applications using pressure matrices for precise continuous position control are rare. One reason designers opt against using these sensors for continuous position control is that when the finger transitions from one sensing electrode to the next, jerky motion, jumps or other non-linear artifacts appear. We demonstrate that interdigitation can improve transition behavior and discuss interpolation algorithms to best leverage such designs. We provide software for reproducing our sensors and experiment, as well as a dataset consisting of 1122 swipe gestures performed on 17 sensors.},
booktitle = {Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {117–126},
numpages = {10},
keywords = {etextile, interdigitation, interpolation, piezoresistive, pressure input, pressure matrix, sensor design},
location = {Tempe, Arizona, USA},
series = {TEI '19}
}

@inproceedings{10.1145/3661167.3661220,
author = {G\'{o}mez-Abajo, Pablo and P\'{e}rez-Soler, Sara and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan},
title = {Mutation Testing for Task-Oriented Chatbots},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661220},
doi = {10.1145/3661167.3661220},
abstract = {Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {232–241},
numpages = {10},
keywords = {Botium, Dialogflow, Mutation testing, Rasa, Task-oriented chatbots},
location = {Salerno, Italy},
series = {EASE '24}
}

@proceedings{10.1145/3623476,
title = {SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3544548.3581317,
author = {Qiu, Huilian Sophie and Lieb, Anna and Chou, Jennifer and Carneal, Megan and Mok, Jasmine and Amspoker, Emily and Vasilescu, Bogdan and Dabbish, Laura},
title = {Climate Coach: A Dashboard for Open-Source Maintainers to Overview Community Dynamics},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581317},
doi = {10.1145/3544548.3581317},
abstract = {Open-source software projects have become an integral part of our daily life, supporting virtually every software we use today. Since open-source software forms the digital infrastructure, maintaining them is of utmost importance. We present Climate Coach, a dashboard that helps open-source project maintainers monitor the health of their community in terms of team climate and inclusion. Through a literature review and an exploratory survey (N=18), we identified important signals that can reflect a project’s health, and display them on a dashboard. We evaluated and refined our dashboard through two rounds of think-aloud studies (N=19). We then conducted a two-week longitudinal diary study (N=10) to test the usefulness of our dashboard. We found that displaying signals that are related to a project’s inclusion help improve maintainers’ management strategies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {552},
numpages = {18},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3377170.3377222,
author = {Yassin, Warusia and Abdullah, Raihana and Abdollah, Mohd Faizal and Mas'ud, Zaki and Bakhari, Farah Adeliena},
title = {An IoT Botnet Prediction Model Using Frequency based Dependency Graph: Proof-of-concept},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377222},
doi = {10.1145/3377170.3377222},
abstract = {Malware attacks are widespread in an era of growing technology by targeting most computing resources. Plenty of the technology nowadays is based on digital data exchange and it leads to the Internet of Things (IoT) development. A massive growth of IoT technology attracts attackers' interest in exploiting a number of IoT devices using a variety of attacks. Consequently, this has caused difficulty to the researcher in distinguishing a characteristic of such variant specifically for IoT botnet-based attack. Current approaches are weak in recognizing such behavior by analyzing registry information more accurately due to the fact that the attack pattern usually hard to construct. Hence, in this paper, selected features of suspicious registry information that's been affected by IoT botnet action i.e. Mirai is further analyzed using the graph-theoretical approach. Using a dependency graph, the similar and dissimilar pattern of distinct botnet composed to facilitate the process of malware variant characteristic identification. As a result of doing this, a precise attack pattern can be constructed and could be considered for future botnet prediction. A series of experiments conducted as a proof-of-concept in order to assess and validate the formed attack pattern. The findings have shown that the proposed prediction model could overcome the issues of undetectable IoT botnet behavior. From this forward, this model could be used to obtain accurate detection results for any variant of malware.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {344–352},
numpages = {9},
keywords = {Dependency Graph, IoT Botnet, Malware, Prediction, Registry Information},
location = {Shanghai, China},
series = {ICIT '19}
}

@inproceedings{10.1145/3357384.3358099,
author = {Fu, Zhenxin and Ji, Feng and Hu, Wenpeng and Zhou, Wei and Zhao, Dongyan and Chen, Haiqing and Yan, Rui},
title = {Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358099},
doi = {10.1145/3357384.3358099},
abstract = {Information-seeking conversation system aims at satisfying the information needs of users through conversations. Text matching between a user query and a pre-collected question is an important part of the information-seeking conversation in E-commerce. In the practical scenario, a sort of questions always correspond to a same answer. Naturally, these questions can form a bag. Learning the matching between user query and bag directly may improve the conversation performance, denoted as query-bag matching. Inspired by such opinion, we propose a query-bag matching model which mainly utilizes the mutual coverage between query and bag and measures the degree of the content in the query mentioned by the bag, and vice verse. In addition, the learned bag representation in word level helps find the main points of a bag in a fine grade and promotes the query-bag matching performance. Experiments on two datasets show the effectiveness of our model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2337–2340},
numpages = {4},
keywords = {bag, coverage, e-commerce, matching, ranking},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.5555/3539845.3539923,
author = {Li, Zhuoran and Zhao, Dan},
title = {ThingNet: a lightweight real-time mirai IoT variants hunter through CPU power fingerprinting},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Internet of Things (IoT) devices have become attractive targets of cyber criminals, whereas attackers have been leveraging these vulnerable devices most notably via the infamous Mirai-based botnets, accounting for nearly 90% of IoT malware attacks in 2020. In this work, we propose a robust, universal and non-invasive Mirai-based malware detection engine employing a compact deep neural network architecture. Our design allows programmatic collection of CPU power footprints with integrated current sensors under various device states, such as idle, service and attack. A lightweight online inference model is deployed in the CPU for on-the-fly classification. Our model is robust against noisy environment with a lucid design of noise reduction function. This work appears to be the first step towards a viable CPU malware detection engine based on power fingerprinting. The extensive simulation study under ARM architecture that is widely used in IoT devices, demonstrates a high detection accuracy of 99.1% at a speed less than 1ms. By analyzing Mirai-based infection under distinguishable phases for power feature extraction, our model has further demonstrated an accuracy of 96.3% on model-unknown variants detection.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {310–315},
numpages = {6},
keywords = {lightweight deep learning, mirai IoT variants detection, noise reduction, power side-channel auditing},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3281278.3281280,
author = {Baudart, Guillaume and Mandel, Louis and Tardieu, Olivier and Vaziri, Mandana},
title = {A reactive language for analyzing cloud logs},
year = {2018},
isbn = {9781450360708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281278.3281280},
doi = {10.1145/3281278.3281280},
abstract = {Log analysis is required in many domains, and especially in the emerging field of cloud computing. Cloud applications are often built by composing diverse services. When something goes wrong, finding the root cause of the problem can be difficult. Many services are only reachable through their Application Programming Interfaces (APIs) with no possibility for live introspection. In this context, logs become an essential tool for monitoring and debugging. Cloud services typically generate very large quantities of log messages, with formats that may not be well specified and may vary over time. In this paper, we present CloudLens, a language for the analysis of semi-structured textual data as found in logs, and specify its formal semantics. CloudLens is a reactive language and views logs as streams of objects. Our objective is to facilitate exploring the contents of logs interactively and to write reusable analyses succinctly, using familiar constructs. We implemented an interpreter for the Apache Zeppelin notebook to provide an interactive IDE. Our prototype implementation is open source and we report on a detailed case study using logs from the Apache OpenWhisk project.},
booktitle = {Proceedings of the 5th ACM SIGPLAN International Workshop on Reactive and Event-Based Languages and Systems},
pages = {61–70},
numpages = {10},
keywords = {Log analysis, Programming language, Reactive programming},
location = {Boston, MA, USA},
series = {REBLS 2018}
}

@inproceedings{10.1145/3213846.3213847,
author = {Madsen, Magnus and Lhot\'{a}k, Ond\v{r}ej},
title = {Safe and sound program analysis with Flix},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213847},
doi = {10.1145/3213846.3213847},
abstract = {Program development tools such as bug finders, build automation tools, compilers, debuggers, integrated development environments, and refactoring tools increasingly rely on static analysis techniques to reason about program behavior. Implementing such static analysis tools is a complex and difficult task with concerns about safety and soundness. Safety guarantees that the fixed point computation -- inherent in most static analyses -- converges and ultimately terminates with a deterministic result. Soundness guarantees that the computed result over-approximates the concrete behavior of the program under analysis. But how do we know if we can trust the result of the static analysis itself? Who will guard the guards?  In this paper, we propose the use of automatic program verification techniques based on symbolic execution and SMT solvers to verify the correctness of the abstract domains used in static analysis tools. We implement a verification toolchain for Flix, a functional and logic programming language tailored for the implementation of static analyses. We apply this toolchain to several abstract domains. The experimental results show that we are able to prove 99.5% and 96.3% of the required safety and soundness properties, respectively.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {38–48},
numpages = {11},
keywords = {lattices, monotonicity, safety, soundness, static analysis},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3377811.3380426,
author = {Wang, Ying and Wen, Ming and Liu, Yepang and Wang, Yibo and Li, Zhenming and Wang, Chao and Yu, Hai and Cheung, Shing-Chi and Xu, Chang and Zhu, Zhiliang},
title = {Watchman: monitoring dependency conflicts for Python library ecosystem},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380426},
doi = {10.1145/3377811.3380426},
abstract = {The PyPI ecosystem has indexed millions of Python libraries to allow developers to automatically download and install dependencies of their projects based on the specified version constraints. Despite the convenience brought by automation, version constraints in Python projects can easily conflict, resulting in build failures. We refer to such conflicts as &lt;u&gt;D&lt;/u&gt;ependency &lt;u&gt;C&lt;/u&gt;onfict (DC) issues. Although DC issues are common in Python projects, developers lack tool support to gain a comprehensive knowledge for diagnosing the root causes of these issues. In this paper, we conducted an empirical study on 235 real-world DC issues. We studied the manifestation patterns and fixing strategies of these issues and found several key factors that can lead to DC issues and their regressions. Based on our findings, we designed and implemented Watchman, a technique to continuously monitor dependency conflicts for the PyPI ecosystem. In our evaluation, Watchman analyzed PyPI snapshots between 11 Jul 2019 and 16 Aug 2019, and found 117 potential DC issues. We reported these issues to the developers of the corresponding projects. So far, 63 issues have been confirmed, 38 of which have been quickly fixed by applying our suggested patches.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {125–135},
numpages = {11},
keywords = {Python, dependency conflicts, software ecosystem},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3544548.3581525,
author = {Wang, April Yi and Head, Andrew and Zhang, Ashley Ge and Oney, Steve and Brooks, Christopher},
title = {Colaroid: A Literate Programming Approach for Authoring Explorable Multi-Stage Tutorials},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581525},
doi = {10.1145/3544548.3581525},
abstract = {Multi-stage programming tutorials are key learning resources for programmers, using progressive incremental steps to teach them how to build larger software systems. A good multi-stage tutorial describes the code clearly, explains the rationale and code changes for each step, and allows readers to experiment as they work through the tutorial. In practice, it is time-consuming for authors to create tutorials with these attributes. In this paper, we introduce Colaroid, an interactive authoring tool for creating high quality multi-stage tutorials. Colaroid tutorials are augmented computational notebooks, where snippets and outputs represent a snapshot of a project, with source code differences highlighted, complete source code context for each snippet, and the ability to load and tinker with any stage of the project in a linked IDE. In two laboratory studies, we found Colaroid makes it easy to create multi-stage tutorials, while offering advantages to readers compared to video and web-based tutorials.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {798},
numpages = {22},
keywords = {computational notebooks, instruction, programming, tutorials},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3379597.3387461,
author = {Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David},
title = {A Machine Learning Approach for Vulnerability Curation},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387461},
doi = {10.1145/3379597.3387461},
abstract = {Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {32–42},
numpages = {11},
keywords = {application security, classifiers ensemble, machine learning, open-source software, self-training},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3502718.3524782,
author = {Druga, Stefania and Otero, Nancy and Ko, Amy J.},
title = {The Landscape of Teaching Resources for AI Education},
year = {2022},
isbn = {9781450392013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502718.3524782},
doi = {10.1145/3502718.3524782},
abstract = {Artificial Intelligence (AI) educational resources such as training tools, interactive demos, and dedicated curriculum are increasingly popular among educators and learners. While prior work has examined pedagogies for promoting AI literacy, it has yet to examine how well technology resources support these pedagogies. To address this gap, we conducted a systematic analysis of existing online resources for AI education, investigating what learning and teaching affordances these resources have to support AI education. We used the Technological Pedagogical Content Knowledge (TPACK) framework to analyze a final corpus of 50 AI resources. We found that most resources support active learning, have digital or physical dependencies, do not include all the five big ideas defined by AI4K12 guidelines, and do not offer built-in support for assessment or feedback. Teaching guides are hard to find or require technical knowledge. Based on our findings, we propose that future AI curricula move from singular activities and demos to more holistic designs that include support, guidance, and flexibility for how AI technology, concepts, and pedagogy play out in the classroom.},
booktitle = {Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 1},
pages = {96–102},
numpages = {7},
keywords = {ai education, k12, teaching support},
location = {Dublin, Ireland},
series = {ITiCSE '22}
}

@inproceedings{10.1145/3560835.3564556,
author = {Okafor, Chinenye and Schorlemmer, Taylor R. and Torres-Arias, Santiago and Davis, James C.},
title = {SoK: Analysis of Software Supply Chain Security by Establishing Secure Design Properties},
year = {2022},
isbn = {9781450398855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560835.3564556},
doi = {10.1145/3560835.3564556},
abstract = {This paper systematizes knowledge about secure software supply chain patterns. It identifies four stages of a software supply chain attack and proposes three security properties crucial for a secured supply chain: transparency, validity, and separation. The paper describes current security approaches and maps them to the proposed security properties, including research ideas and case studies of supply chains in practice. It discusses the strengths and weaknesses of current approaches relative to known attacks and details the various security frameworks put out to ensure the security of the software supply chain. Finally, the paper highlights potential gaps in actor and operation-centered supply chain security techniques.},
booktitle = {Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {15–24},
numpages = {10},
keywords = {collaborative software engineering, security properties, software reuse, software supply chain attacks},
location = {Los Angeles, CA, USA},
series = {SCORED'22}
}

@inproceedings{10.1145/3551349.3559570,
author = {Sarker, Jaydeb},
title = {Identification and Mitigation of Toxic Communications Among Open Source Software Developers},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559570},
doi = {10.1145/3551349.3559570},
abstract = {Toxic and unhealthy conversations during the developer’s communication may reduce the professional harmony and productivity of Free and Open Source Software (FOSS) projects. For example, toxic code review comments may raise pushback from an author to complete suggested changes. A toxic communication with another person may hamper future communication and collaboration. Research also suggests that toxicity disproportionately impacts newcomers, women, and other participants from marginalized groups. Therefore, toxicity is a barrier to promote diversity, equity, and inclusion. Since the occurrence of toxic communications is not uncommon among FOSS communities and such communications may have serious repercussions, the primary objective of my proposed dissertation is to automatically identify and mitigate toxicity during developers’ textual interactions. On this goal, I aim to: i) build an automated toxicity detector for Software Engineering (SE) domain, ii) identify the notion of toxicity across demographics, and iii) analyze the impacts of toxicity on the outcomes of Open Source Software (OSS) projects.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {124},
numpages = {5},
keywords = {NLP, deep learning, developers’ interactions, toxicity},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3274694.3274717,
author = {Ramanathan, Sivaramakrishnan and Mirkovic, Jelena and Yu, Minlan and Zhang, Ying},
title = {SENSS Against Volumetric DDoS Attacks},
year = {2018},
isbn = {9781450365697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274694.3274717},
doi = {10.1145/3274694.3274717},
abstract = {Volumetric distributed denial-of-service (DDoS) attacks can bring any network to a halt. Because of their distributed nature and high volume, the victim often cannot handle these attacks alone and needs help from upstream ISPs. Today's Internet has no automated mechanism for victims to ask ISPs for help in attack handling and ISPs themselves do not offer such services. We propose SENSS, a security service for collaborative mitigation of volumetric DDoS attacks. SENSS enables the victim of an attack to request attack monitoring and filtering on demand, and to pay for the services rendered. Requests can be sent both to the immediate and to remote ISPs, in an automated and secure manner, and can be authenticated by these ISPs, without having prior trust with the victim. Simple and generic SENSS APIs enable victims to build custom detection and mitigation approaches against a variety of DDoS attacks. SENSS is deployable with today's infrastructure, and it has strong economic incentives both for ISPs and for the attack victims. It is also very effective in sparse deployment, offering full protection to direct customers of early adopters, and considerable protection to remote victims when deployed strategically. Deployment on the largest 1% of ISPs protects not just direct customers of these ISPs, but everyone on the Internet, from 90% of volumetric DDoS attacks.},
booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
pages = {266–277},
numpages = {12},
keywords = {DDoS defense, IP spoofing, collaborative defense, traffic filtering},
location = {San Juan, PR, USA},
series = {ACSAC '18}
}

@inproceedings{10.1145/3173574.3173731,
author = {Henley, Austin Z. and Mu\c{c}lu, K\i{}van\c{c} and Christakis, Maria and Fleming, Scott D. and Bird, Christian},
title = {CFar: A Tool to Increase Communication, Productivity, and Review Quality in Collaborative Code Reviews},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173731},
doi = {10.1145/3173574.3173731},
abstract = {Collaborative code review has become an integral part of the collaborative design process in the domain of software development. However, there are well-documented challenges and limitations to collaborative code review---for instance, high-quality code reviews may require significant time and effort for the programmers, whereas faster, lower-quality reviews may miss code defects. To address these challenges, we introduce CFar, a novel tool design for extending collaborative code review systems with an automated code reviewer whose feedback is based on program-analysis technologies. To validate this design, we implemented CFar as a production-quality tool and conducted a mixed-method empirical evaluation of the tool usage at Microsoft. Through the field deployment of our tool and a laboratory study of professional programmers using the tool, we produced several key findings showing that CFar enhances communication, productivity, and review quality in human--human collaborative code review.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {code review, collaborative design, programming environments},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/2910896.2910908,
author = {Schwarzer, Malte and Schubotz, Moritz and Meuschke, Norman and Breitinger, Corinna and Markl, Volker and Gipp, Bela},
title = {Evaluating Link-based Recommendations for Wikipedia},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2910908},
doi = {10.1145/2910896.2910908},
abstract = {Literature recommender systems support users in filtering the vast and increasing number of documents in digital libraries and on the Web. For academic literature, research has proven the ability of citation-based document similarity measures, such as Co-Citation (CoCit), or Co-Citation Proximity Analysis (CPA) to improve recommendation quality. In this paper, we report on the first large-scale investigation of the performance of the CPA approach in generating literature recommendations for Wikipedia, which is fundamentally different from the academic literature domain. We analyze links instead of citations to generate article recommendations. We evaluate CPA, CoCit, and the Apache Lucene MoreLikeThis (MLT) function, which represents a traditional text-based similarity measure. We use two datasets of 779,716 and 2.57 million Wikipedia articles, the Big Data processing framework Apache Flink, and a ten-node computing cluster. To enable our large-scale evaluation, we derive two quasi-gold standards from the links in Wikipedia's "See also" sections and a comprehensive Wikipedia clickstream dataset.Our results show that the citation-based measures CPA and CoCit have complementary strengths compared to the text-based MLT measure. While MLT performs well in identifying narrowly similar articles that share similar words and structure, the citation- based measures are better able to identify topically related information, such as information on the city of a certain university or other technical universities in the region. The CPA approach, which consistently outperformed CoCit, is better suited for identifying a broader spectrum of related articles, as well as popular articles that typically exhibit a higher quality. Additional benefits of the CPA approach are its lower runtime requirements and its language-independence that allows for a cross-language retrieval of articles. We present a manual analysis of exemplary articles to demonstrate and discuss our findings. The raw data and source code of our study, together with a manual on how to use them, are openly available at: https://github.com/wikimedia/citolytics},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {191–200},
numpages = {10},
keywords = {big data, citation analysis, co-citation, co-citation proximity analysis, digital libraries, document similarity measures, large-scale evaluations, link-based, literature recommendations},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/3173574.3173785,
author = {DiFranzo, Dominic and Taylor, Samuel Hardman and Kazerooni, Franccesca and Wherry, Olivia D. and Bazarova, Natalya N.},
title = {Upstanding by Design: Bystander Intervention in Cyberbullying},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173785},
doi = {10.1145/3173574.3173785},
abstract = {Although bystander intervention can mitigate the negative effects of cyberbullying, few bystanders ever attempt to intervene. In this study, we explored the effects of interface design on bystander intervention using a simulated custom-made social media platform. Participants took part in a three-day, in-situ experiment, in which they were exposed to several cyberbullying incidents. Depending on the experimental condition, they received different information about the audience size and viewing notifications intended to increase a sense of personal responsibility in bystanders. Results indicated that bystanders were more likely to intervene indirectly than directly, and information about the audience size and viewership increased the likelihood of flagging cyberbullying posts through serial mediation of public surveillance, accountability, and personal responsibility. The study has implications for understanding bystander effect in cyberbullying, and how to develop design solutions to encourage bystander intervention in social media.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {bystander intervention, cyberbullying, social networking sites},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@article{10.1145/3359276,
author = {Chandrasekharan, Eshwar and Gandhi, Chaitrali and Mustelier, Matthew Wortley and Gilbert, Eric},
title = {Crossmod: A Cross-Community Learning-based System to Assist Reddit Moderators},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359276},
doi = {10.1145/3359276},
abstract = {In this paper, we introduce a novel sociotechnical moderation system for Reddit called Crossmod. Through formative interviews with 11 active moderators from 10 different subreddits, we learned about the limitations of currently available automated tools, and how a new system could extend their capabilities. Developed out of these interviews, Crossmod makes its decisions based on cross-community learning---an approach that leverages a large corpus of previous moderator decisions via an ensemble of classifiers. Finally, we deployed Crossmod in a controlled environment, simulating real-time conversations from two large subreddits with over 10M subscribers each. To evaluate Crossmod's moderation recommendations, 4 moderators reviewed comments scored by Crossmod that had been drawn randomly from existing threads. Crossmod achieved an overall accuracy of 86% when detecting comments that would be removed by moderators, with high recall (over 87.5%). Additionally, moderators reported that they would have removed 95.3% of the comments flagged by Crossmod; however, 98.3% of these comments were still online at the time of this writing (i.e., not removed by the current moderation system). To the best of our knowledge, Crossmod is the first open source, AI-backed sociotechnical moderation system to be designed using participatory methods.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {174},
numpages = {30},
keywords = {ai, community norms, machine learning, mixed initiative, moderation, online communities, online governance, open source., participatory design, sociotechnical systems}
}

@article{10.1145/3664809,
author = {Yu, Jinqiang and Fu, Michael and Ignatiev, Alexey and Tantithamthavorn, Chakkrit and Stuckey, Peter},
title = {A Formal Explainer for Just-In-Time Defect Predictions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664809},
doi = {10.1145/3664809},
abstract = {Just-In-Time (JIT) defect prediction has been proposed to help teams to prioritize the limited resources on the most risky commits (or pull requests), yet it remains largely a black-box, whose predictions are not explainable nor actionable to practitioners. Thus, prior studies have applied various model-agnostic techniques to explain the predictions of JIT models. Yet, explanations generated from existing model-agnostic techniques are still not formally sound, robust, and actionable. In this paper, we propose FoX, a Formal eXplainer for JIT Defect Prediction, which builds on formal reasoning about the behaviour of JIT defect prediction models and hence is able to provide provably correct explanations, which are additionally guaranteed to be minimal. Our experimental results show that FoX&nbsp;is able to efficiently generate provably-correct, robust, and actionable explanations while existing model-agnostic techniques cannot. Our survey study with 54 software practitioners provides valuable insights into the usefulness and trustworthiness of our FoX&nbsp;approach. 86% of participants agreed that our approach is useful, while 74% of participants found it trustworthy. Thus, this paper serves as an important stepping stone towards trustable explanations for JIT models to help domain experts and practitioners better understand why a commit is predicted as defective and what to do to mitigate the risk.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may}
}

@inproceedings{10.1145/3159450.3159629,
author = {Galanos, Ria and Ball, Michael and Dougherty, John and Hummel, Joe and Malan, David J.},
title = {Technology We Can't Live Without!, revisited},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159629},
doi = {10.1145/3159450.3159629},
abstract = {The pace of technology for use in computing education is staggering. In recent years, the following technologies have completely transformed our teaching: Piazza, GradeScope, YouTube, Google Docs, Doodle and whenisgood.net, Skype and Google Hangout, and Khan Academy among others. Hardware has also played a part. We love our Zoom digital voice recorder (for recording CD-quality lecture audio), Blue Yeti USB mike (for audio/videoconferences), and iClickers (for engaging students in class). This panel is an outgrowth of a Technology that Educators of Computing Hail (TECH) Birds of a Feather session that we've held at SIGCSE for seven years, and the panel from SIGCSE 2015 [1] that served as a springboard for a regular column in ACM Inroads [2]. It will provide a chance for seasoned high school and university educators to show you the technologies that have "bubbled to the top" for them, and what key problems they solve. Like concert musicians, they will give live demonstrations and reveal the configuration options required to make their technology "sing". We hope this forum will allow the presenters to dive deeply into the common use cases of these technologies, highlight why they are invaluable, share any "gotchas" they've uncovered, and explain how others can adopt them at their institutions. The highlight of the panel is when the audience, inspired by the presentations, is invited to share their favorite "can't live without" technologies as well.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1043–1044},
numpages = {2},
keywords = {computer science education, technology and teaching},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/3583780.3615116,
author = {Lin, ChungYi and Tung, Shen-Lung and Su, Hung-Ting and Hsu, Winston H.},
title = {CTCam: Enhancing Transportation Evaluation through Fusion of Cellular Traffic and Camera-Based Vehicle Flows},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615116},
doi = {10.1145/3583780.3615116},
abstract = {Traffic prediction utility often faces infrastructural limitations, which restrict its coverage. To overcome this challenge, we present Geographical Cellular Traffic (GCT) flow that leverages cellular network data as a new source for transportation evaluation. The broad coverage of cellular networks allows GCT flow to capture various mobile user activities across regions, aiding city authorities in resource management through precise predictions. Acknowledging the complexity arising from the diversity of mobile users in GCT flow, we supplement it with camera-based vehicle flow data from limited deployments and verify their spatio-temporal attributes and correlations through extensive data analysis. Our two-stage fusion approach integrates these multi-source data, addressing their coverage and magnitude discrepancies, thereby enhancing the prediction of GCT flow for accurate transportation evaluation. Overall, we propose novel uses of telecom data in transportation and verify its effectiveness in multi-source fusion with vision-based data.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5341–5345},
numpages = {5},
keywords = {camera-based flow, cellular traffic, multi-source fusion},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3589334.3645493,
author = {Senol, Asuman and Ukani, Alisha and Cutler, Dylan and Bilogrevic, Igor},
title = {The Double Edged Sword: Identifying Authentication Pages and their Fingerprinting Behavior},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645493},
doi = {10.1145/3589334.3645493},
abstract = {Browser fingerprinting is often associated with cross-site user tracking, a practice that many browsers (e.g., Safari, Brave, Edge, Firefox, and Chrome) want to block. However, less is publicly known about its uses to enhance online safety, where it can provide an additional security layer against service abuses (e.g., in combination with CAPTCHAs) or during user authentication. To the best of our knowledge, no fingerprinting defenses deployed thus far consider this important distinction when blocking fingerprinting attempts, so they might negatively affect website functionality and security.  To address this issue we make three main contributions. First, we introduce a novel machine learning-based method to automatically identify authentication pages (i.e. login and sign-up pages). Our supervised algorithm achieves 96-98% precision and recall on a manually-labelled dataset of almost 1,000 popular sites. Second, we compare our algorithm with methods from prior works on the same dataset, showing that it significantly outperforms all of them. Third, we quantify the prevalence of fingerprinting scripts across login and sign-up pages (10.2%) versus those executed on other pages (9.2%); while the rates of fingerprinting are similar, home pages and authentication pages differ in the third-party scripts they include and how often these scripts are labeled as tracking. We also highlight the substantial differences in fingerprinting on login and sign-up pages. Our work sheds light on the complicated reality that fingerprinting is used to both protect user security and invade user privacy; this dual nature must be considered by fingerprinting mitigations.},
booktitle = {Proceedings of the ACM on Web Conference 2024},
pages = {1690–1701},
numpages = {12},
keywords = {browser fingerprinting, online authentication, privacy, machine learning},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3576915.3616591,
author = {Zhang, Yue and Yang, Yuqing and Lin, Zhiqiang},
title = {Don't Leak Your Keys: Understanding, Measuring, and Exploiting the AppSecret Leaks in Mini-Programs},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3616591},
doi = {10.1145/3576915.3616591},
abstract = {Mobile mini-programs in WeChat have gained significant popularity since their debut in 2017, reaching a scale similar to that of Android apps in the Play Store. Like Google, Tencent, the provider of WeChat, offers APIs to support the development of mini-programs and also maintains a mini-program market within the WeChat app. However, mini-program APIs often manage sensitive user data within the social network platform, both on the WeChat client app and in the cloud. As a result, cryptographic protocols have been implemented to secure data access. In this paper, we demonstrate that WeChat should have required the use of the "appsecret" master key, which is used to authenticate a mini-program, to be used only in the mini-program back-end. If this key is leaked in the front-end of the mini-programs, it can lead to catastrophic attacks on both mini-program developers and users. Using a mini-program crawler and a master key leakage inspector, we measured 3,450,586 crawled mini-programs and found that 40,880 of them had leaked their master keys, allowing attackers to carry out various attacks such as account hijacking, promotion abuse, and service theft. Similar issues were confirmed through testing and measuring of Baidu mini-programs too. We have reported these vulnerabilities and the list of vulnerable mini-programs to Tencent and Baidu, which awarded us with bug bounties, and also Tencent recently released a new API to defend against these attacks based on our findings.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2411–2425},
numpages = {15},
keywords = {credentials leakage, miniprogram security, mobile security, mobile super apps},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/3603216.3624965,
author = {Senol, Asuman and Acar, Gunes},
title = {Unveiling the Impact of User-Agent Reduction and Client Hints: A Measurement Study},
year = {2023},
isbn = {9798400702358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603216.3624965},
doi = {10.1145/3603216.3624965},
abstract = {The user-agent string contains the details of a user's device, browser and platform. Prior work on browser fingerprinting showed that the user-agent string can facilitate covert fingerprinting and tracking of users. In order to address these privacy concerns, browsers including Chrome recently reduced the user-agent string to make it less identifying. Simultaneously, Chrome introduced several highly identifying (or high-entropy) user-agent client hints (UA-CH) to allow access to browser properties that are redacted from the user-agent string. In this empirical study, we attempt to characterize the effects of these major changes through a large-scale web measurement on the top 100K websites. Using an instrumented crawler, we quantify access to high-entropy browser features through UA-CH HTTP headers and the JavaScript API. We measure access delegation to third parties and investigate whether the new client hints are already used by tracking, advertising and browser fingerprinting scripts. Our results show that high-entropy UA-CHs are accessed by one or more scripts on 59.2% of the successfully visited sites and 93.8% of these calls were made by tracking and advertising-related scripts-primarily by those owned by Google. Overall, we find that scripts from -9K distinct registrable (eTLD+1) third-party domains take advantage of their unfettered access and retrieve the high-entropy UA-CHs. We find that on 91.6% of the sites where high-entropy client hints are accessed via the JavaScript API, the high-entropy hints are exfiltrated by a tracker script to a remote server. Turning to high-entropy UA-CHs sent in the HTTP headers-which require opt-in or delegation-we found very limited use. Only 1.3% of the websites use the Accept-CH header to receive high-entropy UA-CHs; and an even smaller fraction of websites (0.4%) delegate high-entropy hints to third-party domains. Overall, our findings indicate that user-agent reduction efforts were effective in minimizing the passive collection of identifying browser features, but third-party tracking and advertising scripts continue to enjoy their unfettered access.},
booktitle = {Proceedings of the 22nd Workshop on Privacy in the Electronic Society},
pages = {91–106},
numpages = {16},
keywords = {client hints, fingerprinting, online tracking, user-agent string, web privacy},
location = {Copenhagen, Denmark},
series = {WPES '23}
}

@inproceedings{10.1145/3131365.3131391,
author = {DeBlasio, Joe and Savage, Stefan and Voelker, Geoffrey M. and Snoeren, Alex C.},
title = {Tripwire: inferring internet site compromise},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131391},
doi = {10.1145/3131365.3131391},
abstract = {Password reuse has been long understood as a problem: credentials stolen from one site may be leveraged to gain access to another site for which they share a password. Indeed, it is broadly understood that attackers exploit this fact and routinely leverage credentials extracted from a site they have breached to access high-value accounts at other sites (e.g., email accounts). However, as a consequence of such acts, this same phenomena of password reuse attacks can be harnessed to indirectly infer site compromises---even those that would otherwise be unknown. In this paper we describe such a measurement technique, in which unique honey accounts are registered with individual third-party websites, and thus access to an email account provides indirect evidence of credentials theft at the corresponding website. We describe a prototype system, called Tripwire, that implements this technique using an automated Web account registration system combined with email account access data from a major email provider. In a pilot study monitoring more than 2,300 sites over a year, we have detected 19 site compromises, including what appears to be a plaintext password compromise at an Alexa top-500 site with more than 45 million active users.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {341–354},
numpages = {14},
keywords = {cybercrime, password reuse, webmail, website compromise},
location = {London, United Kingdom},
series = {IMC '17}
}

@article{10.1145/3338112,
author = {Distefano, Dino and F\"{a}hndrich, Manuel and Logozzo, Francesco and O'Hearn, Peter W.},
title = {Scaling static analyses at Facebook},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3338112},
doi = {10.1145/3338112},
abstract = {Key lessons for designing static analyses tools deployed to find bugs in hundreds of millions of lines of code.},
journal = {Commun. ACM},
month = {jul},
pages = {62–70},
numpages = {9}
}

@inproceedings{10.1145/3469595.3469632,
author = {Weber, Irene},
title = {Low-code from frontend to backend: Connecting conversational user interfaces to backend services via a low-code IoT platform},
year = {2021},
isbn = {9781450389983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469595.3469632},
doi = {10.1145/3469595.3469632},
abstract = {Current chatbot development platforms and frameworks facilitate setting up the language and dialog part of chatbots, while connecting it to backend services and business functions requires substantial manual coding effort and programming skills. This paper proposes an approach to overcome this situation. It proposes an architecture with a chatbot as frontend using an IoT (Internet of Things) platform as a middleware for connections to backend services. Specifically, it elaborates and demonstrates how to combine a chatbot developed on the open source development platform Rasa with the open source platform Node-RED, allowing low-code or no-code development of a transactional conversational user interface from frontend to backend.},
booktitle = {Proceedings of the 3rd Conference on Conversational User Interfaces},
articleno = {37},
numpages = {5},
keywords = {API, IoT, Node-RED, Open Source, Rasa chatbot, conversational user interfaces, end-user programming, integration pattern, low-code, system architecture},
location = {Bilbao (online), Spain},
series = {CUI '21}
}

@proceedings{10.1145/3644032,
title = {AST '24: Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {AST continues to be a venue for researchers and practitioners where they can discuss high quality research contributions on methods for software test automation, and various case studies reporting practices in this field. Indeed, software test automation is a discipline that has produced noteworthy research in the last decade.The special theme of AST 2024 is "Test automation for and with Generative AI". This innovative and promising research direction deals with the application of test automation technologies to the testing of Generative AI applications, as well as the adoption of generative AI to facilitate test automation.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2594291.2594312,
author = {Kuper, Lindsey and Todd, Aaron and Tobin-Hochstadt, Sam and Newton, Ryan R.},
title = {Taming the parallel effect zoo: extensible deterministic parallelism with LVish},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594312},
doi = {10.1145/2594291.2594312},
abstract = {A fundamental challenge of parallel programming is to ensure that the observable outcome of a program remains deterministic in spite of parallel execution. Language-level enforcement of determinism is possible, but existing deterministic-by-construction parallel programming models tend to lack features that would make them applicable to a broad range of problems. Moreover, they lack extensibility: it is difficult to add or change language features without breaking the determinism guarantee.The recently proposed LVars programming model, and the accompanying LVish Haskell library, took a step toward broadly-applicable guaranteed-deterministic parallel programming. The LVars model allows communication through shared monotonic data structures to which information can only be added, never removed, and for which the order in which information is added is not observable. LVish provides a Par monad for parallel computation that encapsulates determinism-preserving effects while allowing a more flexible form of communication between parallel tasks than previous guaranteed-deterministic models provided.While applying LVar-based programming to real problems using LVish, we have identified and implemented three capabilities that extend its reach: inflationary updates other than least-upper-bound writes; transitive task cancellation; and parallel mutation of non-overlapping memory locations. The unifying abstraction we use to add these capabilities to LVish---without suffering added complexity or cost in the core LVish implementation, or compromising determinism---is a form of monad transformer, extended to handle the Par monad. With our extensions, LVish provides the most broadly applicable guaranteed-deterministic parallel programming interface available to date. We demonstrate the viability of our approach both with traditional parallel benchmarks and with results from a real-world case study: a bioinformatics application that we parallelized using our extended version of LVish.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {2–14},
numpages = {13},
keywords = {deterministic parallelism},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@inproceedings{10.1145/3555228.3555244,
author = {Souza, Hugo Henrique Fumero de and Wiese, Igor and Steinmacher, Igor and R\'{e}, Reginaldo},
title = {A characterization study of testing contributors and their contributions in open source projects.},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555228.3555244},
doi = {10.1145/3555228.3555244},
abstract = {Even though open source projects have some different characteristics from projects in the industry, the commitment of maintainers and contributors to achieve a high level of software quality is constant. Therefore, tests are among the main practices of the communities. Thus, motivating contributors to write new tests and maintain regression tests during testing activities is essential for the project’s health. The objective of our work is to characterize testers and their contributions to open source projects as part of a broad study about testers’ motivation. Thus, we conducted a study with 3,936 repositories and 7 different and important programming languages (C, C++, C#, Java, Javascript, Python, and Ruby), analyzing a total of 4,409,142 contributions to classify contributing members and their contributions. Our results show that test-only contributors exist, regardless of programming language or project. We conclude that, despite the unfavorable scenario, there are contributors who feel motivated and dedicate their time and effort to contribute to new tests or to the evolution of existing tests.},
booktitle = {Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
pages = {95–105},
numpages = {11},
location = {Virtual Event, Brazil},
series = {SBES '22}
}

@inproceedings{10.1145/3581641.3584037,
author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
title = {The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584037},
doi = {10.1145/3581641.3584037},
abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {491–514},
numpages = {24},
keywords = {code-fluent large language models, conversational interaction, foundation models, human-centered AI},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3629606.3629620,
author = {Liu, Yunxing and Lee, Minha and Yang, Bin and Martens, Jean-Bernard},
title = {Low Code Conversation-based Hybrid UI Design Case Study and Reflection},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629620},
doi = {10.1145/3629606.3629620},
abstract = {This paper presents a comprehensive case study on the Q-Survey, a chatbot-based qualitative survey tool developed using the Double-Diamond design process. The study delves into the intricacies of balancing user experience (UX) with technical challenges inherent in chatbot development. A significant focus is placed on the role of Low Code (LC) and No Code (NC) tools in facilitating rapid prototype development and testing. While these tools offer agility and ease in the early stages, their limitations become evident as the complexity of the system grows, prompting a reflection on their continued utility in advanced development stages. The Repertory Grid Technique (RGT) is explored as a potential tool for online qualitative surveys, with discussions on its complexity and the potential enhancements using advanced Natural Language Processing (NLP) tools. Through the lens of the Q-Survey and experts’ evaluation of this case, we discuss the broader implications of LC tools in Human-Computer Interaction (HCI) design, emphasizing the need for a structured framework for HCI design with LC. The study concludes with reflections on the current design, potential future directions, and the importance of continuous exploration of LC, especially in the realm of LLMs and coding tools based on LLMs.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {139–145},
numpages = {7},
keywords = {CUI, Chatbot, Low Code Development, Repertory Grid Tool.},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@inproceedings{10.1145/3543873.3587349,
author = {Giakatos, Dimitrios Panteleimon and Sermpezis, Pavlos and Vakali, Athena},
title = {PyPoll: A python library automating mining of networks, discussions and polarization on Twitter},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587349},
doi = {10.1145/3543873.3587349},
abstract = {Today online social networks have a high impact in our society as more and more people use them for communicating with each other, express their opinions, participating in public discussions, etc. In particular, Twitter is one of the most popular social network platforms people mainly use for political discussions. This attracted the interest of many research studies that analyzed social phenomena on Twitter, by collecting data, analysing communication patterns, and exploring the structure of user networks. While previous works share many common methodologies for data collection and analysis, these are mainly re-implemented every time by researchers in a custom way. In this paper, we introduce PyPoll an open-source Python library that operationalizes common analysis tasks for Twitter discussions. With PyPoll users can perform Twitter graph mining, calculate the polarization index and generate interactive visualizations without needing third-party tools. We believe that PyPoll can help researchers automate their tasks by giving them methods that are easy to use. Also, we demonstrate the use of the library by presenting two use cases; the PyPoll visualization app, an online application for graph visualizing and sharing, and the Political Lighthouse, a Web portal for displaying the polarization in various political topics on Twitter.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {208–211},
numpages = {4},
keywords = {Online social networks, graph mining, open-source, polarization},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.5555/3237383.3238045,
author = {Koeman, Vincent J. and Griffioen, Harm J. and Plenge, Danny C. and Hindriks, Koen V.},
title = {StarCraft as a Testbed for Engineering Complex Distributed Systems Using Cognitive Agent Technology},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {It has been argued that the evaluation of cognitive agent systems requires richer benchmark problems. We think that real-time strategy (RTS) games can offer such a testbed, as AI for RTS requires the design of complicated strategies for coordinating hundreds of units that need to solve a range of challenges. Therefore, in this paper, we report on the design and development of the first multi-agent connector that provides full access to StarCraft (Brood War). We provide a new interface that is dedicated to a multi-agent approach by connecting each unit in the game to a cognitive agent. Two main challenges are addressed in this work. First, we decide on the right level of abstraction for unit control by means of agents, designing for instance the percepts that are available to units. Second, a sufficient level of performance needs to be ensured in order to allow a large variety of multi-agent implementations to be successful at tackling challenges of RTS AI. The resulting open-source connector readily supports the hundreds of agents that can come and go during the game. Based on the development of the connector and its initial use by over 200 students, we gained valuable insights.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1983–1985},
numpages = {3},
keywords = {cognitive agents, real-time strategy games},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3368089.3409681,
author = {Huang, Yu and Leach, Kevin and Sharafi, Zohreh and McKay, Nicholas and Santander, Tyler and Weimer, Westley},
title = {Biases and differences in code review using medical imaging and eye-tracking: genders, humans, and machines},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409681},
doi = {10.1145/3368089.3409681},
abstract = {Code review is a critical step in modern software quality assurance, yet it is vulnerable to human biases. Previous studies have clarified the extent of the problem, particularly regarding biases against the authors of code,but no consensus understanding has emerged. Advances in medical imaging are increasingly applied to software engineering, supporting grounded neurobiological explorations of computing activities, including the review, reading, and writing of source code. In this paper, we present the results of a controlled experiment using both medical imaging and also eye tracking to investigate the neurological correlates of biases and differences between genders of humans and machines (e.g., automated program repair tools) in code review. We find that men and women conduct code reviews differently, in ways that are measurable and supported by behavioral, eye-tracking and medical imaging data. We also find biases in how humans review code as a function of its apparent author, when controlling for code quality. In addition to advancing our fundamental understanding of how cognitive biases relate to the code review process, the results may inform subsequent training and tool design to reduce bias.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {456–468},
numpages = {13},
keywords = {automation, code review, eye-tracking, fMRI, gender},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3475960.3475985,
author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
title = {CVEfixes: automated collection of vulnerabilities and their fixes from open-source software},
year = {2021},
isbn = {9781450386807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475960.3475985},
doi = {10.1145/3475960.3475985},
abstract = {Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.},
booktitle = {Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {30–39},
numpages = {10},
keywords = {Security vulnerabilities, dataset, software repository mining, source code repair, vulnerability classification, vulnerability prediction},
location = {Athens, Greece},
series = {PROMISE 2021}
}

@inproceedings{10.1145/3328833.3328873,
author = {Awad, Ahmed and Nagaty, Khaled},
title = {Commit Message Generation from Code Differences using Hidden Markov Models},
year = {2019},
isbn = {9781450361057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328833.3328873},
doi = {10.1145/3328833.3328873},
abstract = {Commit messages are developer-written messages that document code changes. Such change might be adding features, fixing bugs or simply code updates. Although these messages help in understanding the evolution of any software, it is quite often that developers disregard the process of writing these messages, when making a change. Many automated methods have been proposed to generate commit messages. Due to the inability of those techniques to represent higher order understanding of code changes, the quality of these messages in terms of logic and context representation is very low as opposed to developer written messages. To solve this problem, previous work used deep learning models -specifically, sequence-to-sequence models- were used to automate that task. This model delivered promising results on translating code differences to commit messages. However, after the model's performance was thoroughly investigated in previous work. It was found out that code differences corresponding to almost every high quality commit messages generated by the model were very similar to one or more training sample code differences on a token level. Motivated by that observation, a k-nearest neighbor algorithm that outputs the same exact message of the nearest code difference was proposed in previous work. Inspired by the traditional solution to sequence modeling; Hidden Markov Models, we show that HMMs outperforms sequence-to-sequence models without outputting the same exact message of the nearest code diff, our experiments show an enhancement of 4% against sequence to sequence models.},
booktitle = {Proceedings of the 8th International Conference on Software and Information Engineering},
pages = {96–99},
numpages = {4},
keywords = {Commit Message Generation, Hidden Markov Models, Neural machine translation},
location = {Cairo, Egypt},
series = {ICSIE '19}
}

@inproceedings{10.1145/3338906.3338965,
author = {Nie, Pengyu and Rai, Rishabh and Li, Junyi Jessy and Khurshid, Sarfraz and Mooney, Raymond J. and Gligoric, Milos},
title = {A framework for writing trigger-action todo comments in executable format},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338965},
doi = {10.1145/3338906.3338965},
abstract = {Natural language elements, e.g., todo comments, are frequently used to communicate among developers and to describe tasks that need to be performed (actions) when specific conditions hold on artifacts related to the code repository (triggers), e.g., from the Apache Struts project: “remove expectedJDK15 and if() after switching to Java 1.6”. As projects evolve, development processes change, and development teams reorganize, these comments, because of their informal nature, frequently become irrelevant or forgotten. We present the first framework, dubbed TrigIt, to specify trigger-action todo comments in executable format. Thus, actions are executed automatically when triggers evaluate to true. TrigIt specifications are written in the host language (e.g., Java) and are evaluated as part of the build process. The triggers are specified as query statements over abstract syntax trees, abstract representation of build configuration scripts, issue tracking systems, and system clock time. The actions are either notifications to developers or code transformation steps. We implemented TrigIt for the Java programming language and migrated 44 existing trigger-action comments from several popular open-source projects. Evaluation of TrigIt, via a user study, showed that users find TrigIt easy to learn and use. TrigIt has the potential to enforce more discipline in writing and maintaining comments in large code repositories.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {385–396},
numpages = {12},
keywords = {Todo comments, domain specific languages, trigger-action},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00039,
author = {Marginean, A. and Bader, J. and Chandra, S. and Harman, M. and Jia, Y. and Mao, K. and Mols, A. and Scott, A.},
title = {SapFix: automated end-to-end repair at scale},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00039},
doi = {10.1109/ICSE-SEIP.2019.00039},
abstract = {We report our experience with SAPFIX: the first deployment of automated end-to-end fault fixing, from test case design through to deployed repairs in production code1. We have used SAPFIX at Facebook to repair 6 production systems, each consisting of tens of millions of lines of code, and which are collectively used by hundreds of millions of people worldwide.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {269–278},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/3379597.3387500,
author = {Fry, Tanner and Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
title = {A Dataset and an Approach for Identity Resolution of 38 Million Author IDs extracted from 2B Git Commits},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387500},
doi = {10.1145/3379597.3387500},
abstract = {The data collected from open source projects provide means to model large software ecosystems, but often suffer from data quality issues, specifically, multiple author identification strings in code commits might actually be associated with one developer. While many methods have been proposed for addressing this problem, they are either heuristics requiring manual tweaking, or require too much calculation time to do pairwise comparisons for 38M author IDs in, for example, the World of Code collection. In this paper, we propose a method that finds all author IDs belonging to a single developer in this entire dataset, and share the list of all author IDs that were found to have aliases. To do this, we first create blocks of potentially connected author IDs and then use a machine learning model to predict which of these potentially related IDs belong to the same developer. We processed around 38 million author IDs and found around 14.8 million IDs to have an alias, which belong to 5.4 million different developers, with the median number of aliases being 2 per developer. This dataset can be used to create more accurate models of developer behaviour at the entire OSS ecosystem level and can be used to provide a service to rapidly resolve new author IDs.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {518–522},
numpages = {5},
keywords = {Data Sharing, Git Commits, Heuristics, Identity Resolution, Machine Learning},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/2577080.2582208,
author = {Souza, Carlos and Figueiredo, Eduardo},
title = {How do programmers use optional typing? an empirical study},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2582208},
doi = {10.1145/2577080.2582208},
abstract = {The recent popularization of dynamically typed languages, such as Ruby and JavaScript, has brought more attention to the discussion about the impact of typing strategies on development. Types allow the compiler to find type errors earlier and potentially improve the readability and maintainability of code. On the other hand, "untyped" code may be easier to change and require less work from programmers. This paper tries to identify the programmers' point of view about these tradeoffs. An analysis of the source code of 6638 projects written in Groovy, a programming language which features optional typing, shows in which scenarios programmers prefer to type or not to type their declarations. Our results show that types are popular in the definition of module interfaces, but are less used in scripts, test classes and frequently changed code. There is no correlation between the size and age of projects and how their constructs are typed. Finally, we also found evidence that the background of programmers influences how they use types.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {109–120},
numpages = {12},
keywords = {groovy, static analysis, type systems},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/3238147.3238169,
author = {Kovalenko, Vladimir and Palomba, Fabio and Bacchelli, Alberto},
title = {Mining file histories: should we consider branches?},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238169},
doi = {10.1145/3238147.3238169},
abstract = {Modern distributed version control systems, such as Git, offer support for branching — the possibility to develop parts of software outside the master trunk. Consideration of the repository structure in Mining Software Repository (MSR) studies requires a thorough approach to mining, but there is no well-documented, widespread methodology regarding the handling of merge commits and branches. Moreover, there is still a lack of knowledge of the extent to which considering branches during MSR studies impacts the results of the studies. In this study, we set out to evaluate the importance of proper handling of branches when calculating file modification histories. We analyze over 1,400 Git repositories of four open source ecosystems and compute modification histories for over two million files, using two different algorithms. One algorithm only follows the first parent of each commit when traversing the repository, the other returns the full modification history of a file across all branches. We show that the two algorithms consistently deliver different results, but the scale of the difference varies across projects and ecosystems. Further, we evaluate the importance of accurate mining of file histories by comparing the performance of common techniques that rely on file modification history — reviewer recommendation, change recommendation, and defect prediction — for two algorithms of file history retrieval. We find that considering full file histories leads to an increase in the techniques’ performance that is rather modest.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {202–213},
numpages = {12},
keywords = {Branches, Mining Software Repositories, Version Control Systems},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/3417564.3417576,
author = {Tell, Paolo and Raffo, David and Huang, Liguo and Steinmacher, Igor and Britto, Ricardo and T\"{u}z\"{u}n, Eray and Clarke, Paul},
title = {Summary of the 1st ICSSP-ICGSE Joint Event},
year = {2021},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3417564.3417576},
doi = {10.1145/3417564.3417576},
abstract = {Having the common objective of bringing together researchers and industry practitioners to share their research findings, experiences, and new ideas as well as sharing topics of interest, the organizing committees of the 14th International Conference on Software and System Processes (ICSSP) and the 15th International Conference on Global Software Engineering (ICGSE) ceased the opportunity to explore the idea of bringing together the two communities once it was clear that the International Conference on Software Engineering and all its co-located events had to be redesigned as online events.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {31–34},
numpages = {4}
}

@inproceedings{10.1145/3486949.3486965,
author = {ElBatanony, Ahmed and Succi, Giancarlo},
title = {Towards the no-code era: a vision and plan for the future of software development},
year = {2021},
isbn = {9781450391252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486949.3486965},
doi = {10.1145/3486949.3486965},
abstract = {This paper provides a highly opinionated and biased vision and a two-stage plan with guidelines to reach a new era of software development, where anyone can create software without bothering to write code. Moreover, this paper explores in depth the first of these stages, which consists of creating a no-code tool based on six principles: configuration driven development, APIs, open-source, cross-platform, cloud computing, and design systems. An examination of each principle is presented and a case is made for why such a combination of principles would lay the foundation for future development efforts. Possible enquiries are addressed and a path is laid out for future works.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Beyond Code: No Code},
pages = {29–35},
numpages = {7},
keywords = {API, Cloud Computing, Configuration Driven Development, Cross-platform, Design Systems, No-code, Open-source},
location = {Chicago, IL, USA},
series = {BCNC 2021}
}

@inproceedings{10.1145/3477495.3531725,
author = {Lin, Jimmy and Campos, Daniel and Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine},
title = {Fostering Coopetition While Plugging Leaks: The Design and Implementation of the MS MARCO Leaderboards},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531725},
doi = {10.1145/3477495.3531725},
abstract = {We articulate the design and implementation of the MS MARCO document ranking and passage ranking leaderboards. In contrast to "standard" community-wide evaluations such as those at TREC, which can be characterized as simultaneous games, leaderboards represent sequential games, where every player move is immediately visible to the entire community. The fundamental challenge with this setup is that every leaderboard submission leaks information about the held-out evaluation set, which conflicts with the fundamental tenant in machine learning about separation of training and test data. These "leaks", accumulated over long periods of time, threaten the validity of the insights that can be derived from the leaderboards. In this paper, we share our experiences grappling with this issue over the past few years and how our considerations are operationalized into a coherent submission policy. Our work provides a useful guide to help the community understand the design choices made in the popular MS MARCO leaderboards and offers lessons for designers of future leaderboards.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2939–2948},
numpages = {10},
keywords = {community-wide evaluations, datasets, neural models},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.5555/1894483.1894524,
author = {Urban, Josef and Alama, Jesse and Rudnicki, Piotr and Geuvers, Herman},
title = {A wiki for Mizar: motivation, considerations, and initial prototype},
year = {2010},
isbn = {3642141277},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Formal mathematics has so far not taken full advantage of ideas from collaborative tools such as wikis and distributed version control systems (DVCS). We argue that the field could profit from such tools, serving both newcomers and experts alike. We describe a preliminary system for such collaborative development based on the Git DVCS. We focus, initially, on the Mizar system and its library of formalized mathematics.},
booktitle = {Proceedings of the 10th ASIC and 9th MKM International Conference, and 17th Calculemus Conference on Intelligent Computer Mathematics},
pages = {455–469},
numpages = {15},
location = {Paris, France},
series = {AISC'10/MKM'10/Calculemus'10}
}

@inproceedings{10.1145/3459637.3482370,
author = {Gorovits, Alexander and Zhang, Lin and Gujral, Ekta and Papalexakis, Evangelos and Bogdanov, Petko},
title = {Mining Bursty Groups from Interaction Data},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482370},
doi = {10.1145/3459637.3482370},
abstract = {Empirical studies and theoretical models both highlight burstinessas a common temporal pattern in online behavior. A key driver for burstiness is the self-exciting nature of online interactions. For example, posts in online groups often incite posts in response. Such temporal dependencies are easily lost when interaction data is aggregated in snapshots which are subsequently analyzed independently. An alternative is to model individual interactions as a multi-dimensional self-exciting process, thus, enforcing both temporal and network dependencies. Point processes, however, are challenging to employ for large real-world datasets as fitting them incurs super-linear cost in the number of events. How can we efficiently detect online groups exhibiting bursty self-exciting temporal behavior in large real-world datasets?  We propose a bursty group detection framework, called MYRON, which explicitly models self-exciting behavior within groups while also accounting for network-wide baseline activity. MYRON imposes bursty temporal structure within a scalable tensor factorization framework to decouple within-group interactions as interpretable factors. Our framework can incorporate different "shapes"of temporal burstiness via wavelet decomposition or kernels forself-exciting behavior. Our evaluation on both synthetic and real-world data demonstrates MYRON's utility in community detection.It is up to 40% more effective in detecting ground truth groups compared to state-of-the-art baselines. In addition, MYRON is able to uncover interpretable bursty patterns of behavior from user-photo interactions in Flickr.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {596–605},
numpages = {10},
keywords = {community detection, dynamic networks, hawkes process, self-exciting processes, temporal graphs, tensor factorization},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3458817.3476206,
author = {Georganas, Evangelos and Kalamkar, Dhiraj and Avancha, Sasikanth and Adelman, Menachem and Anderson, Cristina and Breuer, Alexander and Bruestle, Jeremy and Chaudhary, Narendra and Kundu, Abhisek and Kutnick, Denise and Laub, Frank and Md, Vasimuddin and Misra, Sanchit and Mohanty, Ramanarayan and Pabst, Hans and Ziv, Barukh and Heinecke, Alexander},
title = {Tensor processing primitives: a programming abstraction for efficiency and portability in deep learning workloads},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476206},
doi = {10.1145/3458817.3476206},
abstract = {During the past decade, novel Deep Learning (DL) algorithms/workloads and hardware have been developed to tackle a wide range of problems. Despite the advances in workload/hardware ecosystems, the programming methodology of DL systems is stagnant. DL workloads leverage either highly-optimized, yet platform-specific and inflexible kernels from DL libraries, or in the case of novel operators, reference implementations are built via DL framework primitives with underwhelming performance. This work introduces the Tensor Processing Primitives (TPP), a programming abstraction striving for efficient, portable implementation of DL workloads with high-productivity. TPPs define a compact, yet versatile set of 2D-tensor operators (or a virtual Tensor ISA), which subsequently can be utilized as building-blocks to construct complex operators on high-dimensional tensors. The TPP specification is platform-agnostic, thus code expressed via TPPs is portable, whereas the TPP implementation is highly-optimized and platform-specific. We demonstrate the efficacy of our approach using standalone kernels and end-to-end DL workloads expressed entirely via TPPs that outperform state-of-the-art implementations on multiple platforms.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {14},
numpages = {14},
location = {St. Louis, Missouri},
series = {SC '21}
}

@article{10.1177/26339137241231912,
author = {Schueller, William and Wachs, Johannes},
title = {Modeling interconnected social and technical risks in open source software ecosystems},
year = {2024},
issue_date = {January-March 2024},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1177/26339137241231912},
doi = {10.1177/26339137241231912},
abstract = {Open source software ecosystems consist of thousands of interdependent libraries, which users can combine to great effect. Recent work has pointed out two kinds of risks in these systems: that technical problems like bugs and vulnerabilities can spread through dependency links, and that relatively few developers are responsible for maintaining even the most widely used libraries. However, a more holistic diagnosis of systemic risk in software ecosystem should consider how these social and technical sources of risk interact and amplify one another. Motivated by the observation that the same individuals maintain several libraries within dependency networks, we present a methodological framework to measure risk in software ecosystems as a function of both dependencies and developers. In our models, a library’s chance of failure increases as its developers leave and as its upstream dependencies fail. We apply our method to data from the Rust ecosystem, highlighting several systemically important libraries that are overlooked when only considering technical dependencies. We compare potential interventions, seeking better ways to deploy limited developer resources with a view to improving overall ecosystem health and software supply chain resilience.},
journal = {Collective Intelligence},
month = {feb},
numpages = {16},
keywords = {Open source software, decentralized collaboration, systemic risk, networks, social-technical systems}
}

@inproceedings{10.1145/3379597.3387472,
author = {Abdellatif, Ahmad and Costa, Diego and Badran, Khaled and Abdalkareem, Rabe and Shihab, Emad},
title = {Challenges in Chatbot Development: A Study of Stack Overflow Posts},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387472},
doi = {10.1145/3379597.3387472},
abstract = {Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development.Therefore, in this paper, we examine the Q&amp;A website, Stack Overflow, to provide insights on the topics that chatbot developers are interested and the challenges they face. In particular, we leverage topic modeling to understand the topics that are being discussed by chatbot developers on Stack Overflow. Then, we examine the popularity and difficulty of those topics. Our results show that most of the chatbot developers are using Stack Overflow to ask about implementation guidelines. We determine 12 topics that developers discuss (e.g., Model Training) that fall into five main categories. Most of the posts belong to chatbot development, integration, and the natural language understanding (NLU) model categories. On the other hand, we find that developers consider the posts of building and integrating chatbots topics more helpful compared to other topics. Specifically, developers face challenges in the training of the chatbot's model. We believe that our study guides future research to propose techniques and tools to help the community at its early stages to overcome the most popular and difficult topics that practitioners face when developing chatbots.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {174–185},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@proceedings{10.1145/3576882,
title = {CompEd 2023: Proceedings of the ACM Conference on Global Computing Education Vol 1},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome participants to the 2nd ACM Global Conference on Computing Education (ACM CompEd 2023) being held in Hyderabad, India, 7th-9th December, 2023 with the Working Groups meetings being held on 5th and 6th December 2023.ACM CompEd is a recent addition to the list of ACM sponsored conferences devoted to research in all aspects of computing education, including education at the school and college levels. The Hyderabad edition is only the second in this promising series. The long hiatus due to Covid-19 pushed this conference by two years, but we are glad that it is finally here!This edition of ACM CompEd partly overlaps with COMPUTE 2023, ACM India's flagship conference on Computing Education. Having the two conferences adjacent to each other is a great way to build synergy between the Indian computing education community and the global community of computing education researchers.},
location = {Hyderabad, India}
}

@article{10.1145/3377869,
author = {Burton, Ren\'{e}e},
title = {Unsupervised Learning Techniques for Malware Characterization: Understanding Certain DNS-based DDoS Attacks},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3377869},
doi = {10.1145/3377869},
abstract = {This article details data science research in the area of Cyber Threat Intelligence applied to a specific type of Distributed Denial of Service (DDoS) attack. We study a DDoS technique prevalent in the Domain Name System (DNS) for which little malware have been recovered. Using data from a globally distributed set of a passive collectors (pDNS), we create a statistical classifier to identify these attacks and then use unsupervised learning to investigate the attack events and the malware that generates them. The first known major study of this technique, this work demonstrates that current attacks have little resemblance to earlier published descriptions and identifies several features of the attacks. Through a combination of text and time-series features, we are able to characterize the dominant malware and demonstrate that the number of global-scale attack systems is relatively small.},
journal = {Digital Threats},
month = {aug},
articleno = {14},
numpages = {26},
keywords = {Domain name service (DNS), botnet, clustering, data science, ddos attacks, malware, threat intelligence}
}

@inproceedings{10.1145/3543507.3583333,
author = {Su, Junhua and Kapravelos, Alexandros},
title = {Automatic Discovery of Emerging Browser Fingerprinting Techniques},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583333},
doi = {10.1145/3543507.3583333},
abstract = {With the progression of modern browsers, online tracking has become the most concerning issue for preserving privacy on the web. As major browser vendors plan to or already ban third-party cookies, trackers have to shift towards browser fingerprinting by incorporating novel browser APIs into their tracking arsenal. Understanding how new browser APIs are abused in browser fingerprinting techniques is a significant step toward ensuring protection from online tracking. In this paper, we propose a novel hybrid system, named BFAD, that automatically identifies previously unknown browser fingerprinting APIs in the wild. The system combines dynamic and static analysis to accurately reveal browser API usage and automatically infer browser fingerprinting behavior. Based on the observation that a browser fingerprint is constructed by pulling information from multiple APIs, we leverage dynamic analysis and a locality-based algorithm to discover all involved APIs and static analysis on the dataflow of fingerprinting information to accurately associate them together. Our system discovers 231 fingerprinting APIs in Alexa top 10K domains, starting with only 35 commonly known fingerprinting APIs and 17 data transmission APIs. Out of 231 APIs, 161 of them are not identified by state-of-the-art detection systems. Since our approach is fully automated, we repeat our experiments 11 months later and discover 18 new fingerprinting APIs that were not discovered in our previous experiment. We present with case studies the fingerprinting ability of a total of 249 detected APIs.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2178–2188},
numpages = {11},
keywords = {Browser Fingerprinting, Online Tracking, Privacy, Program Analysis, Web Measurement},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3583780.3614729,
author = {Cho, Beomsang and Le, Binh M. and Kim, Jiwon and Woo, Simon and Tariq, Shahroz and Abuadbba, Alsharif and Moore, Kristen},
title = {Towards Understanding of Deepfake Videos in the Wild},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614729},
doi = {10.1145/3583780.3614729},
abstract = {Abstract: Deepfakes have become a growing concern in recent years, prompting researchers to develop benchmark datasets and detection algorithms to tackle the issue. However, existing datasets suffer from significant drawbacks that hamper their effectiveness. Notably, these datasets fail to encompass the latest deepfake videos produced by state-of-the-art methods that are being shared across various platforms. This limitation impedes the ability to keep pace with the rapid evolution of generative AI techniques employed in real-world deepfake production. Our contributions in this IRB-approved study are to bridge this knowledge gap from current real-world deepfakes by providing in-depth analysis. We first present the largest and most diverse and recent deepfake dataset, RWDF-23, collected from the wild to date, consisting of 2,000 deepfake videos collected from 4 platforms targeting 4 different languages span created from 21 countries: Reddit, YouTube, TikTok, and Bilibili. By expanding the dataset's scope beyond the previous research, we capture a broader range of real-world deepfake content, reflecting the ever-evolving landscape of online platforms. Also, we conduct a comprehensive analysis encompassing various aspects of deepfakes, including creators, manipulation strategies, purposes, and real-world content production methods. This allows us to gain valuable insights into the nuances and characteristics of deepfakes in different contexts. Lastly, in addition to the video content, we also collect viewer comments and interactions, enabling us to explore the engagements of internet users with deepfake content. By considering this rich contextual information, we aim to provide a holistic understanding of the evolving deepfake phenomenon and its impact on online platforms.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4530–4537},
numpages = {8},
keywords = {deepfake datasets, deepfake detection, deepfake videos},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3610977.3637482,
author = {H\"{u}bert, Heiko and Yun, Hae Seon},
title = {Sobotify: A Framework for Turning Robots into Social Robots},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3637482},
doi = {10.1145/3610977.3637482},
abstract = {Sobotify is a software framework, which aims at simplifying the process of using robots in the field of social robotics. This paper delineates the design and usage of the framework. With Sobotify, even non-technical people should be enabled to use robots for their specific purposes, such as teachers in a classroom, therapists in a physiological or psychological therapy or childcare workers in kindergarten. During the development process of Sobotify, feedback from teachers at a vocational school have been taken into account in order to adjust the tools to their needs. The framework is designed to work with different robots including both humanoid (NAO and Pepper) and non-humanoid robots such as toy robots (Cozmo) with advanced abilities as well as very simple toy robots (MyKeepon). The framework was tested by two research works which proved that Sobotify is applicable in different setups. Further development is already planned for the next months, e.g. integration of additional robots and extension of tools.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {910–914},
numpages = {5},
keywords = {hri, social robotics, software tools},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/2998181.2998360,
author = {Arciniegas-Mendez, Maryi and Zagalsky, Alexey and Storey, Margaret-Anne and Hadwin, Allyson Fiona},
title = {Using the Model of Regulation to Understand Software Development Collaboration Practices and Tool Support},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998360},
doi = {10.1145/2998181.2998360},
abstract = {We developed the Model of Regulation to provide a vocabulary for comparing and analyzing collaboration practices and tools in software engineering. This paper discusses the model's ability to capture how individuals self-regulate their own tasks and activities, how they regulate one another, and how they achieve a shared understanding of project goals and tasks. Using the model, we created an "action-oriented" instrument that individuals, teams, and organizations can use to reflect on how they regulate their work and on the various tools they use as part of regulation. We applied this instrument to two industrial software projects, interviewing one or two stakeholders from each project. The model allowed us to identify where certain processes and communication channels worked well, while recognizing friction points, communication breakdowns, and regulation gaps. We believe this model also shows potential for application in other domains.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1049–1065},
numpages = {17},
keywords = {collaboration, regulation, theory},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3510003.3510156,
author = {Endres, Madeline and Boehnke, Kevin and Weimer, Westley},
title = {Hashing it out: a survey of programmers' cannabis usage, perception, and motivation},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510156},
doi = {10.1145/3510003.3510156},
abstract = {Cannabis is one of the most common mind-altering substances. It is used both medicinally and recreationally and is enmeshed in a complex and changing legal landscape. Anecdotal evidence suggests that some software developers may use cannabis to aid some programming tasks. At the same time, anti-drug policies and tests remain common in many software engineering environments, sometimes leading to hiring shortages for certain jobs. Despite these connections, little is actually known about the prevalence of, and motivation for, cannabis use while programming. In this paper, we report the results of the first large-scale survey of cannabis use by programmers. We report findings about 803 developers' (including 450 full-time programmers') cannabis usage prevalence, perceptions, and motivations. For example, we find that some programmers do regularly use cannabis while programming: 35% of our sample has tried programming while using cannabis, and 18% currently do so at least once a month. Furthermore, this cannabis usage is primarily motivated by a perceived enhancement to certain software development skills (such as brainstorming or getting into a programming zone) rather than medicinal reasons (such as pain relief). Finally, we find that cannabis use while programming occurs at similar rates for programming employees, managers, and students despite differences in cannabis perceptions and visibility. Our results have implications for programming job drug policies and motivate future research into cannabis use while programming.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1107–1119},
numpages = {13},
keywords = {cannabis, corporate drug policy, software development process},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3656579,
author = {Liang, Wanying and Meo, Pasquale De and Tang, Yong and Zhu, Jia},
title = {A Survey of Multi-modal Knowledge Graphs: Technologies and Trends},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3656579},
doi = {10.1145/3656579},
abstract = {In recent years, Knowledge Graphs (KGs) have played a crucial role in the development of advanced knowledge-intensive applications, such as recommender systems and semantic search. However, the human sensory system is inherently multi-modal, as objects around us are often represented by a combination of multiple signals, such as visual and textual. Consequently, Multi-modal Knowledge Graphs (MMKGs), which combine structured knowledge representation with multiple modalities, represent a powerful extension of KGs. Although MMKGs can handle certain types of tasks (e.g., visual query answering) or queries that standard KGs cannot process, and they can effectively tackle some standard problems (e.g., entity alignment), we lack a widely accepted definition of MMKG. In this survey, we provide a rigorous definition of MMKGs along with a classification scheme based on how existing approaches address four fundamental challenges: representation, fusion, alignment, and translation, which are crucial to improving an MMKG. Our classification scheme is flexible and allows for easy incorporation of new approaches, as well as a comparison of two approaches in terms of how they address one of the fundamental challenges mentioned above. As the first comprehensive survey of MMKG, this article aims at inspiring and provide a reference for relevant researchers in the field of Artificial Intelligence.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {273},
numpages = {41},
keywords = {Multi-modal knowledge graphs, four fundamental challenges, pre-training in MMKGs}
}

@inproceedings{10.1145/3643991.3644898,
author = {Casta\~{n}o, Joel and Silverio, Mart\'{\i}nez-Fern\'{a}ndez and Franch, Xavier and Bogner, Justus},
title = {Analyzing the Evolution and Maintenance of ML Models on Hugging Face},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644898},
doi = {10.1145/3643991.3644898},
abstract = {Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF - aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {607–618},
numpages = {12},
keywords = {repository mining, software evolution, maintenance},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3266426,
author = {Sharma, Abhishek and Tian, Yuan and Sulistya, Agus and Wijedasa, Dinusha and Lo, David},
title = {Recommending Who to Follow in the Software Engineering Twitter Space},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3266426},
doi = {10.1145/3266426},
abstract = {With the advent of social media, developers are increasingly using it in their software development activities. Twitter is one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape.” Unfortunately, due to the general-purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets.To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domain-related tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over 1 month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-Measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-Measure by at least 7.63%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
articleno = {16},
numpages = {33},
keywords = {Twitter, recommendation systems, software engineering}
}

@inproceedings{10.1145/3636555.3636895,
author = {Li, Chenglu and Zhu, Wangda and Xing, Wanli and Guo, Rui},
title = {Analyzing Student Attention and Acceptance of Conversational AI for Math Learning: Insights from a Randomized Controlled Trial},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636895},
doi = {10.1145/3636555.3636895},
abstract = {The significance of nurturing a deep conceptual understanding in math learning cannot be overstated. Grounded in the pedagogical strategies of induction, concretization, and exemplification (ICE), we designed and developed a conversational AI using both rule- and generation-based techniques to facilitate math learning. Serving as a preliminary step, this study employed an experimental design involving 151 U.S.-based college students to reveal students’ attention patterns, technology acceptance model, and qualitative feedback when using the developed ConvAI. Our findings suggest that participants in the ConvAI group generally exhibit higher attention levels than those in the control group, aside from the initial stage where the control group was more attentive. Meanwhile, participants appreciated their experience with the ConvAI, particularly valuing the ICE support features. Finally, qualitative analysis of participants’ feedback was conducted to inform future refinement and to inspire educational researchers and practitioners.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {836–842},
numpages = {7},
keywords = {Conversational AI, Large language models, Math learning, Technology design and development},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1109/ICSE43902.2021.00022,
author = {Wang, Ying and Qiao, Liang and Xu, Chang and Liu, Yepang and Cheung, Shing-Chi and Meng, Na and Yu, Hai and Zhu, Zhiliang},
title = {Hero: On the Chaos When PATH Meets Modules},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00022},
doi = {10.1109/ICSE43902.2021.00022},
abstract = {Ever since its first release in 2009, the Go programming language (Golang) has been well received by software communities. A major reason for its success is the powerful support of library-based development, where a Golang project can be conveniently built on top of other projects by referencing them as libraries. As Golang evolves, it recommends the use of a new library-referencing mode to overcome the limitations of the original one. While these two library modes are incompatible, both are supported by the Golang ecosystem. The heterogeneous use of library-referencing modes across Golang projects has caused numerous dependency management (DM) issues, incurring reference inconsistencies and even build failures. Motivated by the problem, we conducted an empirical study to characterize the DM issues, understand their root causes, and examine their fixing solutions. Based on our findings, we developed HERO, an automated technique to detect DM issues and suggest proper fixing solutions. We applied HERO to 19,000 popular Golang projects. The results showed that HERO achieved a high detection rate of 98.5% on a DM issue benchmark and found 2,422 new DM issues in 2,356 popular Golang projects. We reported 280 issues, among which 181 (64.6%) issues have been confirmed, and 160 of them (88.4%) have been fixed or are under fixing. Almost all the fixes have adopted our fixing suggestions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {99–111},
numpages = {13},
keywords = {Dependency Management, Golang Ecosystem},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3577212,
author = {Forlano, Laura E. and Halpern, Megan K.},
title = {Speculative Histories, Just Futures: From Counterfactual Artifacts to Counterfactual Actions},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3577212},
doi = {10.1145/3577212},
abstract = {This article engages with history as a speculative space for the purpose of critically engaging with discourses around the politics of technology in HCI. Drawing on approaches within critical design and based on evidence from two different projects, we develop an approach, counterfactual actions, that moves beyond the creation of artifacts and towards more situated, embodied, and performative engagements. In one project, Reimaging Work, we used a participatory game to engage stakeholders from social and economic justice organizations in Chicago. The other project, Future Design Studio, invited audience members at a futurist festival to create artifacts from the future and then invited improvisational actors to build worlds around them. We argue that a focus on counterfactual actions supports a more relational approach to understanding the politics of socio-technical systems and infrastructures, allowing participants to gain a meaningful understanding of the ways in which technology could be designed otherwise in line with ethics, values and social justice concerns.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {apr},
articleno = {22},
numpages = {37},
keywords = {Histories, futures, prototyping, counterfactual artifacts, speculative design, public engagement}
}

@inproceedings{10.1145/3540250.3549099,
author = {Li, Lingwei and Yang, Li and Jiang, Huaxi and Yan, Jun and Luo, Tiejian and Hua, Zihan and Liang, Geng and Zuo, Chun},
title = {AUGER: automatically generating review comments with pre-training models},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549099},
doi = {10.1145/3540250.3549099},
abstract = {Code review is one of the best practices as a powerful safeguard for software quality. In practice, senior or highly skilled reviewers inspect source code and provide constructive comments, consider- ing what authors may ignore, for example, some special cases. The collaborative validation between contributors results in code being highly qualified and less chance of bugs. However, since personal knowledge is limited and varies, the efficiency and effectiveness of code review practice are worthy of further improvement. In fact, it still takes a colossal and time-consuming effort to deliver useful review comments.  
This paper explores a synergy of multiple practical review comments to enhance code review and proposes AUGER (AUtomatically GEnerating Review comments): a review comments generator with pre-training models. We first collect empirical review data from 11 notable Java projects and construct a dataset of 10,882 code changes. By leveraging Text-to-Text Transfer Transformer (T5) models, the framework synthesizes valuable knowledge in the training stage and effectively outperforms baselines by 37.38% in ROUGE-L. 29% of our automatic review comments are considered useful according to prior studies. The inference generates just in 20 seconds and is also open to training further. Moreover, the performance also gets improved when thoroughly analyzed in case study.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1009–1021},
numpages = {13},
keywords = {Code Review, Machine Learning, Review Comments, Text Generation},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3611643.3616293,
author = {Miller, Courtney and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {“We Feel Like We’re Winging It:” A Study on Navigating Open-Source Dependency Abandonment},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616293},
doi = {10.1145/3611643.3616293},
abstract = {While lots of research has explored how to prevent maintainers from abandoning the open-source projects that serve as our digital infras- tructure, there are very few insights on addressing abandonment when it occurs. We argue open-source sustainability research must expand its focus beyond trying to keep particular projects alive, to also cover the sustainable use of open source by supporting users when they face potential or actual abandonment. We interviewed 33 developers who have experienced open-source dependency aban- donment. Often, they used multiple strategies to cope with aban- donment, for example, first reaching out to the community to find potential alternatives, then switching to a community-accepted alternative if one exists. We found many developers felt they had little to no support or guidance when facing abandonment, leaving them to figure out what to do through a trial-and-error process on their own. Abandonment introduces cost for otherwise seem- ingly free dependencies, but users can decide whether and how to prepare for abandonment through a number of different strategies, such as dependency monitoring, building abstraction layers, and community involvement. In many cases, community members can invest in resources that help others facing the same abandoned dependency, but often do not because of the many other competing demands on their time – a form of the volunteer’s dilemma. We dis- cuss cost reduction strategies and ideas to overcome this volunteer’s dilemma. Our findings can be used directly by open-source users seeking resources on dealing with dependency abandonment, or by researchers to motivate future work supporting the sustainable use of open source.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1281–1293},
numpages = {13},
keywords = {Dependency Management, Human Factors in Software Engineering, Open Source Sustainability},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3551349.3559547,
author = {Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis},
title = {End-to-End Rationale Reconstruction},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559547},
doi = {10.1145/3551349.3559547},
abstract = {The logic behind design decisions, called design rationale, is very valuable. In the past, researchers have tried to automatically extract and exploit this information, but prior techniques are only applicable to specific contexts and there is insufficient progress on an end-to-end rationale information extraction pipeline. Here we outline a path towards such a pipeline that leverages several Machine Learning (ML) and Natural Language Processing (NLP) techniques. Our proposed context-independent approach, called Kantara, produces a knowledge graph representation of decisions and of their rationales, which considers their historical evolution and traceability. We also propose validation mechanisms to ensure the correctness of the extracted information and the coherence of the development process. We conducted a preliminary evaluation of our proposed approach on a small example sourced from the Linux Kernel, which shows promising results.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {176},
numpages = {5},
keywords = {Natural Language Processing, Software rationale},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3127502.3127511,
author = {Manzoni, Pietro and Hern\'{a}ndez-Orallo, Enrique and Calafate, Carlos T. and Cano, Juan-Carlos},
title = {A Proposal for a Publish/Subscribe, Disruption Tolerant Content Island for Fog Computing},
year = {2017},
isbn = {9781450351416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127502.3127511},
doi = {10.1145/3127502.3127511},
abstract = {The performance of applications for the Internet of Things (IoT) depends on the availability of effective transport services offered by the underlying network. In this sense, Fog Computing aims to provide alternative networking models that offer higher fluidity in distributing in-network functions, in addition to allowing fast and scalable processing and exchange of information. In this paper we present the architecture and a working prototype of a fog computing "content island" which interconnects groups of "things" packed-up together to interchange data and processing among themselves and with other content islands. These islands are based on the integration of a publish/subscribe system with DTN (Disruption Tolerant Networks) techniques to provide a higher flexibility with respect to data and computation sharing. Some preliminary results regarding the prototype performance are also given.},
booktitle = {Proceedings of the 3rd Workshop on Experiences with the Design and Implementation of Smart Objects},
pages = {47–52},
numpages = {6},
keywords = {DTN, IoT, MQTT, fog computing, gateway},
location = {Snowbird, Utah, USA},
series = {SMARTOBJECTS '17}
}

@inproceedings{10.1145/3510456.3514152,
author = {Fernandes, Marcelo and Ferino, Samuel and Fernandes, Anny and Kulesza, Uir\'{a} and Aranha, Eduardo and Treude, Christoph},
title = {DevOps education: an interview study of challenges and recommendations},
year = {2022},
isbn = {9781450392259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510456.3514152},
doi = {10.1145/3510456.3514152},
abstract = {Over the last years, the software industry has adopted several DevOps technologies related to practices such as continuous integration and continuous delivery. The high demand for DevOps practitioners requires non-trivial adjustments in traditional software engineering courses and educational methodologies. This work presents an interview study with 14 DevOps educators from different universities and countries, aiming to identify the main challenges and recommendations for DevOps teaching. Our study identified 83 challenges, 185 recommendations, and several association links and conflicts between them. Our findings can help educators plan, execute and evaluate DevOps courses. They also highlight several opportunities for researchers to propose new methods and tools for teaching DevOps.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {90–101},
numpages = {12},
keywords = {DevOps, challenges, recommendations, thematic analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEET '22}
}

@article{10.14778/3007328.3007329,
author = {Chandramouli, Badrish and Fernandez, Raul Castro and Goldstein, Jonathan and Eldawy, Ahmed and Quamar, Abdul},
title = {Quill: efficient, transferable, and rich analytics at scale},
year = {2016},
issue_date = {October 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {14},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007328.3007329},
doi = {10.14778/3007328.3007329},
abstract = {This paper introduces Quill (stands for a quadrillion tuples per day), a library and distributed platform for relational and temporal analytics over large datasets in the cloud. Quill exposes a new abstraction for parallel datasets and computation, called ShardedStreamable. This abstraction provides the ability to express efficient distributed physical query plans that are transferable, i.e., movable from offline to real-time and vice versa. ShardedStreamable decouples incremental query logic specification, a small but rich set of data movement operations, and keying; this allows Quill to express a broad space of plans with complex querying functionality, while leveraging existing temporal libraries such as Trill. Quill's layered architecture provides a careful separation of responsibilities with independently useful components, while retaining high performance. We built Quill for the cloud, with a master-less design where a language-integrated client library directly communicates and coordinates with cloud workers using off-the-shelf distributed cloud components such as queues. Experiments on up to 400 cloud machines, and on datasets up to 1TB, find Quill to incur low overheads and outperform SparkSQL by up to orders-of-magnitude for temporal and 6\texttimes{} for relational queries, while supporting a rich space of transferable, programmable, and expressive distributed physical query plans.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {1623–1634},
numpages = {12}
}

@inproceedings{10.1145/3503222.3507757,
author = {Yang, Boyuan and Chen, Ruirong and Huang, Kai and Yang, Jun and Gao, Wei},
title = {Eavesdropping user credentials via GPU side channels on smartphones},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507757},
doi = {10.1145/3503222.3507757},
abstract = {Graphics Processing Unit (GPU) on smartphones is an effective target for hardware attacks. In this paper, we present a new side channel attack on mobile GPUs of Android smartphones, allowing an unprivileged attacker to eavesdrop the user's credentials, such as login usernames and passwords, from their inputs through on-screen keyboard. Our attack targets on Qualcomm Adreno GPUs and investigate the amount of GPU overdraw when rendering the popups of user's key presses of inputs. Such GPU overdraw caused by each key press corresponds to unique variations of selected GPU performance counters, from which these key presses can be accurately inferred. Experiment results from practical use on multiple models of Android smartphones show that our attack can correctly infer more than 80% of user's credential inputs, but incur negligible amounts of computing overhead and network traffic on the victim device. To counter this attack, this paper suggests mitigations of access control on GPU performance counters, or applying obfuscations on the values of GPU performance counters.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {285–299},
numpages = {15},
keywords = {Input Eavesdropping, Mobile GPU, Performance Counters, Side Channel, Smartphones},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@article{10.5555/3322706.3361994,
author = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
title = {Tunability: importance of hyperparameters of machine learning algorithms},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define databased defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1934–1965},
numpages = {32},
keywords = {classification, hyperparameters, machine learning, meta-learning, supervised learning, tuning}
}

@proceedings{10.1145/3586102,
title = {ICCNS '22: Proceedings of the 2022 12th International Conference on Communication and Network Security},
year = {2022},
isbn = {9781450397520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@article{10.1145/3446372,
author = {Alharbi, Ahmed and Dong, Hai and Yi, Xun and Tari, Zahir and Khalil, Ibrahim},
title = {Social Media Identity Deception Detection: A Survey},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446372},
doi = {10.1145/3446372},
abstract = {Social media have been growing rapidly and become essential elements of many people’s lives. Meanwhile, social media have also come to be a popular source for identity deception. Many social media identity deception cases have arisen over the past few years. Recent studies have been conducted to prevent and detect identity deception. This survey analyzes various identity deception attacks, which can be categorized into fake profile, identity theft, and identity cloning. This survey provides a detailed review of social media identity deception detection techniques. It also identifies primary research challenges and issues in the existing detection techniques. This article is expected to benefit both researchers and social media providers.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {69},
numpages = {35},
keywords = {Identity deception, Sybil, detection techniques, fake profile, identity cloning, identity theft, social botnet, sockpuppet}
}

@article{10.1145/3542937,
author = {P\^{a}r\c{t}achi, Profir-Petru and White, David R. and Barr, Earl T.},
title = {Aide-m\'{e}moire: Improving a Project’s Collective Memory via Pull Request–Issue Links},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3542937},
doi = {10.1145/3542937},
abstract = {Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-m\'{e}moire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-m\'{e}moire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project’s lifetime and continuously improve traceability. Aide-m\'{e}moire is tailored for two specific instances of the general traceability problem—namely, commit to issue and pull request to issue links, with a focus on the latter—and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-m\'{e}moire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {32},
numpages = {36},
keywords = {Traceability, link inference, missing link}
}

@inproceedings{10.1145/3597503.3639097,
author = {Murphy-Hill, Emerson and Elizondo, Alberto and Murillo, Ambar and Harbach, Marian and Vasilescu, Bogdan and Carlson, Delphine and Dessloch, Florian},
title = {GenderMag Improves Discoverability in the Field, Especially for Women: An Multi-Year Case Study of Suggest Edit, a Code Review Feature},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639097},
doi = {10.1145/3597503.3639097},
abstract = {Prior research shows that the GenderMag method can help identify and address usability barriers that are more likely to affect women software users than men. However, the evidence for the effectiveness of GenderMag is limited to small lab studies. In this case study, by combining self-reported gender data from tens of thousands of users of an internal code review tool with software logs data gathered over a five-year period, we quantitatively show that GenderMag helped a team at Google (a) correctly identify discoverability as a usability barrier more likely to affect women than men, and (b) increase discoverability by 2.4x while also achieving gender parity. That is, compared to men using the original code review tool, women and men using the system redesigned with GenderMag were both 2.4x more likely to discover the "Suggest Edit" feature at any given time. Thus, this paper contributes the first large-scale evidence of the effectiveness of GenderMag in the field.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {189},
numpages = {12},
keywords = {software features, feature discovery, UX design, gender, inclusion},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3540250.3549119,
author = {Hong, Yang and Tantithamthavorn, Chakkrit and Thongtanunam, Patanamon and Aleti, Aldeida},
title = {CommentFinder: a simpler, faster, more accurate code review comments recommendation},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549119},
doi = {10.1145/3540250.3549119},
abstract = {Code review is an effective quality assurance practice, but can be labor-intensive since developers have to manually review the code and provide written feedback. Recently, a Deep Learning (DL)-based approach was introduced to automatically recommend code review comments based on changed methods. While the approach showed promising results, it requires expensive computational resource and time which limits its use in practice. To address this limitation, we propose CommentFinder – a retrieval-based approach to recommend code review comments. Through an empirical evaluation of 151,019 changed methods, we evaluate the effectiveness and efficiency of CommentFinder against the state-of-the-art approach. We find that when recommending the best-1 review comment candidate, our CommentFinder is 32% better than prior work in recommending the correct code review comment. In addition, CommentFinder is 49 times faster than the prior work. These findings highlight that our CommentFinder could help reviewers to reduce the manual efforts by recommending code review comments, while requiring less computational time.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {507–519},
numpages = {13},
keywords = {Modern Code Review, Software Quality Assurance},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3442381.3450071,
author = {Szurdi, Janos and Luo, Meng and Kondracki, Brian and Nikiforakis, Nick and Christin, Nicolas},
title = {Where are you taking me?Understanding Abusive Traffic Distribution Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450071},
doi = {10.1145/3442381.3450071},
abstract = {Illicit website owners frequently rely on traffic distribution systems (TDSs) operated by less-than-scrupulous advertising networks to acquire user traffic. While researchers have described a number of case studies on various TDSs or the businesses they serve, we still lack an understanding of how users are differentiated in these ecosystems, how different illicit activities frequently leverage the same advertisement networks and, subsequently, the same malicious advertisers. We design ODIN (Observatory of Dynamic Illicit ad Networks), the first system to study cloaking, user differentiation and business integration at the same time in four different types of traffic sources: typosquatting, copyright-infringing movie streaming, ad-based URL shortening, and illicit online pharmacy websites. ODIN performed 874,494 scrapes over two months (June 19, 2019–August 24, 2019), posing as six different types of users (e.g., mobile, desktop, and crawler) and accumulating over 2TB of data. We observed 81% more malicious pages compared to using only the best performing crawl profile by itself. Three of the traffic sources we study redirect users to the same traffic broker domain names up to 44% of the time and all of them often expose users to the same malicious advertisers. Our experiments show that novel cloaking techniques could decrease by half the number of malicious pages observed. Worryingly, popular blacklists do not just suffer from the lack of coverage and delayed detection, but miss the vast majority of malicious pages targeting mobile users. We use these findings to design a classifier, which can make precise predictions about the likelihood of a user being redirected to a malicious advertiser.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3613–3624},
numpages = {12},
keywords = {Cloaking, Distribution, Mobile, Phone, Security, Traffic, User, Web},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3660650.3660657,
author = {Roberts, Jordan and Mohamed, Abdallah},
title = {Generative AI in CS Education: Literature Review through a SWOT Lens},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660657},
doi = {10.1145/3660650.3660657},
abstract = {The rapid growth of generative artificial intelligence (AI) models introduced challenges for educators, students and administrators across the academic sphere related to how to manage and regulate these tools. While some oppose their use, many researchers have begun to approach the topic of educational AI use from a different perspective. Despite being in its early stages; this field of research has produced notable insights into the capabilities and limitations of models like ChatGPT. This paper utilizes a SWOT analysis framework to analyze and consolidate existing literature, with a specific focus on Computer Science education. Through the analysis of this literature, we have created a set of use cases and guidelines to aid in the future development of strategies and tools within this field. Our findings indicate that while some concerns are valid, such as AI's ability to generate plagiarized work, we identified several promising avenues and opportunities for careful integration of this technology into education.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {10},
numpages = {6},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00008,
author = {Chaaben, Meriem Ben and Burgue\~{n}o, Lola and Sahraoui, Houari},
title = {Towards Using Few-Shot Prompt Learning for Automating Model Completion},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00008},
doi = {10.1109/ICSE-NIER58687.2023.00008},
abstract = {We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {7–12},
numpages = {6},
keywords = {language models, few-shot learning, prompt learning, domain modeling, model completion},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1145/3583780.3615121,
author = {Pierri, Francesco and Liu, Geng and Ceri, Stefano},
title = {ITA-ELECTION-2022: A Multi-Platform Dataset of Social Media Conversations Around the 2022 Italian General Election},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615121},
doi = {10.1145/3583780.3615121},
abstract = {Online social media play a major role in shaping public discourse and opinion, especially during political events. We present the first public multi-platform dataset of Italian-language political conversations, focused on the 2022 Italian general election taking place on September 25th. Leveraging public APIs and a keyword-based search, we collected millions of posts published by users, pages and groups on Facebook, Instagram and Twitter, along with metadata of TikTok and YouTube videos shared on these platforms, over a period of four months. We augmented the dataset with a collection of political ads sponsored on Meta platforms, and a list of social media handles associated with political representatives. Our data resource will allow researchers and academics to further our understanding of the role of social media in the democratic process.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5386–5390},
numpages = {5},
keywords = {advertisement, italy, multi-platform, politics, social media},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3571848,
author = {Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad and Adams, Bram},
title = {On the Discoverability of npm Vulnerabilities in Node.js Projects},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571848},
doi = {10.1145/3571848},
abstract = {The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42%) depend on undisclosed vulnerable packages, 206&nbsp;(4.63%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {91},
numpages = {27},
keywords = {Open source software, software packages, software ecosystems, dependency vulnerabilities}
}

@inproceedings{10.1145/3560835.3564547,
author = {Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan and Indarapu, Aryan and Hyatt, Matt and Schorlemmer, Taylor R. and Thiruvathukal, George K. and Davis, James C.},
title = {An Empirical Study of Artifacts and Security Risks in the Pre-trained Model Supply Chain},
year = {2022},
isbn = {9781450398855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560835.3564547},
doi = {10.1145/3560835.3564547},
abstract = {Deep neural networks achieve state-of-the-art performance on many tasks, but require increasingly complex architectures and costly training procedures. Engineers can reduce costs by reusing a pre-trained model (PTM) and fine-tuning it for their own tasks. To facilitate software reuse, engineers collaborate around model hubs, collections of PTMs and datasets organized by problem domain. Although model hubs are now comparable in popularity and size to other software ecosystems, the associated PTM supply chain has not yet been examined from a software engineering perspective. We present an empirical study of artifacts and security features in 8 model hubs. We indicate the potential threat models and show that the existing defenses are insufficient for ensuring the security of PTMs. We compare PTM and traditional supply chains, and propose directions for further measurements and tools to increase the reliability of the PTM supply chain.},
booktitle = {Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {105–114},
numpages = {10},
keywords = {deep neural networks, empirical software engineering, machine learning, model hubs, software reuse, software supply chain},
location = {Los Angeles, CA, USA},
series = {SCORED'22}
}

@inproceedings{10.1145/3634737.3644992,
author = {Davanian, Ali and Faloutsos, Michail and Lindorfer, Martina},
title = {C2Miner: Tricking IoT Malware into Revealing Live Command &amp; Control Servers},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3644992},
doi = {10.1145/3634737.3644992},
abstract = {How can we identify live Command &amp; Control (C2) servers for a given IoT malware binary? An effective solution to this problem constitutes a significant capability towards detecting and containing botnets. This task is not trivial because C2 servers are short-lived, and they use sophisticated and proprietary communication protocols. We propose C2Miner, a novel approach to trick IoT malware binaries into revealing their currently live C2 servers. Our approach weaponizes old disposable IoT malware binaries and uses them to probe active servers. We provide novel solutions to overcome the following challenges: (a) disambiguating the C2-bound traffic generated by the malware and (b) determining if a target IP:port is indeed a C2 server as opposed to a benign server.In our evaluation, based on 3M distinct exploration attempts over 150K distinct IP addresses, we show that we can identify C2 servers within a given IP:port space with an F1 score of 86%. In addition, we show how our approach can be used in practice and at scale. Conducting a large-scale probing campaign has scalability issues given that the number of probes is proportional to the IP addresses, the number of ports, and the number of binaries from distinct families which we want to explore. To address this challenge, we propose a grammar-based method to fingerprint and cluster C2 communications which, among other applications, allows us to select malware binaries for weaponization efficiently. Additionally, we use spatio-temporal features of C2 servers to narrow down our search in the entire IP space. An optimistic observation from our study is that using only 2 (more than 6 months) old IoT malware binaries, we scan 18K IP:port pairs daily for 6 days and find 6 new live C2 servers.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {112–127},
numpages = {16},
keywords = {IoT, malware, network security, command &amp; control servers},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3093338.3093392,
author = {Simmel, Derek and Filus, Shane},
title = {Flexible Enforcement of Multi-factor Authentication with SSH via Linux-PAM for Federated Identity Users},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3093392},
doi = {10.1145/3093338.3093392},
abstract = {A computational science project with restricted-access data was awarded an allocation by XSEDE in 2016 to use the Bridges supercomputer at the Pittsburgh Supercomputing Center (PSC). As a condition of the license agreement for access to the data, multi-factor authentication (MFA) with XSEDE's Duo MFA service is required for users of this project to login to Bridges via SSH, in addition to filesystem access controls. Since not all Bridges users are required to authenticate to Bridges in this manner, a solution was implemented via Linux-PAM to require XSEDE Duo MFA for SSH login access by specific users, as identified by their local account name or membership in a local group. This paper describes the implementation on Bridges and its extensibility to other systems and environments with similar needs.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {10},
numpages = {9},
keywords = {Duo, GSI-OpenSSH, Kerberos, LDAP, Linux, MFA, MyProxy, OpenSSH, PAM, SSH, X.509, XSEDE, authentication, federated identity, multi-factor authentication},
location = {New Orleans, LA, USA},
series = {PEARC '17}
}

@article{10.1145/3608483,
author = {Falduti, Mattia and Tessaris, Sergio},
title = {Mapping the Interdisciplinary Research on Non-consensual Pornography: Technical and Quantitative Perspectives},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3608483},
doi = {10.1145/3608483},
abstract = {The phenomenon of the non-consensual distribution of intimate or sexually explicit digital images of adults, a.k.a. non-consensual pornography (NCP) or revenge pornography, is under the spotlight for the toll it is taking on society. Law enforcement statistics confirm a dramatic global rise in abuses. For this reason, the research community is investigating different strategies to fight and mitigate the abuses and their effects. Since the phenomenon involves different aspects of personal and social interaction among users of social media and content sharing platforms, in the literature it is addressed under different academic disciplines. However, while most of the literature reviews focus on non-consensual pornography either from a social science or psychological perspective, to the best of our knowledge a systematic review of the research on the technical aspects of the problem is still missing. In this work, we present a Systematic Mapping Study (SMS) of the literature, looking at this interdisciplinary phenomenon through a technical lens. Therefore, we focus on the cyber side of the crime of non-consensual pornography with the aim of describing the state-of-the-art and the future lines of research from a technical and quantitative perspective.},
journal = {Digital Threats},
month = {oct},
articleno = {44},
numpages = {22},
keywords = {Non-consensual pornography, deepfake, revenge pornography, image-based sexual abuse, cybercrime defenses}
}

@inproceedings{10.1145/3600160.3605010,
author = {Chatzoglou, Efstratios and Karopoulos, Georgios and Kambourakis, Georgios and Tsiatsikas, Zisis},
title = {Bypassing antivirus detection: old-school malware, new tricks},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3605010},
doi = {10.1145/3600160.3605010},
abstract = {Being on a mushrooming spree since at least 2013, malware can take a large toll on any system. In a perpetual cat-and-mouse chase with defenders, malware writers constantly conjure new methods to hide their code so as to evade detection by security products. In this context, focusing on the MS Windows platform, this work contributes a comprehensive empirical evaluation regarding the detection capacity of popular, off-the-shelf antivirus and endpoint detection and response engines when facing legacy malware obfuscated via more or less uncommon but publicly known methods. Our experiments exploit a blend of seven traditional AV evasion techniques in 16 executables built in C++, Go, and Rust. Furthermore, we conduct an incipient study regarding the ability of the ChatGPT chatbot in assisting threat actors to produce ready-to-use malware. The derived results in terms of detection rate are highly unexpected: approximately half of the 12 tested AV engines were able to detect less than half of the malware variants, four AVs exactly half of the variants, while only two of the rest detected all but one of the variants.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {150},
numpages = {10},
keywords = {Antivirus software, ChatGPT, Malware, Malware evasion techniques},
location = {Benevento, Italy},
series = {ARES '23}
}

@article{10.1145/2735399.2735400,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2735399.2735400},
doi = {10.1145/2735399.2735400},
journal = {SIGSOFT Softw. Eng. Notes},
month = {apr},
pages = {16–21},
numpages = {6}
}

@article{10.1145/3511215,
author = {Alshammari, Nasser O. and Alharbi, Fawaz D.},
title = {Combining a Novel Scoring Approach with Arabic Stemming Techniques for Arabic Chatbots Conversation Engine},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511215},
doi = {10.1145/3511215},
abstract = {Arabic is recognized as one of the main languages around the world. Many attempts and efforts have been done to provide computing solutions to support the language. Developing Arabic chatbots is still an evolving research field and requires extra efforts due to the nature of the language. One of the common tasks of any natural language processing application is the stemming step. It is important for developing chatbots, since it helps with pre-processing the input data and it can be involved with different phases of the chatbot development process. The aim of this article is to combine a scoring approach with Arabic stemming techniques for developing an Arabic chatbot conversation engine. Two experiments are conducted to evaluate the proposed solution. The first experiment is to select which stemmer is more accurate when applying our solution, since our algorithm can support various stemmers. The second experiment was conducted to evaluate our proposed approach against various machine learning models. The results show that the ISRIS stemming algorithm is the best fit for our solution with accuracy 78.06%. The results also indicate that our novel solution achieved an F1 score of 65.5%, while the other machine learning models achieved slightly lower scores. Our study presents a novel technique by combining scoring mechanisms with stemming processes to produce the best answer for every query sent by chatbots users compared to other approaches. This can be helpful for developing Arabic chatbot and can support many domains such as education, business, and health. This technique is among the first techniques that developed purposefully to serve the development of Arabic chatbots conversation engine.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {84},
numpages = {21},
keywords = {Natural language processing, chatbot, machine learning, stemming, Arabic language}
}

@article{10.1145/3412378,
author = {Ehsan, Osama and Hassan, Safwat and Mezouar, Mariam El and Zou, Ying},
title = {An Empirical Study of Developer Discussions in the Gitter Platform},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3412378},
doi = {10.1145/3412378},
abstract = {Developer chatrooms (e.g., the Gitter platform) are gaining popularity as a communication channel among developers. In developer chatrooms, a developer (asker) posts questions and other developers (respondents) respond to the posted questions. The interaction between askers and respondents results in a discussion thread. Recent studies show that developers use chatrooms to inquire about issues, discuss development ideas, and help each other. However, prior work focuses mainly on analyzing individual messages of a chatroom without analyzing the discussion thread in a chatroom. Developer chatroom discussions are context-sensitive, entangled, and include multiple participants that make it hard to accurately identify threads. Therefore, prior work has limited capability to show the interactions among developers within a chatroom by analyzing only individual messages.In this article, we perform an in-depth analysis of the Gitter platform (i.e., developer chatrooms) by analyzing 6,605,248 messages of 709 chatrooms. To analyze the characteristics of the posted questions and the impact on the response behavior (e.g., whether the posted questions get responses), we propose an approach that identifies discussion threads in chatrooms with high precision (i.e., 0.81 F-score). Our results show that inactive members responded more often and unique questions take longer discussion time than simple questions. We also find that clear and concise questions are more likely to be responded to than poorly written questions.We further manually analyze a randomly selected sample of 384 threads to examine how respondents resolve the raised questions. We observe that more than 80% of the studied threads are resolved. Advanced-level/beginner-level questions along with the edited questions are the mostly resolved questions. Our results can help the project maintainers understand the nature of the discussion threads (e.g., the topic trends). Project maintainers can also benefit from our thread identification approach to spot the common repeated threads and use these threads as frequently asked questions (FAQs) to improve the documentation of their projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {8},
numpages = {39},
keywords = {Chat disentanglement, Gitter, developer chatrooms, developer threads, mixed-effect models, thread identification}
}

@inproceedings{10.1145/3563657.3596042,
author = {Goswami, Lahari and Zeinoddin, Pegah Sadat and Estier, Thibault and Cherubini, Mauro},
title = {Supporting Collaboration in Introductory Programming Classes Taught in Hybrid Mode: A Participatory Design Study},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596042},
doi = {10.1145/3563657.3596042},
abstract = {Hybrid learning modalities, where learners can attend a course in-person or remotely, have gained particular significance in post-pandemic educational settings. In introductory programming courses, novices’ learning behaviour in the collaborative context of classrooms differs in hybrid mode from that of a traditional setting. Reflections from conducting an introductory programming course in hybrid mode led us to recognise the need for re-designing programming tools to support students’ collaborative learning practices. We conducted a participatory design study with nine students, directly engaging them in design to understand their interaction needs in hybrid pedagogical setups to enable effective collaboration during learning. Our findings first highlighted the difficulties that learners face in hybrid modes. The results then revealed learners’ preferences for design functionalities to enable collective notions, communication, autonomy, and regulation. Based on our findings, we discuss design principles and implications to inform the future design of collaborative programming environments for hybrid modes.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {1248–1262},
numpages = {15},
keywords = {collaboration, hybrid classroom, participatory design, programming environment},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3386367.3431317,
author = {Darki, Ahmad and Faloutsos, Michalis},
title = {RIoTMAN: a systematic analysis of IoT malware behavior},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431317},
doi = {10.1145/3386367.3431317},
abstract = {How can we conduct dynamic analysis on IoT malware efficiently? A key challenge is that such malware target a plethora of different devices, which makes identifying the target device non-trivial. This problem does not appear nearly as much in PC and smartphones malware, where the devices are more uniform. The contribution of our work is two fold: (a) we develop RIoTMAN, a comprehensive emulation and dynamic analysis approach, and (b) we study the network behavior of 3024 IoT malware systematically. The power of our approach lies in two key novelties: (a) Iterative Adaptation, and (b) Automated Engagement. First, we employ an intelligent iterative process that incrementally "builds" the configuration of the target device. Second, our platform automates the interaction with the malware even during the C&amp;C server communication phase. In our experiments, we first show that we achieve an activation rate of 93% for our binaries, including 173 binaries, which Virustotal fails to identify as malicious. Second, we impersonate the C&amp;C server for 79% of the malware binaries successfully: we make the malware initiate DDoS attacks, or enter its proliferation phase. Finally, we observe several interesting malware techniques, including unusual communication behaviors. Our goal is to release our platform as an open-source tool to accelerate the efforts for understanding IoT malware in depth and at scale.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {169–182},
numpages = {14},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings{10.1145/3627106.3627140,
author = {Innocenti, Tommaso and Golinelli, Matteo and Onarlioglu, Kaan and Mirheidari, Ali and Crispo, Bruno and Kirda, Engin},
title = {OAuth 2.0 Redirect URI Validation Falls Short, Literally},
year = {2023},
isbn = {9798400708862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627106.3627140},
doi = {10.1145/3627106.3627140},
abstract = {OAuth 2.0 requires a complex redirection trail between websites and Identity Providers (IdPs). In particular, the "redirect URI" parameter included in the popular Authorization Grant Code flow governs the callback endpoint that users are routed to, together with their security tokens. The protocol specification, therefore, includes guidelines on protecting the integrity of the redirect URI. In this work, we analyze the OAuth&nbsp;2.0 specification in light of modern systems-centric attacks and reveal that the prescribed redirect URI validation guidance exposes IdPs to path confusion and parameter pollution attacks. Based on this observation, we propose novel attack techniques and experiment with 16 popular IdPs, empirically verifying that the OAuth&nbsp;2.0 security guidance is under-specified. We finally present end-to-end attack scenarios that combine our attack techniques with common web application vulnerabilities, ultimately resulting in a complete compromise of the secure delegated access that OAuth&nbsp;2.0 promises.},
booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference},
pages = {256–267},
numpages = {12},
keywords = {OAuth 2.0, account takeover, parameter pollution, path confusion, redirect URI},
location = {Austin, TX, USA},
series = {ACSAC '23}
}

@article{10.1145/3615859,
author = {Cusumano, Michael A.},
title = {Generative AI as a New Innovation Platform},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3615859},
doi = {10.1145/3615859},
abstract = {Considering the stability and longevity of a potential new foundational technology.},
journal = {Commun. ACM},
month = {sep},
pages = {18–21},
numpages = {4}
}

@inproceedings{10.1145/3510003.3510048,
author = {Feng, Sidong and Chen, Chunyang},
title = {GIFdroid: automated replay of visual bug reports for Android apps},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510048},
doi = {10.1145/3510003.3510048},
abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using software. However, it is difficult for non-technical users to write clear descriptions about the bug occurrence. Therefore, more and more users begin to record the screen for reporting bugs as it is easy to be created and contains detailed procedures triggering the bug. But it is still tedious and time-consuming for developers to reproduce the bug due to the length and unclear actions within the recording. To overcome these issues, we propose GIFdroid, a lightweight approach to automatically replay the execution trace from visual bug reports. GIFdroid adopts image processing techniques to extract the keyframes from the recording, map them to states in GUI Transitions Graph, and generate the execution trace of those states to trigger the bug. Our automated experiments and user study demonstrate its accuracy, efficiency, and usefulness of the approach.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1045–1057},
numpages = {13},
keywords = {Android testing, bug replay, visual recording},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@proceedings{10.1145/3538969,
title = {ARES '22: Proceedings of the 17th International Conference on Availability, Reliability and Security},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3393822.3432339,
author = {Dinh-Tuan, Hai and Mora-Martinez, Maria and Beierle, Felix and Garzon, Sandro Rodriguez},
title = {Development Frameworks for Microservice-based Applications: Evaluation and Comparison},
year = {2020},
isbn = {9781450377621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393822.3432339},
doi = {10.1145/3393822.3432339},
abstract = {The microservice architectural style has gained much attention from both academia and industry recently as a novel way to design, develop, and deploy cloud-native applications. This concept encourages the decomposition of a monolith into multiple independently deployable units. A typical microservices-based application is formed of two service types: functional services, which provide the core business logic, and infrastructure services, which provide essential functionalities for a microservices ecosystem. To improve developers' productivity, many software frameworks have been developed to provide those reusable infrastructure services, allowing programmers to focus on implementing microservices in arbitrary ways. In this work, we made use of four open source frameworks to develop a cloud-based application in order to compare and evaluate their usability and practicability. While all selected frameworks promote asynchronous microservice design in general, there are differences in the ways each implements services. This leads to interoperability issues, such as message topic naming convention. Additionally, a key finding is the long startup times of JVM-based services that might reduce application's resiliency and portability. Some other advantages come directly from the programming language, such as the ability of Go to generate native binary executables, which results in very small and compact Docker images (up to 78% smaller compared to other languages).},
booktitle = {Proceedings of the 2020 European Symposium on Software Engineering},
pages = {12–20},
numpages = {9},
keywords = {Microservices, framework-based software development, software engineering},
location = {Rome, Italy},
series = {ESSE '20}
}

@inproceedings{10.1145/3209087.3209106,
author = {Androutsopoulos, K. and Aristodemou, L. and Boender, J. and Bottone, M. and Currie, E. and El-Aroussi, I. and Fields, B. and Gheri, L. and Gorogiannis, N. and Heeney, M. and Micheletti, M. and Loomes, M. and Margolis, M. and Petridis, M. and Piermarteri, A. and Primiero, G. and Raimondi, F. and Weldin, N.},
title = {MIRTO: an Open-Source Robotic Platform for Education},
year = {2018},
isbn = {9781450363839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209087.3209106},
doi = {10.1145/3209087.3209106},
abstract = {This paper introduces the Middlesex RoboTic platfOrm (MIRTO), an open-source platform that has been used for teaching First Year Computer Science students since the academic year 2013/2014, with the aim of providing a physical manifestation of Software Engineering concepts that are often delivered using only abstract or synthetic case studies. In this paper we provide a detailed description of the platform, whose hardware specifications and software libraries are all released open source; we describe a number of teaching usages of the platform, report students' projects, and evaluate some of its aspects in terms of effectiveness, usability, and maintenance.},
booktitle = {Proceedings of the 3rd European Conference of Software Engineering Education},
pages = {55–62},
numpages = {8},
keywords = {Computational Thinking, Robotics, Software Engineering Education},
location = {Seeon/ Bavaria, Germany},
series = {ECSEE '18}
}

@inproceedings{10.1145/3355369.3355576,
author = {Pastrana, Sergio and Suarez-Tangil, Guillermo},
title = {A First Look at the Crypto-Mining Malware Ecosystem: A Decade of Unrestricted Wealth},
year = {2019},
isbn = {9781450369480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355369.3355576},
doi = {10.1145/3355369.3355576},
abstract = {Illicit crypto-mining leverages resources stolen from victims to mine cryptocurrencies on behalf of criminals. While recent works have analyzed one side of this threat, i.e.: web-browser cryptojacking, only commercial reports have partially covered binary-based crypto-mining malware.In this paper, we conduct the largest measurement of crypto-mining malware to date, analyzing approximately 4.5 million malware samples (1.2 million malicious miners), over a period of twelve years from 2007 to 2019. Our analysis pipeline applies both static and dynamic analysis to extract information from the samples, such as wallet identifiers and mining pools. Together with OSINT data, this information is used to group samples into campaigns. We then analyze publicly-available payments sent to the wallets from mining-pools as a reward for mining, and estimate profits for the different campaigns. All this together is is done in a fully automated fashion, which enables us to leverage measurement-based findings of illicit crypto-mining at scale.Our profit analysis reveals campaigns with multi-million earnings, associating over 4.4% of Monero with illicit mining. We analyze the infrastructure related with the different campaigns, showing that a high proportion of this ecosystem is supported by underground economies such as Pay-Per-Install services. We also uncover novel techniques that allow criminals to run successful campaigns.},
booktitle = {Proceedings of the Internet Measurement Conference},
pages = {73–86},
numpages = {14},
keywords = {Cryptocurrency, Cybercrime, Malware, Mining, Monero},
location = {Amsterdam, Netherlands},
series = {IMC '19}
}

@inproceedings{10.1145/3468264.3468563,
author = {Yin, Likang and Chen, Zhuangzhi and Xuan, Qi and Filkov, Vladimir},
title = {Sustainability forecasting for Apache incubator projects},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468563},
doi = {10.1145/3468264.3468563},
abstract = {Although OSS development is very popular, ultimately more than 80% of OSS projects fail. Identifying the factors associated with OSS success can help in devising interventions when a project takes a downturn. OSS success has been studied from a variety of angles, more recently in empirical studies of large numbers of diverse projects, using proxies for sustainability, e.g., internal metrics related to productivity and external ones, related to community popularity. The internal socio-technical structure of projects has also been shown important, especially their dynamics. This points to another angle on evaluating software success, from the perspective of self-sustaining and self-governing communities. To uncover the dynamics of how a project at a nascent development stage gradually evolves into a sustainable one, here we apply a socio-technical network modeling perspective to a dataset of Apache Software Foundation Incubator (ASFI), sustainability-labeled projects. To identify and validate the determinants of sustainability, we undertake a mix of quantitative and qualitative studies of ASFI projects’ socio-technical network trajectories. We develop interpretable models which can forecast a project becoming sustainable with 93+% accuracy, within 8 months of incubation start. Based on the interpretable models we describe a strategy for real-time monitoring and suggesting actions, which can be used by projects to correct their sustainability trajectories.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1056–1067},
numpages = {12},
keywords = {Apache Incubator, OSS Sustainability, Sociotechnical System},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3377813.3381348,
author = {Wang, Pei and Ding, Yu and Sun, Mingshen and Wang, Huibo and Li, Tongxin and Zhou, Rundong and Chen, Zhaofeng and Jing, Yiming},
title = {Building and maintaining a third-party library supply chain for productive and secure SGX enclave development},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381348},
doi = {10.1145/3377813.3381348},
abstract = {The big data industry is facing new challenges as concerns about privacy leakage soar. One of the remedies to privacy breach incidents is to encapsulate computations over sensitive data within hardware-assisted Trusted Execution Environments (TEE). Such TEE-powered software is called secure enclaves. Secure enclaves hold various advantages against competing for privacy-preserving computation solutions. However, enclaves are much more challenging to build compared with ordinary software. The reason is that the development of TEE software must follow a restrictive programming model to make effective use of strong memory encryption and segregation enforced by hardware. These constraints transitively apply to all third-party dependencies of the software. If these dependencies do not officially support TEE hardware, TEE developers have to spend additional engineering effort in porting them. High development and maintenance cost is one of the major obstacles against adopting TEE-based privacy protection solutions in production.In this paper, we present our experience and achievements with regard to constructing and continuously maintaining a third-party library supply chain for TEE developers. In particular, we port a large collection of Rust third-party libraries into Intel SGX, one of the most mature trusted computing platforms. Our supply chain accepts upstream patches in a timely manner with SGX-specific security auditing. We have been able to maintain the SGX ports of 159 open-source Rust libraries with reasonable operational costs. Our work can effectively reduce the engineering cost of developing SGX enclaves for privacy-preserving data processing and exchange.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {100–109},
numpages = {10},
keywords = {SGX, privacy-preserving computation, rust, software supply chain, third-party library},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.5555/3535850.3535999,
author = {Viano, Luca and Huang, Yu-Ting and Kamalaruban, Parameswaran and Innes, Craig and Ramamoorthy, Subramanian and Weller, Adrian},
title = {Robust Learning from Observation with Model Misspecification},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Imitation learning (IL) is a popular paradigm for training policies in robotic systems when specifying the reward function is difficult. However, despite the success of IL algorithms, they impose the somewhat unrealistic requirement that the expert demonstrations must come from the same domain in which a new imitator policy is to be learned. We consider a practical setting, where (i) state-only expert demonstrations from the real (deployment) environment are given to the learner, (ii) the imitation learner policy is trained in a simulation (training) environment whose transition dynamics is slightly different from the real environment, and (iii) the learner does not have any access to the real environment during the training phase beyond the batch of demonstrations given. Most of the current IL methods, such as generative adversarial imitation learning and its state-only variants, fail to imitate the optimal expert behavior under the above setting. By leveraging insights from the Robust reinforcement learning (RL) literature and building on recent adversarial imitation approaches, we propose a robust IL algorithm to learn policies that can effectively transfer to the real environment without fine-tuning. Furthermore, we empirically demonstrate on continuous-control benchmarks that our method outperforms the state-of-the-art state-only IL method in terms of the zero-shot transfer performance in the real environment and robust performance under different testing conditions.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1337–1345},
numpages = {9},
keywords = {imitation learning, learning from observation, robust reinforcement learning, sim-to-real transfer},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3544548.3581456,
author = {Garg, Kapil and Gergle, Darren and Zhang, Haoqi},
title = {Orchestration Scripts: A System for Encoding an Organization’s Ways of Working to Support Situated Work},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581456},
doi = {10.1145/3544548.3581456},
abstract = {Ill-structured problems demand that people adopt sophisticated strategies for planning, seeking support, and using available resources along their work process. These practices involve a challenging monitoring and strategizing process that existing tools cannot support since they largely lack an understanding of an organization’s processes, social structures, venues, and tools. We introduce workplace programming for situationally-aware systems–an approach for encoding work situations using computational abstractions of an organization’s ways of working and surfacing support strategies at appropriate times and settings. With this approach, we implement Orchestration Scripts, a system that supports various situated work activities in a socio-technical organization. Through a case study and field study, we show how our approach encodes different aspects of working effectively and helps people identify situations to enact effective strategies using the available support opportunities. Our results show how a programmable technology can provide situated support in today’s workplaces.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {418},
numpages = {17},
keywords = {Ill-Structured Problems, Orchestration Scripts, Organizational Objects, Situated Work, Socio-Technical Ecosystems},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3613905.3650841,
author = {Brachman, Michelle and El-Ashry, Amina and Dugan, Casey and Geyer, Werner},
title = {How Knowledge Workers Use and Want to Use LLMs in an Enterprise Context},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650841},
doi = {10.1145/3613905.3650841},
abstract = {Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers’ current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We discuss implications for adoption and design of generative AI technologies for knowledge work.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {189},
numpages = {8},
keywords = {adoption, knowledge workers, large language models, survey},
location = {
},
series = {CHI EA '24}
}

@article{10.1145/3379542,
author = {Jiang, Xingbin and Lora, Michele and Chattopadhyay, Sudipta},
title = {An Experimental Analysis of Security Vulnerabilities in Industrial IoT Devices},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3379542},
doi = {10.1145/3379542},
abstract = {The revolutionary development of the Internet of Things has triggered a huge demand for Internet of Things devices. They are extensively applied to various fields of social activities, and concerning manufacturing, they are a key enabling concept for the Industry 4.0 ecosystem. Industrial Internet of Things (IIoT) devices share common vulnerabilities with standard IoT devices, which are increasingly exposed to the attackers. As such, connected industrial devices may become sources of cyber, as well as physical, threats for people and assets in industrial environments.In this work, we examine the attack surfaces of a networked embedded system, composed of devices representative of those typically used in the IIoT field. We carry on an analysis of the current state of the security of IIoT technologies. The analysis guides the identification of a set of attack vectors for the examined networked embedded system. We set up the corresponding concrete attack scenarios to gain control of the system actuators and perform some hazardous operations. In particular, we propose a couple of variations of Mirai attack specifically tailored for attacking industrial environments. Finally, we discuss some possible},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {16},
numpages = {24},
keywords = {Industrial IoT security, mirai attack, stealthy attack}
}

@proceedings{10.1145/3615335,
title = {SIGDOC '23: Proceedings of the 41st ACM International Conference on Design of Communication},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/3613904.3642333,
author = {Schaffner, Brennan and Bhagoji, Arjun Nitin and Cheng, Siyuan and Mei, Jacqueline and Shen, Jay L and Wang, Grace and Chetty, Marshini and Feamster, Nick and Lakier, Genevieve and Tan, Chenhao},
title = {"Community Guidelines Make this the Best Party on the Internet": An In-Depth Study of Online Platforms' Content Moderation Policies},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642333},
doi = {10.1145/3613904.3642333},
abstract = {Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {486},
numpages = {16},
keywords = {content moderation, dataset, qualitative analysis, quantitative analysis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3582572,
author = {Guo, Zhaoqiang and Liu, Shiran and Liu, Xutong and Lai, Wei and Ma, Mingliang and Zhang, Xu and Ni, Chao and Yang, Yibiao and Li, Yanhui and Chen, Lin and Zhou, Guoqiang and Zhou, Yuming},
title = {Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582572},
doi = {10.1145/3582572},
abstract = {Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {102},
numpages = {55},
keywords = {Code line, bugginess, defect prediction, quality assurance, static analysis tool}
}

@inproceedings{10.1145/3491102.3517446,
author = {Gamage, Dilrukshi and Ghasiya, Piyush and Bonagiri, Vamshi and Whiting, Mark E. and Sasahara, Kazutoshi},
title = {Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517446},
doi = {10.1145/3491102.3517446},
abstract = {Deepfakes are synthetic content generated using advanced deep learning and AI technologies. The advancement of technology has created opportunities for anyone to create and share deepfakes much easier. This may lead to societal concerns based on how communities engage with it. However, there is limited research available to understand how communities perceive deepfakes. We examined deepfake conversations on Reddit from 2018 to 2021—including major topics and their temporal changes as well as implications of these conversations. Using a mixed-method approach—topic modeling and qualitative coding, we found 6,638 posts and 86,425 comments discussing concerns of the believable nature of deepfakes and how platforms moderate them. We also found Reddit conversations to be pro-deepfake and building a community that supports creating and sharing deepfake artifacts and building a marketplace regardless of the consequences. Possible implications derived from qualitative codes indicate that deepfake conversations raise societal concerns. We propose that there are implications for Human Computer Interaction (HCI) to mitigate the harm created from deepfakes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {103},
numpages = {19},
keywords = {content analysis, deepfake, societal implication, topic modeling},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3143560,
author = {Wasik, Szymon and Antczak, Maciej and Badura, Jan and Laskowski, Artur and Sternal, Tomasz},
title = {A Survey on Online Judge Systems and Their Applications},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3143560},
doi = {10.1145/3143560},
abstract = {Online judges are systems designed for the reliable evaluation of algorithm source code submitted by users, which is next compiled and tested in a homogeneous environment. Online judges are becoming popular in various applications. Thus, we would like to review the state of the art for these systems. We classify them according to their principal objectives into systems supporting organization of competitive programming contests, enhancing education and recruitment processes, facilitating the solving of data mining challenges, online compilers and development platforms integrated as components of other custom systems. Moreover, we introduce a formal definition of an online judge system and summarize the common evaluation methodology supported by such systems. Finally, we briefly discuss an Optil.io platform as an example of an online judge system, which has been proposed for the solving of complex optimization problems. We also analyze the competition results conducted using this platform. The competition proved that online judge systems, strengthened by crowdsourcing concepts, can be successfully applied to accurately and efficiently solve complex industrial- and science-driven challenges.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {3},
numpages = {34},
keywords = {Online judge, challenge, contest, crowdsourcing, evaluation as a service}
}

@inproceedings{10.1145/3548606.3560680,
author = {Jin, Ze and Xing, Luyi and Fang, Yiwei and Jia, Yan and Yuan, Bin and Liu, Qixu},
title = {P-Verifier: Understanding and Mitigating Security Risks in Cloud-based IoT Access Policies},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560680},
doi = {10.1145/3548606.3560680},
abstract = {Modern IoT device manufacturers are taking advantage of the managed Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) IoT clouds (e.g., AWS IoT, Azure IoT) for secure and convenient IoT development/deployment. The IoT access control is achieved by manufacturer-specified, cloud-enforced IoT access policies (cloud-standard JSON documents, called IoT Policies) stating which users can access which IoT devices/resources under what constraints. In this paper, we performed a systematic study on the security of cloud-based IoT access policies on modern PaaS/IaaS IoT clouds. Our research shows that the complexity in the IoT semantics and enforcement logic of the policies leaves tremendous space for device manufacturers to program a flawed IoT access policy, introducing convoluted logic flaws which are non-trivial to reason about. In addition to challenges/mistakes in the design space, it is astonishing to find that mainstream device manufacturers also generally make critical mistakes in deploying IoT Policies thanks to the flexibility offered by PaaS/IaaS clouds and the lack of standard practices for doing so. Our assessment of 36 device manufacturers and 310 open-source IoT projects highlights the pervasiveness and seriousness of the problems, which once exploited, can have serious impacts on IoT users' security, safety, and privacy. To help manufacturers identify and easily fix IoT Policy flaws, we introduce P-Verifier, a formal verification tool that can automatically verify cloud-based IoT Policies. With evaluated high effectiveness and low performance overhead, P-Verifier will contribute to elevating security assurance in modern IoT deployments and access control. We responsibly reported all findings to affected vendors and fixes were deployed or on the way.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1647–1661},
numpages = {15},
keywords = {access control policy, cloud., formal verification, iot},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3631802.3631806,
author = {Kazemitabaar, Majeed and Hou, Xinying and Henley, Austin and Ericson, Barbara Jane and Weintrop, David and Grossman, Tovi},
title = {How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631806},
doi = {10.1145/3631802.3631806},
abstract = {As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners’ utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners’ use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {3},
numpages = {12},
keywords = {ChatGPT, Copilot, Introductory Programming, Large Language Models, OpenAI Codex, Self-paced Learning, Self-regulation},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@article{10.1145/3571282,
author = {Strasser, Ben and Zeitz, Tim},
title = {Using Incremental Many-to-One Queries to Build a Fast and Tight Heuristic for A* in Road Networks},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
issn = {1084-6654},
url = {https://doi.org/10.1145/3571282},
doi = {10.1145/3571282},
abstract = {We study exact, efficient, and practical algorithms for route planning applications in large road networks. On the one hand, such algorithms should be able to answer shortest path queries within milliseconds. On the other hand, routing applications often require integrating the current traffic situation, planning ahead with predictions for future traffic, respecting forbidden turns, and many other features depending on the specific application. Therefore, such algorithms must be flexible and able to support a variety of problem variants. In this work, we revisit the A* algorithm to build a simple, extensible, and unified algorithmic framework applicable to many route planning problems. A* has been previously used for routing in road networks. However, its performance was not competitive because no sufficiently fast and tight distance estimation function was available. We present a novel, efficient, and accurate A* heuristic using Contraction Hierarchies, another popular speedup technique. The core of our heuristic is a new Contraction Hierarchies query algorithm called Lazy RPHAST, which can efficiently compute shortest distances from many incrementally provided sources toward a common target. Additionally, we describe A* optimizations to accelerate the processing of low-degree vertices, which are typical in road networks, and present a new pruning criterion for symmetrical bidirectional A*. An extensive experimental study confirms the practicality of our approach for many applications.},
journal = {ACM J. Exp. Algorithmics},
month = {mar},
articleno = {4.6},
numpages = {28},
keywords = {Route planning, shortest paths, realistic road networks}
}

@inproceedings{10.1145/3639474.3640061,
author = {Frankford, Eduard and Sauerwein, Clemens and Bassner, Patrick and Krusche, Stephan and Breu, Ruth},
title = {AI-Tutoring in Software Engineering Education},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640061},
doi = {10.1145/3639474.3640061},
abstract = {With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {309–319},
numpages = {11},
keywords = {programming education, automated programming assessment systems, artificial intelligence, ChatGPT, OpenAI, ChatBots},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@article{10.1145/3555221,
author = {Mendes, Wendy and Richard, Albert and Tillo, T\"{a}he-Kai and Pinto, Gustavo and Gama, Kiev and Nolte, Alexander},
title = {Socio-technical Constraints and Affordances of Virtual Collaboration - A Study of Four Online Hackathons},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555221},
doi = {10.1145/3555221},
abstract = {Hackathons and similar time-bounded events have become a popular form of collaboration in various domains. They are commonly organized as in-person events during which teams engage in intense collaboration over a short period of time to complete a project that is of interest to them. Most research to date has thus consequently focused on studying how teams collaborate in a co-located setting, pointing towards the advantages of radical co-location. The global pandemic of 2020, however, has led to many hackathons moving online, which challenges our current understanding of how they function. In this paper, we address this gap by presenting findings from a multiple-case study of 10 hackathon teams that participated in 4 hackathon events across two continents. By analyzing the collected data, we found that teams merged synchronous and asynchronous means of communication to maintain a common understanding of work progress as well as to maintain awareness of each other's tasks. Task division was self-assigned based on individual skills or interests, while leaders emerged from different strategies (e.g., participant experience, the responsibility of registering the team in an event). Some of the affordances of in-person hackathons, such as the radical co-location of team members, could be partially reproduced in teams that kept open synchronous communication channels while working (i.e., shared audio territories), in a sort of "radical virtual co-location". However, others, such as interactions with other teams, easy access to mentors, and networking with other participants, decreased. In addition, the technical constraints of the different communication tools and platforms brought technical problems and were overwhelming to participants. Our work contributes to understanding the virtual collaboration of small teams in the context of online hackathons and how technologies and event structures proposed by organizers imply this collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {330},
numpages = {32},
keywords = {online hackathons, time-bounded collaborative events}
}

@inproceedings{10.1145/2983990.2984042,
author = {Haller, Philipp and Loiko, Alex},
title = {LaCasa: lightweight affinity and object capabilities in Scala},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984042},
doi = {10.1145/2983990.2984042},
abstract = {Aliasing is a known source of challenges in the context of imperative object-oriented languages, which have led to important advances in type systems for aliasing control. However, their large-scale adoption has turned out to be a surprisingly difficult challenge. While new language designs show promise, they do not address the need of aliasing control in existing languages.  This paper presents a new approach to isolation and uniqueness in an existing, widely-used language, Scala. The approach is unique in the way it addresses some of the most important obstacles to the adoption of type system extensions for aliasing control. First, adaptation of existing code requires only a minimal set of annotations. Only a single bit of information is required per class. Surprisingly, the paper shows that this information can be provided by the object-capability discipline, widely-used in program security. We formalize our approach as a type system and prove key soundness theorems. The type system is implemented for the full Scala language, providing, for the first time, a sound integration with Scala's local type inference. Finally, we empirically evaluate the conformity of existing Scala open-source code on a corpus of over 75,000 LOC.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {272–291},
numpages = {20},
keywords = {Aliasing, Scala, object capabilities, uniqueness},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@proceedings{10.1145/3660650,
title = {WCCCE '24: Proceedings of the 26th Western Canadian Conference on Computing Education},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kelowna, BC, Canada}
}

@inproceedings{10.1109/ICSE48619.2023.00076,
author = {Li, Jiawei and Ahmed, Iftekhar},
title = {Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00076},
doi = {10.1109/ICSE48619.2023.00076},
abstract = {Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {806–817},
numpages = {12},
keywords = {commit message quality, software defect proneness, empirical analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3648441,
author = {Stein, Benno and Chang, Bor-Yuh Evan and Sridharan, Manu},
title = {Interactive Abstract Interpretation with Demanded Summarization},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3648441},
doi = {10.1145/3648441},
abstract = {We consider the problem of making expressive, interactive static analyzers compositional. Such a technique could help bring the power of server-based static analyses to integrated development environments (IDEs), updating their results live as the code is modified. Compositionality is key for this scenario, as it enables reuse of already-computed analysis results for unmodified code. Previous techniques for interactive static analysis either lack compositionality, cannot express arbitrary abstract domains, or are not from-scratch consistent.We present demanded summarization, the first algorithm for incremental compositional analysis in arbitrary abstract domains that guarantees from-scratch consistency. Our approach analyzes individual procedures using a recent technique for demanded analysis, computing summaries on demand for procedure calls. A dynamically updated summary dependency graph enables precise result invalidation after program edits, and the algorithm is carefully designed to guarantee from-scratch-consistent results after edits, even in the presence of recursion and in arbitrary abstract domains. We formalize our technique and prove soundness, termination, and from-scratch consistency. An experimental evaluation of a prototype implementation on synthetic and real-world program edits provides evidence for the feasibility of this theoretical framework, showing potential for major performance benefits over non-demanded compositional analyses.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {mar},
articleno = {4},
numpages = {40},
keywords = {Abstract interpretation, Incremental computation}
}

@inproceedings{10.1145/3434074.3444876,
author = {Goedicke, David and Tennent, Hamish and Moore, Dylan and Ju, Wendy},
title = {Acoustically Aware Robots: Detecting and Evaluating Sounds Robots Make and Hear},
year = {2021},
isbn = {9781450382908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434074.3444876},
doi = {10.1145/3434074.3444876},
abstract = {The sound a robot or automated system makes and the sounds it listens for in our shared acoustic environment can greatly expand its contextual understanding and to shape its behaviors to the interactions it is trying to perform.People convey significant information with sound in interpersonal communication in social contexts. Para-linguistic information about where we are, how loud we're speaking, or if we sound happy, sad or upset are relevant to understand for a robot that looks to adapt its interactions to be socially appropriate.Similarly, the qualities of the sound an object makes can change how people perceive that object and can alter whether or not it attracts attention, interrupts other interactions, reinforces or contradicts an emotional expression, and as such should be aligned with the designer's intention for the object. In this tutorial, we will introduce the participants to software and design methods to help robots recognize and generate sound for human-robot interaction (HRI). Using open-source tools and methods designers can apply to their own robots, we seek to increase the application of sound to robot design and stimulate HRI research in robot sound.},
booktitle = {Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {697–699},
numpages = {3},
keywords = {acoustic scene analysis, audio, audio, sound recognition, consequential sounds, human robot interaction, non-verbal},
location = {Boulder, CO, USA},
series = {HRI '21 Companion}
}

@inproceedings{10.1145/3468264.3468582,
author = {Liu, Jiakun and Baltes, Sebastian and Treude, Christoph and Lo, David and Zhang, Yun and Xia, Xin},
title = {Characterizing search activities on stack overflow},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468582},
doi = {10.1145/3468264.3468582},
abstract = {To solve programming issues, developers commonly search on Stack Overflow to seek potential solutions. However, there is a gap between the knowledge developers are interested in and the knowledge they are able to retrieve using search engines. To help developers efficiently retrieve relevant knowledge on Stack Overflow, prior studies proposed several techniques to reformulate queries and generate summarized answers. However, few studies performed a large-scale analysis using real-world search logs. In this paper, we characterize how developers search on Stack Overflow using such logs. By doing so, we identify the challenges developers face when searching on Stack Overflow and seek opportunities for the platform and researchers to help developers efficiently retrieve knowledge. To characterize search activities on Stack Overflow, we use search log data based on requests to Stack Overflow's web servers. We find that the most common search activity is reformulating the immediately preceding queries. Related work looked into query reformulations when using generic search engines and found 13 types of query reformulation strategies. Compared to their results, we observe that 71.78% of the reformulations can be fitted into those reformulation strategies. In terms of how queries are structured, 17.41% of the search sessions only search for fragments of source code artifacts (e.g., class and method names) without specifying the names of programming languages, libraries, or frameworks. Based on our findings, we provide actionable suggestions for Stack Overflow moderators and outline directions for future research. For example, we encourage Stack Overflow to set up a database that includes the relations between all computer programming terminologies shared on Stack Overflow, e.g., method name, data structure name, design pattern, and IDE name. By doing so, Stack Overflow could improve the performance of search engines by considering related programming terminologies at different levels of granularity.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {919–931},
numpages = {13},
keywords = {Data Mining, Query Logs, Query Reformulation, Stack Overflow},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3590777.3590790,
author = {Heid, Kris and Andrae, Vincent and Heider, Jens},
title = {Towards detecting device fingerprinting on iOS with API function hooking},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590777.3590790},
doi = {10.1145/3590777.3590790},
abstract = {Device fingerprinting is a technique that got popular at the end of the 90s by websites, to identify and track users. One of the biggest drivers behind such practices are advertising companies to identify users interests to personalize ads. From a user’s perspective, this, of course, raises privacy concerns. While device fingerprinting and its detection has been extensively studied in the context of web browsing, little research has been conducted on device fingerprinting in mobile apps and especially iOS apps. In this paper, we capture the current state of device fingerprinting in iOS apps, and explore possible approaches for fingerprinting detection on mobile devices using static and dynamic app analysis techniques. Finally, we present a first heuristic approach for automatic behavior-based fingerprinting detection on iOS only using spatial and temporal context of relevant API-calls.},
booktitle = {Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
pages = {78–84},
numpages = {7},
keywords = {dynamic analysis, fingerprinting, iOS, privacy, static analysis},
location = {Stavanger, Norway},
series = {EICC '23}
}

@inproceedings{10.1109/ICSE48619.2023.00117,
author = {Wang, Chengpeng and Fan, Gang and Yao, Peisen and Pan, Fuxiong and Zhang, Charles},
title = {Verifying Data Constraint Equivalence in FinTech Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00117},
doi = {10.1109/ICSE48619.2023.00117},
abstract = {Data constraints are widely used in FinTech systems for monitoring data consistency and diagnosing anomalous data manipulations. However, many equivalent data constraints are created redundantly during the development cycle, slowing down the FinTech systems and causing unnecessary alerts. We present EqDAC, an efficient decision procedure to determine the data constraint equivalence. We first propose the symbolic representation for semantic encoding and then introduce two light-weighted analyses to refute and prove the equivalence, respectively, which are proved to achieve in polynomial time. We evaluate EqDAC upon 30,801 data constraints in a FinTech system. It is shown that EqDAC detects 11,538 equivalent data constraints in three hours. It also supports efficient equivalence searching with an average time cost of 1.22 seconds, enabling the system to check new data constraints upon submission.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1329–1341},
numpages = {13},
keywords = {equivalence verification, data constraints, fin-tech systems},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3544548.3580919,
author = {Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi},
title = {Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580919},
doi = {10.1145/3544548.3580919},
abstract = {AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {455},
numpages = {23},
keywords = {AI Coding Assistants, AI-Assisted Pair-Programming, ChatGPT, Copilot, GPT-3, Introductory Programming, K-12 Computer Science Education, Large Language Models, OpenAI Codex},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3540250.3549127,
author = {Robe, Peter and Kuttal, Sandeep K. and AuBuchon, Jake and Hart, Jacob},
title = {Pair programming conversations with agents vs. developers: challenges and opportunities for SE community},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549127},
doi = {10.1145/3540250.3549127},
abstract = {Recent research has shown feasibility of an interactive pair-programming conversational agent, but implementing such an agent poses three challenges: a lack of benchmark datasets, absence of software engineering specific labels, and the need to understand developer conversations. To address these challenges, we conducted a Wizard of Oz study with 14 participants pair programming with a simulated agent and collected 4,443 developer-agent utterances. Based on this dataset, we created 26 software engineering labels using an open coding process to develop a hierarchical classification scheme. To understand labeled developer-agent conversations, we compared the accuracy of three state-of-the-art transformer-based language models, BERT, GPT-2, and XLNet, which performed interchangeably. In order to begin creating a developer-agent dataset, researchers and practitioners need to conduct resource intensive Wizard of Oz studies. Presently, there exists vast amounts of developer-developer conversations on video hosting websites. To investigate the feasibility of using developer-developer conversations, we labeled a publicly available developer-developer dataset (3,436 utterances) with our hierarchical classification scheme and found that a BERT model trained on developer-developer data performed ~10% worse than the BERT trained on developer-agent data, but when using transfer-learning, accuracy improved. Finally, our qualitative analysis revealed that developer-developer conversations are more implicit, neutral, and opinionated than developer-agent conversations. Our results have implications for software engineering researchers and practitioners developing conversational agents.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {319–331},
numpages = {13},
keywords = {Classification, Conversational agents, Datasets, Labels, Language models, Pair programming conversations, Pair programming questions},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3167082,
author = {Watt, Conrad},
title = {Mechanising and verifying the WebAssembly specification},
year = {2018},
isbn = {9781450355865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167082},
doi = {10.1145/3167082},
abstract = {WebAssembly is a new low-level language currently being implemented in all major web browsers. It is designed to become the universal compilation target for the web, obsoleting existing solutions in this area, such as asm.js and Native Client. The WebAssembly working group has incorporated formal techniques into the development of the language, but their efforts so far have focussed on pen and paper formal specification.We present a mechanised Isabelle specification for the WebAssembly language, together with a verified executable interpreter and type checker. Moreover, we present a fully mechanised proof of the soundness of the WebAssembly type system, and detail how our work on this proof has exposed several issues with the official WebAssembly specification, influencing its development. Finally, we give a brief account of our efforts in performing differential fuzzing of our interpreter against industry implementations.},
booktitle = {Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {53–65},
numpages = {13},
keywords = {bytecode, reduction, soundness, stack machine},
location = {Los Angeles, CA, USA},
series = {CPP 2018}
}

@inproceedings{10.1145/3647782.3647803,
author = {Kukreja, Sanjay and Kumar, Tarun and Purohit, Amit and Dasgupta, Abhijit and Guha, Debashis},
title = {A Literature Survey on Open Source Large Language Models},
year = {2024},
isbn = {9798400716652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647782.3647803},
doi = {10.1145/3647782.3647803},
abstract = {Since the 1950s, post the Turing test, humans have been striving hard to make machines learn the art of mastering linguistic intelligence. Language being a complex and intricate tool of expression used by humans, poses a large number of challenges for AI enabled algorithms to grasp its understanding in entirety. Over the past few years, a chain of efforts have been made to make machines understand linguistic intricacies. Small scale models such as BERT and pre-trained language models (PLMs) have demonstrated strong capabilities in understanding and solving various language based tasks. Over the period of years, it is also observed that by increasing the parameters scale to larger size, large language models show a significant improvement in performance and showcase abilities to understand context. For the PLMs of a humongous size i.e in the tune of tens or hundreds of billions of parameters, and to understand the large parametric scales, the scientific community introduced the term LLMs - large language models. The whole world witnessed the launch and quick adoption of ChatGPT, an AI chatbot built on LLMs. As the usage of AI algorithms changes the way the scientific community, society and industry works, it is imperative to review the advances of LLMs. Since 2022, almost daily nearly a dozen LLMs are released. These LLMs are categorized as open and closed source. This paper aims to focus on major aspects of open source LLMs - pre-training covering data collection and pre-processing, model architecture and training. We will select open source models released in June, July and August 2023 with training parameters greater than 70 billion parameters and provide a comprehensive survey on the mentioned aspects. As new models are released on daily / weekly basis in the LLM space, in order to keep the survey concise and targeted to important models, we chose to select time-box of 3 months and a large parameter range of 70 billion in our literature survey. We will also cover historical evolution of LLMs and list open items for future directions.},
booktitle = {Proceedings of the 2024 7th International Conference on Computers in Management and Business},
pages = {133–143},
numpages = {11},
keywords = {Generative AI, Large Language Models, Open Source LLMs},
location = {Singapore, Singapore},
series = {ICCMB '24}
}

@inproceedings{10.1145/3377811.3380361,
author = {Hoang, Thong and Kang, Hong Jin and Lo, David and Lawall, Julia},
title = {CC2Vec: distributed representations of code changes},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380361},
doi = {10.1145/3377811.3380361},
abstract = {Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {518–529},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3377811.3380920,
author = {Tan, Xin and Zhou, Minghui and Fitzgerald, Brian},
title = {Scaling open source communities: an empirical study of the Linux kernel},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380920},
doi = {10.1145/3377811.3380920},
abstract = {Large-scale open source communities, such as the Linux kernel, have gone through decades of development, substantially growing in scale and complexity. In the traditional workflow, maintainers serve as "gatekeepers" for the subsystems that they maintain. As the number of patches and authors significantly increases, maintainers come under considerable pressure, which may hinder the operation and even the sustainability of the community. A few subsystems have begun to use new workflows to address these issues. However, it is unclear to what extent these new workflows are successful, or how to apply them. Therefore, we conduct an empirical study on the multiple-committer model (MCM) that has provoked extensive discussion in the Linux kernel community. We explore the effect of the model on the i915 subsystem with respect to four dimensions: pressure, latency, complexity, and quality assurance. We find that after this model was adopted, the burden of the i915 maintainers was significantly reduced. Also, the model scales well to allow more committers. After analyzing the online documents and interviewing the maintainers of i915, we propose that overloaded subsystems which have trustworthy candidate committers are suitable for adopting the model. We further suggest that the success of the model is closely related to a series of measures for risk mitigation---sufficient precommit testing, strict review process, and the use of tools to simplify work and reduce errors. We employ a network analysis approach to locate candidate committers for the target subsystems and validate this approach and contextual success factors through email interviews with their maintainers. To the best of our knowledge, this is the first study focusing on how to scale open source communities. We expect that our study will help the rapidly growing Linux kernel and other similar communities to adapt to changes and remain sustainable.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1222–1234},
numpages = {13},
keywords = {Linux kernel, maintainer, multiple committers, open source communities, scalability, sustainability, workload},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/ICSE-NIER.2019.00014,
author = {Stephan, Matthew},
title = {Towards a cognizant virtual software modeling assistant using model clones},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00014},
doi = {10.1109/ICSE-NIER.2019.00014},
abstract = {We present our new ideas on taking the first steps towards cultivating synergy between model-driven engineering (MDE), machine learning, and software clones. Specifically, we describe our vision in realizing a cognizant virtual software modeling assistant that uses the latter two to improve software design and MDE. Software engineering has benefited greatly from knowledge-based cognizant source code completion and assistance, but MDE has few and limited analogous capabilities. We outline our research directions by describing our vision for a prototype assistant that provides suggestions to modelers performing model creation or extension in the form of 1) complete models for insertion or guidance, and 2) granular single-step operations. These suggestions are derived by detecting clones of the in-progress model and existing domain, organizational, and exemplar models. We overview our envisioned workflow between modeler and assistant, and, using Simulink as an example, illustrate different manifestations including multiple overlays with percentages and employing variant elements.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {21–24},
numpages = {4},
keywords = {machine learning, model clone detection, model clones, model driven engineering, software modeling},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/2997364.2997371,
author = {K\"{u}hn, Thomas and Bierzynski, Kay and Richly, Sebastian and A\ss{}mann, Uwe},
title = {FRaMED: full-fledge role modeling editor (tool demo)},
year = {2016},
isbn = {9781450344470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2997364.2997371},
doi = {10.1145/2997364.2997371},
abstract = {Since the year 1977, role modeling has been continuously investigated as promising paradigm to model complex, dynamic systems. However, this research had almost no influence on the design of todays increasingly complex and context-sensitive software systems. The reason for that is twofold. First, most modeling languages focused either on the behavioral, relational or context-dependent nature of roles rather than combining them. Second, there is a lack of tool support for the design, validation, and generation of role-based software systems. In particular, there exists no graphical role modeling editor supporting the three natures as well as the various proposed constraints. To overcome this deficiency, we introduce the Full-fledged Role Modeling Editor (FRaMED), a graphical modeling editor embracing all natures of roles and modeling constraints featuring generators for a formal representation and source code of a role-based programming language. To show its applicability for the development of role-based software systems, an example from the banking domain is employed.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {132–136},
numpages = {5},
keywords = {Role-based Modeling},
location = {Amsterdam, Netherlands},
series = {SLE 2016}
}

@article{10.1145/3453478,
author = {Dilhara, Malinda and Ketkar, Ameya and Dig, Danny},
title = {Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3453478},
doi = {10.1145/3453478},
abstract = {Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries.We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings.Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {55},
numpages = {42},
keywords = {Machine learning libraries, Software-2.0, empirical studies}
}

@inproceedings{10.1145/3597503.3639197,
author = {Zhang, Yuxia and Qin, Mian and Stol, Klaas-Jan and Zhou, Minghui and Liu, Hui},
title = {How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639197},
doi = {10.1145/3597503.3639197},
abstract = {It is now commonplace for organizations to pay developers to work on specific open source software (OSS) projects to pursue their business goals. Such paid developers work alongside voluntary contributors, but given the different motivations of these two groups of developers, conflict may arise, which may pose a threat to a project's sustainability. This paper presents an empirical study of paid developers and volunteers in Rust, a popular open source programming language project. Rust is a particularly interesting case given considerable concerns about corporate participation. We compare volunteers and paid developers through contribution characteristics and long-term participation, and solicit volunteers' perceptions on paid developers. We find that core paid developers tend to contribute more frequently; commits contributed by onetime paid developers have bigger sizes; peripheral paid developers implement more features; and being paid plays a positive role in becoming a long-term contributor. We also find that volunteers do have some prejudices against paid developers. This study suggests that the dichotomous view of paid vs. volunteer developers is too simplistic and that further subgroups can be identified. Companies should become more sensitive to how they engage with OSS communities, in certain ways as suggested by this study.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {195},
numpages = {13},
keywords = {open source software, paid developers, volunteers, sustainability},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3276532,
author = {Alimadadi, Saba and Zhong, Di and Madsen, Magnus and Tip, Frank},
title = {Finding broken promises in asynchronous JavaScript programs},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276532},
doi = {10.1145/3276532},
abstract = {Recently, promises were added to ECMAScript 6, the JavaScript standard, in order to provide better support for the asynchrony that arises in user interfaces, network communication, and non-blocking I/O. Using promises, programmers can avoid common pitfalls of event-driven programming such as event races and the deeply nested counterintuitive control ow referred to as “callback hell”. Unfortunately, promises have complex semantics and the intricate control– and data- ow present in promise-based code hinders program comprehension and can easily lead to bugs. The promise graph was proposed as a graphical aid for understanding and debugging promise-based code. However, it did not cover all promise-related features in ECMAScript 6, and did not present or evaluate any technique for constructing the promise graphs. In this paper, we extend the notion of promise graphs to include all promise-related features in ECMAScript 6, including default reactions, exceptions, and the synchronization operations race and all. Furthermore, we report on the construction and evaluation of PromiseKeeper, which performs a dynamic analysis to create promise graphs and infer common promise anti-patterns. We evaluate PromiseKeeper by applying it to 12 open source promise-based Node.js applications. Our results suggest that the promise graphs constructed by PromiseKeeper can provide developers with valuable information about occurrences of common anti-patterns in their promise-based code, and that promise graphs can be constructed with acceptable run-time overhead.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {162},
numpages = {26},
keywords = {Dynamic Analysis, JavaScript, Promise Graph, PromiseKeeper, Promises}
}

@inproceedings{10.1145/3526071.3527515,
author = {Stadler, Marco and Vierhauser, Michael and Cleland-Huang, Jane},
title = {Towards flexible runtime monitoring support for ROS-based applications},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527515},
doi = {10.1145/3526071.3527515},
abstract = {Robotic systems are becoming common in different domains and for various purposes, such as unmanned aerial vehicles performing search and rescue operations, or robots operating in manufacturing plants. Such systems are characterized by close interactions, or even collaborations, between hardware and machinery on the one hand, and humans on the other. Furthermore, as Cyber-Physical Systems (CPS) in general and robotic applications in particular typically operate in an emergent environment, unanticipated events may occur during their operation, making the need for runtime monitoring support a crucial yet often time-consuming task. Runtime monitoring typically requires establishing support for collecting data, aggregating and transporting the data to a monitoring framework for persistence and further processing, and finally, performing checks of functional and non-functional properties. In this paper, we present our initial efforts towards a flexible monitoring framework for ROS-based systems. We report on challenges for establishing runtime monitoring support and present our preliminary architecture that aims to significantly reduce the setup and maintenance effort when creating monitors and establishing constraint checks.},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {43–46},
numpages = {4},
keywords = {ROS, cyber-physical systems, runtime monitoring},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@article{10.1145/3359219,
author = {Crowston, Kevin and Saltz, Jeff S. and Rezgui, Amira and Hegde, Yatish and You, Sangseok},
title = {Socio-technical Affordances for Stigmergic Coordination Implemented in MIDST, a Tool for Data-Science Teams},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359219},
doi = {10.1145/3359219},
abstract = {We present a conceptual framework for socio-technical affordances for stigmergic coordination, that is, coordination supported by a shared work product. Based on research on free/libre open source software development, we theorize that stigmergic coordination depends on three sets of socio-technical affordances: the visibility and combinability of the work, along with defined genres of work contributions. As a demonstration of the utility of the developed framework, we use it as the basis for the design and implementation of a system, MIDST, that supports these affordances and that we thus expect to support stigmergic coordination. We describe an initial assessment of the impact of the tool on the work of project teams of three to six data-science students that suggests that the tool was useful but also in need of further development. We conclude with plans for future research and an assessment of theory-driven system design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {117},
numpages = {25},
keywords = {awareness, data-science teams, stigmergic coordination, translucency}
}

@article{10.1145/3641540,
author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Li, Li},
title = {On the Reliability and Explainability of Language Models for Program Generation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641540},
doi = {10.1145/3641540},
abstract = {Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises a question: are these techniques sufficiently trustworthy for automated program generation? Consequently, further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing overoptimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {126},
numpages = {26},
keywords = {Automated program generation, empirical analysis, explainable AI}
}

@inproceedings{10.1145/3297280.3297467,
author = {Spooren, Jan and Preuveneers, Davy and Desmet, Lieven and Janssen, Peter and Joosen, Wouter},
title = {Detection of algorithmically generated domain names used by botnets: a dual arms race},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297467},
doi = {10.1145/3297280.3297467},
abstract = {Malware typically uses Domain Generation Algorithms (DGAs) as a mechanism to contact their Command and Control server. In recent years, different approaches to automatically detect generated domain names have been proposed, based on machine learning. The first problem that we address is the difficulty to systematically compare these DGA detection algorithms due to the lack of an independent benchmark. The second problem that we investigate is the difficulty for an adversary to circumvent these classifiers when the machine learning models backing these DGA-detectors are known. In this paper we compare two different approaches on the same set of DGAs: classical machine learning using manually engineered features and a 'deep learning' recurrent neural network. We show that the deep learning approach performs consistently better on all of the tested DGAs, with an average classification accuracy of 98.7% versus 93.8% for the manually engineered features. We also show that one of the dangers of manual feature engineering is that DGAs can adapt their strategy, based on knowledge of the features used to detect them. To demonstrate this, we use the knowledge of the used feature set to design a new DGA which makes the random forest classifier powerless with a classification accuracy of 59.9%. The deep learning classifier is also (albeit less) affected, reducing its accuracy to 85.5%.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1916–1923},
numpages = {8},
keywords = {domain generation algorithms, machine learning, malware detection},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3374664.3375741,
author = {Kucuk, Yunus and Yan, Guanhua},
title = {Deceiving Portable Executable Malware Classifiers into Targeted Misclassification with Practical Adversarial Examples},
year = {2020},
isbn = {9781450371070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374664.3375741},
doi = {10.1145/3374664.3375741},
abstract = {Due to voluminous malware attacks in the cyberspace, machine learning has become popular for automating malware detection and classification. In this work we play devil's advocate by investigating a new type of threats aimed at deceiving multi-class Portable Executable (PE) malware classifiers into targeted misclassification with practical adversarial samples. Using a malware dataset with tens of thousands of samples, we construct three types of PE malware classifiers, the first one based on frequencies of opcodes in the disassembled malware code (opcode classifier), the second one the list of API functions imported by each PE sample (API classifier), and the third one the list of system calls observed in dynamic execution (system call classifier). We develop a genetic algorithm augmented with different support functions to deceive these classifiers into misclassifying a PE sample into any target family. Using an Rbot malware sample whose source code is publicly available, we are able to create practical adversarial samples that can deceive the opcode classifier into targeted misclassification with a successful rate of 75%, the API classifier with a successful rate of 83.3%, and the system call classifier with a successful rate of 91.7%.},
booktitle = {Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},
pages = {341–352},
numpages = {12},
keywords = {adversarial machine learning, malware classification},
location = {New Orleans, LA, USA},
series = {CODASPY '20}
}

@inproceedings{10.1145/3640794.3665550,
author = {Zhang, Zhengquan and Tsiakas, Konstantinos and Schneegass, Christina},
title = {Explaining the Wait: How Justifying Chatbot Response Delays Impact User Trust},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665550},
doi = {10.1145/3640794.3665550},
abstract = {In human communication, responding to a question very slowly or quickly influences our trust in the answer. As chatbots evolve to increasingly mimic human speech, response speed can be artificially varied to create certain impressions on users. However, studies remain inconclusive, potentially due to the absence of contextual cues that allow for interpretation of the delay. Thus, this study explores textual explanations that justify the instant and dynamic – dependent on answer length – response delays. We derive five design variations based on prior work and evaluate their impact on the chatbot’s perceived social presence and transparency (N = 10). In a between-subject online study (N = 194), we then evaluate the influence of the highest-rated justification on users’ perceptions of chatbot transparency, social presence, and trust for the two delay conditions. Results demonstrate that while such justifications enhance perceived transparency and trust in the immediate response scenario, they show no effect in the dynamic delay context.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {27},
numpages = {16},
keywords = {Chatbot Response Delay, Explainability, Social Presence, Transparency, Trust},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3600160.3600168,
author = {Verdonck, Jenno and De Boeck, Kevin and Willocx, Michiel and Lapon, Jorn and Naessens, Vincent},
title = {A hybrid anonymization pipeline to improve the privacy-utility balance in sensitive datasets for ML purposes},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3600168},
doi = {10.1145/3600160.3600168},
abstract = {The modern world is data-driven. Businesses increasingly take strategic decisions based on customer data, and companies are founded with a sole focus of performing machine-learning driven data analytics for third parties. External data sources containing sensitive records are often required to build qualitative machine learning models and, hence, perform accurate and meaningful predictions. However, exchanging sensitive datasets is no sinecure. Personal data must be managed according to privacy regulation. Similarly, loss of strategic data can negatively impact the competitiveness of a company. In both cases, dataset anonymization can overcome the aforementioned obstacles. This work proposes a hybrid anonymization pipeline combining masking and (intelligent) sampling to improve the privacy-utility balance of anonymized datasets. The approach is validated via in-depth experiments on a representative machine learning scenario. A quantitative privacy assessment of the proposed hybrid anonymization pipeline is performed and relies on two well-known privacy metrics, namely re-identification risk and certainty. Furthermore, this work shows that the utility level of the anonymized dataset remains acceptable, and that the overall privacy-utility balance increases when complementing masking with intelligent sampling. The study further restrains the common misconception that dataset anonymization is detrimental to the quality of machine learning models. The empirical study shows that anonymous datasets – generated by the hybrid anonymization pipeline – can compete with the original (identifiable) ones when they are used as input for training a machine learning model.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {10},
numpages = {11},
keywords = {Anonymity, ML, privacy, utility},
location = {Benevento, Italy},
series = {ARES '23}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2908131.2908176,
author = {Hooper, Clare J. and Bailey, Brian and Glaser, Hugh and Hendler, James},
title = {Social machines in practice: solutions, stakeholders and scopes},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908176},
doi = {10.1145/2908131.2908176},
abstract = {This paper frames social machines as problem solving entities, demonstrating how their ecosystems address multiple stakeholders' problems. It enumerates aspects relevant to the theory and real-world practice of social machines, based on qualitative observations from our experiences building them. We frame evolving issues including: changing functionality, users, data and context; geographical and temporal scope (considering data granularity and visibility); and social scope. The latter is wide-ranging, including motivation, trust, experience, security, governance, control, provenance, privacy and law. We provide suggestions about building flexibility into social machines to allow for change, and defining social machines in terms of problems and stakeholders.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {156–160},
numpages = {5},
keywords = {linked data, social machines, stakeholders},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1145/3639233.3639332,
author = {Kwok, Raptor Yick-Kan and Au Yeung, Siu-Kei and Li, Zongxi and Hung, Kevin},
title = {Cantonese to Written Chinese Translation via HuggingFace Translation Pipeline},
year = {2024},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639233.3639332},
doi = {10.1145/3639233.3639332},
abstract = {Cantonese, a low-resource language [5] that has been used in Southeastern China for hundreds of years, with over 85 million native speakers worldwide, is poorly supported in the mainstream language model for existing translation platforms such as Baidu, Google and Bing. This paper presents a large parallel corpus of 130 thousand Cantonese and Written Chinese pairs. The data are used to train a translation model using the translation pipeline of the Hugging Face Transformers architecture, a dominant architecture for natural language processing nowadays [18]. The BLEU score and manual assessment evaluate the performance. The translation results achieve a BLEU score of 41.35and chrF++ score of 44.88on the entire validation set. The model also works reasonably well with long sentences of over 20 Chinese characters. It achieves a BLEU score of 48.61and chrF++ score of 39.87on long sentences. Those results are comparable with the existing Baidu Fanyi and Bing Translate. We also establish a Cantonese sentence evaluation metric to classify the quality of the source Cantonese sentence by professional translators. We then compare the BLEU and chrF++ scores with the corresponding evaluation score and found that the better the quality of the source sentence, the higher the BLEU and chrF++ scores. Last, we proved that our corpus enabled the Cantonese translation capability of the Chinese BART pre-trained model.},
booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
pages = {77–84},
numpages = {8},
keywords = {Cantonese, Written Chinese, neural networks, translation},
location = {Seoul, Republic of Korea},
series = {NLPIR '23}
}

@article{10.5555/3606402.3606416,
author = {Reppert, Austin and Montecinos-Velazquez, Brian and Kahl, Harrison and Reid, Rackeem and Rivas, Danielle and Spampinato, Dominic and Zhong, Hudson and Ngo, Linh B.},
title = {A Kubernetes Framework for Learning Cloud Native Development},
year = {2023},
issue_date = {April 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {8},
issn = {1937-4771},
abstract = {This work describes a Kubernetes framework that can be automatically deployed on CloudLab, a federal cloud resource, to support learning activities in cloud computing education. The framework enables instructors and students to study cloud services' full product development life-cycle, including aspects such as automated deployment, availability, and security. This framework is easy to deploy, is freely available to academic institutions, and can be extended to support more advanced learning scenarios. The effectiveness of this framework is demonstrated through two complex and full-stack student projects.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {99–108},
numpages = {10}
}

@proceedings{10.1145/3635059,
title = {PCI '23: Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
year = {2023},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lamia, Greece}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3503222.3507709,
author = {Yang, Yanan and Zhao, Laiping and Li, Yiming and Zhang, Huanyu and Li, Jie and Zhao, Mingyang and Chen, Xingzhen and Li, Keqiu},
title = {INFless: a native serverless system for low-latency, high-throughput inference},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507709},
doi = {10.1145/3503222.3507709},
abstract = {Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services. While simply ”patching” general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-the-art systems by 2\texttimes{}-5\texttimes{} on system throughput, meeting the latency goals of ML services.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {768–781},
numpages = {14},
keywords = {Inference System, Machine Learning, Serverless Computing},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3546157.3546173,
author = {Balfagih, Ahmed and Keselj, Vlado and Taylor, Stacey},
title = {N-gram and Word2Vec Feature Engineering Approaches for Spam Recognition on Some Influential Twitter Topics in Saudi Arabia},
year = {2022},
isbn = {9781450396257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546157.3546173},
doi = {10.1145/3546157.3546173},
abstract = {Social media platforms, such as Twitter, have become powerful sources of information on people's perception of major events. Many people use Twitter to express their views on various issues and events and use it to develop their opinion on the diverse economic, political, technical, and social occurrences related to their daily lives. Spam and non-relevant tweets are a major challenge for Twitter trend detection. Saudi Arabia is a top ranked country in Twitter usage worldwide, and in recent years has experienced difficulties due to the use and rise of hashtags based on misleading tweets and spam. The goal of this paper is to apply machine learning techniques to identify spam on the Saudi tweets collected to the end of 2020. To date, spam detection on Twitter data has been mostly done in English, leaving other major languages, such as Arabic, insufficiently covered. Additionally, publicly accessible Arabic Twitter datasets are hard to find. For our research, we use eight Twitter datasets on some significant topics in politics, health, national affairs, economy, and sport, to train and evaluate different machine learning algorithms, with a focus on two feature generation techniques based on N-grams and Word2Vec embeddings. One contribution of this paper is providing these new labelled datasets with embeddings. The experimental results show improvement from using embeddings over N-grams in more balanced datasets vs. more unbalanced ones. We also find a superior performance of the Random Forest algorithm over other algorithms in most experiments.},
booktitle = {Proceedings of the 6th International Conference on Information System and Data Mining},
pages = {101–107},
numpages = {7},
location = {Silicon Valley, CA, USA},
series = {ICISDM '22}
}

@inproceedings{10.1145/3297662.3365811,
author = {Kalles, Dimitrios and Giagtzoglou, Kyriakos and Mitropoulos, Konstantinos},
title = {Gaming Ecosystems for Education and Research: Where Artificial Intelligence meets with Software Engineering, at scale},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365811},
doi = {10.1145/3297662.3365811},
abstract = {We present aspects of ecosystem engineering for a strategy board game. Human and machine players of the ecosystem can pick opponents or form teams and play against other teams or players. We present the key features of the ecosystem, the highlights of the development process and we propose concrete potential uses of the ecosystem in research and education.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {199–204},
numpages = {6},
keywords = {Artificial Intelligence, Machine Learning, Machine Learning Competitions, Reinforcement Learning, Software Engineering},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@article{10.1145/3585004,
author = {Badampudi, Deepika and Unterkalmsteiner, Michael and Britto, Ricardo},
title = {Modern Code Reviews—Survey of Literature and Practice},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3585004},
doi = {10.1145/3585004},
abstract = {Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners’ opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues.Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners’ perception of the relevance of MCR research, and analyzed the primary studies’ research impact.Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor– and support systems–related research.Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {107},
numpages = {61},
keywords = {Modern code review, literature survey, practitioner survey}
}

@proceedings{10.1145/3596454,
title = {EICS '23 Companion: Companion Proceedings of the 2023 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
year = {2023},
isbn = {9798400702068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Swansea, United Kingdom}
}

@proceedings{10.1145/3579375,
title = {ACSW '23: Proceedings of the 2023 Australasian Computer Science Week},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@article{10.1145/3555129,
author = {Yin, Likang and Chakraborti, Mahasweta and Yan, Yibo and Schweik, Charles and Frey, Seth and Filkov, Vladimir},
title = {Open Source Software Sustainability: Combining Institutional Analysis and Socio-Technical Networks},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555129},
doi = {10.1145/3555129},
abstract = {Sustainable Open Source Software (OSS) forms much of the fabric of our digital society, especially successful and sustainable ones. But many OSS projects do not become sustainable, resulting in abandonment and even risks for the world's digital infrastructure. Prior work has looked at the reasons for this mainly from two very different perspectives. In software engineering, the focus has been on understanding success and sustainability from the socio-technical perspective: the OSS programmers' day-to-day activities and the artifacts they create. In institutional analysis, on the other hand, emphasis has been on institutional designs (e.g., policies, rules, and norms) that structure project governance. Even though each is necessary for a comprehensive understanding of OSS projects, the connection and interaction between the two approaches have been barely explored.In this paper, we make the first effort toward understanding OSS project sustainability using a dual-view analysis, by combining institutional analysis with socio-technical systems analysis. In particular, we (i) use linguistic approaches to extract institutional rules and norms from OSS contributors' communications to represent the evolution of their governance systems, and (ii) construct socio-technical networks based on longitudinal collaboration records to represent each project's organizational structure. We combined the two methods and applied them to a dataset of developer digital traces from 253 nascent OSS projects within the Apache Software Foundation (ASF) incubator. We find that the socio-technical and institutional features relate to each other, and provide complimentary views into the progress of the ASF's OSS projects. Refining these combined analyses can help provide a more precise understanding of the synchronization between the evolution of institutional governance and organizational structure.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {404},
numpages = {23},
keywords = {OSS sustainability, institutional design, socio-technical systems}
}

@inproceedings{10.1145/3576915.3623122,
author = {Ryan, Ita and Roedig, Utz and Stol, Klaas-Jan},
title = {Unhelpful Assumptions in Software Security Research},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623122},
doi = {10.1145/3576915.3623122},
abstract = {In the study of software security many factors must be considered. Once venturing beyond the simplest of laboratory experiments, the researcher is obliged to contend with exponentially complex conditions. Software security has been shown to be affected by priming, tool usability, library documentation, organisational security culture, the content and format of internet resources, IT team and developer interaction, Internet search engine ordering, developer personality, security warning placement, mentoring, developer experience and more. In a systematic review of software security papers published since 2016, we have identified a number of unhelpful assumptions that are commonly made by software security researchers. In this paper we list these assumptions, describe why they sometimes do not reflect reality, and suggest implications for researchers.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3460–3474},
numpages = {15},
keywords = {secure software development, software security},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@article{10.1145/3565800,
author = {Wang, Chengpeng and Wang, Wenyang and Yao, Peisen and Shi, Qingkai and Zhou, Jinguo and Xiao, Xiao and Zhang, Charles},
title = {
Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3565800},
doi = {10.1145/3565800},
abstract = {Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {66},
numpages = {39},
keywords = {Abstract interpretation, value-flow analysis, data structure analysis}
}

@inproceedings{10.1145/3377811.3380376,
author = {Zhang, Yuxia and Zhou, Minghui and Stol, Klaas-Jan and Wu, Jianyu and Jin, Zhi},
title = {How do companies collaborate in open source ecosystems? an empirical study of OpenStack},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380376},
doi = {10.1145/3377811.3380376},
abstract = {Open Source Software (OSS) has come to play a critical role in the software industry. Some large ecosystems enjoy the participation of large numbers of companies, each of which has its own focus and goals. Indeed, companies that otherwise compete, may become collaborators within the OSS ecosystem they participate in. Prior research has largely focused on commercial involvement in OSS projects, but there is a scarcity of research focusing on company collaborations within OSS ecosystems. Some of these ecosystems have become critical building blocks for organizations worldwide; hence, a clear understanding of how companies collaborate within large ecosystems is essential. This paper presents the results of an empirical study of the OpenStack ecosystem, in which hundreds of companies collaborate on thousands of project repositories to deliver cloud distributions. Based on a detailed analysis, we identify clusters of collaborations, and identify four strategies that companies adopt to engage with the OpenStack ecosystem. We alsofind that companies may engage in intentional or passive collaborations, or may work in an isolated fashion. Further, wefi nd that a company's position in the collaboration network is positively associated with its productivity in OpenStack. Our study sheds light on how large OSS ecosystems work, and in particular on the patterns of collaboration within one such large ecosystem.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1196–1208},
numpages = {13},
keywords = {OSS ecosystem, company participation, open collaboration, open source software, openstack, software development},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/ICSE48619.2023.00168,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Su, Yuhui and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Ex Pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00168},
doi = {10.1109/ICSE48619.2023.00168},
abstract = {Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1983–1995},
numpages = {13},
keywords = {GUI testing, deep learning, program analysis, empirical study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@inproceedings{10.5555/3581644.3581714,
author = {Ky, Jo\"{e}l Roman and Mathieu, Bertrand and Lahmadi, Abdelkader and Boutaba, Raouf},
title = {Assessing Unsupervised Machine Learning Solutions for Anomaly Detection in Cloud Gaming Sessions},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Cloud gaming applications have gained great adoption on the Internet particularly benefiting from the wide availability of broadband access networks. However, they still fail to meet users' quality requirements when accessed using cellular networks due to common wireless channel degradations. Machine Learning (ML) techniques can be leveraged to detect such anomalies during users' cloud gaming sessions. In this respect, unsupervised ML approaches are particularly interesting since they do not require labeled datasets. In this work, we investigate these approaches to understand their performance and their robustness. Our dataset consists of game sessions played on the public Google Stadia Cloud Gaming servers. The game sessions are played using a 4G network emulation replicating the capacity variations sampled on a commercial 4G network. We compare different models ranging from traditional approaches to deep learning and we evaluate their default performance while varying the level of contamination in their training datasets. Our experiments show that Auto-Encoders models achieve the best performance without contamination while the OC-SVM and the Isolation Forest are the most robust to data contamination.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {56},
numpages = {7},
keywords = {AI, QoE, anomaly detection, cloud gaming, mobile networks, unsupervised learning},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@inproceedings{10.1145/3345629.3345634,
author = {Dey, Tapajit and Ma, Yuxing and Mockus, Audris},
title = {Patterns of Effort Contribution and Demand and User Classification based on Participation Patterns in NPM Ecosystem},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345634},
doi = {10.1145/3345629.3345634},
abstract = {Background: Open source requires participation of volunteer and commercial developers (users) in order to deliver functional high-quality components. Developers both contribute effort in the form of patches and demand effort from the component maintainers to resolve issues reported against it. Open source components depend on each other directly and transitively, and evidence suggests that more effort is required for reporting and resolving the issues reported further upstream in this supply chain. Aim: Identify and characterize patterns of effort contribution and demand throughout the open source supply chain and investigate if and how these patterns vary with developer activity; identify different groups of developers; and predict developers' company affiliation based on their participation patterns. Method: 1,376,946 issues and pull-requests created for 4433 NPM packages with over 10,000 monthly downloads and full (public) commit activity data of the 272,142 issue creators is obtained and analyzed and dependencies on NPM packages are identified. Fuzzy c-means clustering algorithm is used to find the groups among the users based on their effort contribution and demand patterns, and Random Forest is used as the predictive modeling technique to identify their company affiliations. Result: Users contribute and demand effort primarily from packages that they depend on directly with only a tiny fraction of contributions and demand going to transitive dependencies. A significant portion of demand goes into packages outside the users' respective supply chains (constructed based on publicly visible version control data). Three and two different groups of users are observed based on the effort demand and effort contribution patterns respectively. The Random Forest model used for identifying the company affiliation of the users gives a AUC-ROC value of 0.68, and variables representing aggregate participation patterns proved to be the important predictors. Conclusion: Our results give new insights into effort demand and supply at different parts of the supply chain of the NPM ecosystem and its users and suggests the need to increase visibility further upstream.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {36–45},
numpages = {10},
keywords = {Clustering, NPM Packages, Random Forest model, Software Dependencies, Software Issue Reporting, User Contribution},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/3517745.3561427,
author = {Kupoluyi, Jesutofunmi and Chaqfeh, Moumena and Varvello, Matteo and Coke, Russell and Hashmi, Waleed and Subramanian, Lakshmi and Zaki, Yasir},
title = {Muzeel: assessing the impact of JavaScript dead code elimination on mobile web performance},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561427},
doi = {10.1145/3517745.3561427},
abstract = {To quickly create interactive web pages, developers heavily rely on (large) general-purpose JavaScript libraries. This practice bloats web pages with complex unused functions dead code which are unnecessarily downloaded and processed by the browser. The identification and the elimination of these functions is an open problem, which this paper tackles with Muzeel, a black-box approach requiring neither knowledge of the code nor execution traces. While the state-of-the-art solutions stop analyzing JavaScript when the page loads, the core design principle of Muzeel is to address the challenge of dynamically analyzing JavaScript after the page is loaded, by emulating all possible user interactions with the page, such that the used functions (executed when interactivity events fire) are accurately identified, whereas unused functions are filtered out and eliminated. We run Muzeel against 15,000 popular web pages and show that half of the 300,000 JavaScript files used in these pages have at least 70% of unused functions, accounting for 55% of the files' sizes. To assess the impact of dead code elimination on Mobile Web performance, we serve 200 Muzeel-ed pages to several Android phones and browsers, under variable network conditions. Our evaluation shows that Muzeel can speed up page loads by 25-30% thanks to a combination of lower CPU and bandwidth usage. Most importantly, we show that such savings are achieved while maintaining the pages' visual appearance and interactive functionality.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {335–348},
numpages = {14},
keywords = {JavaScript, mobile web, user experience, web simplification},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1109/ICSE48619.2023.00119,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Che, Xing and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00119},
doi = {10.1109/ICSE48619.2023.00119},
abstract = {Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1355–1367},
numpages = {13},
keywords = {text input generation, GUI testing, android app, large language model, prompt-tuning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3510458.3513011,
author = {Rossi, Davide and Zacchiroli, Stefano},
title = {Worldwide gender differences in public code contributions: and how they have been affected by the COVID-19 pandemic},
year = {2022},
isbn = {9781450392273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510458.3513011},
doi = {10.1145/3510458.3513011},
abstract = {Gender imbalance is a well-known phenomenon observed throughout sciences which is particularly severe in software development and Free/Open Source Software communities. Little is know yet about the geography of this phenomenon in particular when considering large scales for both its time and space dimensions.We contribute to fill this gap with a longitudinal study of the population of contributors to publicly available software source code. We analyze the development history of 160 million software projects for a total of 2.2 billion commits contributed by 43 million distinct authors over a period of 50 years. We classify author names by gender using name frequencies and author geographical locations using heuristics based on email addresses and time zones. We study the evolution over time of contributions to public code by gender and by world region.For the world overall, we confirm previous findings about the low but steadily increasing ratio of contributions by female authors. When breaking down by world regions we find that the long-term growth of female participation is a world-wide phenomenon. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women's ability to contribute to public code has been more hindered than that of men.Software developers around the world work together to produce publicly available software (or public code). They do so using public identities and disclosing information about their work that include their names and when a software change was made. We use this information to characterize the gender gap in public code, that is, the difference in participation to public software development between men and women. Specifically, we study the development history of 160 million pieces of public software, developed over a period of 50 years by 43 million authors. We characterize the gender gap on this corpus over time and by world region. To determine author genders we rely on public data about name frequencies by gender around the world. To determine author locations we use email addresses, name frequencies around the world, and the timezone associated to each software change. We confirm that the gender gap in public code is huge. Female authors are only 8.1% of the total and have authored only 13.5% software versions. The gender gap is however shrinking, with women participation having increased steadily over the past 12 years. This improvement is a global phenomenon, observable in most world regions. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women have been more hindered than men in their ability to contribute to public code.},
booktitle = {Proceedings of the 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society},
pages = {172–183},
numpages = {12},
keywords = {commit, covid19, diversity, gender, open source, software heritage},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIS '22}
}

@inproceedings{10.1145/3377811.3380414,
author = {Egelman, Carolyn D. and Murphy-Hill, Emerson and Kammer, Elizabeth and Hodges, Margaret Morrow and Green, Collin and Jaspan, Ciera and Lin, James},
title = {Predicting developers' negative feelings about code review},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380414},
doi = {10.1145/3377811.3380414},
abstract = {During code review, developers critically examine each others' code to improve its quality, share knowledge, and ensure conformance to coding standards. In the process, developers may have negative interpersonal interactions with their peers, which can lead to frustration and stress; these negative interactions may ultimately result in developers abandoning projects. In this mixed-methods study at one company, we surveyed 1,317 developers to characterize the negative experiences and cross-referenced the results with objective data from code review logs to predict these experiences. Our results suggest that such negative experiences, which we call "pushback", are relatively rare in practice, but have negative repercussions when they occur. Our metrics can predict feelings of pushback with high recall but low precision, making them potentially appropriate for highlighting interactions that may benefit from a self-intervention.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {174–185},
numpages = {12},
keywords = {code review, interpersonal conflict},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3137616.3137619,
author = {Varshney, Gaurav and Misra, Manoj and Atrey, Pradeep K.},
title = {Detecting Spying and Fraud Browser Extensions: Short Paper},
year = {2017},
isbn = {9781450352062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3137616.3137619},
doi = {10.1145/3137616.3137619},
abstract = {Due to the flaws in policy followed by web browsers for granting permissions to browser extensions and due to a lack of effective static and dynamic detection systems for identifying malicious extensions uploaded on the web stores, malicious browser extensions have become the easiest way to carry out phishing, spying, fraud and other kinds of advanced attacks. This paper identifies and analyzes a subset of these attacks which can be performed with the use of malicious browser extensions (using Google Chrome) and discusses the research gaps of the existing prevention and detection schemes to adequately defend against these attacks. An initial set of malicious signatures responsible for cyber fraud and spying is identified during the study. We use this set of signatures to develop a lightweight malicious extension detection system which can alert users of suspected spying or fraud extensions installed on the Chrome browser on a PC. Results show that the proposed detection system performs better than known malicious extension detectors such as Chrome Cleanup tool and Chrome safeguard tool.},
booktitle = {Proceedings of the 2017 on Multimedia Privacy and Security},
pages = {45–52},
numpages = {8},
keywords = {browser security, malicious browser extension, phishing, spying, static code analysis},
location = {Dallas, Texas, USA},
series = {MPS '17}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/3340531.3417416,
author = {Romero, Julien and Razniewski, Simon},
title = {Inside Quasimodo: Exploring Construction and Usage of Commonsense Knowledge},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3417416},
doi = {10.1145/3340531.3417416},
abstract = {Quasimodo is an open-source commonsense knowledge base that significantly advanced the state of salient commonsense knowledge base construction. It introduced a pipeline that gathers, normalizes, validates and scores statements coming from query log and question answering forums. In this demonstration, we present a companion web portal which allows (i) to explore the data, (ii) to run and analyze the extraction pipeline live, and (iii) inspect the usage of Quasimodo's knowledge in several downstream use cases. The web portal is available at https://quasimodo.r2.enst.fr.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3445–3448},
numpages = {4},
keywords = {commonsense, datasets, knowledge base, visualisation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1145/3329786,
author = {Or-Meir, Ori and Nissim, Nir and Elovici, Yuval and Rokach, Lior},
title = {Dynamic Malware Analysis in the Modern Era—A State of the Art Survey},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329786},
doi = {10.1145/3329786},
abstract = {Although malicious software (malware) has been around since the early days of computers, the sophistication and innovation of malware has increased over the years. In particular, the latest crop of ransomware has drawn attention to the dangers of malicious software, which can cause harm to private users as well as corporations, public services (hospitals and transportation systems), governments, and security institutions. To protect these institutions and the public from malware attacks, malicious activity must be detected as early as possible, preferably before it conducts its harmful acts. However, it is not always easy to know what to look for—especially when dealing with new and unknown malware that has never been seen. Analyzing a suspicious file by static or dynamic analysis methods can provide relevant and valuable information regarding a file's impact on the hosting system and help determine whether the file is malicious or not, based on the method's predefined rules. While various techniques (e.g., code obfuscation, dynamic code loading, encryption, and packing) can be used by malware writers to evade static analysis (including signature-based anti-virus tools), dynamic analysis is robust to these techniques and can provide greater understanding regarding the analyzed file and consequently can lead to better detection capabilities. Although dynamic analysis is more robust than static analysis, existing dynamic analysis tools and techniques are imperfect, and there is no single tool that can cover all aspects of malware behavior. The most recent comprehensive survey performed in this area was published in 2012. Since that time, the computing environment has changed dramatically with new types of malware (ransomware, cryptominers), new analysis methods (volatile memory forensics, side-channel analysis), new computing environments (cloud computing, IoT devices), new machine-learning algorithms, and more. The goal of this survey is to provide a comprehensive and up-to-date overview of existing methods used to dynamically analyze malware, which includes a description of each method, its strengths and weaknesses, and its resilience against malware evasion techniques. In addition, we include an overview of prominent studies presenting the usage of machine-learning methods to enhance dynamic malware analysis capabilities aimed at detection, classification, and categorization.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {88},
numpages = {48},
keywords = {Dynamic analysis, behavioral analysis, detection, evasion, malware}
}

@inproceedings{10.1145/2983323.2983884,
author = {Shankaralingappa, Darshan M. and De Fransicsi Morales, Gianmarco and Gionis, Aristides},
title = {Extracting Skill Endorsements from Personal Communication Data},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983884},
doi = {10.1145/2983323.2983884},
abstract = {People are increasingly communicating and collaborating via digital platforms, such as email and messaging applications. Data exchanged on these digital communication platforms can be a treasure trove of information on people who participate in the discussions: who they are collaborating with, what they are working on, what their expertise is, and so on. Yet, personal communication data is very rarely analyzed due to the sensitivity of the information it contains. In this paper, we mine personal communication data with the goal of generating skill endorsements of the type "person A endorses person B on skill X." To address privacy concerns, we consider that each person has access only to their own data (i.e., conversations with their peers). By using our method, they can generate endorsements for their peers, which they can inspect and opt to publish. To identify meaningful skills we use a knowledge base created from the StackExchange Q&amp;A forum. We study two different approaches, one based on building a skill graph, and one based on information retrieval techniques. We find that the latter approach outperforms the graph-based algorithms when tested on a dataset of user profiles from StackOverflow. We also conduct a user study on email data from nine volunteers, and we find that the information retrieval-based approach achieves a MAP@10 score of 0.617.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1961–1964},
numpages = {4},
keywords = {e-mail mining, personal data, skill endorsements},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@article{10.1145/3505263,
author = {Wu, Siwei and Wu, Lei and Zhou, Yajin and Li, Runhuai and Wang, Zhi and Luo, Xiapu and Wang, Cong and Ren, Kui},
title = {Time-travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3505263},
doi = {10.1145/3505263},
abstract = {Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage.In this article, we propose a scalable attack detection framework named EthScope, which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around  ( text{2,300}times )  when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {54},
numpages = {33},
keywords = {Ethereum, attack detection, vulnerability}
}

@inproceedings{10.1145/3485447.3512138,
author = {Sepahpour-Fard, Melody and Quayle, Michael},
title = {How Do Mothers and Fathers Talk About Parenting to Different Audiences? Stereotypes and Audience Effects: An Analysis of r/Daddit, r/Mommit, and r/Parenting Using Topic Modelling},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512138},
doi = {10.1145/3485447.3512138},
abstract = {While major strides have been made towards gender equality in public life, serious inequality remains in the domestic sphere, especially around parenting. The present study analyses discussions about parenting on Reddit (i.e., a content aggregation website) to explore audience effects and gender stereotypes. It suggests a novel method to study topical variation in individuals’ language when interacting with different audiences. Comments posted in 2020 were collected from three parenting subreddits (i.e., topical communities), described as being for fathers (r/Daddit), mothers (r/Mommit), and all parents (r/Parenting). Users posting on r/Parenting and r/Daddit or on r/Parenting and r/Mommit were assumed to identify as fathers or mothers, respectively, allowing gender comparison. Users’ comments on r/Parenting (to a mixed-gender audience) were compared with their comments to single-gender audiences on r/Daddit or r/Mommit using Latent Dirichlet Allocation (LDA) topic modelling. Results show that the most discussed topic among parents is about education and family advice, a topic mainly discussed in the mixed-gender subreddit and more by fathers than mothers. The topic model also indicates that, when it comes to the basic needs of children (sleep, food, and medical care), mothers seem to be more concerned regardless of the audience. In contrast, topics such as birth and pregnancy announcements and physical appearance are more discussed by fathers in the father-centric subreddit. Overall, findings seem to show that mothers are generally more concerned about the practical sides of parenting while fathers’ expressed concerns are more contextual: with other fathers, there seems to be a desire to show their fatherhood and be recognized for it while they discuss education with mothers. These results demonstrate that concerns expressed by parents on Reddit are context-sensitive but also consistent with gender stereotypes, potentially reflecting a persistent gendered and unequal division of labour in parenting.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2696–2706},
numpages = {11},
keywords = {LDA topic modelling, Reddit, audience, computational social science, gender stereotypes, natural language processing, parenting, social identity performance, social psychology},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3411764.3445659,
author = {Kuttal, Sandeep Kaur and Ong, Bali and Kwasny, Kate and Robe, Peter},
title = {Trade-offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445659},
doi = {10.1145/3411764.3445659},
abstract = {Pair programming has a documented history of benefits, such as increased code quality, productivity, self-efficacy, knowledge transfer, and reduced gender gap. Research uncovered problems with pair programming related to scheduling, collocating, role imbalance, and power dynamics. We investigated the trade-offs of substituting a human with an agent to simultaneously provide benefits and alleviate obstacles in pair programming. We conducted gender-balanced studies with human-human pairs in a remote lab with 18 programmers and Wizard-of-Oz studies with 14 programmers, then analyzed results quantitatively and qualitatively. Our comparative analysis of the two studies showed no significant differences in productivity, code quality, and self-efficacy. Further, agents facilitated knowledge transfer; however, unlike humans, agents were unable to provide logical explanations or discussions. Human partners trusted and showed humility towards agents. Our results demonstrate that agents can act as effective pair programming partners and open the way towards new research on conversational agents for programming.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {243},
numpages = {20},
keywords = {Pair programming, avatars, conversational agents, empirical evaluation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3649884,
author = {Ren, Yuqing and Clement, Jeffrey},
title = {Augmenting Human Teams with Robots in Knowledge Work Settings: Insights from the Literature},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
url = {https://doi.org/10.1145/3649884},
doi = {10.1145/3649884},
abstract = {Recent developments in large language models open doors for Artificial Intelligence and robots to augment knowledge workers and teams in a variety of domains, such as customer service, data science, legal work, and software development. In this article, we review 317 articles from multiple disciplines and summarize the insights in a theoretical framework linking key robot attributes to human perceptions and behaviors. The robot attributes include embodiment, nonverbal and verbal communication, perceived gender and race, emotions, perceived personality, and competence. The outcomes include human perceptions, acceptance, engagement, compliance, trust, and willingness to help. We identify four differences between one human and one robot settings and team settings and use them as the springboard to generalize insights from the literature review to the design and impact of a robot in assisting humans in knowledge work teams. We report two high-level observations around the interplay among robot attributes and context dependent designs and discuss their implications.},
journal = {J. Hum.-Robot Interact.},
month = {jun},
articleno = {20},
numpages = {34},
keywords = {Human-robot interaction, Generative AI, robot design, human robot team}
}

@inproceedings{10.5555/3291656.3291706,
author = {Andreadis, Georgios and Versluis, Laurens and Mastenbroek, Fabian and Iosup, Alexandru},
title = {A reference architecture for datacenter scheduling: design, validation, and experiments},
year = {2018},
publisher = {IEEE Press},
abstract = {Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {37},
numpages = {15},
keywords = {datacenter, reference architecture, scheduling},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/3658271.3658338,
author = {Santos, Caio Nery Matos and Claro, Daniela Barreiro and Medrado Gondim, Jo\~{a}o and Mane, Babacar},
title = {Suspicious Behavior Detection near Vehicles in University Environment: An Approach using Object Detection and Body Angles},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658271.3658338},
doi = {10.1145/3658271.3658338},
abstract = {Context: With the advancement of smart cities, the University environment demands surveillance camera systems to increase their monitoring capabilities to prevent malicious behaviors without prohibiting people’s circulation. Problem: Universities have large parking lots with many vehicles and face daily security problems, such as robberies and kidnappings, due to the lack of cameras capable of detecting suspicious behavior and alerting security personnel. Solution: Our approach enhances security in the university environment by developing a system capable of recognizing vehicles and individuals, assessing their proximity, and detecting gestures and actions labeled as suspicious behavior while interoperating with camera systems to alert the appropriate security authorities. Information systems theory: This work was conceived based on the General System Theory to interact with pre-existing heterogeneous systems. It relates to the Technological Frames of Reference theory, which involves the perception and interpretation of real-time object detection technology to monitor, alert, and ensure security. Method: Our research method is an experimental, descriptive investigation of collecting quantitative data, and our evaluation is conducted through the proof of concept. Results: Our artifact demonstrated its feasibility by exhibiting good performance, enabling the detection of pre-defined suspicious behaviors near vehicles with a precision of 94,25% and accuracy of 86,99%. Contributions and Impact in the area of information systems: Our contributions are two-fold: From the organization’s perspective, our security system interoperability, generating interoperable alerts; the artifact to detect suspicious behaviors, protecting people in the University environment. Our approach impacts the three pilars from IS area: People, Process and Technology. Additionally, we provide a dataset of security camera videos in university parking lots.},
booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems},
articleno = {66},
numpages = {10},
keywords = {abnormal hehavior, activity recognition, body angles, parking lots, pose detection, public security},
location = {Juiz de Fora, Brazil},
series = {SBSI '24}
}

@inproceedings{10.1145/3551349.3556923,
author = {Zhang, Chen and Chen, Bihuan and Hu, Junhao and Peng, Xin and Zhao, Wenyun},
title = {BuildSonic: Detecting and Repairing Performance-Related Configuration Smells for Continuous Integration Builds},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556923},
doi = {10.1145/3551349.3556923},
abstract = {Despite the benefits, continuous integration (CI) can incur high&nbsp;costs. One of the well-recognized costs is long build time, which greatly&nbsp;affects the speed of software development and increases the&nbsp;cost&nbsp;in&nbsp;computational resources. While there exist configuration options&nbsp;in&nbsp;the&nbsp;CI infrastructure to accelerate builds, the&nbsp;CI infrastructure is often&nbsp;not&nbsp;optimally configured, leading to CI configuration smells. Attempts&nbsp;have been made to detect or repair CI configuration smells.&nbsp;However,&nbsp;none of them is specifically&nbsp;designed to improve build performance&nbsp;in&nbsp;CI. In this paper, we first create a catalog of 20 performance-related&nbsp;CI configuration smells (PCSs) in three tools (i.e., Travis CI, Maven&nbsp;and Gradle) of the CI infrastructure for Java projects. Then, we propose an automated approach, named BuildSonic, to detect and repair&nbsp;15 types of PCSs by analyzing configuration files.&nbsp;We&nbsp;have&nbsp;conducted large-scale experiments to evaluate BuildSonic. We detected&nbsp;20,318 PCSs in 99.0% of the 4,140 Java projects, with a precision of 0.998&nbsp;and a recall of 0.991. We submitted 1,138 pull requests&nbsp;for&nbsp;sampled&nbsp;PCSs of each PCS type, 246 and 11 of which&nbsp;have&nbsp;been respectively&nbsp;merged and accepted by developers. We successfully triggered CI builds&nbsp;before and&nbsp;after merging 288 pull requests, and observed an average build performance improvement of 12.4% after repairing a PCS.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {18},
numpages = {13},
keywords = {build performance, configuration smells, continuous integration},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/2872427.2874809,
author = {Pellissier Tanon, Thomas and Vrande\v{c}i\'{c}, Denny and Schaffert, Sebastian and Steiner, Thomas and Pintscher, Lydia},
title = {From Freebase to Wikidata: The Great Migration},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2874809},
doi = {10.1145/2872427.2874809},
abstract = {Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two major collaborative knowledge bases are Wikimedia's Wikidata and Google's Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Primary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {1419–1428},
numpages = {10},
keywords = {crowdsourcing systems, freebase, semantic web, wikidata},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1145/3517745.3561446,
author = {Karl, Manuel and Musch, Marius and Ma, Guoli and Johns, Martin and Lekies, Sebastian},
title = {No keys to the kingdom required: a comprehensive investigation of missing authentication vulnerabilities in the wild},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561446},
doi = {10.1145/3517745.3561446},
abstract = {Nowadays, applications expose administrative endpoints to the Web that can be used for a plethora of security sensitive actions. Typical use cases range from running small snippets of user-provided code for rapid prototyping, administering databases, and running CI/CD pipelines, to managing job scheduling on whole clusters of computing devices. While accessing these applications over the Web make the lives of their users easier, they can be leveraged by attackers to compromise the underlying infrastructure if not properly configured.In this paper, we comprehensively investigate inadequate authentication mechanisms in such web endpoints. For this, we looked at 25 popular applications and exposed 18 of them to the Internet because they were either vulnerable in their default configuration or were easy to misconfigure. We identified ongoing attacks against 7 of them, some were even compromised within a few hours from the deployment. In an Internet-wide scan of the IPv4 address space, we examine the prevalence of such vulnerable applications at scale. Thereby, we found 4,221 vulnerable instances, enough to create a small botnet with little technical knowledge. We observed these vulnerable instances and found that even after four weeks, more than half of them were still online and vulnerable.Currently, most of the identified vulnerabilities are seen as features of the software and are often not yet considered by common security scanners or vulnerability databases. However, via our experiments, we found missing authentication vulnerabilities to be common and already actively exploited at scale. They thus represent a prevalent but often disregarded danger.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {619–632},
numpages = {14},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3558535.3559782,
author = {St\"{u}tz, Rainer and Stockinger, Johann and Moreno-Sanchez, Pedro and Haslhofer, Bernhard and Maffei, Matteo},
title = {Adoption and Actual Privacy of Decentralized CoinJoin Implementations in Bitcoin},
year = {2023},
isbn = {9781450398619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558535.3559782},
doi = {10.1145/3558535.3559782},
abstract = {We present a first measurement study on the adoption and actual privacy of two popular decentralized CoinJoin implementations, Wasabi and Samourai, in the broader Bitcoin ecosystem. By applying highly accurate (¿ 99%) algorithms we can effectively detect 30,251 Wasabi and 223,597 Samourai transactions within the block range 530,500 to 725,348 (2018-07-05 to 2022-02-28). We also found a steady adoption of these services with a total value of mixed coins of ca. 4.74 B USD and average monthly mixing amounts of ca. 172.93 M USD) for Wasabi and ca. 41.72 M USD for Samourai. Furthermore, we could trace ca. 322 M USD directly received by cryptoasset exchanges and ca. 1.16 B USD indirectly received via two hops. Our analysis further shows that the traceability of addresses during the pre-mixing and post-mixing narrows down the anonymity set provided by these coin mixing services. It also shows that the selection of addresses for the CoinJoin transaction can harm anonymity. Overall, this is the first paper to provide a comprehensive picture of the adoption and privacy of distributed CoinJoin transactions. Understanding this picture is particularly interesting in the light of ongoing regulatory efforts that will, on the one hand, affect compliance measures implemented in cryptocurrency exchanges and, on the other hand, the privacy of end-users.},
booktitle = {Proceedings of the 4th ACM Conference on Advances in Financial Technologies},
pages = {254–267},
numpages = {14},
keywords = {cryptoassets, mixing, CoinJoin},
location = {Cambridge, MA, USA},
series = {AFT '22}
}

@inbook{10.1145/3617448.3617494,
title = {Index/Author’s Biography},
year = {2024},
isbn = {9798400709494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617448.3617494},
abstract = {This book provides a comprehensive study of the many ways to interact with computers and computerized devices. An “interaction technique” starts when the user performs an action that causes an electronic device to respond, and includes the direct feedback from the device to the user. Examples include physical buttons and switches, on-screen menus and scrollbars operated by a mouse, touchscreen widgets, gestures such as flick-to-scroll, text entry on computers and touchscreens, input for virtual reality systems, interactions with conversational agents such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, and adaptations of all of these for people with disabilities. Pick, Click, Flick! is written for anyone interested in interaction techniques, including computer scientists and designers working on human-computer interaction, as well as implementers and consumers who want to understand and get the most out of their digital devices.REVIEWS“Pick, Click, Flick! is an impressive reference manual of the many years of interaction design development. It is a reference book, invaluable when questions arise, whether while you are busy designing something, or learning, or teaching, where assigning sections of the reference will be a valuable resource and learning tool for students. Brad Myers has provided a great service to the interaction community.” ‐ Don Norman, Distinguished Prof. Emeritus, Design Lab, University of California, San Diego“Every UX professional should immerse themselves in this book. Not only does it unravel the fascinating and complex history of GUI widgets that will captivate any user interface nerd, but it also stands as the definitive guide to an incredibly diverse array of interaction techniques. This is not just an engaging read; it’s an essential toolkit.” ‐ Jakob Nielsen, Principal, Nielsen Design Group},
booktitle = {Pick, Click, Flick!: The Story of Interaction Techniques}
}

@inproceedings{10.1145/3339252.3341498,
author = {Kantola, Raimo},
title = {6G Network Needs to Support Embedded Trust},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3341498},
doi = {10.1145/3339252.3341498},
abstract = {A slogan coined at the recent first Levi 6G Summit by Peter Wetter of Nokia Bell Labs was "the 6G is about the 6th sense". This can be understood in at least two ways. One is that the network just knows what to do in all kinds of situations because of the use of AI and the second is that 6G radio will be widely used to sense the environment where the users are. In this view, 6G is seen as a continuation of the merge of the physical and the virtual words. An outcome of the Summit is a 6G White Paper documenting the ideas of some 60 invited people from the 300 participants about the future generation coming after 5G. This paper provides further discussion and justification of the trust and security aspects of the Networking Chapter in the White Paper. The paper focuses on principles and only refers to some verification in order not to clutter the discussion with technical detail. Opinions expressed here are of the author of this paper who was also the main editor of the Networking Chapter. The members of the White paper group on Networking or the editors of the White paper should not be held liable for the views expressed in the paper.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {104},
numpages = {5},
keywords = {6G, ID/Locator split, networking, reputation, trust},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3578245.3585032,
author = {Somashekar, Gagan and Kumar, Rajat},
title = {Enhancing the Configuration Tuning Pipeline of Large-Scale Distributed Applications Using Large Language Models (Idea Paper)},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3585032},
doi = {10.1145/3578245.3585032},
abstract = {The performance of distributed applications implemented using microservice architecture depends heavily on the configuration of various parameters, which are hard to tune due to large configuration search space and inter-dependence of parameters. While the information in product manuals and technical documents guides the tuning process, manual collection of meta-data for all application parameters is laborious and not scalable. Prior works have largely overlooked the automated use of product manuals, technical documents and source code for extracting such meta-data. In the current work, we propose using large language models for automated meta-data extraction and enhancing the configuration tuning pipeline. We further ideate on building an in-house knowledge system using experimental data to learn important parameters in configuration tuning using historical data on parameter dependence, workload statistics, performance metrics and resource utilization. We expect productionizing the proposed system will reduce the total time and experimental iterations required for configuration tuning in new applications, saving an organization both developer time and money.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {39–44},
numpages = {6},
keywords = {information retrieval, large language models, microservice architecture, parameter tuning},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3325917.3325942,
author = {Miraz, Mahdi H. and Donald, David C.},
title = {LApps: Technological, Legal and Market Potentials of Blockchain Lightning Network Applications},
year = {2019},
isbn = {9781450366359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325917.3325942},
doi = {10.1145/3325917.3325942},
abstract = {Following in the footsteps of pioneer Bitcoin, many altcoins as well as coloured coins have been being developed and merchandised adopting blockchain as the core enabling technology. However, since interoperability and scalability, due to high and capped (in particular cases) transaction latency are deep-rooted in the architecture of blockchain technology, they are by default inherited in any blockchain based applications. Lightning Network (LN) is one of the supporting technologies developed to eliminate this impediment of blockchain technology by facilitating instantaneous transfers of cryptos. Since the potentials of LN is still relatively unknown, this paper investigates the current states of development along with possible non-monetary usage of LN, especially in settlement coloured coins such as securities, as well as creation of new business models based on Lightning Applications (LApps) and microchannel payments as well as micro-trades. The legal challenges that may act as impediment to the adoption of LN is also discussed.},
booktitle = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining},
pages = {185–189},
numpages = {5},
keywords = {Atomic Swap, Blockchain, Coloured Coins, Contracts (HTLC), Cross-chain Trading, Cross-listing, Cryptocurrencies, DAO, DApps, Hashed Timelock Payment Channels, ICO, LApps, Layer 2, Lightning Network, Off-Chain, On-Chain, State Channels, Wallet-to-Wallet Transfer},
location = {Houston, TX, USA},
series = {ICISDM '19}
}

@inproceedings{10.1145/2740908.2741749,
author = {Hadgu, Asmelash Teka},
title = {Mining Scholarly Communication and Interaction on the Social Web},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741749},
doi = {10.1145/2740908.2741749},
abstract = {The explosion of Web 2.0 platforms including social networking sites such as Twitter, blogs and wikis affects all web users: scholars included. As a result, there is a need for a comprehensive approach to gain a broader understanding and timely signals of scientific communication as well as how researchers interact on the social web. Most current work in this area deals with either a low number of researchers and heavily relies on manual annotation or large-scale analysis without deep understanding of the underlying researcher population. In this proposal, we present a holistic approach to solve these problems. This research proposes novel methods to collect, filter, analyze and make sense of scholars and scholarly communication by integrating heterogeneous data sources from fast social media streams as well as the academic web. Applying reproducible research, contributing applications and data sets, the thesis proposal strives to add value by mining the social web for social good.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {499–503},
numpages = {5},
keywords = {altmetrics, scholars, scientific content, social web},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3630106.3658982,
author = {Antoniak, Maria and Naik, Aakanksha and Alvarado, Carla S. and Wang, Lucy Lu and Chen, Irene Y.},
title = {NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658982},
doi = {10.1145/3630106.3658982},
abstract = {Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of guiding principles. We propose nine principles for ethical use of NLP for maternal healthcare, grouped into three themes: (i) recognizing contextual significance (ii) holistic measurements, and (iii) who/what is valued. For each principle, we describe its underlying rationale and provide practical advice. This set of principles can provide a methodological pattern for other researchers and serve as a resource to practitioners working on maternal health and other healthcare fields to emphasize the importance of technical nuance, historical context, and inclusive design when developing NLP technologies for clinical use.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1446–1463},
numpages = {18},
keywords = {ethical guidelines, large language models, maternal health, natural language processing},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/2851581.2892591,
author = {Ahmad, Muhammad Aurangzeb},
title = {After Death: Big Data and the Promise of Resurrection by Proxy},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892591},
doi = {10.1145/2851581.2892591},
abstract = {With the advent of Big Data and the possibility of capturing massive personal data it is possible to simulate some aspects of a person's personality. The imitation game is based on the observation that it is possible to convince a person of a fake identity if sufficient information is available about the identity being faked. Imitation is however not limited to a person who is alive but also a person who is not alive; the question of simulating a deceased person for the purpose of having the simulation interact with a person is addressed. Various challenges and background considerations for such an endeavor are discussed. The goal of the paper is to open up discussion on this subject and examine its feasibility.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {397–408},
numpages = {12},
keywords = {ATA after death, death, postmortem data, techno-social systems},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3576915.3623063,
author = {Kuchhal, Dhruv and Saad, Muhammad and Oest, Adam and Li, Frank},
title = {Evaluating the Security Posture of Real-World FIDO2 Deployments},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623063},
doi = {10.1145/3576915.3623063},
abstract = {FIDO2 is a suite of protocols that combines the usability of local authentication (e.g., biometrics) with the security of public-key cryptography to deliver passwordless authentication. It eliminates shared authentication secrets (i.e., passwords, which could be leaked or phished) and provides strong security guarantees assuming the benign behavior of the client-side protocol components.However, when this assumption does not hold true, such as in the presence of malware, client authentications pose a risk that FIDO2 deployments must account for. FIDO2 provides recommendations for deployments to mitigate such situations. Yet, to date, there has been limited empirical investigation into whether deployments adopt these mitigations and what risks compromised clients present to real-world FIDO2 deployments, such as unauthorized account access or registration.In this work, we aim to fill in the gap by: 1) systematizing the threats to FIDO2 deployments when assumptions about the client-side protocol components do not hold, 2) empirically evaluating the security posture of real-world FIDO2 deployments across the Tranco Top 1K websites, considering both the server-side and client-side perspectives, and 3) synthesizing the mitigations that the ecosystem can adopt to further strengthen the practical security provided by FIDO2. Through our investigation, we identify that compromised clients pose a practical threat to FIDO2 deployments due to weak configurations, and known mitigations exhibit critical shortcomings and/or minimal adoption. Based on our findings, we propose directions for the ecosystem to develop additional defenses into their FIDO2 deployments. Ultimately, our work aims to drive improvements to FIDO2's practical security.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2381–2395},
numpages = {15},
keywords = {fido2, malware, security measurements, webauthn},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/3605764.3623985,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79–90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {Copenhagen, Denmark},
series = {AISec '23}
}

@proceedings{10.5555/3590145,
title = {ASONAM '22: Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
year = {2022},
isbn = {9781665456616},
publisher = {IEEE Press},
abstract = {We were delighted to welcome each participant at ASONAM 2022 and thank you for having contributed virtually or in person in Istanbul. ASONAM 2022 was the fourteenth annual conference in the successful ASONAM conferences series and also the first hybrid version of the conference. Previous ASONAM conferences were held in Athens (2009), Odense (2010), Kaohsiung (2011), Istanbul (2012), Niagara Falls (2013), Beijing (2014), Paris (2015), San Francisco (2016), Sydney (2017), Barcelona (2018), Vancouver (2019), Virtual (2020), Virtual (2021). The pre-pandemic locations of the conferences have enabled the participants to enjoy local sights and to engage in person-to-person interactions, making new contacts and form new scientific collaborations. These possibilities were only available in a limited form during the virtual conferences. As the covid pandemic seems to be moving towards an endemic form it was decided to have the conference in the hybrid form, as a move towards normal endemic in-person conferences.For more than a century, social networks have been studied in a variety of disciplines including sociology, anthropology, psychology, and economics. The Internet, the social Web, and other large-scale, sociotechnological infrastructures have triggered a growing interest and resulted in significant methodological advancements in social network analysis and mining. Method development in graph theory, statistics, data mining, machine learning, and AI have inspired new research problems and, in turn, opens up further possibilities for application. These spiraling trends have led to a rising prominence of social network analysis and mining methods and tools in academia, politics, security, and business.},
location = {Istanbul, Turkey}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3528588,
title = {NLBSE '22: Proceedings of the 1st International Workshop on Natural Language-based Software Engineering},
year = {2022},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.5555/3623290,
title = {ICSE-SEIS '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
abstract = {We are delighted to introduce the Software Engineering in Society (SEIS) track program as part of the 45th IEEE/ACM International Conference on Software Engineering, to be held in Melbourne, Australia, on May 14-20, 2023. The aim of the track is to bring together researchers studying various roles that software engineering plays in society.},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3613372.3613405,
author = {Gomes, Anderson and Maia, Paulo Henrique M.},
title = {DoME: An Architecture for Domain Model Evolution at Runtime Using NLP},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613405},
doi = {10.1145/3613372.3613405},
abstract = {In traditional information systems, domain models are represented as database tables with attributes and relationships. Changes in the domain models exist due to system evolution and the emergence of new requirements. In these applications, domain models evolve using CRUD operations requested by users. However, it is necessary to support changes in domain models during the applications’ runtime when new (unforeseen) situations may occur. This work presents an architecture called DoME, which relies on natural language processing (NLP) to allow users to trigger changes in the domain models and self-adaptation techniques to update the models at runtime. It is instantiated in a concrete architecture using a chatbot in Telegram and Transformers Libraries for NLP. The architecture has been preliminary evaluated regarding its assertiveness and user satisfaction, resulting in an 82.55% hit rate and confirming that NL provides good usability and facilitates data manipulation.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {186–195},
numpages = {10},
keywords = {Domain Modelling., Generative Artificial Intelligence, Natural Language Processing, Software Architecture},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@article{10.1145/3543845,
author = {Klare, Heiko and Gleitze, Joshua},
title = {Termination and Expressiveness of Execution Strategies for Networks of Bidirectional Model Transformations},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0934-5043},
url = {https://doi.org/10.1145/3543845},
doi = {10.1145/3543845},
abstract = {When developers describe a software system with multiple models, such as architecture diagrams, deployment descriptions, and source code, these models must represent the system in a uniform way, i.e., they must be and stay consistent. One means to automatically preserve consistency after changes to models are model transformations, of which bidirectional transformations that preserve consistency between two models have been well researched. To preserve consistency between multiple models, such transformations can be combined to networks. When transformations are developed independently and reused modularly, the resulting network can be of arbitrary topology. For such networks, no universal strategy exists to orchestrate the execution of transformations such that the resulting models are consistent.In this article, we prove that termination of such a strategy can only be guaranteed if it is incomplete, i.e., if it is allowed to fail to restore consistency for some changes although an execution order of transformations exists that yields consistent models. We propose such a strategy, for which we prove termination and show that and why it makes it easier for users of model transformation networks to understand the reasons whenever the strategy fails. In addition, we provide a simulator for the comparison of different execution strategies. These findings help transformation developers and users in understanding when and why they can expect the execution of a transformation network to terminate and when they can even expect it to succeed. Furthermore, the proposed strategy guarantees them termination and supports them in finding the reason whenever it is not successful.},
journal = {Form. Asp. Comput.},
month = {sep},
articleno = {15},
numpages = {35}
}

@inproceedings{10.1145/3324884.3416627,
author = {Li, Mingyang and Shi, Lin and Yang, Ye and Wang, Qing},
title = {A deep multitask learning approach for requirements discovery and annotation from open forum},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416627},
doi = {10.1145/3324884.3416627},
abstract = {The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91% and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {336–348},
numpages = {13},
keywords = {deep learning, multitask learning, requirements annotation, requirements discovery},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3463676.3485599,
author = {Fietkau, Julian and Thimmaraju, Kashyap and Kybranz, Felix and Neef, Sebastian and Seifert, Jean-Pierre},
title = {The Elephant in the Background: A Quantitative Approachto Empower Users Against Web Browser Fingerprinting},
year = {2021},
isbn = {9781450385275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463676.3485599},
doi = {10.1145/3463676.3485599},
abstract = {Tracking users is a ubiquitous practice on the web today. User activity is recorded and analyzed on a large scale to create personalized products, forecast future behavior, and prevent online fraud. While HTTP cookies have been the weapon of choice so far, new and more pervasive techniques such as browser fingerprinting are gaining traction. This paper describes how users can be empowered against fingerprinting by showing them when, how, and who is tracking them. To this end, we conduct a systematic analysis of various fingerprinting tools to create FPMON: a browser extension to measure and rate fingerprinting activity on any website in real-time. With FPMON, we evaluate the 10k most popular websites to i) study the pervasiveness of fingerprinting; ii) review the latest countermeasures; and iii) identify the networks that foster the use of fingerprinting. Our evaluations reveal that i) fingerprinters subvert privacy regulations; ii) they are present on privacy-sensitive websites (insurance, finances, NGOs); and iii) current countermeasures cannot sufficiently protect users. Hence, we publish FPMON as a free browser extension to empower users against this growing threat.},
booktitle = {Proceedings of the 20th Workshop on Workshop on Privacy in the Electronic Society},
pages = {167–180},
numpages = {14},
keywords = {browser fingerprinting, online tracking, web privacy},
location = {Virtual Event, Republic of Korea},
series = {WPES '21}
}

@proceedings{10.5555/3623288,
title = {ICSE-NIER '23: Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3639476,
title = {ICSE-NIER'24: Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@inproceedings{10.1145/2970276.2970312,
author = {Wei, Lili and Liu, Yepang and Cheung, Shing-Chi},
title = {Taming Android fragmentation: characterizing and detecting compatibility issues for Android apps},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970312},
doi = {10.1145/2970276.2970312},
abstract = {Android ecosystem is heavily fragmented. The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps. As a result, various compatibility issues arise, causing poor user experience. However, little is known on the characteristics of such fragmentation-induced compatibility issues and no mature tools exist to help developers quickly diagnose and fix these issues. To bridge the gap, we conducted an empirical study on 191 real-world compatibility issues collected from popular open-source Android apps. Our study characterized the symptoms and root causes of compatibility issues, and disclosed that the patches of these issues exhibit common patterns. With these findings, we propose a technique named FicFinder to automatically detect compatibility issues in Android apps. FicFinder performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues are triggered. FicFinder reports actionable debugging information to developers when it detects potential issues. We evaluated FicFinder with 27 large-scale open-source Android apps. The results show that FicFinder can precisely detect compatibility issues in these apps and uncover previously-unknown issues.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {226–237},
numpages = {12},
keywords = {Android fragmentation, compatibility issues},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1109/ICSE43902.2021.00092,
author = {Haering, Marlo and Stanik, Christoph and Maalej, Walid},
title = {Automatically Matching Bug Reports With Related App Reviews},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00092},
doi = {10.1109/ICSE43902.2021.00092},
abstract = {App stores allow users to give valuable feedback on apps, and developers to find this feedback and use it for the software evolution. However, finding user feedback that matches existing bug reports in issue trackers is challenging as users and developers often use a different language. In this work, we introduce DeepMatcher, an automatic approach using state-of-the-art deep learning methods to match problem reports in app reviews to bug reports in issue trackers. We evaluated DeepMatcher with four open-source apps quantitatively and qualitatively. On average, DeepMatcher achieved a hit ratio of 0.71 and a Mean Average Precision of 0.55. For 91 problem reports, DeepMatcher did not find any matching bug report. When manually analyzing these 91 problem reports and the issue trackers of the studied apps, we found that in 47 cases, users actually described a problem before developers discovered and documented it in the issue tracker. We discuss our findings and different use cases for DeepMatcher.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {970–981},
numpages = {12},
keywords = {app store analytics, deep learning, mining software repositories, natural language processing, software evolution},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3404397.3404435,
author = {Zou, Pengfei and Li, Ang and Barker, Kevin and Ge, Rong},
title = {Detecting Anomalous Computation with RNNs on GPU-Accelerated HPC Machines},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404435},
doi = {10.1145/3404397.3404435},
abstract = {This paper presents a workload classification framework that accurately discriminates illicit computation from authorized workloads on GPU-accelerated HPC systems at runtime. As such systems become increasingly powerful and widely-adopted, attackers have begun to run illicit and for-profit programs that typically require extremely high computing capability to be successful, depriving mission-critical and authorized workloads of execution cycles and increasing risks of data leaking and empowered attacks. Traditional measures on CPU hosts are oblivious to such attacks. Our classification framework leverages the distinctive signatures between illicit and authorized GPU workloads, and explores machine learning methods and workload profiling to classify them. We face multiple challenges in designing the framework: achieving high detection accuracy, maintaining low profiling and inference overhead, and overcoming the limitation of lacking data types and volumes typically required by deep learning models. To address these challenges, we use lightweight, non-intrusive, high-level workload profiling, collect multiple sequences of easily obtainable multimodal input data, and build recurrent neural networks (RNNs) to learn from history for online anomalous workload detection. Evaluation results on three generations of GPU machines demonstrate that the workload classification framework can tell apart the illicit workloads with a high accuracy of over 95%. The collected dataset, detection framework, and neural network models are released on github1.},
booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
articleno = {52},
numpages = {11},
keywords = {GPU accelerated systems, HPC security, workload classification.},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@inproceedings{10.1109/SC.2018.00040,
author = {Andreadis, Georgios and Versluis, Laurens and Mastenbroek, Fabian and Iosup, Alexandru},
title = {A reference architecture for datacenter scheduling: design, validation, and experiments},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00040},
doi = {10.1109/SC.2018.00040},
abstract = {Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {37},
numpages = {15},
keywords = {datacenter, reference architecture, scheduling},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/2957792.2957804,
author = {Zangerle, Eva and Gassler, Wolfgang and Pichl, Martin and Steinhauser, Stefan and Specht, G\"{u}nther},
title = {An Empirical Evaluation of Property Recommender Systems for Wikidata and Collaborative Knowledge Bases},
year = {2016},
isbn = {9781450344517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957792.2957804},
doi = {10.1145/2957792.2957804},
abstract = {The Wikidata platform is a crowdsourced, structured knowledgebase aiming to provide integrated, free and language-agnostic facts which are---amongst others---used by Wikipedias. Users who actively enter, review and revise data on Wikidata are assisted by a property suggesting system which provides users with properties that might also be applicable to a given item. We argue that evaluating and subsequently improving this recommendation mechanism and hence, assisting users, can directly contribute to an even more integrated, consistent and extensive knowledge base serving a huge variety of applications. However, the quality and usefulness of such recommendations has not been evaluated yet. In this work, we provide the first evaluation of different approaches aiming to provide users with property recommendations in the process of curating information on Wikidata. We compare the approach currently facilitated on Wikidata with two state-of-the-art recommendation approaches stemming from the field of RDF recommender systems and collaborative information systems. Further, we also evaluate hybrid recommender systems combining these approaches. Our evaluations show that the current recommendation algorithm works well in regards to recall and precision, reaching a recall@7 of 79.71% and a precision@7 of 27.97%. We also find that generally, incorporating contextual as well as classifying information into the computation of property recommendations can further improve its performance significantly.},
booktitle = {Proceedings of the 12th International Symposium on Open Collaboration},
articleno = {18},
numpages = {8},
keywords = {Evaluation, Recommender Systems, Wikidata, Wikipedia},
location = {Berlin, Germany},
series = {OpenSym '16}
}

@inproceedings{10.1145/3442520.3442530,
author = {Cui, Pinchen and Umphress, David},
title = {Towards Unsupervised Introspection of Containerized Application},
year = {2021},
isbn = {9781450389037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442520.3442530},
doi = {10.1145/3442520.3442530},
abstract = {Container (or containerization) as one of the new concepts of virtualization, has attracted increasing attention and occupied a considerable amount of market size owing to the inherent lightweight characteristic. However, the lightweight advantage is achieved at the price of the security. Attacks against weak isolation of the container have been reported, and the use of a shared kernel is another targeted vulnerable point. This work aims to provide secure monitoring of containerized applications, which can help i) the infrastructure owner to ensure the running application is harmless, ii) the application owner to detect anomalous behaviors. We propose to use unsupervised introspection tools to perform the non-intrusive monitoring, which leverages the system call traces to classify the anomalies. Since the traditional dataset used for anomaly detection either only focus on network traces or limited to few attributes of system calls, we crafted and collected various normal and abnormal behaviors of a containerized application, and an optimized and open-source system call based dataset has been built. Unsupervised machine learning classifiers are trained over the proposed dataset, a comprehensive case study has been performed and analyzed. The results show the feasibility of unsupervised introspection of containerized applications.},
booktitle = {Proceedings of the 2020 10th International Conference on Communication and Network Security},
pages = {42–51},
numpages = {10},
keywords = {Anomaly Detection, Container, Docker, Introspection, Open Source Dataset, Unsupervised},
location = {Tokyo, Japan},
series = {ICCNS '20}
}

@article{10.1109/TASLP.2022.3181350,
author = {An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim, Misuk},
title = {Domain-Slot Relationship Modeling Using a Pre-Trained Language Encoder for Multi-Domain Dialogue State Tracking},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3181350},
doi = {10.1109/TASLP.2022.3181350},
abstract = {Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. As using pre-trained language models is the de facto standard for natural language processing tasks, many recent studies use them to encode the dialogue context for predicting the dialogue states. Model architectures that have certain inductive biases for modeling the relationship among different domain-slot pairs are also emerging. Our work is based on these research approaches on multi-domain dialogue state tracking. We propose a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$[CLS]$&lt;/tex-math&gt;&lt;/inline-formula&gt; token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results on the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our model outperforms other models with the same setting. Our ablation studies incorporate three main parts. The first component shows the effectiveness of our approach exploiting the relationship modeling. The second component compares the effect of using different pre-trained language encoders. The final component involves comparing different initialization methods that could be used for the special tokens. Qualitative analysis of the attention map of the pre-trained language encoder shows that our special tokens encode relevant information through the encoding process by attending to each other.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2091–2102},
numpages = {12}
}

@proceedings{10.1145/3639477,
title = {ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3649217,
title = {ITiCSE 2024: Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 29th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2024), hosted by Universit\`{a} degli Studi di Milano in Milan, Italy.ITiCSE 2024 will take place from Friday July 5 to Wednesday July 10. The conference program includes a keynote address, paper sessions, a panel, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet July 5-7 and will submit draft reports before the conference begins on July 8.The submissions to ITiCSE 2024 were reviewed by 446 researchers and practitioners from computing education and related fields, including 44 program committee members and 402 reviewers. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.},
location = {Milan, Italy}
}

@proceedings{10.1145/3660354,
title = {WDC '24: Proceedings of the 3rd ACM Workshop on the Security Implications of Deepfakes and Cheapfakes},
year = {2024},
isbn = {9798400706493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3540250.3558944,
author = {Christakis, Maria and Cottenier, Thomas and Filieri, Antonio and Luo, Linghui and Mansur, Muhammad Numair and Pike, Lee and Rosner, Nicol\'{a}s and Sch\"{a}f, Martin and Sengupta, Aritra and Visser, Willem},
title = {Input splitting for cloud-based static application security testing platforms},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558944},
doi = {10.1145/3540250.3558944},
abstract = {As software development teams adopt DevSecOps practices, application security is increasingly the responsibility of development teams, who are required to set up their own Static Application Security Testing (SAST) infrastructure. Since development teams often do not have the necessary infrastructure and expertise to set up a custom SAST solution, there is an increased need for cloud-based SAST platforms that operate as a service and run a variety of static analyzers. Adding a new static analyzer to a cloud-based SAST platform can be challenging because static analyzers greatly vary in complexity, from linters that scale efficiently to interprocedural dataflow engines that use cubic or even more complex algorithms. Careful manual evaluation is needed to decide whether a new analyzer would slow down the overall response time of the platform or may timeout too often. We explore the question of whether this can be simplified by splitting the input to the analyzer into partitions and analyzing the partitions independently. Depending on the complexity of the static analyzer, the partition size can be adjusted to curtail the overall response time. We report on an experiment where we run different analysis tools with and without splitting the inputs. The experimental results show that simple splitting strategies can effectively reduce the running time and memory usage per partition without significantly affecting the findings produced by the tool.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1367–1378},
numpages = {12},
keywords = {API usage checking, software security, static analysis in the cloud},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3640794.3665585,
author = {Edwards, Justin and Nguyen, Andy and Sobocinski, Marta and L\"{a}ms\"{a}, Joni and de Araujo, Adelson and Dang, Belle and Whitehead, Ridwan and Roberts, Anni-Sofia and Kaarlela, Matti and Jarvela, Sanna},
title = {MAI - A Proactive Speech Agent for Metacognitive Mediation in Collaborative Learning},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665585},
doi = {10.1145/3640794.3665585},
abstract = {We introduce MAI - a proactive speech agent aimed at enhancing metacognitive awareness among learners in collaborative learning settings. Background is presented around Socially Shared Regulation of Learning and the role of metacognition in learning. Next, the design of the rules that MAI uses to prompt learners and mediate metacognition are introduced. We describe the conditions in which MAI has been piloted thus far, including as a Wizard of Oz prototype and as a fully functional prototype using natural language processing. We discuss the ethical considerations that went into the prototyping and testing of MAI. Finally, we describe our next steps for understanding the interactions learners had with MAI already, planned design changes, and the future of testing the agent.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {46},
numpages = {5},
keywords = {collaborative learning, proactive agents, regulation of learning, speech agent, speech interfaces},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3450337.3483496,
author = {Allameh, Mahdieh and Zaman, Loutfouz},
title = {Jessy: A Conversational Assistant for Tutoring Digital Board Games},
year = {2021},
isbn = {9781450383561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450337.3483496},
doi = {10.1145/3450337.3483496},
abstract = {We present Jessy, an interactive intelligent digital board game assistant for The Royal Game of Ur. Jessy is capable of answering questions regarding the game rules, giving suggestions for best moves considering the player's state, and informing the player about the important events in the game. An explanatory non-comparative study was conducted to evaluate the usability and usefulness of Jessy in engaging and learning how to play the game. The study showed Jessy was helpful in general and the findings suggest insights on how to craft Jessy for its target application – difficult board games.},
booktitle = {Extended Abstracts of the 2021 Annual Symposium on Computer-Human Interaction in Play},
pages = {168–173},
numpages = {6},
keywords = {conversational agents, digital assistant, digital board games, game tutorials},
location = {Virtual Event, Austria},
series = {CHI PLAY '21}
}

@inproceedings{10.1145/3640794.3665560,
author = {Zhou, Jijie and Hu, Yuhan},
title = {Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665560},
doi = {10.1145/3640794.3665560},
abstract = {Recently, large language models have facilitated the emergence of highly intelligent conversational AI capable of engaging in human-like dialogues. However, a notable distinction lies in the fact that these AI models predominantly generate responses rapidly, often producing extensive content without emulating the thoughtful process characteristic of human cognition and typing. This paper presents a design aimed at simulating human-like typing behaviors, including patterns such as hesitation and self-editing, as well as a preliminary user experiment to understand whether and to what extent the agent with human-like typing behaviors could potentially affect conversational engagement and its trustworthiness. We’ve constructed an interactive platform featuring user-adjustable parameters, allowing users to personalize the AI’s communication style and thus cultivate a more enriching and immersive conversational experience. Our user experiment, involving interactions with three types of agents—a baseline agent, one simulating hesitation, and another integrating both hesitation and self-editing behaviors—reveals a preference for the agent that incorporates both behaviors, suggesting an improvement in perceived naturalness and trustworthiness. Through the insights from our design process and both quantitative and qualitative feedback from user experiments, this paper contributes to the multimodal interaction design and user experience for conversational AI, advocating for a more human-like, engaging, and trustworthy communication paradigm.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {24},
numpages = {12},
keywords = {AI, ChatGPT, Human-Computer Interaction (HCI), chatbot, conversational interface, delay, hesitation, interaction design, multimodal, self-correction, self-editing, typing},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3170427.3174367,
author = {Edge, Darren and Larson, Jonathan and White, Christopher},
title = {Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3174367},
doi = {10.1145/3170427.3174367},
abstract = {The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {ai, business intelligence, data, hci, visual analytics},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3540250.3560885,
author = {Gu, Taotao and Li, Xiang and Lu, Shuaibing and Tian, Jianwen and Nie, Yuanping and Kuang, Xiaohui and Lin, Zhechao and Liu, Chenyifan and Liang, Jie and Jiang, Yu},
title = {Group-based corpus scheduling for parallel fuzzing},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3560885},
doi = {10.1145/3540250.3560885},
abstract = {Parallel fuzzing relies on hardware resources to guarantee test throughput and efficiency. In industrial practice, it is well known that parallel fuzzing faces the challenge of task division, but most works neglect the important process of corpus allocation. In this paper, we proposed a group-based corpus scheduling strategy to address these two issues, which has been accepted by the LLVM community. And we implement a parallel fuzzer based on this strategy called glibFuzzer. glibFuzzer first groups the global corpus into different subsets and then assigns different energy scores and different scores to them. The energy scores were mainly determined by the seed size and the length of coverage information, and the difference score can describe the degree of difference in the code covered by different subsets of seeds. In each round of key local corpus construction, the master node selects high-quality seeds by combining the two scores to improve test efficiency and avoid task conflict. To prove the effectiveness of the strategy, we conducted an extensive evaluation on the real-world programs and FuzzBench. After 4\texttimes{}24 CPU-hours, glibFuzzer covered 22.02% more branches and executed 19.42 times more test cases than libFuzzer in 18 real-world programs. glibFuzzer showed an average branch coverage increase of 73.02%, 55.02%, 55.86% over AFL, PAFL, UniFuzz, respectively. More importantly, glibFuzzer found over 100 unique vulnerabilities.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1521–1532},
numpages = {12},
keywords = {Parallel fuzzing, Seed scheduling, Vulnerability detection},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3635033,
author = {Chen, Ninghan and Chen, Xihui and Zhong, Zhiqiang and Pang, Jun},
title = {Bridging Performance of X (formerly known as Twitter) Users: A Predictor of Subjective Well-Being During the Pandemic},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3635033},
doi = {10.1145/3635033},
abstract = {The outbreak of the COVID-19 pandemic triggered the perils of misinformation over social media. By amplifying the spreading speed and popularity of trustworthy information, influential social media users have been helping overcome the negative impacts of such flooding misinformation. In this article, we use the COVID-19 pandemic as a representative global health crisisand examine the impact of the COVID-19 pandemic on these influential users’ subjective well-being (SWB), one of the most important indicators of mental health. We leverage X (formerly known as Twitter) as a representative social media platform and conduct the analysis with our collection of 37,281,824 tweets spanning almost two years. To identify influential X users, we propose a new measurement called user bridging performance (UBM) to evaluate the speed and wideness gain of information transmission due to their sharing. With our tweet collection, we manage to reveal the more significant mental sufferings of influential users during the COVID-19 pandemic. According to this observation, through comprehensive hierarchical multiple regression analysis, we are the first to discover the strong relationship between individual social users’ subjective well-being and their bridging performance. We proceed to extend bridging performance from individuals to user subgroups. The new measurement allows us to conduct a subgroup analysis according to users’ multilingualism and confirm the bridging role of multilingual users in the COVID-19 information propagation. We also find that multilingual users not only suffer from a much lower SWB in the pandemic, but also experienced a more significant SWB drop.},
journal = {ACM Trans. Web},
month = {jan},
articleno = {15},
numpages = {23},
keywords = {Information diffusion, bridging performance, Twitter, COVID-19, subjective well-being, datasets}
}

@article{10.1145/3274343,
author = {Im, Jane and Zhang, Amy X. and Schilling, Christopher J. and Karger, David},
title = {Deliberation and Resolution on Wikipedia: A Case Study of Requests for Comments},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274343},
doi = {10.1145/3274343},
abstract = {Resolving disputes in a timely manner is crucial for any online production group. We present an analysis of Requests for Comments (RfCs), one of the main vehicles on Wikipedia for formally resolving a policy or content dispute. We collected an exhaustive dataset of 7,316 RfCs on English Wikipedia over the course of 7 years and conducted a qualitative and quantitative analysis into what issues affect the RfC process. Our analysis was informed by 10 interviews with frequent RfC closers. We found that a major issue affecting the RfC process is the prevalence of RfCs that could have benefited from formal closure but that linger indefinitely without one, with factors including participants' interest and expertise impacting the likelihood of resolution. From these findings, we developed a model that predicts whether an RfC will go stale with 75.3% accuracy, a level that is approached as early as one week after dispute initiation.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {74},
numpages = {24},
keywords = {collaboration, comments, consensus, dataset, deliberation, dispute resolution, online communities, online discussion, wikipedia}
}

@inproceedings{10.1145/3631802.3631830,
author = {Liffiton, Mark and Sheese, Brad E and Savelka, Jaromir and Denny, Paul},
title = {CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631830},
doi = {10.1145/3631802.3631830},
abstract = {Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students’ usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {8},
numpages = {11},
keywords = {Guardrails, Intelligent programming tutors, Intelligent tutoring systems, Large language models, Natural language interfaces, Novice programmers, Programming assistance},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@article{10.1145/3386323,
author = {Bright, Walter and Alexandrescu, Andrei and Parker, Michael},
title = {Origins of the D programming language},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386323},
doi = {10.1145/3386323},
abstract = {As its name suggests, the initial motivation for the D programming language was to improve on C and C++ while keeping their spirit. The D language was to preserve those languages' efficiency, low-level access, and Algol-style syntax. The areas D set out to improve focused initially on rapid development, convenience, and simplifying the syntax without hampering expressiveness.  The genesis of D has its peculiarities, as is the case with many other languages. Walter Bright, D's creator, is a mechanical engineer by education who started out working for Boeing designing gearboxes for the 757. He was programming games on the side, and in trying to make his game Empire run faster, became interested in compilers. Despite having no experience, Bright set out in 1982 to implement a compiler that produced better code than those on the market at the time.  This interest materialized into a C compiler, followed by compilers for C++, Java, and JavaScript. Best known of these would be the Zortech C++ compiler, the first (and to date only) C++-to-native compiler developed by a single person. The D programming language began in 1999 as an effort to pull the best features of these languages into a new one. Fittingly, D would use the by that time mature C/C++ back end (optimizer and code generator) that had been under continued development and maintenance since 1982.  Between 1999 and 2006, Bright worked alone on the D language definition and its implementation, although a steadily increasing volume of patches from users was incorporated. The new language would be based on the past successes of the languages he'd used and implemented, but would be clearly looking to the future. D started with choices that are obvious today but were less clear winners back in the 1990s: full support for Unicode, IEEE floating point, 2s complement arithmetic, and flat memory addressing (memory is treated as a linear address space with no segmentation). It would do away with certain compromises from past languages imposed by shortages of memory (for example, forward declarations would not be required). It would primarily appeal to C and C++ users, as expertise with those languages would be readily transferrable. The interface with C was designed to be zero cost.  The language design was begun in late 1999. An alpha version appeared in 2001 and the initial language was completed, somewhat arbitrarily, at version 1.0 in January 2007. During that time, the language evolved considerably, both in capability and in the accretion of a substantial worldwide community that became increasingly involved with contributing. The front end was open-sourced in April 2002, and the back end was donated by Symantec to the open source community in 2017. Meanwhile, two additional open-source back ends became mature in the 2010s: `gdc` (using the same back end as the GNU C++ compiler) and `ldc` (using the LLVM back end).  The increasing use of the D language in the 2010s created an impetus for formalization and development management. To that end, the D Language Foundation was created in September 2015 as a nonprofit corporation overseeing work on D's definition and implementation, publications, conferences, and collaborations with universities.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {73},
numpages = {38},
keywords = {Programming Languages}
}

@article{10.1145/3439769,
author = {Uddin, Gias and Khomh, Foutse and Roy, Chanchal K.},
title = {Automatic API Usage Scenario Documentation from Technical Q&amp;A Sites},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3439769},
doi = {10.1145/3439769},
abstract = {The online technical Q&amp;A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {31},
numpages = {45},
keywords = {API, crowd-sourced developer forum, documentation, usage scenario}
}

@article{10.1145/3448291,
author = {Tripathi, Nikhil and Hubballi, Neminath},
title = {Application Layer Denial-of-Service Attacks and Defense Mechanisms: A Survey},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3448291},
doi = {10.1145/3448291},
abstract = {Application layer Denial-of-Service (DoS) attacks are generated by exploiting vulnerabilities of the protocol implementation or its design. Unlike volumetric DoS attacks, these are stealthy in nature and target a specific application running on the victim. There are several attacks discovered against popular application layer protocols in recent years. In this article, we provide a structured and comprehensive survey of the existing application layer DoS attacks and defense mechanisms. We classify existing attacks and defense mechanisms into different categories, describe their working, and compare them based on relevant parameters. We conclude the article with directions for future research.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {86},
numpages = {33},
keywords = {Protocol-specific and generic DoS attacks, defense mechanisms, distributed DoS attacks}
}

@inproceedings{10.1145/3604915.3609487,
author = {Hidasi, Bal\'{a}zs and Czapp, \'{A}d\'{a}m Tibor},
title = {The Effect of Third Party Implementations on Reproducibility},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3609487},
doi = {10.1145/3604915.3609487},
abstract = {Reproducibility of recommender systems research has come under scrutiny during recent years. Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility. We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility. Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets. In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {272–282},
numpages = {11},
keywords = {offline evaluation, recommender systems, reimplementation, reproducibility, sequential recommendations, session-based recommendations, third-party implementation, unofficial implementation},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{10.1145/3641006,
author = {Zhang, Amy X. and Bernstein, Michael S. and Karger, David R. and Ackerman, Mark S.},
title = {Form-From: A Design Space of Social Media Systems},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3641006},
doi = {10.1145/3641006},
abstract = {Social media systems are as varied as they are pervasive. They have been almost universally adopted for a broad range of purposes including work, entertainment, activism, and decision making. As a result, they have also diversified, with many distinct designs differing in content type, organization, delivery mechanism, access control, and many other dimensions. In this work, we aim to characterize and then distill a concise design space of social media systems that can help us understand similarities and differences, recognize potential consequences of design choice, and identify spaces for innovation. Our model, which we call Form-From, characterizes social media based on (1) the form of the content, either threaded or flat, and (2) from where or from whom one might receive content, ranging from spaces to networks to the commons. We derive Form-From inductively from a larger set of 62 dimensions organized into 10 categories. To demonstrate the utility of our model, we trace the history of social media systems as they traverse the Form-From space over time, and we identify common design patterns within cells of the model.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {167},
numpages = {47},
keywords = {design space, social computing systems, social media, taxonomy}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@inproceedings{10.1145/3605770.3625216,
author = {Wyss, Elizabeth and De Carli, Lorenzo and Davidson, Drew},
title = {(Nothing But) Many Eyes Make All Bugs Shallow},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605770.3625216},
doi = {10.1145/3605770.3625216},
abstract = {Open source package repositories have become a crucial component of the modern software supply chain since they enable developers to easily and rapidly import code written by others. However, low quality, poorly vetted code residing in such repositories exposes developers and end-users to dangerous bugs and vulnerabilities at a large scale. Such issues have recently led to the creation of government-backed verification standards pertaining to packages, as well as a significant body of developer folklore regarding what constitutes a reliable package. However, there exists little academic research assessing the relationships between recommended development practices and known package issues in this domain. Motivated by this gap in understanding, we conduct a large-scale study that formally evaluates whether adherence to these guidelines meaningfully impacts reported issues and bug maintenance activity across the most widely utilized npm packages (encompassing 7,162 packages with over 100K weekly downloads each), which unveiled wide disparities across package-level metrics. We find that it is only recommendations pertaining to a broad notion of scrutiny that provide strong and reliable insights into the reporting and resolving of package issues. These findings pose significant implications for developers, who seek to identify well-maintained packages for use, as well as security researchers, who seek to identify suspicious packages for critical observation.},
booktitle = {Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {53–63},
numpages = {11},
keywords = {open-source, package repositories, software supply chain},
location = {Copenhagen, Denmark},
series = {SCORED '23}
}

@inproceedings{10.1145/3453483.3454066,
author = {Stanford, Caleb and Veanes, Margus and Bj\o{}rner, Nikolaj},
title = {Symbolic Boolean derivatives for efficiently solving extended regular expression constraints},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454066},
doi = {10.1145/3453483.3454066},
abstract = {The manipulation of raw string data is ubiquitous in security-critical software, and verification of such software relies on efficiently solving string and regular expression constraints via SMT. However, the typical case of Boolean combinations of regular expression constraints exposes blowup in existing techniques. To address solvability of such constraints, we propose a new theory of derivatives of symbolic extended regular expressions (extended meaning that complement and intersection are incorporated), and show how to apply this theory to obtain more efficient decision procedures. Our implementation of these ideas, built on top of Z3, matches or outperforms state-of-the-art solvers on standard and handwritten benchmarks, showing particular benefits on examples with Boolean combinations.  Our work is the first formalization of derivatives of regular expressions which both handles intersection and complement and works symbolically over an arbitrary character theory. It unifies existing approaches involving derivatives of extended regular expressions, alternating automata and Boolean automata by lifting them to a common symbolic platform. It relies on a parsimonious augmentation of regular expressions: a construct for symbolic conditionals is shown to be sufficient to obtain relevant closure properties for derivatives over extended regular expressions.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {620–635},
numpages = {16},
keywords = {SMT, automaton, derivative, regex, regular expression, string},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3196398.3196423,
author = {Zampetti, Fiorella and Serebrenik, Alexander and Di Penta, Massimiliano},
title = {Was self-admitted technical debt removal a real removal? an in-depth perspective},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196423},
doi = {10.1145/3196398.3196423},
abstract = {Technical Debt (TD) has been defined as "code being not quite right yet", and its presence is often self-admitted by developers through comments. The purpose of such comments is to keep track of TD and appropriately address it when possible. Building on a previous quantitative investigation by Maldonado et al. on the removal of self-admitted technical debt (SATD), in this paper we perform an in-depth quantitative and qualitative study of how SATD is addressed in five Java open source projects. On the one hand, we look at whether SATD is "accidentally" removed, and the extent to which the SATD removal is being documented. We found that that (i) between 20% and 50% of SATD comments are accidentally removed while entire classes or methods are dropped, (ii) 8% of the SATD removal is acknowledged in commit messages, and (iii) while most of the changes addressing SATD require complex source code changes, very often SATD is addressed by specific changes to method calls or conditionals. Our results can be used to better plan TD management or learn patterns for addressing certain kinds of TD and provide recommendations to developers.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {526–536},
numpages = {11},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3563766.3564109,
author = {Le, Franck and Srivatsa, Mudhakar and Ganti, Raghu and Sekar, Vyas},
title = {Rethinking data-driven networking with foundation models: challenges and opportunities},
year = {2022},
isbn = {9781450398992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563766.3564109},
doi = {10.1145/3563766.3564109},
abstract = {Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
pages = {188–197},
numpages = {10},
keywords = {foundational models, machine learning, network management and security},
location = {Austin, Texas},
series = {HotNets '22}
}

@inproceedings{10.1145/3510003.3510108,
author = {Shi, Lin and Mu, Fangwen and Zhang, Yumin and Yang, Ye and Chen, Junjie and Chen, Xiao and Jiang, Hanzhi and Jiang, Ziyou and Wang, Qing},
title = {BugListener: identifying and synthesizing bug reports from collaborative live chats},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510108},
doi = {10.1145/3510003.3510108},
abstract = {In community-based software development, developers frequently rely on live-chatting to discuss emergent bugs/errors they encounter in daily development tasks. However, it remains a challenging task to accurately record such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the task of identifying and synthesizing bug reports from community live chats, and propose a novel approach, named BugListener, to address the challenges. Specifically, BugListener automates three sub-tasks: 1) Disentangle the dialogs from massive chat logs by using a Feed-Forward neural network; 2) Identify the bug-report dialogs from separated dialogs by leveraging the Graph neural network to learn the contextual information; 3) Synthesize the bug reports by utilizing Transfer Learning techniques to classify the sentences into: observed behaviors (OB), expected behaviors (EB), and steps to reproduce the bug (SR). BugListener is evaluated on six open source projects. The results show that: for bug report identification, BugListener achieves the average F1 of 77.74%, improving the best baseline by 12.96%; and for bug report synthesis task, BugListener could classify the OB, EB, and SR sentences with the F1 of 84.62%, 71.46%, and 73.13%, improving the best baselines by 9.32%, 12.21%, 10.91%, respectively. A human evaluation study also confirms the effectiveness of BugListener in generating relevant and accurate bug reports. These demonstrate the significant potential of applying BugListener in community-based software development, for promoting bug discovery and quality improvement.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {299–311},
numpages = {13},
keywords = {bug report generation, live chats mining, open source},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1109/TASLP.2021.3078368,
author = {Kim, Seokhwan and Galley, Michel and Gunasekara, Chulaka and Lee, Sungjin and Atkinson, Adam and Peng, Baolin and Schulz, Hannes and Gao, Jianfeng and Li, Jinchao and Adada, Mahmoud and Huang, Minlie and Lastras, Luis and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Hori, Chiori and Cherian, Anoop and Marks, Tim K. and Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav},
title = {Overview of the Eighth Dialog System Technology Challenge: DSTC8},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3078368},
doi = {10.1109/TASLP.2021.3078368},
abstract = {This paper introduces the Eighth Dialog System Technology Challenge. In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {2529–2540},
numpages = {12}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@inproceedings{10.1145/3492547.3492567,
author = {Sambangi, Swathi and Gondi, Lakshmeeswari},
title = {Multiple Linear Regression Prediction Model for DDOS Attack Detection in Cloud ELB},
year = {2021},
isbn = {9781450390446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492547.3492567},
doi = {10.1145/3492547.3492567},
abstract = {An ongoing research challenge in cloud computing is to address the security and data availability challenges. Although, DDoS attacks in cloud are not new but still they have been continuously throwing new challenges to the network community which makes detection of these attacks an ongoing research challenge with respect to cloud security. One of the reasons for these challenges is the high non-linearity of the real-world data. Thus, we bring into light the importance of understanding the non-linearity of data. Understanding nature of traffic instances in network datasets helps to build efficient machine learning models. For building a machine learning model, we choose to apply regression analysis. Two datasets namely CICIDS 2017 and CICIDS 2019 are considered for the present study as these datasets show high non-linearity. In this paper, we propose to apply regression analysis after performing feature engineering addressing the problem of DDoS attack detection. We propose to visualize the regression model by plotting residual plot, fit chart. The learning models can also be evaluated by comparing their respective MAPE and accuracy values. To the best of our knowledge, the research addressed in this paper is the first contribution in cloud computing which depicts the importance data visualization in analyzing the machine learning models. We believe that this paper paves a way for future researchers in cloud computing to concentrate on data visualization.},
booktitle = {The 7th International Conference on Engineering &amp; MIS 2021},
articleno = {4},
numpages = {9},
keywords = {Cloud, DDoS, DoS, Fit chart, Regression model, Residual plot},
location = {Almaty, Kazakhstan},
series = {ICEMIS'21}
}

@inproceedings{10.1109/ICSE.2019.00034,
author = {Nilizadeh, Shirin and Noller, Yannic and P\u{a}s\u{a}reanu, Corina S.},
title = {DifFuzz: differential fuzzing for side-channel analysis},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00034},
doi = {10.1109/ICSE.2019.00034},
abstract = {Side-channel attacks allow an adversary to uncover secret program data by observing the behavior of a program with respect to a resource, such as execution time, consumed memory or response size. Side-channel vulnerabilities are difficult to reason about as they involve analyzing the correlations between resource usage over multiple program paths. We present DifFuzz, a fuzzing-based approach for detecting side-channel vulnerabilities related to time and space. DifFuzz automatically detects these vulnerabilities by analyzing two versions of the program and using resource-guided heuristics to find inputs that maximize the difference in resource consumption between secret-dependent paths. The methodology of DifFuzz is general and can be applied to programs written in any language. For this paper, we present an implementation that targets analysis of Java programs, and uses and extends the Kelinci and AFL fuzzers. We evaluate DifFuzz on a large number of Java programs and demonstrate that it can reveal unknown side-channel vulnerabilities in popular applications. We also show that DifFuzz compares favorably against Blazer and Themis, two state-of-the-art analysis tools for finding side-channels in Java programs.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {176–187},
numpages = {12},
keywords = {dynamic analysis, fuzzing, side-channel analysis, vulnerability detection},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@proceedings{10.1145/3634737,
title = {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3400806.3400833,
author = {Baasanjav, Undrah},
title = {Social Media and Credibility: Civil Society Organizations in Mongolia},
year = {2020},
isbn = {9781450376884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400806.3400833},
doi = {10.1145/3400806.3400833},
abstract = {This study aims at conveying an understanding and perception of the potentials and pitfalls of social media by Mongolians who work in not-for-profit organizations. By speaking to the experts in civil society institutions the researcher analyzed why the participants use social media, and how they assess the credibility of information. This exploratory study documents journalists’, educators’, and civil society experts’ accounts in relation to political campaign and mobilization tactics on social media. The participants' accounts to a great extent speak to communicative and deliberative potentials and affordances of social media use for civic discourses posited by the scholars in the tradition of the expanding “deliberative sphere.” They also speak to the platform-specific affordances that either constrain or enable different potentials and possibilities.},
booktitle = {International Conference on Social Media and Society},
pages = {230–237},
numpages = {8},
keywords = {Information credibility, Mongolia, civil society, disinformation, focus group interviews, media literacy, platform affordances, social media},
location = {Toronto, ON, Canada},
series = {SMSociety'20}
}

@article{10.1145/3631504.3631508,
author = {Seufitelli, Danilo B. and Brand\~{a}o, Michele A. and Fernandes, Ayane C. A. and Siqueira, Kayque M. and Moro, Mirella M.},
title = {Where do Databases and Digital Forensics meet? A Comprehensive Survey and Taxonomy},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3631504.3631508},
doi = {10.1145/3631504.3631508},
abstract = {We present a systematic literature review and propose a taxonomy for research at the intersection of Digital Forensics and Databases. The merge between these two areas has become more prolific due to the growing volume of data and mobile apps on the Web, and the consequent rise in cyber attacks. Our review has identified 91 relevant papers. The taxonomy categorizes such papers into: Cyber-Attacks (subclasses SQLi, Attack Detection, Data Recovery) and Criminal Intelligence (subclasses Forensic Investigation, Research Products, Crime Resolution). Overall, we contribute to better understanding the intersection between digital forensics and databases, and open opportunities for future research and development with potential for significant social, economic, and technical-scientific contributions.},
journal = {SIGMOD Rec.},
month = {nov},
pages = {18–29},
numpages = {12}
}

@article{10.1145/3434300,
author = {Pavlinovic, Zvonimir and Su, Yusen and Wies, Thomas},
title = {Data flow refinement type inference},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {POPL},
url = {https://doi.org/10.1145/3434300},
doi = {10.1145/3434300},
abstract = {Refinement types enable lightweight verification of functional programs. Algorithms for statically inferring refinement types typically work by reduction to solving systems of constrained Horn clauses extracted from typing derivations. An example is Liquid type inference, which solves the extracted constraints using predicate abstraction. However, the reduction to constraint solving in itself already signifies an abstraction of the program semantics that affects the precision of the overall static analysis. To better understand this issue, we study the type inference problem in its entirety through the lens of abstract interpretation. We propose a new refinement type system that is parametric with the choice of the abstract domain of type refinements as well as the degree to which it tracks context-sensitive control flow information. We then derive an accompanying parametric inference algorithm as an abstract interpretation of a novel data flow semantics of functional programs. We further show that the type system is sound and complete with respect to the constructed abstract semantics. Our theoretical development reveals the key abstraction steps inherent in refinement type inference algorithms. The trade-off between precision and efficiency of these abstraction steps is controlled by the parameters of the type system. Existing refinement type systems and their respective inference algorithms, such as Liquid types, are captured by concrete parameter instantiations. We have implemented our framework in a prototype tool and evaluated it for a range of new parameter instantiations (e.g., using octagons and polyhedra for expressing type refinements). The tool compares favorably against other existing tools. Our evaluation indicates that our approach can be used to systematically construct new refinement type inference algorithms that are both robust and precise.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {19},
numpages = {31},
keywords = {Liquid types, abstract interpretation, refinement type inference}
}

@inproceedings{10.1145/3591196.3593364,
author = {Rezwana, Jeba and Maher, Mary Lou},
title = {User Perspectives on Ethical Challenges in Human-AI Co-Creativity: A Design Fiction Study},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591196.3593364},
doi = {10.1145/3591196.3593364},
abstract = {In a human-AI co-creation, AI not only categorizes, evaluates and interprets data but also generates new content and interacts with humans. As co-creative AI is a form of intelligent technology that directly involves humans, it is critical to anticipate and address ethical issues during all design stages. The open-ended nature of human-AI interactions in co-creation poses many challenges for designing ethical co-creative AI systems. Researchers have been exploring ethical issues associated with autonomous AI in recent years, but ethics in human-AI co-creativity is a relatively new research area. In order to design human-centered ethical AI, it is important to understand the perspectives, expectations, and ethical concerns of potential users. In this paper, we present a study with 18 participants to explore several ethical dilemmas and challenges in human-AI co-creation from the perspective of potential users using a design fiction (DF) methodology. DF is a speculative research method that depicts a new concept or technology through stories as an intangible prototype. We present the findings from the study as potential users’ perspectives, stances, and expectations around ethical challenges in human-AI co-creativity as a basis for designing human-centered ethical AI partners for human-AI co-creation.},
booktitle = {Proceedings of the 15th Conference on Creativity and Cognition},
pages = {62–74},
numpages = {13},
keywords = {AI Ethics, Co-creativity, Design Fiction, Ethical AI, Ethical Issues, Human-AI Co-Creation},
location = {Virtual Event, USA},
series = {C&amp;C '23}
}

@inproceedings{10.1145/3611643.3616264,
author = {Wang, Huiyan and Liu, Shuguan and Zhang, Lingyu and Xu, Chang},
title = {Automatically Resolving Dependency-Conflict Building Failures via Behavior-Consistent Loosening of Library Version Constraints},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616264},
doi = {10.1145/3611643.3616264},
abstract = {Python projects grow quickly by code reuse and building automation based on third-party libraries. However, the version constraints associated with these libraries are prone to mal-configuration, and this forms a major obstacle to correct project building (known as dependency-conflict (DC) building failure). Our empirical findings suggest that such mal-configured version constraints were mainly prepared manually, and could essentially be refined for better quality to improve the chance of successful project building. We propose a LooCo approach to refining Python projects’ library version constraints by automatically loosening them to maximize their solutions, while keeping the libraries to observe their original behaviors. Our experimental results with real-life Python projects report that LooCo could efficiently refine library version constraints (0.4s per version loosening) by effective loosening (5.5 new versions expanded on average) automatically, and transform 54.8% originally unsolvable cases into solvable ones (i.e., successful building) and significantly increase solutions (21 more on average) for originally solvable cases.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {198–210},
numpages = {13},
keywords = {Dependency conflict, loosening resolution, version constraint},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3649217.3653557,
author = {Farinetti, Laura and Canale, Lorenzo},
title = {Chatbot Development Using LangChain: A Case Study to Foster Critical Thinking and Creativity},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653557},
doi = {10.1145/3649217.3653557},
abstract = {Critical thinking and creativity are fundamental skills for engineers and computer scientists. The emergence of Large Language Models (LLMs) able to create chatbots that use natural language is an opportunity for educators to foster these skills. The well-known risk of generative AI for potential misinformation offers fertile ground to practice critical thinking.This paper describes a hands-on experience within a database course, where students had to develop a chatbot using the LangChain framework, and to evaluate it from different points of view. The students were free to choose the domain of their chatbot. The learning goal was twofold: on the one hand, to make them practice with state-of-the-art technologies, and on the other hand to stimulate critical analysis on their output. The paper discusses the students' evaluation of the chatbots under several metrics, including document retrieval, syntax and grammar accuracy, semantic relevance and information reliability. Students' assessments were also compared to the teachers' ones, to gain an insight on the critical attitude of the students and to offer a ground for discussion.The experience was stimulating and appreciated by the students. The final results highlight that the majority of students successfully produced chatbot responses that were grammatically and syntactically correct, and that consistently extracted pertinent sections from documents, yielding semantically relevant outputs. Despite these achievements, a significant portion of students expressed reservations about the reliability of the chatbot's responses to prompts, gaining awareness of LLMs' capability to generate responses that make sense to humans but may be potentially misleading.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {401–407},
numpages = {7},
keywords = {chatbot development, creativity and critical thinking, database education, information retrieval, langchain framework, large language models, natural language interfaces},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3442442.3453701,
author = {West, Robert and Bhagat, Smriti and Groth, Paul and Zitnik, Marinka and Couto, Francisco M. and Lisena, Pasquale and Mero\~{n}o-Pe\~{n}uela, Albert and Zhao, Xiangyu and Fan, Wenqi and Yin, Dawei and Tang, Jiliang and Shou, Linjun and Gong, Ming and Pei, Jian and Geng, Xiubo and Zhou, Xingjie and Jiang, Daxin and Ricaud, Benjamin and Aspert, Nicolas and Miz, Volodymyr and Dy, Jennifer and Ioannidis, Stratis and Y\i{}ld\i{}z, undefinedlkay and Rezapour, Rezvaneh and Aref, Samin and Dinh, Ly and Diesner, Jana and Drutsa, Alexey and Ustalov, Dmitry and Popov, Nikita and Baidakova, Daria and Mishra, Shubhanshu and Gopalan, Arjun and Juan, Da-Cheng and Ilharco Magalhaes, Cesar and Ferng, Chun-Sung and Heydon, Allan and Lu, Chun-Ta and Pham, Philip and Yu, George and Fan, Yicheng and Wang, Yueqi and Laurent, Florian and Schraner, Yanick and Scheller, Christian and Mohanty, Sharada and Chen, Jiawei and Wang, Xiang and Feng, Fuli and He, Xiangnan and Teinemaa, Irene and Albert, Javier and Goldenberg, Dmitri and Vasile, Flavian and Rohde, David and Jeunen, Olivier and Benhalloum, Amine and Sakhi, Otmane and Rong, Yu and Huang, Wenbing and Xu, Tingyang and Bian, Yatao and Cheng, Hong and Sun, Fuchun and Huang, Junzhou and Fakhraei, Shobeir and Faloutsos, Christos and \c{C}elebi, Onur and M\"{u}ller, Martin and Schneider, Manuel and Altunina, Olesia and Wingerath, Wolfram and Wollmer, Benjamin and Gessert, Felix and Succo, Stephan and Ritter, Norbert and Courdier, Evann and Avram, Tudor Mihai and Cvetinovic, Dragan and Tsinadze, Levan and Jose, Johny and Howell, Rose and Koenig, Mario and Defferrard, Micha\"{e}l and Kenthapadi, Krishnaram and Packer, Ben and Sameki, Mehrnoosh and Sephus, Nashlie},
title = {Summary of Tutorials at The Web Conference 2021},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3453701},
doi = {10.1145/3442442.3453701},
abstract = {This report summarizes the 23 tutorials hosted at The Web Conference 2021: nine lecture-style tutorials and 14 hands-on tutorials.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {727–733},
numpages = {7},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3636555.3636882,
author = {Dunder, Nora and Lundborg, Saga and Wong, Jacqueline and Viberg, Olga},
title = {Kattis vs ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636882},
doi = {10.1145/3636555.3636882},
abstract = {AI-powered education technologies can support students and teachers in computer science education. However, with the recent developments in generative AI, and especially the increasingly emerging popularity of ChatGPT, the effectiveness of using large language models for solving programming tasks has been underexplored. The present study examines ChatGPT’s ability to generate code solutions at different difficulty levels for introductory programming courses. We conducted an experiment where ChatGPT was tested on 127 randomly selected programming problems provided by Kattis, an automatic software grading tool for computer science programs, often used in higher education. The results showed that ChatGPT independently could solve 19 out of 127 programming tasks generated and assessed by Kattis. Further, ChatGPT was found to be able to generate accurate code solutions for simple problems but encountered difficulties with more complex programming tasks. The results contribute to the ongoing debate on the utility of AI-powered tools in programming education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {821–827},
numpages = {7},
keywords = {Academic Integrity, Automated Grading, ChatGPT, Programming Education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3546118.3546152,
author = {Jancok, Vladimir and Ries, Michal},
title = {Security Aspects of Behavioral Biometrics for Strong User Authentication},
year = {2022},
isbn = {9781450396448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546118.3546152},
doi = {10.1145/3546118.3546152},
abstract = {Efficient user identification and authentication are fundamental for securing access to systems processing sensitive data. This paper provides an analysis of current research in the field of user identification and identity verification with a focus on the behavioral biometrics supported by Machine Learning. It identifies the methods for user modeling with the potential of application in real-world scenarios such as strong authentication and fraud detection domain. This paper further elaborates on the current state-of-the-art approaches, feature extraction, and classification methods. We describe our experimental setup and provide an evaluation of our method in the selected deployment. We focus on user interactions in a controlled web environment. We performed classification experiments with the machine learning models on various datasets showing promising results in the robustness and proving relevance as a modern non-intrusive security measure.},
booktitle = {Proceedings of the 23rd International Conference on Computer Systems and Technologies},
pages = {57–63},
numpages = {7},
keywords = {authentication, behavioral biometrics, biometrics, intrusion detection, machine learning, user identification},
location = {University of Ruse, Ruse, Bulgaria},
series = {CompSysTech '22}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3597638.3608403,
author = {P. Carvalho, Lu\'{\i}s and Guerreiro, Tiago and Lawson, Shaun and Montague, Kyle},
title = {Towards real-time and large-scale web accessbility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608403},
doi = {10.1145/3597638.3608403},
abstract = {We rely on large-scale web accessibility evaluations to obtain snapshots of Internet Health and understand trends and behaviours impacting overall web accessibility. Such evaluations are financially and time exhaustive, making the possibility of more real-time measurements of Internet Health infeasible. In this paper, we investigate the impacts of optimising the page selection processes of large-scale web accessibility evaluations. We set out to conduct an automated accessibility evaluation of 1500 websites using the ‘Home+’ sampling method (for each website, we evaluated the home page and all pages linked belonging to the same domain) as our baseline; then compared the agreement rates of web accessibility evaluations on further sub-sampled datasets. Accessibility data was successfully captured on 987 websites. Our findings demonstrate that a strong accessibility evaluation agreement between the baseline and the sub-sample datasets could be reached with a sub-sample of just 20% of the pages, significantly reducing the effort and resources required to conduct large-scale web accessibility evaluations.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {35},
numpages = {9},
keywords = {Internet Health, Large-Scale Analysis, Web Accessibility},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3325112.3325267,
author = {J. Domanski, Robert},
title = {The A.I. Pandorica: Linking Ethically-challenged Technical Outputs to Prospective Policy Approaches},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325267},
doi = {10.1145/3325112.3325267},
abstract = {Artificial intelligence increasingly drives the modern world. Its rapid and continuous integration into the economic and political fabric of societies across the globe has placed the ethical challenges associated with A.I. onto national political agendas. This paper will deconstruct both the technical and regulatory challenges wrought by A.I. through an ethical lens. By providing a brief overview of how modern A.I. works, and defining and mapping its associated ethical issues to specific technical outputs, this paper will explore several promising paths forward.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {409–416},
numpages = {8},
keywords = {Artificial intelligence, algorithmic bias, digital government, ethics, machine learning},
location = {Dubai, United Arab Emirates},
series = {dg.o '19}
}

@inproceedings{10.1145/3560905.3578264,
author = {Chung, Ming-Kuang and Ching, Fu-Shiang and Chen, Ling-Jyh},
title = {From Participatory Sensing to Public-Private Partnership: The Development of AirBox Project in Taiwan},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3578264},
doi = {10.1145/3560905.3578264},
abstract = {A complete sensor network should include sensors, data processing, and data services. However, to establish the legitimacy of sensor data for urban governance, sensor networks should go beyond simple deployment of sensors in the built environment and strive for deeper integration of data services within civil society. This paper presents the Taiwan AirBox Project as an exemplary case of practical deployment of a sensor network to discuss the topics of open data, value-added services, and joint calibration services; as well as how these services generate productive public-private partnerships.The AirBox project adopted a strategy of combining open-source hardware, flexible database API, multiple value-added data services, and open-joint calibration to gradually enhance the data quality. The results suggested that: 1. open hardware and open source software are keys to expanding the deployment of the sensor network; 2. open data and diverse value-added services enhance the public's environmental awareness and advocacy; 3. the open joint-calibration system helps connect government policy formulation with public environmental awareness.In addition, the AirBox project demonstrates the feasibility of a democratized deployment strategy. "Openness" serves as the foundation for mutual trust, communication, cooperation, and co-creation among stakeholders involved in the deployment process.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {1000–1006},
numpages = {7},
keywords = {AirBox, open joint-calibration system, participatory sensing, sensor network deployment},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1109/ICSE48619.2023.00212,
author = {Zhang, Lyuye and Liu, Chengwei and Xu, Zhengzi and Chen, Sen and Fan, Lingling and Zhao, Lida and Wu, Jiahui and Liu, Yang},
title = {Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00212},
doi = {10.1109/ICSE48619.2023.00212},
abstract = {With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compilation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that CORAL not only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2540–2552},
numpages = {13},
keywords = {remediation, compatibility, Java, open-source software},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3447865.3457969,
author = {Braun, Susanne and Bieniusa, Annette and Elberzhager, Frank},
title = {Advanced Domain-Driven Design for Consistency in Distributed Data-Intensive Systems},
year = {2021},
isbn = {9781450383387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447865.3457969},
doi = {10.1145/3447865.3457969},
abstract = {More and more data-intensive systems have emerged lately. Big Data, Artificial Intelligence, or cloud-native applications all require high scalability and availability. Data is no longer persisted in one central relational database with serialized and transactional access, but rather distributed and replicated among different nodes running only under eventual consistency. This poses a number of design challenges for software architects, as they cannot rely on a single system to mask the concurrency anomalies of concurrent access to distributed and replicated data. Based on three case studies, we developed a theory regarding how practitioners handle synchronization and consistency design challenges in distributed data-intensive applications. We also identified the "white spots" of missing design guidance needed by practitioners to handle the aforementioned challenges appropriately. We are currently evaluating our theory in the context of an action research study. In this study, we are also evaluating the novel design guidelines we are proposing in this regard, which, according to our theory, meet the needs of practitioners. Our design guidelines integrate with Domain-Driven Design, which is widely used in practice. Following the idea of multilevel serializability, we investigate the compatibility of business operations beyond commutativity. We provide concrete practical design guidance to achieve compatibility of non-commutative business operations. We also describe the basic infrastructure guarantees our design guidelines require from replication frameworks.},
booktitle = {Proceedings of the 8th Workshop on Principles and Practice of Consistency for Distributed Data},
articleno = {9},
numpages = {12},
keywords = {data-intensive systems, domain-driven design, eventual consistency},
location = {Online, United Kingdom},
series = {PaPoC '21}
}

@inproceedings{10.1145/3510458.3513019,
author = {Qiu, Huilian Sophie and Vasilescu, Bogdan and K\"{a}stner, Christian and Egelman, Carolyn and Jaspan, Ciera and Murphy-Hill, Emerson},
title = {Detecting interpersonal conflict in issues and code review: cross pollinating open- and closed-source approaches},
year = {2022},
isbn = {9781450392273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510458.3513019},
doi = {10.1145/3510458.3513019},
abstract = {Interpersonal conflict in code review, such as toxic language or an unnecessary pushback, is associated with negative outcomes such as stress and turnover. Automatic detection is one approach to prevent and mitigate interpersonal conflict. Two recent automatic detection approaches were developed in different settings: a toxicity detector using text analytics for open source issue discussions and a pushback detector using logs-based metrics for corporate code reviews. This paper tests how the toxicity detector and the pushback detector can be generalized beyond their respective contexts and discussion types, and how the combination of the two can help improve interpersonal conflict detection. The results reveal connections between the two concepts.Software engineers often communicate with one another on platforms that support tasks like discussing bugs and inspecting each others' code. Such discussions sometimes contain interpersonal conflict, which can lead to stress and abandonment. In this paper, we investigate how to automatically detect interpersonal conflict, both by analyzing the text of the what the engineers are saying and by analyzing the properties of that text.},
booktitle = {Proceedings of the 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society},
pages = {41–55},
numpages = {15},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIS '22}
}

@inproceedings{10.1145/3597503.3639223,
author = {Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin},
title = {Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639223},
doi = {10.1145/3597503.3639223},
abstract = {Understanding and identifying the causes behind developers' emotions (e.g., Frustration caused by 'delays in merging pull requests') can be crucial towards finding solutions to problems and fostering collaboration in open-source communities. Effectively identifying such information in the high volume of communications across the different project channels, such as chats, emails, and issue comments, requires automated recognition of emotions and their causes. To enable this automation, large-scale software engineering-specific datasets that can be used to train accurate machine learning models are required. However, such datasets are expensive to create with the variety and informal nature of software projects' communication channels.In this paper, we explore zero-shot LLMs that are pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our evaluation indicates that these recently available models can identify emotion categories when given detailed emotions, although they perform worse than the top-rated models. For emotion cause identification, our results indicate that zero-shot LLMs are effective at recognizing the correct emotion cause with a BLEU-2 score of 0.598. To highlight the potential use of these techniques, we conduct a case study of the causes of Frustration in the last year of development of a popular open-source project, revealing several interesting insights.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {182},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3631974,
author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
title = {A Survey of Learning-based Automated Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631974},
doi = {10.1145/3631974},
abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {55},
numpages = {69},
keywords = {Automatic program repair, deep learning, neural machine translation, AI and software engineering}
}

@inproceedings{10.1145/3639477.3639734,
author = {Zhao, Shengyu and Xia, Xiaoya and Fitzgerald, Brian and Li, Xiaozhou and Lenarduzzi, Valentina and Taibi, Davide and Wang, Rong and Wang, Wei and Tian, Chunqi},
title = {OpenRank Leaderboard: Motivating Open Source Collaborations Through Social Network Evaluation in Alibaba},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639734},
doi = {10.1145/3639477.3639734},
abstract = {Open source has revolutionized how software development is carried out, with a growing number of individuals and organizations contributing to open source projects. As the importance of open source continues to grow, companies also expect to grow thriving and sustainable open source communities with continued contributions and better collaborations. In this study, we applied the contribution leaderboard to seven open source projects initiated by Alibaba. We conducted a case study to investigate the perceptions and facts regarding how to motivate collaboration through gamification. Specifically, we employed a social network algorithm, OpenRank, to evaluate and steer developers' contributions. We validated the effectiveness of OpenRank by comparing it with other evaluation metrics and surveying developers. Through semi-structured interviews and project metric analysis, we found that the OpenRank Leaderboard can promote transparent communication environments, a better community atmosphere, and improved collaboration behavior.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {346–357},
numpages = {12},
keywords = {open source contribution, social network, leaderboard, gamification},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@proceedings{10.1145/3641822,
title = {CHASE '24: Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHASE 2024 continues the tradition of a high-quality venue for research related to the cooperative and human aspects of software engineering. Researchers and practitioners have long recognized the need to investigate the cooperative and human aspects. However, their articles have been scattered across many conferences and communities. The CHASE conference provides academics and practitioners with a unified forum for discussing high-quality research studies, models, methods, and tools for human and cooperative aspects of software engineering.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3614419,
title = {WEBSCI '24: Proceedings of the 16th ACM Web Science Conference},
year = {2024},
isbn = {9798400703348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stuttgart, Germany}
}

@article{10.1145/3434168,
author = {Chen, Yan and Lasecki, Walter S. and Dong, Tao},
title = {Towards Supporting Programming Education at Scale via Live Streaming},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434168},
doi = {10.1145/3434168},
abstract = {Live streaming, which allows streamers to broadcast their work to live viewers, is an emerging practice for teaching and learning computer programming. Participation in live streaming is growing rapidly, despite several apparent challenges, such as a general lack of training in pedagogy among streamers and scarce signals about a stream's characteristics (e.g., difficulty, style, and usefulness) to help viewers decide what to watch. To understand why people choose to participate in live streaming for teaching or learning programming, and how they cope with both apparent and non-obvious challenges, we interviewed 14 streamers and viewers about their experience with live streaming programming. Among other results, we found that the casual and impromptu nature of live streaming makes it easier to prepare than pre-recorded videos, and viewers have the opportunity to shape the content and learning experience via real-time communication with both the streamer and each other. Nonetheless, we identified several challenges that limit the potential of live streaming as a learning medium. For example, streamers voiced privacy and harassment concerns, and existing streaming platforms do not adequately support viewer-streamer interactions, adaptive learning, and discovery and selection of streaming content. Based on these findings, we suggest specialized tools to facilitate knowledge sharing among people teaching and learning computer programming online, and we offer design recommendations that promote a healthy, safe, and engaging learning environment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {259},
numpages = {19},
keywords = {informal learning, live coding, live streaming, programming education}
}

@inproceedings{10.1145/3618257.3624804,
author = {Lim, Kyungchan and Kwon, Yonghwi and Kim, Doowon},
title = {A Longitudinal Study of Vulnerable Client-side Resources and Web Developers' Updating Behaviors},
year = {2023},
isbn = {9798400703829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618257.3624804},
doi = {10.1145/3618257.3624804},
abstract = {Modern Websites rely on various client-side web resources, such as JavaScript libraries, to provide end-users with rich and interactive web experiences. Unfortunately, anecdotal evidence shows that improperly managed client-side resources could open up attack surfaces that adversaries can exploit. However, there is still a lack of a comprehensive understanding of the updating practices among web developers and the potential impact of inaccuracies in Common Vulnerabilities and Exposures (CVE) information on the security of the web ecosystem. In this paper, we conduct a longitudinal (four-year) measurement study of the security practices and implications on client-side resources (e.g., JavaScript libraries and Adobe Flash) across the Web. Specifically, we first collect a large-scale dataset of 157.2M webpages of Alexa Top 1M websites for four years in the wild. Analyzing the dataset, we find an average of 41.2% of websites (in each year of the four years) carry at least one vulnerable client-side resource (e.g., JavaScript or Adobe Flash). We also reveal that vulnerable JavaScript library versions are frequently observed in the wild, suggesting a concerning level of lagging update practice in the wild. On average, we observe 531.2 days with 25,337 websites of the window of vulnerability due to the unpatched client-side resources from the release of security patches. Furthermore, we manually investigate the fidelity of CVE (Common Vulnerabilities and Exposures) reports on client-side resources, leveraging PoC (Proof of Concept) code. We find that 13 CVE reports (out of 27) have incorrect vulnerable version information, which may impact security-related tasks such as security updates.},
booktitle = {Proceedings of the 2023 ACM on Internet Measurement Conference},
pages = {162–180},
numpages = {19},
keywords = {adobe flash, cve, javascript library, web security},
location = {Montreal QC, Canada},
series = {IMC '23}
}

@article{10.1145/3579527,
author = {Murphy-Hill, Emerson and Dicker, Jillian and Horvath, Amber and Hodges, Maggie Morrow and Egelman, Carolyn D. and Weingart, Laurie R. and Jaspan, Ciera and Green, Collin and Chen, Nina},
title = {Systemic Gender Inequities in Who Reviews Code},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579527},
doi = {10.1145/3579527},
abstract = {Code review is an essential task for modern software engineers, where the author of a code change assigns other engineers the task of providing feedback on the author's code. In this paper, we investigate the task of code review through the lens of equity, the proposition that engineers should share reviewing responsibilities fairly. Through this lens, we quantitatively examine gender inequities in code review load at Google. We found that, on average, women perform about 25% fewer reviews than men, an inequity with multiple systemic antecedents, including authors' tendency to choose men as reviewers, a recommender system's amplification of human biases, and gender differences in how reviewer credentials are assigned and earned. Although substantial work remains to close the review load gap, we show how one small change has begun to do so.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {94},
numpages = {59},
keywords = {code review, equity}
}

@inproceedings{10.1145/3524842.3528433,
author = {Le, Triet Huynh Minh and Babar, M. Ali},
title = {On the use of fine-grained vulnerable code statements for software vulnerability assessment models},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528433},
doi = {10.1145/3524842.3528433},
abstract = {Many studies have developed Machine Learning (ML) approaches to detect Software Vulnerabilities (SVs) in functions and fine-grained code statements that cause such SVs. However, there is little work on leveraging such detection outputs for data-driven SV assessment to give information about exploitability, impact, and severity of SVs. The information is important to understand SVs and prioritize their fixing. Using large-scale data from 1,782 functions of 429 SVs in 200 real-world projects, we investigate ML models for automating function-level SV assessment tasks, i.e., predicting seven Common Vulnerability Scoring System (CVSS) metrics. We particularly study the value and use of vulnerable statements as inputs for developing the assessment models because SVs in functions are originated in these statements. We show that vulnerable statements are 5.8 times smaller in size, yet exhibit 7.5--114.5% stronger assessment performance (Matthews Correlation Coefficient (MCC)) than non-vulnerable statements. Incorporating context of vulnerable statements further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score). Overall, we provide the initial yet promising ML-based baselines for function-level SV assessment, paving the way for further research in this direction.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {621–633},
numpages = {13},
keywords = {machine learning, mining software repositories, security vulnerability, vulnerability assessment},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3511808.3557620,
author = {Lima, Susana and Morla, Ricardo and Routar, Jo\~{a}o},
title = {JavaScript&amp;Me, A Tool to Support Research into Code Transformation and Browser Security},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557620},
doi = {10.1145/3511808.3557620},
abstract = {Doing research into code variations and their applications to browser security is challenging. One of the most important aspects of this research is to choose a relevant dataset on which machine learning algorithms can be applied to yield useful results. Although JavaScript code is widely available on various sources, such as package managers, code hosting platforms, and websites, collecting a large corpus of JavaScript and curating it is not a simple task. We present a novel open-source tool that helps with this task by allowing the automatic and systematic collection, processing, and transformation of JavaScript code. These three steps are performed by independent modules, and each one can be extended to incorporate new features, such as additional code sources, or transformation tools, adding to the flexibility of our tool and expanding its usability. Additionally, we use our tool to create a corpus of around 270k JavaScript files, including regular, minified, and obfuscated code, on which we perform a brief analysis. The conclusions from this analysis show the importance of properly curating a dataset before using it in research tasks, such as machine learning classifiers, reinforcing the relevance of our tool.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4224–4228},
numpages = {5},
keywords = {code collection, code processing, code transformations, javascript},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/3379597.3387489,
author = {Zhang, Xunhui and Rastogi, Ayushi and Yu, Yue},
title = {On the Shoulders of Giants: A New Dataset for Pull-based Development Research},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387489},
doi = {10.1145/3379597.3387489},
abstract = {Pull-based development is a widely adopted paradigm for collaboration in distributed software development, attracting eyeballs from both academic and industry. To better study pull-based development model, this paper presents a new dataset containing 96 features collected from 11,230 projects and 3,347,937 pull requests. We describe the creation process and explain the features in details. To the best of our knowledge, our dataset is the most comprehensive and largest one toward a complete picture for pull-based development research.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {543–547},
numpages = {5},
keywords = {distributed software development, pull request, pull-based development},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@proceedings{10.1145/3527188,
title = {HAI '22: Proceedings of the 10th International Conference on Human-Agent Interaction},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@inproceedings{10.1145/3491101.3519735,
author = {Zuniga, Evelyn and Milani, Stephanie and Leroy, Guy and Rzepecki, Jaroslaw and Georgescu, Raluca and Momennejad, Ida and Bignell, Dave and Sun, Mingfei and Shaw, Alison and Costello, Gavin and Jacob, Mikhail and Devlin, Sam and Hofmann, Katja},
title = {How Humans Perceive Human-like Behavior in Video Game Navigation},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519735},
doi = {10.1145/3491101.3519735},
abstract = {The goal of this paper is to understand how people assess human-likeness in human- and AI-generated behavior. To this end, we present a qualitative study of hundreds of crowd-sourced assessments of human-likeness of behavior in a 3D video game navigation task. In particular, we focus on an AI agent that has passed a Turing Test, in the sense that human judges were not able to reliably distinguish between videos of a human and AI agent navigating on a quantitative level. Our insights shine a light on the characteristics that people consider as human-like. Understanding these characteristics is a key first step for improving AI agents in the future.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {391},
numpages = {11},
keywords = {Believable AI, Human-AI Interaction, Human-subject Study},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@article{10.1145/3569934,
author = {Gong, Lina and Zhang, Jingxuan and Wei, Mingqiang and Zhang, Haoxiang and Huang, Zhiqiu},
title = {What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3569934},
doi = {10.1145/3569934},
abstract = {There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {69},
numpages = {57},
keywords = {Software engineering for artificial intelligence, pre-trained models, model reuse, model contract}
}

@article{10.1109/TNET.2018.2854795,
author = {Liu, Zhuotao and Jin, Hao and Hu, Yih-Chun and Bailey, Michael},
title = {Practical Proactive DDoS-Attack Mitigation via Endpoint-Driven In-Network Traffic Control},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2854795},
doi = {10.1109/TNET.2018.2854795},
abstract = {Volumetric attacks, which overwhelm the bandwidth of a destination, are among the most common distributed denial-of-service DDoS attacks today. Despite considerable effort made by both research and industry, our recent interviews with over 100 potential DDoS victims in over 10 industry segments indicate that today’s DDoS prevention is far from perfect. On one hand, few academical proposals have ever been deployed in the Internet; on the other hand, solutions offered by existing DDoS prevention vendors are not silver bullet to defend against the entire attack spectrum. Guided by such large-scale study of today’s DDoS defense, in this paper, we present MiddlePolice, the first readily deployable and proactive DDoS prevention mechanism. We carefully architect MiddlePolice such that it requires no changes from both the Internet core and the network stack of clients, yielding instant deployability in the current Internet architecture. Further, relying on our novel capability feedback mechanism, MiddlePolice is able to enforce destination-driven traffic control so that it guarantees to deliver victim-desired traffic regardless of the attacker strategies. We implement a prototype of MiddlePolice and demonstrate its feasibility via extensive evaluations in the Internet, hardware testbed, and large-scale simulations.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1948–1961},
numpages = {14}
}

@inproceedings{10.1145/3361149.3361161,
author = {Abidi, Mouna and Grichi, Manel and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {Code smells for multi-language systems},
year = {2019},
isbn = {9781450362061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361149.3361161},
doi = {10.1145/3361149.3361161},
abstract = {Software quality becomes a necessity and no longer an advantage. In fact, with the advancement of technologies, companies must provide software with good quality. Many studies introduce the use of design patterns as improving software quality and discuss the presence of occurrences of design defects as decreasing software quality. Code smells include low-level problems in source code, poor coding decisions that are symptoms of the presence of anti-patterns in the code. Most of the studies present in the literature discuss the occurrences of design defects for mono-language systems. However, nowadays most of the systems are developed using a combination of several programming languages, in order to use particular features of each of them. As the number of languages increases, so does the number of design defects. They generally do not prevent the program from functioning correctly, but they indicate a higher risk of future bugs and makes the code less readable and harder to maintain. We analysed open-source systems, developers' documentation, bug reports, and programming language specifications and extracted bad practices related to multi-language systems. We encoded these practices in the form of code smells. We report in this paper 12 code smells.},
booktitle = {Proceedings of the 24th European Conference on Pattern Languages of Programs},
articleno = {12},
numpages = {13},
keywords = {code analysis, code smells, multi-language systems, software quality},
location = {Irsee, Germany},
series = {EuroPLop '19}
}

@inproceedings{10.1145/3460120.3484765,
author = {Kondracki, Brian and Azad, Babak Amin and Starov, Oleksii and Nikiforakis, Nick},
title = {Catching Transparent Phish: Analyzing and Detecting MITM Phishing Toolkits},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484765},
doi = {10.1145/3460120.3484765},
abstract = {For over a decade, phishing toolkits have been helping attackers automate and streamline their phishing campaigns. Man-in-the- Middle (MITM) phishing toolkits are the latest evolution in this space, where toolkits act as malicious reverse proxy servers of online services, mirroring live content to users while extracting cre- dentials and session cookies in transit. These tools further reduce the work required by attackers, automate the harvesting of 2FA- authenticated sessions, and substantially increase the believability of phishing web pages.In this paper, we present the first analysis of MITM phishing toolkits used in the wild. By analyzing and experimenting with these toolkits, we identify intrinsic network-level properties that can be used to identify them. Based on these properties, we develop a machine learning classifier that identifies the presence of such toolkits in online communications with 99.9% accuracy.We conduct a large-scale longitudinal study of MITM phishing toolkits by creating a data-collection framework that monitors and crawls suspicious URLs from public sources. Using this infrastruc- ture, we capture data on 1,220 MITM phishing websites over the course of a year. We discover that MITM phishing toolkits occupy a blind spot in phishing blocklists, with only 43.7% of domains and 18.9% of IP addresses associated with MITM phishing toolkits present on blocklists, leaving unsuspecting users vulnerable to these attacks. Our results show that our detection scheme is resilient to the cloaking mechanisms incorporated by these tools, and is able to detect previously hidden phishing content. Finally, we propose methods that online services can utilize to fingerprint requests origi- nating from these toolkits and stop phishing attempts as they occur.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {36–50},
numpages = {15},
keywords = {phishing, social engineering, web security},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3279981.3279990,
author = {Hoey, Jesse and Schr\"{o}der, Tobias and Morgan, Jonathan H. and Rogers, Kimberly B. and Nagappan, Meiyappan},
title = {Affective Dynamics and Control in Group Processes},
year = {2018},
isbn = {9781450360777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279981.3279990},
doi = {10.1145/3279981.3279990},
abstract = {The computational modeling of groups requires models that connect micro-level with macro-level processes and outcomes. Recent research in computational social science has started from simple models of human behaviour, and attempted to link to social structures. However, these models make simplifying assumptions about human understanding of culture that are of ten not realistic and may be limiting in their generality. In this paper, we present work on Bayesian affect control theory as a more comprehensive, yet highly parsimonious model that integrates artificial intelligence, social psychology, and emotions into a single predictive model of human activities in groups. We illustrate these developments with examples from an ongoing research project aimed at computational analysis of virtual software development teams.},
booktitle = {Proceedings of the Group Interaction Frontiers in Technology},
articleno = {1},
numpages = {7},
keywords = {affect, affect control theory, agents, collaboration, emotion, group interaction, team dynamics},
location = {Boulder, CO, USA},
series = {GIFT'18}
}

@inproceedings{10.1145/3626252.3630937,
author = {Grover, Shuchi},
title = {Teaching AI to K-12 Learners: Lessons, Issues, and Guidance},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630937},
doi = {10.1145/3626252.3630937},
abstract = {There is growing recognition of the need to teach artificial intelli- gence (AI) and machine learning (ML) at the school level. This push acknowledges the meteoric growth in the range and diversity of ap- plications of ML in all industries and everyday consumer products, with Large Language Models (LLMs) being only the latest and most compelling example yet. Efforts to bring AI, especially ML educa- tion to school learners are being propelled by substantial industry interest, research efforts, as well as technological developments that make sophisticated ML tools readily available to learners of all ages. These early efforts span a variety of learning goals captured by the AI4K12 "big ideas" framework and employ a plurality of pedagogies.This paper provides a sense for the current state of the field, shares lessons learned from early K-12 AI education as well as CS education efforts that can be leveraged, highlights issues that must be addressed in designing for teaching AI in K-12, and provides guidance for future K-12 AI education efforts and tackle what to many feels like "the next new thing".},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {422–428},
numpages = {7},
keywords = {artificial intelligence, k-12 ai education, k-12 cs education, machine learning},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3428658.3431075,
author = {Ngomo, Jean Gabriel Nguema and Lopes, Giseli Rabello and Campos, Maria Luiza Machado and Cavalcanti, Maria Claudia Reis},
title = {An Approach for Improving DBpedia as a Research Data Hub},
year = {2020},
isbn = {9781450381963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428658.3431075},
doi = {10.1145/3428658.3431075},
abstract = {Extracted from Wikipedia content, DBpedia is considered one of the most important knowledge bases of the Semantic Web, which has editions in several languages, among which those in English (DBpedia EN) and Portuguese (DBpedia PT). All DBpedia editions are subject to quality issues, more especially DBpedia PT suffers from inconsistencies and lack of data in several domains. This paper describes a semi-automatic and incremental process for publishing data on DBpedia, coming from reliable external sources, while seeking to improve aspects of its quality. In an open science context, the proposal aims at consolidating DBpedia as a reference hub for research data, so that research from any area supported by the Semantic Web data can use its data reliably. Although the approach is independent from a specific DBpedia edition, the supporting prototype tool, named ETL4DBpedia, was built for DBpedia PT, based on ETL workflows (Extract, Transform, Load). This paper also describes the assessment of the approach, applying the tool in a real-usage scenario involving data from the field of botany. This application resulted in an increase by 127% in the completeness of species of medicinal plants in DBpedia PT, besides showing satisfactory performance for ETL4Bpedia components.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {65–72},
numpages = {8},
keywords = {DBpedia, Data Quality, ETL, Open Science, Semantic Web},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {WebMedia '20}
}

@inproceedings{10.1145/3530019.3535304,
author = {Ikegami, Ayano and Kula, Raula Gaikovina and Chinthanet, Bodin and Maeprasart, Vittunyuta and Ouni, Ali and Ishio, Takashi and Matsumoto, Kenichi},
title = {On the Use of Refactoring in Security Vulnerability Fixes: An Exploratory Study on Maven Libraries},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3535304},
doi = {10.1145/3530019.3535304},
abstract = {Third-party library dependencies are commonplace in today’s software development. With the growing threat of security vulnerabilities, applying security fixes in a timely manner is important to protect software systems. As such, the community developed a list of software and hardware weakness known as Common Weakness Enumeration (CWE) to assess vulnerabilities. Prior work has revealed that maintenance activities such as refactoring code potentially correlate with security-related aspects in the source code. In this work, we explore the relationship between refactoring and security by analyzing refactoring actions performed jointly with vulnerability fixes in practice. We conducted a case study to analyze 143 maven libraries in which 351 known vulnerabilities had been detected and fixed. Surprisingly, our exploratory results show that developers incorporate refactoring operations in their fixes, with 31.9% (112 out of 351) of the vulnerabilities paired with refactoring actions. We envision this short paper to open up potential new directions to motivate automated tool support, allowing developers to deliver fixes faster, while maintaining their code.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {288–293},
numpages = {6},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@proceedings{10.1145/3644815,
title = {CAIN '24: Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goal of the CAIN Conference Series is to bring together researchers and practitioners in software engineering, data science, and artificial intelligence (AI) as part of a growing community that is targeting the challenges of Software Engineering for AI-enabled systems.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3569951,
title = {PEARC '23: Practice and Experience in Advanced Research Computing},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Portland, OR, USA}
}

@proceedings{10.1145/3555050,
title = {CoNEXT '22: Proceedings of the 18th International Conference on emerging Networking EXperiments and Technologies},
year = {2022},
isbn = {9781450395083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CoNEXT is a premier and highly selective venue in computer networking. This year's exciting technical program helps us better understand and improve the performance, reliability and security of networks in all layers.},
location = {Roma, Italy}
}

@article{10.1145/3449232,
author = {Klug, Daniel and Bogart, Christopher and Herbsleb, James D.},
title = {"They Can Only Ever Guide": How an Open Source Software Community Uses Roadmaps to Coordinate Effort},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449232},
doi = {10.1145/3449232},
abstract = {Unlike in commercial software development, open source software (OSS) projects do not generally have managers with direct control over how developers spend their time, yet for projects with large, diverse sets of contributors, the need exists to focus and steer development in a particular direction in a coordinated way. This is especially important for "infrastructure" projects, such as critical libraries and programming languages that many other people depend on. Some projects have taken the approach of borrowing planning tools that originated in commercial development, despite the fact that these techniques were designed for very different contexts, e.g. strong top-down control and profit motives. Little research has been done to understand how these practices are adapted to a new context. In this paper, we examine the Rust project's use of roadmaps: how has an important OSS infrastructure project adapted an inherently top-down tool to the freewheeling world of OSS? We find that because Rust's roadmaps are built in part by summarizing what motivated developers most prefer to work on, they are in some ways more a description of the motivated labor available than they are a directive that the community move in a particular direction. They allow the community to avoid wasting time on unpopular proposals by revealing that there will be little help in building them, and encouraging work on popular features by making visible the amount of consensus in those features. Roadmaps generate a collective focus without limiting the full scope of what developers work on: roadmap issues consume proportionally more effort than other issues, but constitute a minority of the work done (i.e issues and pull requests made) by both central and peripheral participants. They also create transparency among and beyond the community into what central contributors' plans are, and allow more rational decision-making by providing a way for evidence about community needs to be linked to decision-making.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {158},
numpages = {28},
keywords = {collaboration, common pool resources, open source, rust language}
}

@proceedings{10.1145/3605770,
title = {SCORED '23: Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '23, the second edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Copenhagen, Denmark with extensive support for in-person and virtual attendance.This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {Copenhagen, Denmark}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@article{10.1145/2656877.2656893,
author = {Krenc, Thomas and Hohlfeld, Oliver and Feldmann, Anja},
title = {An internet census taken by an illegal botnet: a qualitative assessment of published measurements},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2656877.2656893},
doi = {10.1145/2656877.2656893},
abstract = {On March 17, 2013, an Internet census data set and an accompanying report were released by an anonymous author or group of authors. It created an immediate media buzz, mainly because of the unorthodox and unethical data collection methodology (i.e., exploiting default passwords to form the Carna botnet), but also because of the alleged unprecedented large scale of this census (even though legitimate census studies of similar and even larger sizes have been performed in the past). Given the unknown source of this released data set, little is known about it. For example, can it be ruled out that the data is faked? Or if it is indeed real, what is the quality of the released data?The purpose of this paper is to shed light on these and related questions and put the contributions of this anonymous Internet census study into perspective. Indeed, our findings suggest that the released data set is real and not faked, but that the measurements suffer from a number of methodological flaws and also lack adequate meta-data information. As a result, we have not been able to verify several claims that the anonymous author(s) made in the published report.In the process, we use this study as an educational example for illustrating how to deal with a large data set of unknown quality, hint at pitfalls in Internet-scale measurement studies, and discuss ethical considerations concerning third-party use of this released data set for publications.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {103–111},
numpages = {9},
keywords = {carna botnet, census, ipv4}
}

@inproceedings{10.1145/3368089.3409671,
author = {Yan, Shenao and Tao, Guanhong and Liu, Xuwei and Zhai, Juan and Ma, Shiqing and Xu, Lei and Zhang, Xiangyu},
title = {Correlations between deep neural network model coverage criteria and model quality},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409671},
doi = {10.1145/3368089.3409671},
abstract = {Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {775–787},
numpages = {13},
keywords = {Deep Neural Networks, Software Testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@proceedings{10.1145/3627341,
title = {ICCVIT '23: Proceedings of the 2023 International Conference on Computer, Vision and Intelligent Technology},
year = {2023},
isbn = {9798400708701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chenzhou, China}
}

@article{10.1145/3640018,
author = {Kapugama Geeganage, Dakshi Tharanga and Wynn, Moe Thandar and ter Hofstede, Arthur H. M.},
title = {Text2EL+: Expert Guided Event Log Enrichment Using Unstructured Text},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3640018},
doi = {10.1145/3640018},
abstract = {Through the application of process mining, business processes can be improved on the basis of process execution data captured in event logs. Naturally, the quality of this data determines the quality of the improvement recommendations. Improving data quality is non-trivial, and there is great potential to exploit unstructured text, e.g., from notes, reviews, and comments, for this purpose and to enrich event logs. To this end, this article introduces Text2EL+&nbsp;, a three-phase approach to enrich event logs using unstructured text. In its first phase, events and (case and event) attributes are derived from unstructured text linked to organisational processes. In its second phase, these events and attributes undergo a semantic and contextual validation before their incorporation in the event log. In its third and final phase, recognising the importance of human domain expertise, expert guidance is used to further improve data quality by removing redundant and irrelevant events. Expert input is used to train a Named Entity Recognition (NER) model with customised tags to detect event log elements. The approach applies natural language processing techniques, sentence embeddings, training pipelines and models, as well as contextual and expression validation. Various unstructured clinical notes associated with a healthcare case study were analysed, and completeness, concordance, and correctness of the derived event log elements were evaluated through experiments. The results show that the proposed method is feasible and applicable.},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {8},
numpages = {28},
keywords = {Event data quality, process mining, event log, unstructured text, natural language processing, semantic validation}
}

@inproceedings{10.1145/3430665.3456312,
author = {Gama, Kiev and Zimmerle, Carlos and Rossi, Pedro},
title = {Online Hackathons as an Engaging Tool to Promote Group Work in Emergency Remote Learning},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456312},
doi = {10.1145/3430665.3456312},
abstract = {In 2020, due to the COVID-19 pandemic, educational activities had to be done remotely as a way to avoid the spread of the disease. Instead of shifting to an online learning model, it consisted of a transition to what was called Emergency Remote Teaching. This is a strategy to keep activities going on until it is safe again to return to the physical facilities of universities. This new setting became a challenge to both teachers and students. The lack of interaction and classroom socialization became obstacles for students to continue engaged.Before the pandemic, hackathons -- short-lived events (1 to 3 days) of intensive collaboration to develop software prototypes -- were being explored as an alternative venue to engage students in acquiring and practicing technical skills. In this paper, we present an experience report on the usage of an online hackathon as a resource to engage students in the development of their semester project in a distributed applications course during this emergency remote teaching period. We describe the intervention and analyze the students' perspective of the approach. One of the findings was the importance of the Discord communication tool -- used by students for playing games -- which helped them socialize and be engaged in synchronous group work, "virtually collocated".},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {345–351},
numpages = {7},
keywords = {emergency remote teaching, hackathons, online learning},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@inproceedings{10.1145/3391800.3398175,
author = {Han, Xueyuan and Mickens, James and Gehani, Ashish and Seltzer, Margo and Pasquier, Thomas},
title = {Xanthus: Push-button Orchestration of Host Provenance Data Collection},
year = {2020},
isbn = {9781450379779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391800.3398175},
doi = {10.1145/3391800.3398175},
abstract = {Host-based anomaly detectors generate alarms by inspecting audit logs for suspicious behavior. Unfortunately, evaluating these anomaly detectors is hard. There are few high-quality, publicly-available audit logs, and there are no pre-existing frameworks that enable push-button creation of realistic system traces. To make trace generation easier, we created Xanthus, an automated tool that orchestrates virtual machines to generate realistic audit logs. Using Xanthus' simple management interface, administrators select a base VM image, configure a particular tracing framework to use within that VM, and define post-launch scripts that collect and save trace data. Once data collection is finished, Xanthus~creates a self-describing archive, which contains the VM, its configuration parameters, and the collected trace data. We demonstrate that Xanthus~hides many of the tedious (yet subtle) orchestration tasks that humans often get wrong; Xanthus~avoids mistakes that lead to non-replicable experiments.},
booktitle = {Proceedings of the 3rd International Workshop on Practical Reproducible Evaluation of Computer Systems},
pages = {27–32},
numpages = {6},
keywords = {IDS, anomaly detection, audit log, computer security, data provenance, data replicability, intrusion detection systems, penetration testing},
location = {Stockholm, Sweden},
series = {P-RECS '20}
}

@inproceedings{10.1145/3607199.3607223,
author = {Trampert, Leon and Stock, Ben and Roth, Sebastian},
title = {Honey, I Cached our Security Tokens Re-usage of Security Tokens in the Wild},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607223},
doi = {10.1145/3607199.3607223},
abstract = {In order to mitigate the effect of Web attacks, modern browsers support a plethora of different security mechanisms. Mechanisms such as anti-Cross-Site Request Forgery (CSRF) tokens or nonces in a Content Security Policy rely on a random number that must only be used once. Notably, those Web security mechanisms are shipped through HTML tags or HTTP response headers from the server to the client side. To decrease the server load and the traffic burdened on the server infrastructure, many Web applications are served via a Content Delivery Network (CDN), which caches certain responses from the server to deliver them to multiple clients. This, however, affects not only the content but also the settings of the security mechanisms deployed via HTML meta tags or HTTP headers. If those are also cached, their content is fixed, and the security tokens are no longer random for each request. Even if the responses are not cached, operators may re-use tokens, as generating random numbers that are unique for each request introduces additional complexity for preserving the state on the server side. This work sheds light on the re-usage of security tokens in the wild, investigates what caused the static tokens, and elaborates on the security impact of the non-random security tokens.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {714–726},
numpages = {13},
keywords = {CSP Nonces, CSRF, Security Tokens, Web Security},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1145/3542929.3563466,
author = {Meiklejohn, Christopher and Stark, Lydia and Celozzi, Cesare and Ranney, Matt and Miller, Heather},
title = {Method overloading the circuit},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542929.3563466},
doi = {10.1145/3542929.3563466},
abstract = {Circuit breakers are frequently deployed in microservice applications to improve their reliability. They achieve this by short circuiting RPC invocations issued to overloaded or failing services, thereby relieving pressure on those services and allowing them to recover. In this paper, we systematically examine the state of the art in industrial circuit breakers designs. We first present a taxonomy of existing, open-source circuit breaker designs and implementations based on a systematic mapping study. We then examine the relationship between these circuit breaker designs and application reliability. We make a clear case that incorrect application of circuit breakers to an application can hurt reliability in the process of trying to improve it. To address the deficiencies in the state of the art, we propose two new circuit breaker designs and provide guidance on how to properly structure microservice applications for the best circuit breaker use. Finally, we identify several open challenges in circuit breaker usage and design for future researchers.},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {273–288},
numpages = {16},
keywords = {fault injection, fault tolerance, verification},
location = {San Francisco, California},
series = {SoCC '22}
}

@inproceedings{10.1145/2908080.2908115,
author = {Petricek, Tomas and Guerra, Gustavo and Syme, Don},
title = {Types from data: making structured data first-class citizens in F#},
year = {2016},
isbn = {9781450342612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908080.2908115},
doi = {10.1145/2908080.2908115},
abstract = {Most modern applications interact with external services and access data in structured formats such as XML, JSON and CSV. Static type systems do not understand such formats, often making data access more cumbersome. Should we give up and leave the messy world of external data to dynamic typing and runtime checks? Of course, not! We present F# Data, a library that integrates external structured data into F#. As most real-world data does not come with an explicit schema, we develop a shape inference algorithm that infers a shape from representative sample documents. We then integrate the inferred shape into the F# type system using type providers. We formalize the process and prove a relative type soundness theorem. Our library significantly reduces the amount of data access code and it provides additional safety guarantees when contrasted with the widely used weakly typed techniques.},
booktitle = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {477–490},
numpages = {14},
keywords = {F#, Inference, JSON, Type Providers, XML},
location = {Santa Barbara, CA, USA},
series = {PLDI '16}
}

@inproceedings{10.1145/3589334.3645399,
author = {Chouaki, Salim and Chakraborty, Abhijnan and Goga, Oana and Zannettou, Savvas},
title = {What News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645399},
doi = {10.1145/3589334.3645399},
abstract = {Understanding how exposure to news on social media impacts public discourse and exacerbates political polarization is a significant endeavor in both computer and social sciences. Unfortunately, progress in this area is hampered by limited access to data due to the closed nature of social media platforms. Consequently, prior studies have been constrained to considering only fragments of users' news exposure and reactions. To overcome this obstacle, we present an innovative measurement approach centered on donating personal data for scientific purposes, facilitated through a privacy-preserving tool that captures users' interactions with news on Facebook. This approach offers a nuanced perspective on users' news exposure and consumption, encompassing different types of news exposure: selective, incidental, algorithmic, and targeted, driven by the diverse underlying mechanisms governing news appearance on users' feeds. Our analysis of data from 472 participants based in the U.S. reveals several interesting findings. For instance, users are more prone to encountering misinformation because of their active selection of low-quality news sources rather than being exposed solely due to friends or platform algorithms. Furthermore, our study uncovers that users are open to engaging with news sources with opposite political ideology as long as these interactions are not visible to their immediate social circles. Overall, our study showcases the viability of data donation as a means to provide clarity to longstanding questions in this field, offering new perspectives on the intricate dynamics of social media news consumption and its effects.},
booktitle = {Proceedings of the ACM on Web Conference 2024},
pages = {2371–2382},
numpages = {12},
keywords = {data donation, news exposure, social media},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3656478,
author = {Gooch, Daniel and Waugh, Kevin and Richards, Mike and Slaymaker, Mark and Woodthorpe, John},
title = {Exploring the Profile of University Assessments Flagged as Containing AI-Generated Material},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/3656478},
doi = {10.1145/3656478},
journal = {ACM Inroads},
month = {may},
pages = {39–47},
numpages = {9}
}

@inproceedings{10.1145/3236024.3236080,
author = {Ram, Achyudh and Sawant, Anand Ashok and Castelluccio, Marco and Bacchelli, Alberto},
title = {What makes a code change easier to review: an empirical investigation on code change reviewability},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236080},
doi = {10.1145/3236024.3236080},
abstract = {Peer code review is a practice widely adopted in software projects to improve the quality of code. In current code review practices, code changes are manually inspected by developers other than the author before these changes are integrated into a project or put into production. We conducted a study to obtain an empirical understanding of what makes a code change easier to review. To this end, we surveyed published academic literature and sources from gray literature (blogs and white papers), we interviewed ten professional developers, and we designed and deployed a reviewability evaluation tool that professional developers used to rate the reviewability of 98 changes. We find that reviewability is defined through several factors, such as the change description, size, and coherent commit history. We provide recommendations for practitioners and researchers. Public preprint [https://doi.org/10.5281/zenodo.1323659]; data and materials [https://doi.org/10.5281/zenodo.1323659].},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {201–212},
numpages = {12},
keywords = {Code quality, code review, pull request},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3466752.3480127,
author = {Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
title = {RecPipe: Co-designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480127},
doi = {10.1145/3466752.3480127},
abstract = {Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 \texttimes{} and 6 \texttimes{}.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {870–884},
numpages = {15},
keywords = {datacenter, deep learning, hardware accelerator, personalized recommendation},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@article{10.1145/3576037,
author = {Venturini, Daniel and Cogo, Filipe Roseiro and Polato, Ivanilton and Gerosa, Marco A. and Wiese, Igor Scaliante},
title = {I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3576037},
doi = {10.1145/3576037},
abstract = {Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages’ builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider’s version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {94},
numpages = {26},
keywords = {Breaking changes, Semantic Version, npm, dependency management, change impact}
}

@article{10.1145/3555190,
author = {Frluckaj, Hana and Dabbish, Laura and Widder, David Gray and Qiu, Huilian Sophie and Herbsleb, James D.},
title = {Gender and Participation in Open Source Software Development},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555190},
doi = {10.1145/3555190},
abstract = {Open source software represents an important form of digital infrastructure as well as a pathway to technical careers for many developers, but women are drastically underrepresented in this setting. Although there is a good body of literature on open source participation, there is very little understanding of the participation trajectories and contribution experiences of women developers, and how they compare to those of men developers, in open source software projects. In order to understand their joining and participation trajectories, we conducted interviews with 23 developers (11 men and 12 women) who became core in an open source project. We identify differences in women and men's motivations for initial contributions and joining processes (e.g. women participating in projects that they have been invited to) and sustained involvement in a project. We also describe unique negative experiences faced by women contributors in this setting in each stage of participation. Our results have implications for diversifying participation in open source software and understanding open source as a pathway to technical careers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {299},
numpages = {31},
keywords = {diversity, gender, inclusion, open collaboration, open source software}
}

@inproceedings{10.1145/3510003.3510620,
author = {Gissurarson, Matth\'{\i}as P\'{a}ll and Applis, Leonhard and Panichella, Annibale and van Deursen, Arie and Sands, David},
title = {PropR: property-based automatic program repair},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510620},
doi = {10.1145/3510003.3510620},
abstract = {Automatic program repair (APR) regularly faces the challenge of overfitting patches --- patches that pass the test suite, but do not actually address the problems when evaluated manually. Currently, overfit detection requires manual inspection or an oracle making quality control of APR an expensive task. With this work, we want to introduce properties in addition to unit tests for APR to address the problem of overfitting. To that end, we design and implement PropR, a program repair tool for Haskell that leverages both property-based testing (via QuickCheck) and the rich type system and synthesis offered by the Haskell compiler. We compare the repair-ratio, time-to-first-patch and overfitting-ratio when using unit tests, property-based tests, and their combination. Our results show that properties lead to quicker results and have a lower overfit ratio than unit tests. The created overfit patches provide valuable insight into the underlying problems of the program to repair (e.g., in terms of fault localization or test quality). We consider this step towards fitter, or at least insightful, patches a critical contribution to bring APR into developer workflows.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1768–1780},
numpages = {13},
keywords = {automatic program repair, property-based testing, search based software engineering, synthesis, typed holes},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3674147,
author = {Hirtum, Lennart Van and Causmaecker, Patrick De and Goemaere, Jens and Kenter, Tobias and Riebler, Heinrich and Lass, Michael and Plessl, Christian},
title = {A Computation of the Ninth Dedekind Number using FPGA Supercomputing},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-7406},
url = {https://doi.org/10.1145/3674147},
doi = {10.1145/3674147},
abstract = {This manuscript makes the claim of having computed the  (9^{th})  Dedekind number, D(9). This was done by accelerating the core operation of the process with an efficient FPGA design that outperforms an optimized 64-core CPU reference by 95 (times) . The FPGA execution was parallelized on the Noctua 2 supercomputer at Paderborn University. The resulting value for D(9) is (286386577668298411128469151667598498812366) 
. This value can be verified in two steps. We have made the data file containing the 490M results available, each of which can be verified separately on CPU, and the whole file sums to our proposed value. The paper explains the mathematical approach in the first part, before putting the focus on a deep dive into the FPGA accelerator implementation followed by a performance analysis. The FPGA implementation was done in RTL using a dual-clock architecture and shows how we achieved an impressive FMax of 450MHz on the targeted Stratix 10 GX 2800 FPGAs. The total compute time used was 47’000 FPGA Hours.},
note = {Just Accepted},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {jul},
keywords = {dedekind number, counting, combinatorics, accelerators, FPGA, supercomputing}
}

@inproceedings{10.1145/3487552.3487824,
author = {Moura, Giovane C. M. and Castro, Sebastian and Heidemann, John and Hardaker, Wes},
title = {TsuNAME: exploiting misconfiguration and vulnerability to DDoS DNS},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487824},
doi = {10.1145/3487552.3487824},
abstract = {TheInternet's Domain Name System (DNS) is a part of every web request and e-mail exchange, so DNS failures can be catastrophic, taking out major websites and services. This paper identifies TsuNAME, a vulnerability where some recursive resolvers can greatly amplify queries, potentially resulting in a denial-of-service to DNS services. TsuNAME is caused by cyclical dependencies in DNS records. A recursive resolver repeatedly follows these cycles, coupled with insufficient caching and application-level retries greatly amplify an initial query, stressing authoritative servers. Although issues with cyclic dependencies are not new, the scale of amplification has not previously been understood. We document real-world events in .nz (a country-level domain), where two misconfigured domains resulted in a 50% increase on overall traffic. We reproduce and document root causes of this event through experiments, and demostrate a 500\texttimes{} amplification factor. In response to our disclosure, several DNS software vendors have documented their mitigations, including Google public DNS and Cisco OpenDNS. For operators of authoritative DNS services we have developed and released CycleHunter, an open-source tool that detects cyclic dependencies and prevents attacks. We use CycleHunter to evaluate roughly 184 million domain names in 7 large, top-level domains (TLDs), finding 44 cyclic dependent NS records used by 1.4k domain names. The TsuNAME vulnerability is weaponizable, since an adversary can easily create cycles to attack the infrastructure of a parent domains. Documenting this threat and its solutions is an important step to ensuring it is fully addressed.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {398–418},
numpages = {21},
location = {Virtual Event},
series = {IMC '21}
}

@proceedings{10.1145/3625007,
title = {ASONAM '23: Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
year = {2023},
isbn = {9798400704093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The ASONAM conference series brings together researchers from around the world to share the latest advances in the attractive field of Social Networks Analysis and Mining.},
location = {Kusadasi, Turkiye}
}

@inproceedings{10.1145/3487553.3524718,
author = {Yu, Shuo and Huang, Huafei and Dao, Minh N. and Xia, Feng},
title = {Graph Augmentation Learning},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524718},
doi = {10.1145/3487553.3524718},
abstract = {Graph Augmentation Learning (GAL) provides outstanding solutions for graph learning in handling incomplete data, noise data, etc. Numerous GAL methods have been proposed for graph-based applications such as social network analysis and traffic flow forecasting. However, the underlying reasons for the effectiveness of these GAL methods are still unclear. As a consequence, how to choose optimal graph augmentation strategy for a certain application scenario is still in black box. There is a lack of systematic, comprehensive, and experimentally validated guideline of GAL for scholars. Therefore, in this survey, we in-depth review GAL techniques from macro (graph), meso (subgraph), and micro (node/edge) levels. We further detailedly illustrate how GAL enhance the data quality and the model performance. The aggregation mechanism of augmentation strategies and graph learning models are also discussed by different application scenarios, i.e., data-specific, model-specific, and hybrid scenarios. To better show the outperformance of GAL, we experimentally validate the effectiveness and adaptability of different GAL strategies in different downstream tasks. Finally, we share our insights on several open issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability, and generalization.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1063–1072},
numpages = {10},
keywords = {Graph augmentation learning, Graph neural networks, Graph representation learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3098954.3098992,
author = {Le Sceller, Quentin and Karbab, ElMouatez Billah and Debbabi, Mourad and Iqbal, Farkhund},
title = {SONAR: Automatic Detection of Cyber Security Events over the Twitter Stream},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3098992},
doi = {10.1145/3098954.3098992},
abstract = {Everyday, security experts face a growing number of security events that affecting people well-being, their information systems and sometimes the critical infrastructure. The sooner they can detect and understand these threats, the more they can mitigate and forensically investigate them. Therefore, they need to have a situation awareness of the existing security events and their possible effects. However, given the large number of events, it can be difficult for security analysts and researchers to handle this flow of information in an adequate manner and answer the following questions in near-real time: what are the current security events? How long do they last? In this paper, we will try to answer these issues by leveraging social networks that contain a massive amount of valuable information on many topics. However, because of the very high volume, extracting meaningful information can be challenging. For this reason, we propose SONAR: an automatic, self-learned framework that can detect, geolocate and categorize cyber security events in near-real time over the Twitter stream. SONAR is based on a taxonomy of cyber security events and a set of seed keywords describing type of events that we want to follow in order to start detecting events. Using these seed keywords, it automatically discovers new relevant keywords such as malware names to enhance the range of detection while staying in the same domain. Using a custom taxonomy describing all type of cyber threats, we demonstrate the capabilities of SONAR on a dataset of approximately 47.8 million tweets related to cyber security in the last 9 months. SONAR could efficiently and effectively detect, categorize and monitor cyber security related events before getting on the security news, and it could automatically discover new security terminologies with their event. Additionally, SONAR is highly scalable and customizable by design; therefore we could adapt SONAR framework for virtually any type of events that experts are interested in.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {23},
numpages = {11},
keywords = {Cyber security events detection, Twitter, framework, security awareness, social media, word embedding},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3639478.3639805,
author = {Sun, Jiayi},
title = {Sustaining Scientific Open-Source Software Ecosystems: Challenges, Practices, and Opportunities},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639805},
doi = {10.1145/3639478.3639805},
abstract = {Scientific open-source software (scientific OSS) has facilitated scientific research due to its transparent and collaborative nature. The sustainability of such software is becoming crucial given its pivotal role in scientific endeavors. While past research has proposed strategies for the sustainability of the scientific software or general OSS communities in isolation, it remains unclear when the two scenarios are merged if these approaches are directly applicable to developing scientific OSS. In this research, we propose to investigate the unique challenges in sustaining the scientific OSS ecosystems. We first conduct a case study to empirically understand the interdisciplinary team's collaboration in scientific OSS ecosystems and identify the collaboration challenges. Further, to generalize our findings, we plan to conduct a large-scale quantitative study in broader scientific OSS ecosystems to identify the cross-project collaboration inefficiencies. Finally, we would like to design and develop interventions to mitigate the problems identified.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {234–236},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3542945,
author = {Araujo, Hugo and Mousavi, Mohammad Reza and Varshosaz, Mahsa},
title = {Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3542945},
doi = {10.1145/3542945},
abstract = {We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {51},
numpages = {61},
keywords = {Verification and validation, robotics, autonomous systems, testing, literature survey}
}

@proceedings{10.1145/3560107,
title = {ICEGOV '22: Proceedings of the 15th International Conference on Theory and Practice of Electronic Governance},
year = {2022},
isbn = {9781450396356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guimar\~{a}es, Portugal}
}

@inproceedings{10.1145/3372297.3417232,
author = {Pashchenko, Ivan and Vu, Duc-Ly and Massacci, Fabio},
title = {A Qualitative Study of Dependency Management and Its Security Implications},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417232},
doi = {10.1145/3372297.3417232},
abstract = {Several large scale studies on the Maven, NPM, and Android ecosystems point out that many developers do not often update their vulnerable software libraries thus exposing the user of their code to security risks. The purpose of this study is to qualitatively investigate the choices and the interplay of functional and security concerns on the developers' overall decision-making strategies for selecting, managing, and updating software dependencies.We run 25 semi-structured interviews with developers of both large and small-medium enterprises located in nine countries. All interviews were transcribed, coded, and analyzed according to applied thematic analysis. They highlight the trade-offs that developers are facing and that security researchers must understand to provide effective support to mitigate vulnerabilities (for example bundling security fixes with functional changes might hinder adoption due to lack of resources to fix functional breaking changes).We further distill our observations to actionable implications on what algorithms and automated tools should achieve to effectively support (semi-)automatic dependency management.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1513–1531},
numpages = {19},
keywords = {dependency management, interviews, qualitative study, security, vulnerable dependencies},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3559712.3559716,
author = {Moreira, Mateus Gabi and De Fran\c{c}a, Breno Bernard Nicolau},
title = {Analysis of Microservice Evolution using Cohesion Metrics},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559712.3559716},
doi = {10.1145/3559712.3559716},
abstract = {The adoption of Microservices Architecture (MSA) has increased in recent years due to several claimed benefits, such as reducing deployment complexity, supporting technology diversity, and better scalability. However, MSA is not free from maintainability issues, especially the lack of cohesion, in which microservices possibly concentrate or miss responsibilities. Also, the lack of empirically-validated cohesion metrics for MSA makes the quantitative assessment even more challenging. In this paper, we empirically explore the practical applicability of service-level cohesion metrics in an open-source MSA application context. The qualitative results show the possibility of assessing MSA cohesion using these service-level metrics, the feasibility of tracking software evolution, and an indication of possible technical debts along the way.},
booktitle = {Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {40–49},
numpages = {10},
keywords = {Cohesion Metrics, Microservices, Software architecture, Software evolution},
location = {Uberlandia, Brazil},
series = {SBCARS '22}
}

@inproceedings{10.1145/3437963.3441698,
author = {Mahajan, Khyati and Roy Choudhury, Sourav and Levens, Sara and Gallicano, Tiffany and Shaikh, Samira},
title = {Community Connect: A Mock Social Media Platform to Study Online Behavior},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441698},
doi = {10.1145/3437963.3441698},
abstract = {We present Community Connect, a custom social media platform for conducting controlled experiments of human behavior. The key distinguishing factor of Community Connect is the ability to control the visibility of user posts based on the groups they belong to, allowing careful and controlled investigation into how information propagates through a social network. We release this platform as a resource to the broader community, to facilitate research on data collected through controlled experiments on social networks.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1073–1076},
numpages = {4},
keywords = {computational social science, data collection and analysis, social networking sites},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.5555/3463952.3464091,
author = {Shinde, Aditya and Doshi, Prashant and Setayeshfar, Omid},
title = {Cyber Attack Intent Recognition and Active Deception using Factored Interactive POMDPs},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper presents an intelligent and adaptive agent that employs deception to recognize a cyber adversary's intent on a honeypot host. Unlike previous approaches to cyber deception, which mainly focus on delaying or confusing the attackers, we focus on engaging with them to learn their intent. We model cyber deception as a sequential decision-making problem in a two-agent context. We introduce factored finitely-nested interactive POMDPs (I-POMDPX) and use this framework to model the problem with multiple attacker types. Our approach models cyber attacks on a single honeypot host across multiple phases from the attacker's initial entry to reaching its adversarial objective. The defending I-POMDPX-based agent uses decoys to engage with the attacker at multiple phases to form increasingly accurate predictions of the attacker's behavior and intent. The use of I-POMDPs also enables us to model the adversary's mental state and investigate how deception affects their beliefs. Our experiments in both simulation and with the agent deployed on a host system show that the I-POMDPX-based agent performs significantly better at intent recognition than commonly used deception strategies on honeypots. This emerging application of autonomous agents offers a new approach that contrasts with the traditional action-reaction dynamic that has defined interactions between cyber attackers and defenders for years.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1200–1208},
numpages = {9},
keywords = {cyber deception, emerging application, honeypots, multi-agent decision making},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/2660398.2660424,
author = {Wang, Ting-Yu and Harper, F. Maxwell and Hecht, Brent},
title = {Designing Better Location Fields in User Profiles},
year = {2014},
isbn = {9781450330435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660398.2660424},
doi = {10.1145/2660398.2660424},
abstract = {Twitter, Facebook, Pinterest and many other online communities ask their users to populate a location field in their user profiles. The information that is entered into this field has many uses in both industry and academia, with location field data providing valuable geographic context for operators of online communities and playing key roles in numerous research projects. However, despite the importance of location field entries, we know little about how to design location fields effectively. In this paper, we report the results of the first controlled study of the design of location fields in user profiles. After presenting a survey of location field design decisions in use across many online communities, we show that certain design decisions can lead to more granular location information or a higher percentage of users that fill out the field, but that there is a trade-off between granularity and the percent of non-empty fields. We also add context to previous work that found that location fields tend to have a high rate of non-geographic information (e.g. Location: "Justin Bieber's Heart"), showing that this result may be site-specific rather than endemic to all location fields. Finally, we provide evidence that verifying users' location field entries against a database of known-valid locations can eliminate toponym (place name) ambiguity and any non-geographic location field entries while at the same time having little effect on field population rate and granularity.},
booktitle = {Proceedings of the 2014 ACM International Conference on Supporting Group Work},
pages = {73–80},
numpages = {8},
keywords = {geographic user-generated content, geotagging, location field, user profile, volunteered geographic information},
location = {Sanibel Island, Florida, USA},
series = {GROUP '14}
}

@inproceedings{10.1145/3491101.3516384,
author = {Rothschild, Annabel and Booker, Justin and Davoll, Christa and Hill, Jessica and Ivey, Venise and DiSalvo, Carl and Rydal Shapiro, Ben and DiSalvo, Betsy},
title = {Towards fair and pro-social employment of digital pieceworkers for sourcing machine learning training data},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3516384},
doi = {10.1145/3491101.3516384},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {2},
numpages = {9},
keywords = {Amazon Mechanical Turk, Platform labor, computing ethics, crowd collaboration, crowd working},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3442381.3450111,
author = {Pelrine, Kellin and Danovitch, Jacob and Rabbany, Reihaneh},
title = {The Surprising Performance of Simple Baselines for Misinformation Detection},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450111},
doi = {10.1145/3442381.3450111},
abstract = {As social media becomes increasingly prominent in our day to day lives, it is increasingly important to detect informative content and prevent the spread of disinformation and unverified rumours. While many sophisticated and successful models have been proposed in the literature, they are often compared with older NLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the performance of a broad set of modern transformer-based language models and show that with basic fine-tuning, these models are competitive with and can even significantly outperform recently proposed state-of-the-art methods. We present our framework as a baseline for creating and evaluating new methods for misinformation detection. We further study a comprehensive set of benchmark datasets, and discuss potential data leakage and the need for careful design of the experiments and understanding of datasets to account for confounding variables. As an extreme case example, we show that classifying only based on the first three digits of tweet ids, which contain information on the date, gives state-of-the-art performance on a commonly used benchmark dataset for fake news detection –Twitter16. We provide a simple tool to detect this problem and suggest steps to mitigate it in future datasets.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3432–3441},
numpages = {10},
keywords = {COVID-19, datasets, misinformation, natural language processing, social media},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3500868,
title = {CSCW'22 Companion: Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2022},
isbn = {9781450391900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Taiwan}
}

@inproceedings{10.1145/3576915.3623120,
author = {Li, Zongjie and Wang, Chaozheng and Wang, Shuai and Gao, Cuiyun},
title = {Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623120},
doi = {10.1145/3576915.3623120},
abstract = {The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their "synonyms" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API.We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2336–2350},
numpages = {15},
keywords = {code generation, large language model, watermark},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@proceedings{10.1145/3523227,
title = {RecSys '22: Proceedings of the 16th ACM Conference on Recommender Systems},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/3582515.3609549,
author = {Iannuzzi, Nicola and Manca, Marco and Patern\`{o}, Fabio and Santoro, Carmen},
title = {Large Scale Automatic Web Accessibility Validation},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609549},
doi = {10.1145/3582515.3609549},
abstract = {Digital accessibility is considered an important aspect to allow all people, including those with permanent or temporary disabilities, to access the continuously increasing number of digital services. This raises the need for tools able to provide support for monitoring the level of accessibility of a large number of websites in order to understand their actual level of accessibility, and identify the areas that need more interventions for their improvement. We present how we have extended a tool for accessibility validation for this purpose, and the results that we obtained in the validation of about 2.7 million Web pages of Italian public administration Web sites.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {307–314},
numpages = {8},
keywords = {Accessibility, Automatic Validation Tools, Large-scale validations},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@inproceedings{10.1145/3540250.3558961,
author = {Chen, Yifen and Rigby, Peter C. and Chen, Yulin and Jiang, Kun and Dehghani, Nader and Huang, Qianying and Cottle, Peter and Andrews, Clayton and Lee, Noah and Nagappan, Nachiappan},
title = {Workgraph: personal focus vs. interruption for engineers at Meta},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558961},
doi = {10.1145/3540250.3558961},
abstract = {All engineers dislike interruptions because it takes away from the deep focus time needed to write complex code. Our goal is to reduce unnecessary interruptions at . We first describe our Workgraph platform that logs how engineers use our internal work tools at . Using these anonymized logs, we create sessions. sessions are defined in opposition to interruption and are the amount of time until the engineer is interrupted by, for example, a work chat message. We describe descriptive statistics related to how long engineers are able to focus. We find that at Meta, Engineers have a total of 14.25 hours of personal-focus time per week. These numbers are comparable with those reported by other software firms. We then create a Random Forest model to understand which factors influence the median daily personal-focus time. We find that the more time an engineer spends in the IDE the longer their focus. We also find that the more central an engineer is in the social work network, the shorter their personal-focus time. Other factors such as role and domain/pillar have little impact on personal-focus at Meta. To help engineers achieve longer blocks of personal-focus and help them stay in flow, Meta developed the AutoFocus tool that blocks work chat notifications when an engineer is working on code for 12 minutes or longer. AutoFocus allows the sender to still force a work chat message using “@notify” ensuring that urgent messages still get through, but allowing the sender to reflect on the importance of the message. In a large experiment, we find that AutoFocus increases the amount of personal-focus time by 20.27%, and it has now been rolled out widely at Meta.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1390–1397},
numpages = {8},
keywords = {A/B Testing, Flow, Focus},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3561386,
author = {De Salve, Andrea and Franceschi, Luca and Lisi, Andrea and Mori, Paolo and Ricci, Laura},
title = {L2DART: A Trust Management System Integrating Blockchain and Off-Chain Computation},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3561386},
doi = {10.1145/3561386},
abstract = {The blockchain technology has been gaining an increasing popularity for the last years, and smart contracts are being used for a growing number of applications in several scenarios. The execution of smart contracts on public blockchains can be invoked by any user with a transaction, although in many scenarios there would be the need for restricting the right of executing smart contracts only to a restricted set of users. To help deal with this issue, this article proposes a system based on a popular access control framework called RT, Role-based Trust Management, to regulate smart contracts execution rights. The proposed system, called Layer 2 DecentrAlized Role-based Trust management (L2DART), implements the RT framework on a public blockchain, and it is designed as a layer-2 technology that involves both on-chain and off-chain functionalities to reduce the blockchain costs while keeping blockchain auditability, i.e., immutability and transparency. The on-chain costs of L2DART have been evaluated on Ethereum and compared with a previous solution implementing on-chain all the functionalities. The results show that the on-chain costs of L2DART are relatively low, making the system deployable in real-world scenarios.},
journal = {ACM Trans. Internet Technol.},
month = {feb},
articleno = {14},
numpages = {30},
keywords = {Blockchain, smart contract, layer-2, off-chain computation, Trust Management, access control}
}

@article{10.1145/3649598,
author = {Robillard, Martin P. and Arya, Deeksha M. and Ernst, Neil A. and Guo, Jin L. C. and Lamothe, Maxime and Nassif, Mathieu and Novielli, Nicole and Serebrenik, Alexander and Steinmacher, Igor and Stol, Klaas-Jan},
title = {Communicating Study Design Trade-offs in Software Engineering},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649598},
doi = {10.1145/3649598},
abstract = {Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs. We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers’ work.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {112},
numpages = {10},
keywords = {Empirical software engineering, threats to validity, empirical study design, metascience, research validity, research design trade-offs}
}

@inproceedings{10.1145/3377813.3381365,
author = {Strand, Anton and Gunnarson, Markus and Britto, Ricardo and Usman, Muhmmad},
title = {Using a context-aware approach to recommend code reviewers: findings from an industrial case study},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381365},
doi = {10.1145/3377813.3381365},
abstract = {Code review is a commonly used practice in software development. It refers to the process of reviewing new code changes before they are merged with the code base. However, to perform the review, developers are mostly assigned manually to code changes. This may lead to problems such as: a time-consuming selection process, limited pool of known candidates and risk of over-allocation of a few reviewers. To address the above problems, we developed Carrot, a machine learning-based tool to recommend code reviewers. We conducted an improvement case study at Ericsson. We evaluated Carrot using a mixed approach. we evaluated the prediction accuracy using historical data and the metrical Mean Reciprocal Rank (MRR). Furthermore, we deployed the tool in one Ericsson project and evaluated how adequate the recommendations were from the point of view of the tool users and the recommended reviewers. We also asked the opinion of senior developers about the usefulness of the tool. The results show that Carrot can help identify relevant non-obvious reviewers and be of great assistance to new developers. However, there were mixed opinions on Carrot's ability to assist with workload balancing and the decrease code review lead time.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {1–10},
numpages = {10},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@proceedings{10.1145/3626252,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@inproceedings{10.1145/2940299.2940300,
author = {Maceli, Monica},
title = {Co-design in the wild: a case study on meme creation tools},
year = {2016},
isbn = {9781450340465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940299.2940300},
doi = {10.1145/2940299.2940300},
abstract = {The internet meme has become a vital form of self-expression in social communities throughout the Internet. The tools facilitating meme-creation, specifically image macros, have been little-studied but endow non-technical users with the ability to create the multi-layered graphics typical to such memes. The use of these creativity tools provides a unique setting in which to explore the concept of co-design, wherein tools are shaped in response to emergent user needs. Users and designers of meme-creation tools have evolved ways of collaborating and communicating over time, in a fully naturalistic setting. This study explores these processes through survey and interviews of tool designer/developers and an analysis of users' design ideas generated over time. The study finds that, while co-design may be commonly practiced today, these activities raise numerous challenges to participatory design, including: creating trust between designers and users, managing unwieldy system growth, and supporting features specific to aspiring end-user crafters.},
booktitle = {Proceedings of the 14th Participatory Design Conference: Full Papers - Volume 1},
pages = {161–170},
numpages = {10},
keywords = {co-design, image macros, internet memes, participatory design, social technologies},
location = {Aarhus, Denmark},
series = {PDC '16}
}

@proceedings{10.1145/3578741,
title = {MLNLP '22: Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
year = {2022},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@inproceedings{10.1145/3338906.3340449,
author = {Asthana, Sumit and Kumar, Rahul and Bhagwan, Ranjita and Bird, Christian and Bansal, Chetan and Maddila, Chandra and Mehta, Sonu and Ashok, B.},
title = {WhoDo: automating reviewer suggestions at scale},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340449},
doi = {10.1145/3338906.3340449},
abstract = {Today's software development is distributed and involves continuous changes for new features and yet, their development cycle has to be fast and agile. An important component of enabling this agility is selecting the right reviewers for every code-change - the smallest unit of the development cycle. Modern tool-based code review is proven to be an effective way to achieve appropriate code review of software changes. However, the selection of reviewers in these code review systems is at best manual. As software and teams scale, this poses the challenge of selecting the right reviewers, which in turn determines software quality over time. While previous work has suggested automatic approaches to code reviewer recommendations, it has been limited to retrospective analysis. We not only deploy a reviewer suggestions algorithm - WhoDo - and evaluate its effect but also incorporate load balancing as part of it to address one of its major shortcomings: of recommending experienced developers very frequently. We evaluate the effect of this hybrid recommendation + load balancing system on five repositories within Microsoft. Our results are based around various aspects of a commit and how code review affects that. We attempt to quantitatively answer questions which are supposed to play a vital role in effective code review through our data and substantiate it through qualitative feedback of partner repositories.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {937–945},
numpages = {9},
keywords = {code-review, recommendation, software-engineering},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3584976,
author = {Srinivasa, Shreyas and Pedersen, Jens Myrup and Vasilomanolakis, Emmanouil},
title = {Gotta Catch ’em All: A Multistage Framework for Honeypot Fingerprinting},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3584976},
doi = {10.1145/3584976},
abstract = {Honeypots are decoy systems that lure attackers by presenting them with a seemingly vulnerable system. They provide an early detection mechanism as well as a method for learning how adversaries work and think. However, over the past years, several researchers have shown methods for fingerprinting honeypots. This significantly decreases the value of a honeypot; if an attacker is able to recognize the existence of such a system, they can evade it. In this article, we revisit the honeypot identification field, by providing a holistic framework that includes state-of-the-art and novel fingerprinting components. We decrease the probability of false positives by proposing a rigid multi-step approach for labeling a system as a honeypot. We perform extensive scans covering 2.9 billion addresses of the IPv4 space and identify a total of 21,855 honeypot instances. Moreover, we present several interesting side findings such as the identification of around 355,000 non-honeypot systems that represent potentially misconfigured or unpatched vulnerable servers (e.g., SSH servers with default password configurations and vulnerable versions). We ethically disclose our findings to network administrators about the default configuration and the honeypot developers about the gaps in implementation that lead to possible honeypot fingerprinting. Last, we discuss countermeasures against honeypot fingerprinting techniques.},
journal = {Digital Threats},
month = {oct},
articleno = {42},
numpages = {28},
keywords = {Honeypots, fingerprinting, honeypot attacks, honeypot detection, honeypot evasion}
}

@inproceedings{10.1145/3194810.3194817,
author = {Afzal, Afsoon and Lacomis, Jeremy and Goues, Claire Le and Timperley, Christopher S.},
title = {A turing test for genetic improvement},
year = {2018},
isbn = {9781450357531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194810.3194817},
doi = {10.1145/3194810.3194817},
abstract = {Genetic improvement is a research field that aims to develop search-based techniques for improving existing code. GI has been used to automatically repair bugs, reduce energy consumption, and to improve run-time performance. In this paper, we reflect on the often-overlooked relationship between GI and developers within the context of continually evolving software systems. We introduce a distinction between transparent and opaque patches based on intended lifespan and developer interaction. Finally, we outline a Turing test for assessing the ability of a GI system to produce opaque patches that are acceptable to humans. This motivates research into the role GI systems will play in transparent development contexts.},
booktitle = {Proceedings of the 4th International Workshop on Genetic Improvement Workshop},
pages = {17–18},
numpages = {2},
location = {Gothenburg, Sweden},
series = {GI '18}
}

@inproceedings{10.1145/3548606.3559358,
author = {Shen, Yun and Han, Yufei and Zhang, Zhikun and Chen, Min and Yu, Ting and Backes, Michael and Zhang, Yang and Stringhini, Gianluca},
title = {Finding MNEMON: Reviving Memories of Node Embeddings},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3559358},
doi = {10.1145/3548606.3559358},
abstract = {Previous security research efforts orbiting around graphs have been exclusively focusing on either (de-)anonymizing the graphs or understanding the security and privacy issues of graph neural networks. Little attention has been paid to understand the privacy risks of integrating the output from graph embedding models (e.g., node embeddings) with complex downstream machine learning pipelines. In this paper, we fill this gap and propose a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes. We show that an adversary can recover edges with decent accuracy by only gaining access to the node embedding matrix of the original graph without interactions with the node embedding models. We demonstrate the effectiveness and applicability of our graph recovery attack through extensive experiments.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2643–2657},
numpages = {15},
keywords = {graph embedding, machine learning security and privacy},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3411764.3445667,
author = {Hamm, Andrea and Shibuya, Yuya and Ullrich, Stefan and Cerratto Pargman, Teresa Cerratto},
title = {What Makes Civic Tech Initiatives To Last Over Time? Dissecting Two Global Cases},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445667},
doi = {10.1145/3411764.3445667},
abstract = {Civic tech initiatives dedicated to environmental issues have become a worldwide phenomenon and made invaluable contributions to data, community building, and publics. However, many of them stop after a relatively short time. Therefore, we studied two long-lasting civic tech initiatives of global scale, to understand what makes them sustain over time. To this end, we conducted two mixed-method case studies, combining social network analysis and qualitative content analysis of Twitter data with insights from expert interviews. Drawing on our findings, we identified a set of key factors that help the studied civic tech initiatives to grow and last. Contributing to Digital Civics in HCI, we argue that the civic tech initiatives’ scaling and sustaining are configured through the entanglement of (1) civic data both captured and owned by the citizens for the citizens, (2) the use of open and accessible technology, and (3) the initiatives’ public narrative, giving them a voice on the environmental issue.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {87},
numpages = {17},
keywords = {Citizen Science, Civic IoT, Civic Tech, Digital Civics, Scaling, Sustainability},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3377930.3390173,
author = {Medvet, Eric and Bartoli, Alberto and De Lorenzo, Andrea and Fidel, Giulio},
title = {Evolution of distributed neural controllers for voxel-based soft robots},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390173},
doi = {10.1145/3377930.3390173},
abstract = {Voxel-based soft robots (VSRs) are aggregations of elastic, cubic blocks that have sparkled the interest of Robotics and Artificial Life researchers. VSRs can move by varying the volume of individual blocks, according to control signals dictated by a controller, possibly based on inputs coming from sensors embedded in the blocks. Neural networks (NNs) have been used as centralized processing units for those sensing controllers, with weights optimized using evolutionary computation. This structuring breaks the intrinsic modularity of VSRs: decomposing a VSR into modules to be assembled in a different way is very hard.In this work we propose an alternative approach that enables full modularity and is based on a distributed neural controller. Each block contains a small NN that outputs signals to adjacent blocks and controls the local volume, based on signals from adjacent blocks and on local sensor readings. We show experimentally for the locomotion task that our controller is as effective as the centralized one. Our experiments also suggest that the proposed framework indeed allows exploiting modularity: VSRs composed of pre-trained parts (body and controller) can be evolved more efficiently than starting from scratch.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {112–120},
numpages = {9},
keywords = {evolutionary robotics, modularity, neuroevolution},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3639474.3640064,
author = {Lau, Yi Meng and Koh, Christian Michael and Jiang, Lingxiao},
title = {Teaching Software Development for Real-World Problems using a Microservice-Based Collaborative Problem-Solving Approach},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640064},
doi = {10.1145/3639474.3640064},
abstract = {Experienced and skillful software developers are needed in organizations to develop software products effective for their business with shortened time-to-market. Such developers will not only need to code but also be able to work in teams and collaboratively solve real-world problems that organizations are facing. It is challenging for educators to nurture students to become such developers with strong technical, social, and cognitive skills.Towards addressing the challenge, this study presents a Collaborative Software Development Project Framework for a course that focuses on learning microservices architectures and developing a software application for a real-world business. Students get to work in teams to solve a real-world problem of their own choice. They are given opportunities to recognize that the software development process goes beyond writing code and that social and cognitive skills in engaging with each other are also essential. By adopting microservices architectures in the course, students learn to break down the functionalities of their applications into smaller pieces of code with standardized interfaces that can be developed, tested, and deployed independently. This not only helps students to learn various technical skills needed for developing and implementing the functionalities needed by the application in the form of microservices but also facilitates task allocation and coordination among their team members and provides a platform for them to solve problems collaboratively. Upon completion of their projects, students are also asked to reflect on their development process and encouraged to think beyond the basics for better software design and development approaches.The course curriculum incorporates the framework, especially for the student team projects. The earlier teaching weeks introduce a combination of concepts and lab exercises to students as the building blocks. The survey studies show that the framework is effective in enhancing the students' learning of technical, social, and cognitive skills, while further improvements, such as closer collaboration with other courses, can be done to improve a holistic learning curriculum.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {22–33},
numpages = {12},
keywords = {software development, collaborative problem-solving, real-world solutions, microservices architectures},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3611643.3616327,
author = {Davis, Matthew C. and Choi, Sangheon and Estep, Sam and Myers, Brad A. and Sunshine, Joshua},
title = {NaNofuzz: A Usable Tool for Automatic Test Generation},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616327},
doi = {10.1145/3611643.3616327},
abstract = {In the United States alone, software testing labor is estimated to cost $48 billion USD per year. Despite widespread test execution automation and automation in other areas of software engineering, test suites continue to be created manually by software engineers. We have built a test generation tool, called NaNofuzz, that helps users find bugs in their code by suggesting tests where the output is likely indicative of a bug, e.g., that return NaN (not-a-number) values. NaNofuzz is an interactive tool embedded in a development environment to fit into the programmer's workflow. NaNofuzz tests a function with as little as one button press, analyses the program to determine inputs it should evaluate, executes the program on those inputs, and categorizes outputs to prioritize likely bugs. We conducted a randomized controlled trial with 28 professional software engineers using NaNofuzz as the intervention treatment and the popular manual testing tool, Jest, as the control treatment. Participants using NaNofuzz on average identified bugs more accurately (p &lt; .05, by 30%), were more confident in their tests (p &lt; .03, by 20%), and finished their tasks more quickly (p &lt; .007, by 30%).},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1114–1126},
numpages = {13},
keywords = {Empirical software engineering, automatic test generation, experiments, human subjects, software testing, usable testing, user study},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3638246,
author = {Jodat, Baharin A. and Chandar, Abhishek and Nejati, Shiva and Sabetzadeh, Mehrdad},
title = {Test Generation Strategies for Building Failure Models and Explaining Spurious Failures},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638246},
doi = {10.1145/3638246},
abstract = {Test inputs fail not only when the system under test is faulty but also when the inputs are invalid or unrealistic. Failures resulting from invalid or unrealistic test inputs are spurious. Avoiding spurious failures improves the effectiveness of testing in exercising the main functions of a system, particularly for compute-intensive (CI) systems where a single test execution takes significant time. In this article, we propose to build failure models for inferring interpretable rules on test inputs that cause spurious failures. We examine two alternative strategies for building failure models: (1)&nbsp;machine learning (ML)-guided test generation and (2)&nbsp;surrogate-assisted test generation. ML-guided test generation infers boundary regions that separate passing and failing test inputs and samples test inputs from those regions. Surrogate-assisted test generation relies on surrogate models to predict labels for test inputs instead of exercising all the inputs. We propose a novel surrogate-assisted algorithm that uses multiple surrogate models simultaneously, and dynamically selects the prediction from the most accurate model. We empirically evaluate the accuracy of failure models inferred based on surrogate-assisted and ML-guided test generation algorithms. Using case studies from the domains of cyber-physical systems and networks, we show that our proposed surrogate-assisted approach generates failure models with an average accuracy of 83%, significantly outperforming ML-guided test generation and two baselines. Further, our approach learns failure-inducing rules that identify genuine spurious failures as validated against domain knowledge.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {93},
numpages = {32},
keywords = {Search-based testing, machine learning, surrogate models, failure models, test-input validity, and spurious failures}
}

@inproceedings{10.1145/3550356.3556502,
author = {Boubekeur, Younes and Singh, Prabhsimran and Mussbacher, Gunter},
title = {A DSL and model transformations to specify learning corpora for modeling assistants},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3556502},
doi = {10.1145/3550356.3556502},
abstract = {Software engineering undergraduate students spend a significant time learning various topics related to software design, including notably model-driven engineering (MDE), where different types of structural and behavioral models are used to design, implement, and validate an application. MDE instructors spend a lot of time covering modeling concepts, which is more difficult with ever-increasing class sizes. Online resources, such as learning corpora for domain modeling, can aid in this learning process by serving as a more dynamic textbook alternative or as part of a larger interactive application with domain modeling exercises and tutorials. A Learning Corpus (LC) is an extensible list of entries representing possible mistakes that could occur when defining a model, e.g., Missing Abstraction-Occurrence pattern in the case of a domain model. Each LC entry includes progressive levels of feedback, including written responses, quizzes, and references to external resources. To make it easy for instructors to customize the entries as well as add their own, we propose a novel, simple, and intuitive approach based on an internal domain-specific language that supports features such as context-specific information and concise arbitrary metamodel navigation with shorthands. Transformations to source code as well as Markdown and LATEX enable use of the LC entries in different contexts. These transformations as well as the integration of the generated code in a sample Modeling Assistant application verify and validate the LC metamodel and specification.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {95–102},
numpages = {8},
keywords = {domain modeling, feedback, learning corpus, model transformation, model-driven engineering (MDE)},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3394171.3413802,
author = {Song, Yi-Fan and Zhang, Zhang and Shan, Caifeng and Wang, Liang},
title = {Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413802},
doi = {10.1145/3394171.3413802},
abstract = {One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the State-Of-The-Art (SOTA) models of this task tends to be exceedingly sophisticated and over-parameterized, where the low efficiency in model training and inference has obstructed the development in the field, especially for large-scale action datasets. In this work, we propose an efficient but strong baseline based on Graph Convolutional Network (GCN), where three main improvements are aggregated, i.e., early fused Multiple Input Branches (MIB), Residual GCN (ResGCN) with bottleneck structure and Part-wise Attention (PartAtt) block. Firstly, an MIB is designed to enrich informative skeleton features and remain compact representations at an early fusion stage. Then, inspired by the success of the ResNet architecture in Convolutional Neural Network (CNN), a ResGCN module is introduced in GCN to alleviate computational costs and reduce learning difficulties in model training while maintain the model accuracy. Finally, a PartAtt block is proposed to discover the most essential body parts over a whole action sequence and obtain more explainable representations for different skeleton action sequences. Extensive experiments on two large-scale datasets, i.e., NTU RGB+D 60 and 120, validate that the proposed baseline slightly outperforms other SOTA models and meanwhile requires much fewer parameters during training and inference procedures, e.g., at most 34 times less than DGNN, which is one of the best SOTA methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1625–1633},
numpages = {9},
keywords = {ResGCN, action recognition, bottleneck, part attention, skeleton},
location = {Seattle, WA, USA},
series = {MM '20}
}

@proceedings{10.1145/3627106,
title = {ACSAC '23: Proceedings of the 39th Annual Computer Security Applications Conference},
year = {2023},
isbn = {9798400708862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3616855,
title = {WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 17th ACM International Conference on Web Search and Data Mining - WSDM 2024. WSDM is one of the premier conferences in the fields of web search and data mining, with a dynamic and growing community from academia and industry. After two years of virtual conferences and in-person conferences in Singapore, the 2024 edition is an in-person conference with virtual elements. We hope you enjoy the conference at the "Centro Internacional de Congresos de Yucatan (CIC)" in Merida from March 4 to March 8, 2024.We are excited to kick off the program with a dynamic mix of Tutorials and Industry Day. Our seven tutorials will cover a broad range of search and data mining topics. Industry Day will provide valuable insights from leaders at major technology companies. The core technical program continues WSDM's tradition of a single-track format, featuring 109 thought-provoking papers from both academic and industry experts. We're honored to have inspiring keynote speakers each day: Nicolas Christin (CMU), Elizabeth Reid (Google), and Saiph Savage (Civic A.I. Lab). Additionally, 17 interactive demonstrations will showcase the latest prototypes and systems. The final day offers a stimulating Doctoral Consortium and six engaging workshops on topics including integrity in social networks, large language model for society, psychology-informed information access system, interactive and scalable information retrieval system and machine learning on graphs. WSDM 2024 proudly presents WSDM day on information retrieval and Web in the region. WSDM Cup Day highlights finalists' presentations addressing challenges in Conversational Multi-Doc QA. This diverse and stimulating program promises to be an enriching experience for all!.},
location = {Merida, Mexico}
}

@proceedings{10.1145/3563357,
title = {BuildSys '22: Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the past thirteen years, BuildSys has been an interdisciplinary conference that brings together various stakeholders, including researchers, practitioners, and policymakers from different disciplines, including civil engineering, mechanical engineering, environmental science, electrical and computer engineering, computer science, system management and control, and many others. This year is no exception, with papers and attendees from all these disciplines and regions worldwide. The conference's focus extends beyond building systems to the built environment more generally.},
location = {Boston, Massachusetts}
}

@inproceedings{10.1145/3131365.3131389,
author = {Thomas, Daniel R. and Pastrana, Sergio and Hutchings, Alice and Clayton, Richard and Beresford, Alastair R.},
title = {Ethical issues in research using datasets of illicit origin},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131389},
doi = {10.1145/3131365.3131389},
abstract = {We evaluate the use of data obtained by illicit means against a broad set of ethical and legal issues. Our analysis covers both the direct collection, and secondary uses of, data obtained via illicit means such as exploiting a vulnerability, or unauthorized disclosure. We extract ethical principles from existing advice and guidance and analyse how they have been applied within more than 20 recent peer reviewed papers that deal with illicitly obtained datasets. We find that existing advice and guidance does not address all of the problems that researchers have faced and explain how the papers tackle ethical issues inconsistently, and sometimes not at all. Our analysis reveals not only a lack of application of safeguards but also that legitimate ethical justifications for research are being overlooked. In many cases positive benefits, as well as potential harms, remain entirely unidentified. Few papers record explicit Research Ethics Board (REB) approval for the activity that is described and the justifications given for exemption suggest deficiencies in the REB process.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {445–462},
numpages = {18},
keywords = {cybercrime, data of illicit origin, ethics, found data, law, leaked data, menlo report, unintentionally public data},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1109/ICSE48619.2023.00064,
author = {Tan, Xin and Chen, Yiran and Wu, Haohua and Zhou, Minghui and Zhang, Li},
title = {Is it Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00064},
doi = {10.1109/ICSE48619.2023.00064},
abstract = {Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {653–664},
numpages = {12},
keywords = {newcomer, mentoring, open source, good first issue},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3491039,
author = {Calefato, Fabio and Lanubile, Filippo},
title = {Using Personality Detection Tools for Software Engineering Research: How Far Can We Go?},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3491039},
doi = {10.1145/3491039},
abstract = {Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-the-shelf solutions trained on non-technical corpora, which might not be readily applicable to technical domains like software engineering. In this article, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers’ e-mails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in software engineering by replacing the personality detection tool used to infer developers’ personalities from pull-request discussions and e-mails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {42},
numpages = {48},
keywords = {Computational personality detection, automatic personality recognition, Big Five, Five-Factor Model, replication, negative results, LIWC, IBM personality insights}
}

@article{10.1145/3469652,
author = {Han, Zhao and Phillips, Elizabeth and Yanco, Holly A.},
title = {The Need for Verbal Robot Explanations and How People Would Like a Robot to Explain Itself},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
url = {https://doi.org/10.1145/3469652},
doi = {10.1145/3469652},
abstract = {Although non-verbal cues such as arm movement and eye gaze can convey robot intention, they alone may not provide enough information for a human to fully understand a robot’s behavior. To better understand how to convey robot intention, we conducted an experiment (N = 366) investigating the need for robots to explain, and the content and properties of a desired explanation such as timing, engagement importance, similarity to human explanations, and summarization. Participants watched a video where the robot was commanded to hand an almost-reachable cup and one of six reactions intended to show the unreachability : doing nothing (No Cue), turning its head to the cup (Look), or turning its head to the cup with the addition of repeated arm movement pointed towards the cup (Look &amp; Point), and each of these with or without a Headshake. The results indicated that participants agreed robot behavior should be explained across all conditions, in situ, in a similar manner as what human explain, and provide concise summaries and respond to only a few follow-up questions by participants. Additionally, we replicated the study again with N = 366 participants after a 15-month span and all major conclusions still held.},
journal = {J. Hum.-Robot Interact.},
month = {sep},
articleno = {36},
numpages = {42},
keywords = {Robot explanation, behavior explanation, system transparency}
}

@proceedings{10.1145/3640543,
title = {IUI '24: Proceedings of the 29th International Conference on Intelligent User Interfaces},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Greenville, SC, USA}
}

@inproceedings{10.1145/3583780.3615295,
author = {Alonso, Omar and Church, Kenneth},
title = {Some Useful Things to Know When Combining IR and NLP: the Easy, the Hard and the Ugly},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615295},
doi = {10.1145/3583780.3615295},
abstract = {Deep nets such as GPT are at the core of the current advances in many systems and applications. Things are moving very fast, and it appears that techniques are out of date within weeks. How can we take advantage of new discoveries and incorporate them into our existing work? Are these radical new developments, repetitions of older concepts, or both?In this tutorial, we aim to bring interested researchers and practitioners up to speed on the recent and ongoing techniques around ML and Deep learning in the context of IR and NLP. Additionally, our goal is to clarify terminology, emphasize fundamentals, and outline new research opportunities.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5196–5199},
numpages = {4},
keywords = {LLMs, benchmarks, fine-tunning, inference, pre-training},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3460120.3485372,
author = {Xiong, Junjie and Wei, Mingkui and Lu, Zhuo and Liu, Yao},
title = {Warmonger: Inflicting Denial-of-Service via Serverless Functions in the Cloud},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485372},
doi = {10.1145/3460120.3485372},
abstract = {We debut the Warmonger attack, a novel attack vector that can cause denial-of-service between a serverless computing platform and an external content server. The Warmonger attack exploits the fact that a serverless computing platform shares the same set of egress IPs among all serverless functions, which belong to different users, to access an external content server. As a result, a malicious user on this platform can purposefully misbehave and cause these egress IPs to be blocked by the content server, resulting in a platform-wide denial of service. To validate the Warmonger attack, we ran months-long experiments, collected and analyzed the egress IP usage pattern of four major serverless service providers (SSPs). We also conducted an in-depth evaluation of an attacker's possible moves to inflict an external server and cause IP-blockage. We demonstrate that some SSPs use surprisingly small numbers of egress IPs (as little as only four) and share them among their users, and that the serverless platform provides sufficient leverage for a malicious user to conduct well-known misbehaviors and cause IP-blockage. Our study unveiled a potential security threat on the emerging serverless computing platform, and shed light on potential mitigation approaches.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {955–969},
numpages = {15},
keywords = {cloud computing, denial-of-service, edge computing, serverless functions},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@proceedings{10.5555/3623295,
title = {ICSE-SEET '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/2807442.2807469,
author = {Guo, Philip J.},
title = {Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming},
year = {2015},
isbn = {9781450337793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2807442.2807469},
doi = {10.1145/2807442.2807469},
abstract = {One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming. But there are usually far fewer experts than learners. To enable a single expert to help more learners at once, we built Codeopticon, an interface that enables a programming tutor to monitor and chat with dozens of learners in real time. Each learner codes in a workspace that consists of an editor, compiler, and visual debugger. The tutor sees a real-time view of each learner's actions on a dashboard, with each learner's workspace summarized in a tile. At a glance, the tutor can see how learners are editing and debugging their code, and what errors they are encountering. The dashboard automatically reshuffles tiles so that the most active learners are always in the tutor's main field of view. When the tutor sees that a particular learner needs help, they can open an embedded chat window to start a one-on-one conversation. A user study showed that 8 first-time Codeopticon users successfully tutored anonymous learners from 54 countries in a naturalistic online setting. On average, in a 30-minute session, each tutor monitored 226 learners, started 12 conversations, exchanged 47 chats, and helped 2.4 learners.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {599–608},
numpages = {10},
keywords = {computer programming, learning at scale, remote tutoring},
location = {Charlotte, NC, USA},
series = {UIST '15}
}

@article{10.1145/3477579,
author = {Beyer, Dirk and Dangl, Matthias and Dietsch, Daniel and Heizmann, Matthias and Lemberger, Thomas and Tautschnig, Michael},
title = {Verification Witnesses},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3477579},
doi = {10.1145/3477579},
abstract = {Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker, UAutomizer, CPA-witness2test, and FShell-witness2test.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {57},
numpages = {69},
keywords = {Violation witness, correctness witness, witness validation, software verification, program analysis, model checking, data-flow analysis, formal methods, certifying algorithm}
}

@proceedings{10.1145/3477997,
title = {DYNAMICS '20: Proceedings of the 2020 Workshop on DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security},
year = {2020},
isbn = {9781450387149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lexington, MA, USA}
}

@inproceedings{10.1145/3544548.3581225,
author = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
title = {Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581225},
doi = {10.1145/3544548.3581225},
abstract = {Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron’s usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report reflections both from our interviewees and from independent reviewers who critiqued performances of several of the scripts to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations—including plagiarism and bias—and participatory models for the design and deployment of such tools.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {355},
numpages = {34},
keywords = {co-creativity, computational creativity, human-computer interaction, improvisation, natural language evaluation, natural language generation, theatre},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3524842.3527937,
author = {Obie, Humphrey O. and Ilekura, Idowu and Du, Hung and Shahin, Mojtaba and Grundy, John and Li, Li and Whittle, Jon and Turhan, Burak},
title = {On the violation of honesty in mobile apps: automated detection and categories},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527937},
doi = {10.1145/3524842.3527937},
abstract = {Human values such as integrity, privacy, curiosity, security, and honesty are guiding principles for what people consider important in life. Such human values may be violated by mobile software applications (apps), and the negative effects of such human value violations can be seen in various ways in society. In this work, we focus on the human value of honesty. We present a model to support the automatic identification of violations of the value of honesty from app reviews from an end-user perspective. Beyond the automatic detection of honesty violations by apps, we also aim to better understand different categories of honesty violations expressed by users in their app reviews. The result of our manual analysis of our honesty violations dataset shows that honesty violations can be characterised into ten categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. Based on these results, we argue for a conscious effort in developing more honest software artefacts including mobile apps, and the promotion of honesty as a key value in software development practices. Furthermore, we discuss the role of app distribution platforms as enforcers of ethical systems supporting human values, and highlight some proposed next steps for human values in software engineering (SE) research.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {321–332},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@article{10.1145/3467021,
author = {Ge, Mengmeng and Cho, Jin-Hee and Kim, Dongseong and Dixit, Gaurav and Chen, Ing-Ray},
title = {Proactive Defense for Internet-of-things: Moving Target Defense With Cyberdeception},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3467021},
doi = {10.1145/3467021},
abstract = {Resource constrained Internet-of-Things (IoT) devices are highly likely to be compromised by attackers, because strong security protections may not be suitable to be deployed. This requires an alternative approach to protect vulnerable components in IoT networks. In this article, we propose an integrated defense technique to achieve intrusion prevention by leveraging cyberdeception (i.e., a decoy system) and moving target defense (i.e., network topology shuffling). We evaluate the effectiveness and efficiency of our proposed technique analytically based on a graphical security model in a software-defined networking (SDN)-based IoT network. We develop four strategies (i.e., fixed/random and adaptive/hybrid) to address “when” to perform network topology shuffling and three strategies (i.e., genetic algorithm/decoy attack path-based optimization/random) to address “how” to perform network topology shuffling on a decoy-populated IoT network, and we analyze which strategy can best achieve a system goal, such as prolonging the system lifetime, maximizing deception effectiveness, maximizing service availability, or minimizing defense cost. We demonstrated that a software-defined IoT network running our intrusion prevention technique at the optimal parameter setting prolongs system lifetime, increases attack complexity of compromising critical nodes, and maintains superior service availability compared with a counterpart IoT network without running our intrusion prevention technique. Further, when given a single goal or a multi-objective goal (e.g., maximizing the system lifetime and service availability while minimizing the defense cost) as input, the best combination of “when” and “how” strategies is identified for executing our proposed technique under which the specified goal can be best achieved.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {24},
numpages = {31},
keywords = {Internet-of-Things, moving target defense, graphical security models, software defined networking}
}

@article{10.1145/3211968,
author = {O'Hearn, Peter},
title = {Separation logic},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3211968},
doi = {10.1145/3211968},
abstract = {Separation logic is a key development in formal reasoning about programs, opening up new lines of attack on longstanding problems.},
journal = {Commun. ACM},
month = {jan},
pages = {86–95},
numpages = {10}
}

@inproceedings{10.1145/3102071.3102091,
author = {Heijne, Norbert and Bakkes, Sander},
title = {Procedural zelda: a PCG environment for player experience research},
year = {2017},
isbn = {9781450353199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102071.3102091},
doi = {10.1145/3102071.3102091},
abstract = {To contribute to the domain of player experience research, this paper presents a new PCG environment with a relatively wide expressive range that builds upon the iconic The Legend of Zelda: A Link to the Past action-RPG game; it contributes by providing the openly-available Procedural Zelda environment for gaming research. The paper presents the design goals and design context of the research environment, and provides a detailed overview of the procedural capabilities of Procedural Zelda, together with its capabilities for data logging, to benefit, e.g., player modelling investigations.},
booktitle = {Proceedings of the 12th International Conference on the Foundations of Digital Games},
articleno = {11},
numpages = {10},
keywords = {player experience research, player modelling, procedural content generation, zelda},
location = {Hyannis, Massachusetts},
series = {FDG '17}
}

@article{10.5555/3648699.3648939,
author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
title = {PaLM: scaling language modeling with pathways},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
journal = {J. Mach. Learn. Res.},
month = {mar},
articleno = {240},
numpages = {113},
keywords = {large language models, few-shot learning, natural language processing, scalable deep learning}
}

@inproceedings{10.1145/3282308.3282319,
author = {Stocker, Mirko and Zimmermann, Olaf and Zdun, Uwe and L\"{u}bke, Daniel and Pautasso, Cesare},
title = {Interface Quality Patterns: Communicating and Improving the Quality of Microservices APIs},
year = {2018},
isbn = {9781450363877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282308.3282319},
doi = {10.1145/3282308.3282319},
abstract = {The design and evolution of Application Programming Interfaces (APIs) in microservices architectures is challenging. General design issues in integration and programming have been covered in great detail in many pattern languages since the beginnings of the patterns movement, and service-oriented infrastructure design patterns have also been published in the last decade. However, the interface representations (i.e., the content of message payloads) have received less attention. We presented five structural representation patterns in our previous work; in this paper we continue our coverage of the API design space and propose five interface quality patterns that deal with the observable aspects of quality-attribute-driven interface design for efficiency, security, and manageability: An API Key allows API providers to identify clients. Providers may offer rich data contracts in their responses, which not all consumers might need. A Wish List allows the client to request only the attributes in a response data set that it is interested in. If a client makes many API calls, the provider can employ a Rate Limit and bill clients according to a specified Rate Plan. A provider has to provide a high-quality service while at the same time having to use its available resources economically. The resulting compromise is expressed in a provider's Service Level Agreement.},
booktitle = {Proceedings of the 23rd European Conference on Pattern Languages of Programs},
articleno = {10},
numpages = {16},
location = {Irsee, Germany},
series = {EuroPLoP '18}
}

@inproceedings{10.1145/3442144.3442146,
author = {Aung, Yan Lin and Tiang, Hui Hui and Wijaya, Herman and Ochoa, Mart\'{\i}n and Zhou, Jianying},
title = {Scalable VPN-forwarded Honeypots: Dataset and Threat Intelligence Insights},
year = {2021},
isbn = {9781450390026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442144.3442146},
doi = {10.1145/3442144.3442146},
abstract = {After distributed denial-of-service attacks by the Mirai malware in 2016, large-scale attacks exploiting IoT devices raise significant security concerns for the stakeholders involved. The efficacy of setting up honeypots to survey the threat landscape and for early detection of threats to IoT devices is evident. However, the availability of dataset collected by these IoT honeypots to advance research on IoT security has been scarce and limited. With this paper, we contribute network traffic dataset collected by a high-interaction IoT honeypots deployed in the wild for 1.5 years during 2017-2018. The honeypots are manifested on 40 public IP addresses in the wild while forwarding the traffic to 11 real IoT devices. Using Zeek tool, the dataset is generated in JSON format from 258,871 PCAP files resulting more than 81.5 million logs. To foster further research, the attacks, exploitation and intrusion attempts present in the dataset as well as threat intelligence insights are provided with an aid of an open-source threat-hunting and security monitoring platform.},
booktitle = {Sixth Annual Industrial Control System Security (ICSS) Workshop},
pages = {21–30},
numpages = {10},
keywords = {High-Interaction IoT Honeypot, Network Traffic Dataset},
location = {Austin, TX, USA},
series = {ICSS 2020}
}

@article{10.1145/3437254,
author = {Spinellis, Diomidis},
title = {Why computing students should contribute to open source software projects},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3437254},
doi = {10.1145/3437254},
abstract = {Acquiring developer-prized practical skills, knowledge, and experiences.},
journal = {Commun. ACM},
month = {jun},
pages = {36–38},
numpages = {3}
}

@article{10.1145/3505245,
author = {Gruetzemacher, Ross and Paradice, David},
title = {Deep Transfer Learning &amp; Beyond: Transformer Language Models in Information Systems Research},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3505245},
doi = {10.1145/3505245},
abstract = {AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving transformer language models (TLMs) offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizing text mining in top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher quality text analytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {204},
numpages = {35},
keywords = {Natural language processing, text mining, artificial intelligence, deep learning, transfer learning, language models}
}

@inproceedings{10.1145/3379597.3387493,
author = {Chatterjee, Preetha and Damevski, Kostadin and Kraft, Nicholas A. and Pollock, Lori},
title = {Software-related Slack Chats with Disentangled Conversations},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387493},
doi = {10.1145/3379597.3387493},
abstract = {More than ever, developers are participating in public chat communities to ask and answer software development questions. With over ten million daily active users, Slack is one of the most popular chat platforms, hosting many active channels focused on software development technologies, e.g., python, react. Prior studies have shown that public Slack chat transcripts contain valuable information, which could provide support for improving automatic software maintenance tools or help researchers understand developer struggles or concerns.In this paper, we present a dataset of software-related Q&amp;A chat conversations, curated for two years from three open Slack communities (python, clojure, elm). Our dataset consists of 38,955 conversations, 437,893 utterances, contributed by 12,171 users. We also share the code for a customized machine-learning based algorithm that automatically extracts (or disentangles) conversations from the downloaded chat transcripts.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {588–592},
numpages = {5},
keywords = {chat disentanglement, online software developer chats},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3442381.3449961,
author = {Ju, Mingxuan and Song, Wei and Sun, Shiyu and Ye, Yanfang and Fan, Yujie and Hou, Shifu and Loparo, Kenneth and Zhao, Liang},
title = {Dr.Emotion: Disentangled Representation Learning for Emotion Analysis on Social Media to Improve Community Resilience in the COVID-19 Era and Beyond},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449961},
doi = {10.1145/3442381.3449961},
abstract = {During the pandemic caused by coronavirus disease (COVID-19), social media has played an important role by enabling people to discuss their experiences and feelings of this global crisis. To help combat the prolonged pandemic that has exposed vulnerabilities impacting community resilience, in this paper, based on our established large-scale COVID-19 related social media data, we propose and develop an integrated framework (named Dr.Emotion) to learn disentangled representations of social media posts (i.e., tweets) for emotion analysis and thus to gain deep insights into public perceptions towards COVID-19. In Dr.Emotion, for given social media posts, we first post-train a transformer-based model to obtain the initial post embeddings. Since users may implicitly express their emotions in social media posts which could be highly entangled with other descriptive information in the post content, to address this challenge for emotion analysis, we propose an adversarial disentangler by integrating emotion-independent (i.e., sentiment-neutral) priors of the posts generated by another post-trained transformer-based model to separate and disentangle the implicitly encoded emotions from the content in latent space for emotion classification at the first attempt. Extensive experimental studies are conducted to fully evaluate Dr.Emotion and promising results demonstrate its performance in emotion analysis by comparison with the state-of-the-art baseline methods. By exploiting our developed Dr.Emotion, we further perform emotion analysis over a large number of social media posts and provide in-depth investigation from both temporal and geographical perspectives, based on which additional work can be conducted to extract and transform the constructive ideas, experiences and support into actionable information to improve community resilience in responses to a variety of crises created by COVID-19 and well beyond.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {518–528},
numpages = {11},
keywords = {COVID-19, Community Resilience., Disentangled Representation Learning, Emotion Analysis, Social Media},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1613/jair.1.11524,
author = {Sridharan, Mohan and Gelfond, Michael and Zhang, Shiqi and Wyatt, Jeremy},
title = {REBA: a refinement-based architecture for knowledge representation and reasoning in robotics},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11524},
doi = {10.1613/jair.1.11524},
abstract = {This article describes REBA, a knowledge representation and reasoning architecture for robots that is based on tightly-coupled transition diagrams of the domain at two different levels of granularity. An action language is extended to support non-boolean fluents and non-deterministic causal laws, and used to describe the domain's transition diagrams, with the fine-resolution transition diagram being defined as a refinement of the coarse-resolution transition diagram. The coarse-resolution system description, and a history that includes prioritized defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this abstract transition. The zoomed fine-resolution system description, and a probabilistic representation of the uncertainty in sensing and actuation, are used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract transition as a sequence of concrete actions. The fine-resolution outcomes of executing these concrete actions are used to infer coarse-resolution outcomes that are added to the coarse-resolution history and used for subsequent coarse-resolution reasoning. The architecture thus combines the complementary strengths of declarative programming and probabilistic graphical models to represent and reason with non-monotonic logic-based and probabilistic descriptions of uncertainty and incomplete domain knowledge. In addition, we describe a general methodology for the design of software components of a robot based on these knowledge representation and reasoning tools, and provide a path for proving the correctness of these components. The architecture is evaluated in simulation and on a mobile robot finding and moving target objects to desired locations in indoor domains, to show that the architecture supports reliable and efficient reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {87–180},
numpages = {94}
}

@proceedings{10.1145/3578503,
title = {WebSci '23: Proceedings of the 15th ACM Web Science Conference 2023},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3587819,
title = {MMSys '23: Proceedings of the 14th ACM Multimedia Systems Conference},
year = {2023},
isbn = {9798400701481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@proceedings{10.1145/3613905,
title = {CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3555183,
author = {Gunawardena, Sanuri Dananja and Devine, Peter and Beaumont, Isabelle and Garden, Lola Piper and Murphy-Hill, Emerson and Blincoe, Kelly},
title = {Destructive Criticism in Software Code Review Impacts Inclusion},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555183},
doi = {10.1145/3555183},
abstract = {The software industry lacks gender diversity. Recent research has suggested that a toxic working culture is to blame. Studies have found that communications in software repositories directed towards women are more negative in general. In this study, we use a destructive criticism lens to examine gender differences in software code review feedback. Software code review is a practice where code is peer reviewed and negative feedback is often delivered. We explore differences in perceptions, frequency, and impact of destructive criticism across genders. We surveyed 93 software practitioners eliciting perceived reactions to hypothetical scenarios (or vignettes) where participants are asked to imagine receiving either constructive or destructive criticism. In addition, the survey collected general opinions on feedback obtained during software code review as well as the frequency that participants give and receive destructive criticism.We found that opinions on destructive criticism vary. Women perceive destructive criticism as less appropriate and are less motivated to continue working with the developer after receiving destructive criticism. Destructive criticism is fairly common with more than half of respondents having received nonspecific negative feedback and nearly a quarter having received inconsiderate negative feedback in the past year. Our results suggest that destructive criticism in code review could be a contributing factor to the lack of gender diversity observed in the software industry.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {292},
numpages = {29},
keywords = {destructive criticism, diversity and inclusion, software code review, software engineering}
}

@article{10.1145/3449249,
author = {Geiger, R. Stuart and Howard, Dorothy and Irani, Lilly},
title = {The Labor of Maintaining and Scaling Free and Open-Source Software Projects},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449249},
doi = {10.1145/3449249},
abstract = {Free and/or open-source software (or F/OSS) projects now play a major and dominant role in society, constituting critical digital infrastructure relied upon by companies, academics, non-profits, activists, and more. As F/OSS has become larger and more established, we investigate the labor of maintaining and sustaining those projects at various scales. We report findings from an interview-based study with contributors and maintainers working in a wide range of F/OSS projects. Maintainers of F/OSS projects do not just maintain software code in a more traditional software engineering understanding of the term: fixing bugs, patching security vulnerabilities, and updating dependencies. F/OSS maintainers also perform complex and often-invisible interpersonal and organizational work to keep their projects operating as active communities of users and contributors. We particularly focus on how this labor of maintaining and sustaining changes as projects and their software grow and scale across many dimensions. In understanding F/OSS to be as much about maintaining a communal project as it is maintaining software code, we discuss broadly applicable considerations for peer production communities and other socio-technical systems more broadly.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {175},
numpages = {28},
keywords = {free software, infrastructure, labor, maintenance, open source}
}

@proceedings{10.1145/3605731,
title = {ICPP Workshops '23: Proceedings of the 52nd International Conference on Parallel Processing Workshops},
year = {2023},
isbn = {9798400708428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/3427228.3427665,
author = {Sankaran, Arnav and Datta, Pubali and Bates, Adam},
title = {Workflow Integration Alleviates Identity and Access Management in Serverless Computing},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427665},
doi = {10.1145/3427228.3427665},
abstract = {As serverless computing continues to revolutionize the design and deployment of web services, it has become an increasingly attractive target to attackers. These adversaries are developing novel tactics for circumventing the ephemeral nature of serverless functions, exploiting container reuse optimizations and achieving lateral movement by “living off the land” provided by legitimate serverless workflows. Unfortunately, the traditional security controls currently offered by cloud providers are inadequate to counter these new threats. In this work, we propose will.iam,1 a workflow-aware access control model and reference monitor that satisfies the functional requirements of the serverless computing paradigm. will.iam encodes the protection state of a serverless application as a permissions graph that describes the permissible transitions of its workflows, associating web requests with a permissions set at the point of ingress according to a graph-based labeling state. By proactively enforcing the permissions requirements of downstream workflow components, will.iam is able to avoid the costs of partially processing unauthorized requests and reduce the attack surface of the application. We implement the will.iam framework in Go and evaluate its performance as compared to recent related work against the well-established Nordstrom “Hello, Retail!” application. We demonstrate that will.iam imposes minimal burden to requests, averaging 0.51% overhead across representative workflows, but dramatically improves performance when handling unauthorized requests (e.g., DDoS attacks) as compared to past solutions. will.iam thus demonstrates an effective and practical alternative for authorization in the serverless paradigm.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {496–509},
numpages = {14},
keywords = {Access Control, Information Flow Control, Serverless Computing},
location = {Austin, USA},
series = {ACSAC '20}
}

@article{10.1145/3357385.3357388,
author = {Spooren, Jan and Preuveneers, Davy and Desmet, Lieven and Janssen, Peter and Joosen, Wouter},
title = {On the use of DGAs in malware: an everlasting competition of detection and evasion},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3357385.3357388},
doi = {10.1145/3357385.3357388},
abstract = {Malware typically makes use of Domain Generation Algorithms (DGAs) as a mechanism to contact their Command and Control server. In recent years, different approaches to automatically detect generated domain names have been proposed, based on machine learning. The first problem that we address is the difficulty to systematically compare these DGA detection algorithms due to the lack of an independent benchmark. The second problem that we investigate is the difficulty for an adversary to circumvent these classifiers when the machine learning models backing these DGA-detectors are known. In this paper we compare two different approaches on the same set of DGAs: classical machine learning using manually engineered features and a 'deep learning' recurrent neural network. We show that the deep learning approach performs consistently better on all of the tested DGAs, with an average classification accuracy of 98.7% versus 93.8% for the manually engineered features. We demonstrate that the deep learning solution yields better results even when only 10,000 malicious samples are available. We also show that one of the dangers of manual feature engineering is that DGAs can adapt their strategy, based on knowledge of the features used to detect them. To demonstrate this, we use the knowledge of the used feature set to design a new DGA which makes the Random Forest classifier powerless with a classification accuracy of 57.3%. The deep learning classifier is also (albeit less) affected, reducing its accuracy to 78.9%.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {aug},
pages = {31–43},
numpages = {13},
keywords = {domain generation algorithms, machine learning, malware detection}
}

@inproceedings{10.1145/3510456.3514141,
author = {Vo, Nhi N. Y. and Vu, Nam H. and Vu, Tu A. and Vu, Quang T. and Mach, Bang D.},
title = {CRS: a hybrid course recommendation system for software engineering education},
year = {2022},
isbn = {9781450392259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510456.3514141},
doi = {10.1145/3510456.3514141},
abstract = {With the increasing numbers of elective courses at universities and the Massive Open Online Courses (MOOCs), Software Engineering (SE) students are facing challenges in selecting their study paths in tech. On the other hand, the skills in SE-related fields have been changing significantly for the past decade, which requires more frequent updates to the curriculum and teaching materials. There is a strong demand for a better course guide and recommendation system to aid higher education in SE to keep up with the industry requirements. In this work, we incorporate data mining techniques, a natural language processing model, and a recommendation system in a web application that helps SE students and university faculty with those challenges. Our proposed hybrid Course Recommendation System (CRS) consists of two web applications (user and admin web apps) to provide multiple features, including user-specific suggestions for university courses, careers, jobs, industry-demanded skills together with online materials to learn those skills, and various analysis dashboards for both SE students and lecturers. We conduct a survey on SE students and faculty members to evaluate the initial impact of our CRS on the end users, which proved the effectiveness of our approach in addressing the mentioned issues. Demo: CRS User Web App and CRS Admin Web App.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {62–68},
numpages = {7},
keywords = {course recommendation system, software engineering education},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEET '22}
}

@inproceedings{10.1145/3460120.3484780,
author = {Roth, Sebastian and Gr\"{o}ber, Lea and Backes, Michael and Krombholz, Katharina and Stock, Ben},
title = {12 Angry Developers - A Qualitative Study on Developers' Struggles with CSP},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484780},
doi = {10.1145/3460120.3484780},
abstract = {The Web has improved our ways of communicating, collaborating, teaching, and entertaining us and our fellow human beings. However, this cornerstone of our modern society is also one of the main targets of attacks, most prominently Cross-Site Scripting (XSS). A correctly crafted Content Security Policy (CSP) is capable of effectively mitigating the effect of those Cross-Site Scripting attacks. However, research has shown that the vast majority of all policies in the wild are trivially bypassable.To uncover the root causes behind the omnipresent misconfiguration of CSP, we conducted a qualitative study involving 12 real-world Web developers. By combining a semi-structured interview, a drawing task, and a programming task, we were able to identify the participant's misconceptions regarding the attacker model covered by CSP as well as roadblocks for secure deployment or strategies used to create a CSP.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3085–3103},
numpages = {19},
keywords = {content security policy, roadblocks, usable security, web security},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3622748.3622751,
author = {Melo de Ara\'{u}jo, Marcos Henrique and Costa, Catarina and Font\~{a}o, Awdren},
title = {Analysis of the Technical Debt of Software Projects Based on Merge Code Comments},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622751},
doi = {10.1145/3622748.3622751},
abstract = {Developers use code comments for various reasons, such as explaining the produced code, documenting specifications, communicating with other developers, and highlighting future tasks. Software projects with minimal documentation often have a significant number of comments. In this regard, code comment analysis techniques can be used as tools to examine more complex aspects of software projects, such as technical debt generated by merge conflicts. Technical debt resulting from the resolution of merge conflicts occurs when the resulting code contains comments indicating tasks to be performed in the future. No studies directly linking merge conflicts and technical debt were found in the literature. The objective of this work is to identify and analyze code comments generated during the resolution of merge conflicts from this perspective. This process can lead to improvements in software quality and assist in managing technical debt. To achieve this, an exploratory analysis was conducted on 100 software projects, specifically focusing on task annotations originating from the merge conflict resolution. The results revealed that 60.61% of the analyzed projects have at least one code comment indicating the creation or maintenance of technical debt.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {21–30},
numpages = {10},
keywords = {Merge conflict, code comments, technical debt},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@proceedings{10.1145/3650105,
title = {FORGE '24: Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {FORGE aims to bring researchers, practitioners, and educators from the AI and Software Engineering community to solve the new challenges we meet in the era of foundation models.},
location = {Lisbon, Portugal}
}

@article{10.1145/3479497,
author = {Ferreira, Isabella and Cheng, Jinghui and Adams, Bram},
title = {The "Shut the f**k up" Phenomenon: Characterizing Incivility in Open Source Code Review Discussions},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479497},
doi = {10.1145/3479497},
abstract = {Code review is an important quality assurance activity for software development. Code review discussions among developers and maintainers can be heated and sometimes involve personal attacks and unnecessary disrespectful comments, demonstrating, therefore, incivility. Although incivility in public discussions has received increasing attention from researchers in different domains, the knowledge about the characteristics, causes, and consequences of uncivil communication is still very limited in the context of software development, and more specifically, code review. To address this gap in the literature, we leverage the mature social construct of incivility as a lens to understand confrontational conflicts in open source code review discussions. For that, we conducted a qualitative analysis on 1,545 emails from the Linux Kernel Mailing List (LKML) that were associated with rejected changes. We found that more than half (66.66%) of the non-technical emails included uncivil features. Particularly, frustration, name calling, and impatience are the most frequent features in uncivil emails. We also found that there are civil alternatives to address arguments, while uncivil comments can potentially be made by any people when discussing any topic. Finally, we identified various causes and consequences of such uncivil communication. Our work serves as the first study about the phenomenon of in(civility) in open source software development, paving the road for a new field of research about collaboration and communication in the context of software engineering activities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {353},
numpages = {35},
keywords = {civility, code review, communication, incivility, online communities, open source}
}

@article{10.1145/3628162,
author = {Shoufan, Abdulhadi},
title = {Can Students without Prior Knowledge Use ChatGPT to Answer Test Questions? An Empirical Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
url = {https://doi.org/10.1145/3628162},
doi = {10.1145/3628162},
abstract = {With the immense interest in ChatGPT worldwide, education has seen a mix of both excitement and skepticism. To properly evaluate its impact on education, it is crucial to understand how far it can help students without prior knowledge answer assessment questions. This study aims to address this question as well as the impact of the question type. We conducted multiple experiments with computer engineering students (experiment group: n=41 to 56), who were asked to use ChatGPT to answer previous test questions before learning about the related topics. Their scores were then compared with the scores of previous-term students who answered the same questions in a quiz or exam setting (control group: n=24 to 61). The results showed a wide range of effect sizes, from -2.55 to 1.23, depending on the question type and content. The experiment group performed best answering code analysis and conceptual questions but struggled with code completion and questions that involved images. However, the performance in code generation tasks was inconsistent. Overall, the ChatGPT group’s answers lagged slightly behind the control group’s answers with an effect size of -0.16. We conclude that ChatGPT, at least in the field of this study, is not yet ready to rely on by students who do not have sufficient background to evaluate generated answers. We suggest that educators try using ChatGPT and educate students on effective questioning techniques and how to assess the generated responses. This study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.},
journal = {ACM Trans. Comput. Educ.},
month = {dec},
articleno = {45},
numpages = {29},
keywords = {ChatGPT, large language models}
}

@inproceedings{10.1145/3397481.3450644,
author = {Chromik, Michael and Eiband, Malin and Buchner, Felicitas and Kr\"{u}ger, Adrian and Butz, Andreas},
title = {I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450644},
doi = {10.1145/3397481.3450644},
abstract = {Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework. We observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding decreases when it is examined.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {307–317},
numpages = {11},
keywords = {Shapley explanation, cognitive bias, explainable AI, understanding},
location = {College Station, TX, USA},
series = {IUI '21}
}

@inproceedings{10.1109/ICSE.2019.00088,
author = {Ramsauer, Ralf and Lohmann, Daniel and Mauerer, Wolfgang},
title = {The list is the process: reliable pre-integration tracking of commits on mailing lists},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00088},
doi = {10.1109/ICSE.2019.00088},
abstract = {A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history.We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth.Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {807–818},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@proceedings{10.1145/3658271,
title = {SBSI '24: Proceedings of the 20th Brazilian Symposium on Information Systems},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Juiz de Fora, Brazil}
}

@inproceedings{10.1145/3524842.3527932,
author = {AlOmar, Eman Abdullah and Chouchen, Moataz and Mkaouer, Mohamed Wiem and Ouni, Ali},
title = {Code review practices for refactoring changes: an empirical study on OpenStack},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527932},
doi = {10.1145/3524842.3527932},
abstract = {Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the code base. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {689–701},
numpages = {13},
keywords = {code review, developer perception, refactoring, software quality},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1109/ASE51524.2021.9678525,
author = {Bhagwan, Ranjita and Mehta, Sonu and Radhakrishna, Arjun and Garg, Sahil},
title = {Learning patterns in configuration},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678525},
doi = {10.1109/ASE51524.2021.9678525},
abstract = {Large services depend on correct configuration to run efficiently and seamlessly. Checking such configuration for correctness is important because services use a large and continuously increasing number of configuration files and parameters. Yet, very few such tools exist because the permissible values for a configuration parameter are seldom specified or documented, existing at best as tribal knowledge among a few domain experts.In this paper, we address the problem of configuration pattern mining: learning configuration rules from examples. Using program synthesis and a novel string profiling algorithm, we show that we can use file contents and histories of commits to learn patterns in configuration. We have built a tool called ConfMiner that implements configuration pattern mining and have evaluated it on four large repositories containing configuration for a large-scale enterprise service. Our evaluation shows that ConfMiner learns a large variety of configuration rules with high precision and is very useful in flagging anomalous configuration.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {817–828},
numpages = {12},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3611643.3613897,
author = {Lu, Chengjie and Xu, Qinghua and Yue, Tao and Ali, Shaukat and Schwitalla, Thomas and Nyg\r{a}rd, Jan F.},
title = {EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613897},
doi = {10.1145/3611643.3613897},
abstract = {The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1973–1984},
numpages = {12},
keywords = {active learning, cyber-cyber digital twin, digital twin, neural network, transfer learning, validation system},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/2786805.2786866,
author = {Zheng, Qimu and Mockus, Audris and Zhou, Minghui},
title = {A method to identify and correct problematic software activity data: exploiting capacity constraints and data redundancies},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786866},
doi = {10.1145/2786805.2786866},
abstract = {Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10% and rapidly rising. We found the corrected values to be 50% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {637–648},
numpages = {12},
keywords = {capacity constraint, data quality, data redundancy, mining software repositories},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@proceedings{10.1145/3585059,
title = {SIGITE '23: Proceedings of the 24th Annual Conference on Information Technology Education},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Marietta, GA, USA}
}

@proceedings{10.1145/2998392,
title = {SCALA 2016: Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
year = {2016},
isbn = {9781450346481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3461778.3462016,
author = {Buford, Mikayla and Nattar Ranganathan, Vaishnavi and Roseway, Asta and Seyed, Teddy},
title = {Crisis Couture: A Study on Motivations and Practices of Mask Makers During A Crisis},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462016},
doi = {10.1145/3461778.3462016},
abstract = {The landscape of everyday fashion has been transformed due to the COVID-19 pandemic. Part of this can be attributed to different types of communities (e.g. fashion, makers, sewing), who have designed and fabricated masks to counter global shortages and negative culture backlashes. In this paper, we present a mix-methods study of individuals and groups within these communities on their motivations and practices in designing and creating face masks during a global crisis. We conducted a survey with 66 mask makers in the Summer of 2020, and we interviewed 23 of them about their attitudes and reflections on their mask making processes, their unique innovations, and the meaning of contributing in an impactful manner in local and global communities. We unpack themes around technology, self-expression and statement making, making and remixing, sustainable practices, as well as the role of design inspirations on methods and practices for mask makers during a crisis.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {31–47},
numpages = {17},
keywords = {couture, covid, covid-19, design, fashion, makers, making, masks, wearables},
location = {Virtual Event, USA},
series = {DIS '21}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@inproceedings{10.1109/ICSE48619.2023.00182,
author = {Ciniselli, Matteo and Pascarella, Luca and Aghajani, Emad and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
title = {Source Code Recommender Systems: The Practitioners' Perspective},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00182},
doi = {10.1109/ICSE48619.2023.00182},
abstract = {The automatic generation of source code is one of the long-lasting dreams in software engineering research. Several techniques have been proposed to speed up the writing of new code. For example, code completion techniques can recommend to developers the next few tokens they are likely to type, while retrieval-based approaches can suggest code snippets relevant for the task at hand. Also, deep learning has been used to automatically generate code statements starting from a natural language description. While research in this field is very active, there is no study investigating what the users of code recommender systems (i.e., software practitioners) actually need from these tools. We present a study involving 80 software developers to investigate the characteristics of code recommender systems they consider important. The output of our study is a taxonomy of 70 "requirements" that should be considered when designing code recommender systems. For example, developers would like the recommended code to use the same coding style of the code under development. Also, code recommenders being "aware" of the developers' knowledge (e.g., what are the framework/libraries they already used in the past) and able to customize the recommendations based on this knowledge would be appreciated by practitioners. The taxonomy output of our study points to a wide set of future research directions for code recommenders.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2161–2172},
numpages = {12},
keywords = {code recommender systems, empirical study, practitioners' survey},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3613372.3613389,
author = {Braga, Carlos and Santos Jr, Paulo and Barcellos, Monalessa},
title = {Help! I need somebody. A Mapping Study about Expert Identification in Software Development},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613389},
doi = {10.1145/3613372.3613389},
abstract = {Context: Software development is a knowledge-intensive activity, and its success in an organization relies deeply on knowledge sharing. Knowledge management challenges are often increased in agile environments, which involve a lot of tacit knowledge, commonly acquired through experiences and hard to be made explicit. Therefore, knowledge sharing among practitioners is crucial. However, identifying suitable experts to share specific knowledge is not trivial. It involves not only discovering the individuals with the desired knowledge but also considering other factors that may improve the expert responsiveness, such as social connections and availability. Objective: Considering the important role experts play in knowledge sharing, we decided to investigate approaches that help identify experts that can share knowledge in software development. Our goal is to provide a panorama of the existing approaches and shine a light on research opportunities. Method: We carried out a systematic literature mapping and analyzed 17 publications. Results: The results show that most approaches have relied on code repositories as a source of evidence for identifying experts and, consequently, focus on supporting developers and aiding in the codification activity. Additionally, expert identification has been mostly automated, and factors beyond possessing the desired knowledge have often been disregarded. Conclusion: Although there are several expert identification approaches, there has been a lack of concern with factors that influence reaching the most suitable expert for a specific situation (e.g., considering the characteristics of the person seeking knowledge). Moreover, there is a need for deeper reflection on how to better explore different artifacts as sources of expert evidence and how to combine them to improve expert identification.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {154–163},
numpages = {10},
keywords = {Expert Identification, Knowledge Sharing, Mapping Study},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@proceedings{10.1145/3524846,
title = {MET '22: Proceedings of the 7th International Workshop on Metamorphic Testing},
year = {2022},
isbn = {9781450393072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MET series has provided a good forum for discussing novel ideas, new perspectives, new applications, and the state of research, related to or inspired by Metamorphic Testing. This workshop aims to bring together researchers and practitioners in both academia and industry to discuss their research results, experiences and insights into Metamorphic Testing. In this year, we are facing the similar situation of the pandemic as the previous MET in 2020 and 2021, and hence we will run this workshop virtually again.},
location = {Pittsburgh, Pennsylvania}
}

@article{10.1145/3563304,
author = {Parreaux, Lionel and Chau, Chun Yin},
title = {MLstruct: principal type inference in a Boolean algebra of structural types},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563304},
doi = {10.1145/3563304},
abstract = {Intersection and union types are becoming more popular by the day, entering the mainstream in programming languages like TypeScript and Scala 3. Yet, no language so far has managed to combine these powerful types with principal polymorphic type inference. We present a solution to this problem in MLstruct, a language with subtyped records, equirecursive types, first-class unions and intersections, class-based instance matching, and ML-style principal type inference. While MLstruct is mostly structurally typed, it contains a healthy sprinkle of nominality for classes, which gives it desirable semantics, enabling the expression of a powerful form of extensible variants that does not need row variables. Technically, we define the constructs of our language using conjunction, disjunction, and negation connectives, making sure they form a Boolean algebra, and we show that the addition of a few nonstandard but sound subtyping rules gives us enough structure to derive a sound and complete type inference algorithm. With this work, we hope to foster the development of better type inference for present and future programming languages with expressive subtyping systems.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {141},
numpages = {30},
keywords = {principal type inference, structural typing, union and intersection types}
}

@proceedings{10.1145/3600160,
title = {ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Benevento, Italy}
}

@proceedings{10.1145/3627217,
title = {COMPUTE '23: Proceedings of the 16th Annual ACM India Compute Conference},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hyderabad, India}
}

@proceedings{10.1145/3571560,
title = {ICAAI '22: Proceedings of the 6th International Conference on Advances in Artificial Intelligence},
year = {2022},
isbn = {9781450396943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Birmingham, United Kingdom}
}

@inproceedings{10.1145/3428658.3430965,
author = {da Silva, Fl\'{a}vio Roberto Matias and Freire, Paulo M\'{a}rcio Souza and de Souza, Marcelo Pereira and de A. B. Plenamente, Gustavo and Goldschmidt, Ronaldo Ribeiro},
title = {FakeNewsSetGen: a Process to Build Datasets that Support Comparison Among Fake News Detection Methods},
year = {2020},
isbn = {9781450381963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428658.3430965},
doi = {10.1145/3428658.3430965},
abstract = {Due to easy access and low cost, social media online news consumption has increased significantly for the last decade. Despite their benefits, some social media allow anyone to post news with intense spreading power, which amplifies an old problem: the dissemination of Fake News. In the face of this scenario, several machine learning-based methods to automatically detect Fake News (MLFN) have been proposed. All of them require datasets to train and evaluate their detection models. Although recent MLFN were designed to consider data regarding the news propagation on social media, most of the few available datasets do not contain this kind of data. Hence, comparing the performances amid those recent MLFN and the others is restricted to a very limited number of datasets. Moreover, all existing datasets with propagation data do not contain news in Portuguese, which impairs the evaluation of the MLFN in this language. Thus, this work proposes FakeNewsSetGen, a process that builds Fake News datasets that contain news propagation data and support comparison amid the state-of-the-art MLFN. FakeNewsSetGen's software engineering process was guided to include all kind of data required by the existing MLFN. In order to illustrate FakeNewsSetGen's viability and adequacy, a case study was carried out. It encompassed the implementation of a FakeNewsSetGen prototype and the application of this prototype to create a dataset called FakeNewsSet, with news in Portuguese. Five MLFN with different kind of data requirements (two of them demanding news propagation data) were applied to FakeNewsSet and compared, demonstrating the potential use of both the proposed process and the created dataset.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {241–248},
numpages = {8},
keywords = {Dataset building process, Fake News detection, social media},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {WebMedia '20}
}

@inproceedings{10.1145/3368089.3409753,
author = {Zhang, Changjian and Garlan, David and Kang, Eunsuk},
title = {A behavioral notion of robustness for software systems},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409753},
doi = {10.1145/3368089.3409753},
abstract = {Software systems are designed and implemented with assumptions about the environment. However, once the system is deployed, the actual environment may deviate from its expected behavior, possibly undermining desired properties of the system. To enable systematic design of systems that are robust against potential environmental deviations, we propose a rigorous notion of robustness for software systems. In particular, the robustness of a system is defined as the largest set of deviating environmental behaviors under which the system is capable of guaranteeing a desired property. We describe a new set of design analysis problems based on our notion of robustness, and a technique for automatically computing robustness of a system given its behavior description. We demonstrate potential applications of our robustness notion on two case studies involving network protocols and safety-critical interfaces.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {compositional reasoning, formal methods, labelled transition systems, software robustness},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3274192.3274217,
author = {de Almeida Melo, Leandro and da Silva Junior, F\'{a}bio Freire and Leite, Tiago Henrique da S. and Filho, Fernando Figueira and de Souza, Cleidson R. B.},
title = {Going Beyond The Challenge! Investigating The Aspects That Attract People to Participate in Hackathons},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274217},
doi = {10.1145/3274192.3274217},
abstract = {Hackathons are events where their participants face the challenge of working intensively and collaboratively with other people. In these events, the participants have the opportunity to develop functional prototypes and solutions to real problems in a short time period (one to three days). Those kind of events have become increasingly popular and spread to several fields of knowledge. Also, they have been adopted by many organizations over the world. However, there is little scientific data that explains why people have participated in hackathons. In this paper, we report findings from an exploratory study in two hackathons organized by a big IT company. We aim to understand why their participants have voluntarily participated in such events. We identified four sets of motivational factors: technical, social, individual and business motivations. Technical motivations are associated with skill acquisition, while social motivations are related to the interaction between people during the event. Also, individual motivations are associated with a sense of autonomy and enjoyment of work. Finally, business motivations include the opportunity of publicizing work and the possibility of establishing partnerships. Our findings contributes to a broader understanding of the motivations for participating in hackathons, as well as providing recommendations for the organization of such events.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {25},
numpages = {10},
keywords = {Collocation, Corporate Hackathons, Engagement, Motivation},
location = {Bel\'{e}m, Brazil},
series = {IHC '18}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00064,
author = {Melo, Glaucia},
title = {Designing Adaptive Developer-Chatbot Interactions: Context Integration, Experimental Studies, and Levels of Automation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00064},
doi = {10.1109/ICSE-Companion58688.2023.00064},
abstract = {The growing demand for software developers and the increasing development complexity have emphasized the need for support in software engineering projects. This is especially relevant in light of advancements in artificial intelligence, such as conversational systems. A significant contributor to the complexity of software development is the multitude of tools and methods used, creating various contexts in which software developers must operate. Moreover, there has been limited investigation into the interaction between context-based chatbots and software developers through experimental user studies. Assisting software developers in their work becomes essential. In particular, understanding the context surrounding software development and integrating this context into chatbots can lead to novel insight into what software developers expect concerning these human-chatbot interactions and their levels of automation. In my research, I study the design of context-based adaptive interactions between software developers and chatbots to foster solutions and knowledge to support software developers at work.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {235–239},
numpages = {5},
keywords = {software engineering, context, chatbot, levels of automation, autonomous systems, interactions},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3510003.3510168,
author = {Wyss, Elizabeth and De Carli, Lorenzo and Davidson, Drew},
title = {What the fork? finding hidden code clones in npm},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510168},
doi = {10.1145/3510003.3510168},
abstract = {This work presents findings and mitigations on an understudied issue, which we term shrinkwrapped clones, that is endemic to the npm software package ecosystem. A shrink-wrapped clone is a package which duplicates, or near-duplicates, the code of another package without any indication or reference to the original package. This phenomenon represents a challenge to the hygiene of package ecosystems, as a clone package may siphon interest from the package being cloned, or create hidden duplicates of vulnerable, insecure code which can fly under the radar of audit processes.Motivated by these considerations, we propose unwrapper, a mechanism to programmatically detect shrinkwrapped clones and match them to their source package. unwrapper uses a package difference metric based on directory tree similarity, augmented with a prefilter which quickly weeds out packages unlikely to be clones of a target. Overall, our prototype can compare a given package within the entire npm ecosystem (1,716,061 packages with 20,190,452 different versions) in 72.85 seconds, and it is thus practical for live deployment. Using our tool, we performed an analysis of a subset of npm packages, which resulted in finding up to 6,292 previously unknown shrinkwrapped clones, of which up to 207 carried vulnerabilities from the original package that had already been fixed in the original package. None of such vulnerabilities were discoverable via the standard npm audit process.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2415–2426},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@proceedings{10.1145/3658852,
title = {MOCO '24: Proceedings of the 9th International Conference on Movement and Computing},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Utrecht, Netherlands}
}

@inproceedings{10.1145/3350768.3350786,
author = {Souza, Renata and Rocha, Larissa and Silva, Franklin and Machado, Ivan},
title = {Investigating Agile Practices in Software Startups},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350786},
doi = {10.1145/3350768.3350786},
abstract = {Software development practices have smoothly shifted from traditional software development to new approaches that fit better to the real and unpredictable world. Agile practices might help practitioners respond quickly to customer change requests and deliver a working software on-schedule. Software startups are companies that develop innovative and software-intensive products and services in a dynamic and fast-growing market. This study aims to investigate the use of agile practices in software startups. We conducted 14 in-depth semi-structured interviews with the CEO and CTO from early-stage software startups. The results indicate that DevOps, Fundamentals, Design and Extreme Programming are the most used agile practice areas. Our results open up an opportunity to improve software engineering practices in early-stage software startups.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {317–321},
numpages = {5},
keywords = {Agile practices, Interview, Software engineering, Software startups},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3180155.3180217,
author = {German, Daniel M. and Robles, Gregorio and Poo-Caama\~{n}o, Germ\'{a}n and Yang, Xin and Iida, Hajimu and Inoue, Katsuro},
title = {"Was my contribution fairly reviewed?": a framework to study the perception of fairness in modern code reviews},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180217},
doi = {10.1145/3180155.3180217},
abstract = {Modern code reviews improve the quality of software products. Although modern code reviews rely heavily on human interactions, little is known regarding whether they are performed fairly. Fairness plays a role in any process where decisions that affect others are made. When a system is perceived to be unfair, it affects negatively the productivity and motivation of its participants. In this paper, using fairness theory we create a framework that describes how fairness affects modern code reviews. To demonstrate its applicability, and the importance of fairness in code reviews, we conducted an empirical study that asked developers of a large industrial open source ecosystem (OpenStack) what their perceptions are regarding fairness in their code reviewing process. Our study shows that, in general, the code review process in OpenStack is perceived as fair; however, a significant portion of respondents perceive it as unfair. We also show that the variability in the way they prioritize code reviews signals a lack of consistency and the existence of bias (potentially increasing the perception of unfairness). The contributions of this paper are: (1) we propose a framework---based on fairness theory---for studying and managing social behaviour in modern code reviews, (2) we provide support for the framework through the results of a case study on a large industrial-backed open source project, (3) we present evidence that fairness is an issue in the code review process of a large open source ecosystem, and, (4) we present a set of guidelines for practitioners to address unfairness in modern code reviews.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {523–534},
numpages = {12},
keywords = {fairness, human and social aspects, modern code review, open source software, software development, transparency},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@proceedings{10.1145/3603555,
title = {MuC '23: Proceedings of Mensch und Computer 2023},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rapperswil, Switzerland}
}

@proceedings{10.1145/3600211,
title = {AIES '23: Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1145/3652153,
author = {Guo, Yimeng and Chen, Zhifei and Chen, Lin and Xu, Wenjie and Li, Yanhui and Zhou, Yuming and Xu, Baowen},
title = {Generating Python Type Annotations from Type Inference: How Far Are We?},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3652153},
doi = {10.1145/3652153},
abstract = {In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go.In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {123},
numpages = {38},
keywords = {Type annotations, type inference, Python, empirical study}
}

@article{10.1145/3458919,
author = {Jacucci, Giulio and Daee, Pedram and Vuong, Tung and Andolina, Salvatore and Klouche, Khalil and Sj\"{o}berg, Mats and Ruotsalo, Tuukka and Kaski, Samuel},
title = {Entity Recommendation for Everyday Digital Tasks},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3458919},
doi = {10.1145/3458919},
abstract = {Recommender systems can support everyday digital tasks by retrieving and recommending useful information contextually. This is becoming increasingly relevant in services and operating systems. Previous research often focuses on specific recommendation tasks with data captured from interactions with an individual application. The quality of recommendations is also often evaluated addressing only computational measures of accuracy, without investigating the usefulness of recommendations in realistic tasks. The aim of this work is to synthesize the research in this area through a novel approach by (1) demonstrating comprehensive digital activity monitoring, (2) introducing entity-based computing and interaction, and (3) investigating the previously overlooked usefulness of entity recommendations and their actual impact on user behavior in real tasks. The methodology exploits context from screen frames recorded every 2 seconds to recommend information entities related to the current task. We embodied this methodology in an interactive system and investigated the relevance and influence of the recommended entities in a study with participants resuming their real-world tasks after a 14-day monitoring phase. Results show that the recommendations allowed participants to find more relevant entities than in a control without the system. In addition, the recommended entities were also used in the actual tasks. In the discussion, we reflect on a research agenda for entity recommendation in context, revisiting comprehensive monitoring to include the physical world, considering entities as actionable recommendations, capturing drifting intent and routines, and considering explainability and transparency of recommendations, ethics, and ownership of data.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {aug},
articleno = {29},
numpages = {41},
keywords = {Proactive search, user intent modeling}
}

@article{10.1145/3615666,
author = {Connolly, Kate and Klempay, Anna and McCann, Mary and Brenner, Paul},
title = {Dark Web Marketplaces: Data for Collaborative Threat Intelligence},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3615666},
doi = {10.1145/3615666},
abstract = {The dark web has become an increasingly important landscape for the sale of illicit cyber goods. Given the prevalence of malware and tools that are used to steal data from individuals on these markets, it is crucial that every company, governing body, and cyber professional be aware of what information is sold on these marketplaces. Knowing this information will allow these entities to protect themselves against cyber attacks and from information breaches. In this article, we announce the public release of a data set on dark web marketplaces’ cybersecurity-related listings. We spent multiple years seeking out websites that sold illicit digital goods and collected data on the available products. Due to the marketplaces’ varied and complex layers of security, we leveraged the flexible Selenium WebDriver with Python to navigate the web pages and collect data. We present analysis of categories of malicious cyber goods sold on marketplaces, prices, persistent vendors, ratings, and other basic information on marketplace storefronts. Additionally, we share the tools and techniques we’ve compiled, enabling others to scrape dark web marketplaces at a significantly lower risk. We invite professionals who opt to gather data from the dark web to contribute to the publicly shared threat intelligence resource.},
journal = {Digital Threats},
month = {oct},
articleno = {49},
numpages = {12},
keywords = {Dark web, markets, threat intelligence, malware}
}

@inproceedings{10.1145/3538969.3538973,
author = {Schmidt, Holger and van Aerssen, Max and Leich, Christian and Benni, Abdulkader and Al Ali, Salar and Tanz, Jakob},
title = {CopypastaVulGuard – A browser extension to prevent copy and paste spreading of vulnerable source code in forum posts},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3538973},
doi = {10.1145/3538969.3538973},
abstract = {Forums such as Stack Overflow are used by many software developers to find a solution for a given coding problem. Found solutions, i.e. forum posts containing relevant source code, are utilized in a copy and paste manner. This behavior carries the risk that vulnerabilities contained in the source code of the forum posts are spread. Software developers should be able to identify vulnerable source code at an early stage, thereby preventing copying the corresponding source code. In this paper, we introduce the tool CopypastaVulGuard that identifies vulnerable source code in forum posts and allows software developers to omit the source code by marking the forum posts as dangerous. Our tool consists of a browser extension and a management application capable to address as examples SQL injections, remote code executions and deprecated functions based on a dump of the archive.org Stack Overflow data set. We present an evaluation of our tool’s possible impact and relevance considering pros/cons and selected research questions.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {148},
numpages = {8},
keywords = {Browser-Plugin, Copy &amp; Paste, Detection, Stack Overflow, Vulnerabilities},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3510003.3510233,
author = {Eiers, William and Sankaran, Ganesh and Li, Albert and O'Mahony, Emily and Prince, Benjamin and Bultan, Tevfik},
title = {Quantifying permissiveness of access control policies},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510233},
doi = {10.1145/3510003.3510233},
abstract = {Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information, without verification and validation techniques that can assist developers in writing policies, complex policy specifications are likely to have errors that can lead to unintended and unauthorized access to data, possibly with disastrous consequences. In this paper, we present a quantitative and differential policy analysis framework that not only identifies if one policy is more permissive than another policy, but also quantifies the relative permissiveness of access control policies. We quantify permissiveness of policies using a model counting constraint solver. We present a heuristic that transforms constraints extracted from access control policies and significantly improves the model counting performance. We demonstrate the effectiveness of our approach by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language and Microsoft's Azure policy language.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1805–1817},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@proceedings{10.1145/3598469,
title = {dg.o '23: Proceedings of the 24th Annual International Conference on Digital Government Research},
year = {2023},
isbn = {9798400708374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Gda?sk, Poland}
}

@proceedings{10.1145/3630106,
title = {FAccT '24: Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rio de Janeiro, Brazil}
}

@proceedings{10.1145/3605098,
title = {SAC '24: Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Organizing Committee, I extend a warm welcome to you at the 39th Annual ACM Symposium on Applied Computing (SAC 2024), taking place in \'{A}vila, Spain, and hosted by the University of Salamanca. For more than three decades, this international forum has been dedicated to computer scientists, engineers, and practitioners, providing a platform for presenting their research findings and results in various areas of applied computing. The organizing committee sincerely appreciates your participation in this exciting international event, and we hope that the conference proves interesting and beneficial for all attendees.},
location = {Avila, Spain}
}

@article{10.1145/3576038,
author = {Jin, Xianhao and Servant, Francisco},
title = {HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3576038},
doi = {10.1145/3576038},
abstract = {Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice—Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {93},
numpages = {39},
keywords = {Software maintenance, Continuous Integration, build selection, test selection}
}

@inproceedings{10.1145/2025113.2025139,
author = {Brun, Yuriy and Holmes, Reid and Ernst, Michael D. and Notkin, David},
title = {Proactive detection of collaboration conflicts},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025139},
doi = {10.1145/2025113.2025139},
abstract = {Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results.First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems.Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations.Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {168–178},
numpages = {11},
keywords = {collaboration conflicts, collaborative development, crystal, developer awareness, speculative analysis, version control},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.5555/3291656.3291678,
author = {Boushehrinejadmoradi, Nader and Yoga, Adarsh and Nagarakatte, Santosh},
title = {A parallelism profiler with what-if analyses for OpenMP programs},
year = {2018},
publisher = {IEEE Press},
abstract = {This paper proposes OMP-WhIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series-parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WhIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are addressed. We have used OMP-WhIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {16},
numpages = {14},
keywords = {parallel programming, performance analysis},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/3350768.3352456,
author = {Paschoal, Leo Natan and Turci, Lucas Fernandes and Conte, Tayana Uch\^{o}a and Souza, Simone R. S.},
title = {Towards a Conversational Agent to Support the Software Testing Education},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3352456},
doi = {10.1145/3350768.3352456},
abstract = {The training of professionals in the field of software testing is increasing its relevance in the past few years and, therefore, efforts in appropriate methodologies for the learning-teaching process in this context have been proposed and appreciated. The emergence of pedagogical models, such as flipped classroom and team-based learning, which demand from the students a previous study of the theory before the lecture, creates a concern: how to support the before class learning? Because of the hybrid nature of these pedagogical models, which means they mix elements from traditional and distance education, it is possible that the support mechanisms used in distance learning platforms, such as conversational agents, can be applied for this matter. At the same time in which the academic work tries carefully to provide a proper software testing formation, there are also many contributions being established regarding the training and non-formal learning. Improvement and personal training courses about criteria, tools, and software testing good practices are being created by teaching institutes and offered in Massive Open Online Courses platforms (MOOCs). However, in this type of course, in the absence of a teacher, the student might be in a situation where there is nobody available to answer their questions about the topic. In this paper, we propose the use of conversational agents in solving the problems and challenges which encompass the learning through MOOCs and hybrid models. A conversational agent, called TOB-SST is proposed to support software testing education. A viability study was conducted to understand the quality of the given answers by TOB-SST and the possibility of it serving as a learning support tool. The results indicate that it is promising to employ a conversational agent to guide student study.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {57–66},
numpages = {10},
keywords = {Chatbot, Computer Science Education, Software Testing},
location = {Salvador, Brazil},
series = {SBES '19}
}

@proceedings{10.1145/3631204,
title = {CSCS '23: Proceedings of the 7th ACM Computer Science in Cars Symposium},
year = {2023},
isbn = {9798400704543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@proceedings{10.1145/3617650,
title = {CompEd 2023: Proceedings of the ACM Conference on Global Computing Education Vol 2},
year = {2023},
isbn = {9798400703744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome participants to the 2nd ACM Global Conference on Computing Education (ACM CompEd 2023) being held in Hyderabad, India, 7th-9th December, 2023 with the Working Groups meetings being held on 5th and 6th December 2023.ACM CompEd is a recent addition to the list of ACM sponsored conferences devoted to research in all aspects of computing education, including education at the school and college levels. The Hyderabad edition is only the second in this promising series. The long hiatus due to Covid-19 pushed this conference by two years, but we are glad that it is finally here!This edition of ACM CompEd partly overlaps with COMPUTE 2023, ACM India's flagship conference on Computing Education. Having the two conferences adjacent to each other is a great way to build synergy between the Indian computing education community and the global community of computing education researchers.},
location = {Hyderabad, India}
}

@inproceedings{10.1109/SC.2018.00019,
author = {Boushehrinejadmoradi, Nader and Yoga, Adarsh and Nagarakatte, Santosh},
title = {A parallelism profiler with what-if analyses for OpenMP programs},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00019},
doi = {10.1109/SC.2018.00019},
abstract = {This paper proposes OMP-WhIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series-parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WhIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are addressed. We have used OMP-WhIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {16},
numpages = {14},
keywords = {parallel programming, performance analysis},
location = {Dallas, Texas},
series = {SC '18}
}

@proceedings{10.1145/3567445,
title = {IoT '22: Proceedings of the 12th International Conference on the Internet of Things},
year = {2022},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@proceedings{10.1145/3628034,
title = {EuroPLoP '23: Proceedings of the 28th European Conference on Pattern Languages of Programs},
year = {2023},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@article{10.1145/3441140,
author = {Zoppi, Tommaso and Ceccarelli, Andrea and Capecchi, Tommaso and Bondavalli, Andrea},
title = {Unsupervised Anomaly Detectors to Detect Intrusions in the Current Threat Landscape},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1922},
url = {https://doi.org/10.1145/3441140},
doi = {10.1145/3441140},
abstract = {Anomaly detection aims at identifying unexpected fluctuations in the expected behavior of a given system. It is acknowledged as a reliable answer to the identification of zero-day attacks to such extent, several ML algorithms that suit for binary classification have been proposed throughout years. However, the experimental comparison of a wide pool of unsupervised algorithms for anomaly-based intrusion detection against a comprehensive set of attacks datasets was not investigated yet. To fill such gap, we exercise 17 unsupervised anomaly detection algorithms on 11 attack datasets. Results allow elaborating on a wide range of arguments, from the behavior of the individual algorithm to the suitability of the datasets to anomaly detection. We conclude that algorithms as Isolation Forests, One-Class Support Vector Machines, and Self-Organizing Maps are more effective than their counterparts for intrusion detection, while clustering algorithms represent a good alternative due to their low computational complexity. Further, we detail how attacks with unstable, distributed, or non-repeatable behavior such as Fuzzing, Worms, and Botnets are more difficult to detect. Ultimately, we digress on capabilities of algorithms in detecting anomalies generated by a wide pool of unknown attacks, showing that achieved metric scores do not vary with respect to identifying single attacks.},
journal = {ACM/IMS Trans. Data Sci.},
month = {apr},
articleno = {7},
numpages = {26},
keywords = {Anomaly detection, intrusion detection, unsupervised algorithms, comparison, attacks datasets, machine learning}
}

@inproceedings{10.1145/3597503.3623297,
author = {Jackson, Victoria and Prikladnicki, Rafael and van der Hoek, Andre},
title = {Co-Creation in Fully Remote Software Teams},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623297},
doi = {10.1145/3597503.3623297},
abstract = {In this paper, we use the lens of co-creation---a concept originally coined and applied in the fields of management and design that denotes how groups of people collaboratively create something of meaning through an orchestration of people, activities, and tools---to study how fully remote software teams co-create digital artifacts that can be considered as a form of documentation. We report on the results of a qualitative, interview-based study with 25 software professionals working in remote teams. Our primary findings are the definition of four models of co-creation, examples of sequencing these models into work chains to produce artifacts, factors that influence how developers match tasks to models and chains, and insights into tool support for co-creation. Together, our findings illustrate how co-creation is an intentional activity that has a significant role in how remote software teams' choose to structure their collaborative activities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {51},
numpages = {12},
keywords = {collaboration, remote software development, developer tools, virtual software teams, software team practices},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE43902.2021.00121,
author = {Ferreira, Gabriel and Jia, Limin and Sunshine, Joshua and K\"{a}stner, Christian},
title = {Containing Malicious Package Updates in npm with a Lightweight Permission System},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00121},
doi = {10.1109/ICSE43902.2021.00121},
abstract = {The large amount of third-party packages available in fast-moving software ecosystems, such as Node.js/npm, enables attackers to compromise applications by pushing malicious updates to their package dependencies. Studying the npm repository, we observed that many packages in the npm repository that are used in Node.js applications perform only simple computations and do not need access to filesystem or network APIs. This offers the opportunity to enforce least-privilege design per package, protecting applications and package dependencies from malicious updates. We propose a lightweight permission system that protects Node.js applications by enforcing package permissions at runtime. We discuss the design space of solutions and show that our system makes a large number of packages much harder to be exploited, almost for free.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1334–1346},
numpages = {13},
keywords = {design trade-offs, malicious package updates, package management, permission system, sand-boxing, security, supply-chain security},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3610228,
author = {Yan, Bo and Yang, Cheng and Shi, Chuan and Fang, Yong and Li, Qi and Ye, Yanfang and Du, Junping},
title = {Graph Mining for Cybersecurity: A Survey},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3610228},
doi = {10.1145/3610228},
abstract = {The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {nov},
articleno = {47},
numpages = {52},
keywords = {Cybersecurity, cyber attack, graph mining, graph embedding, graph neural network}
}

@proceedings{10.1145/3565287,
title = {MobiHoc '23: Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ACM MobiHoc is a premier international annual conference with a highly selective single-track technical program dedicated to addressing the challenges emerging from networked systems that must operate in the face of dynamics.},
location = {Washington, DC, USA}
}

@article{10.1145/3490390,
author = {Braun, Lennart and Demmler, Daniel and Schneider, Thomas and Tkachenko, Oleksandr},
title = {MOTION – A Framework for Mixed-Protocol Multi-Party Computation},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3490390},
doi = {10.1145/3490390},
abstract = {We present MOTION, an efficient and generic open-source framework for mixed-protocol secure multi-party computation&nbsp;(MPC). MOTION is built in a user-friendly, modular, and extensible way, intended to be used as a tool in MPC research and to increase adoption of MPC protocols in practice. Our framework incorporates several important engineering decisions such as full communication serialization, which enables MPC over arbitrary messaging interfaces and removes the need of owning network sockets. MOTION also incorporates several performance optimizations that improve the communication complexity and latency, e.g.,  ( 2times ) &nbsp;better online round complexity of precomputed correlated&nbsp;Oblivious Transfer&nbsp;(OT).We instantiate our framework with protocols for N&nbsp;parties and security against up to  ( N-1 )  passive corruptions: the MPC protocols of Goldreich-Micali-Wigderson&nbsp;(GMW) in its arithmetic and Boolean version and OT-based BMR&nbsp;(Ben-Efraim et&nbsp;al., CCS’16), as well as novel and highly efficient conversions between them, including a non-interactive conversion from BMR to arithmetic GMW.MOTION is highly efficient, which we demonstrate in our experiments. Compared to secure evaluation of AES-128 with  ( N=3 )  parties in a high-latency network with OT-based BMR, we achieve a 16 ( times )  better throughput of 16&nbsp;AES evaluations per second using BMR. With this, we show that BMR is much more competitive than previously assumed. For  ( N=3 )  parties and full-threshold protocols in a LAN, MOTION is  ( 10times ) – ( 18times )  faster than the previous best passively secure implementation from the MP-SPDZ framework, and  ( 190times ) – ( 586times )  faster than the actively secure SCALE-MAMBA framework. Finally, we show that our framework is highly efficient for privacy-preserving neural network inference.},
journal = {ACM Trans. Priv. Secur.},
month = {mar},
articleno = {8},
numpages = {35},
keywords = {Secure multi-party computation, hybrid protocols, efficiency, outsourcing}
}

@article{10.1145/3604605,
author = {Ibrohim, Muhammad Okky and Bosco, Cristina and Basile, Valerio},
title = {Sentiment Analysis for the Natural Environment: A Systematic Review},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604605},
doi = {10.1145/3604605},
abstract = {In this systematic review, Kitchenham’s framework is used to explore what tasks, techniques, and benchmarks for Sentiment Analysis have been developed for addressing topics about the natural environment. We comprehensively analyze seven dimensions including contribution, topical focus, data source and query, annotation, language, detail of the task, and technology/algorithm used. By showing how this research area has grown during the last few years, our investigation provides important findings about the results achieved and the challenges that need to be still addressed for making this technology actually helpful for stakeholders such as policymakers and governments.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {88},
numpages = {37},
keywords = {Natural environment, data-driven policy, sentiment analysis, natural language processing (NLP), systematic review}
}

@proceedings{10.1145/3584931,
title = {CSCW '23 Companion: Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Minneapolis, MN, USA}
}

@article{10.1145/3274566,
author = {van Allen, Philip},
title = {Prototyping ways of prototyping AI},
year = {2018},
issue_date = {November - December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {6},
issn = {1072-5520},
url = {https://doi.org/10.1145/3274566},
doi = {10.1145/3274566},
journal = {Interactions},
month = {oct},
pages = {46–51},
numpages = {6}
}

@inproceedings{10.1145/3598469.3598470,
author = {Sprenkamp, Kilian and Zavolokina, Liudmila and Angst, Mario and Dolata, Mateusz},
title = {Data-Driven Governance in Crises: Topic Modelling for the Identification of Refugee Needs},
year = {2023},
isbn = {9798400708374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3598469.3598470},
doi = {10.1145/3598469.3598470},
abstract = {The war in Ukraine and the following refugee crisis have recently again highlighted the need for effective refugee management across European countries. Refugee management contemporarily mostly relies on top-down management approaches by governments. These often lead to suboptimal policies for refugees and highlight a need to better identify and integrate refugee needs into management. Here, we show that modern applications of Natural Language Processing (NLP) allow for the effective analysis of large text corpora linked to refugee needs, making it possible to complement top-down approaches with bottom-up knowledge centered around the current needs of the refugee population. By following a Design Science Research Methodology, we utilize 58 semi-structured stakeholder interviews within Switzerland to develop design requirements for NLP applications for refugee management. Based on the design requirements, we developed R2G – “Refugees to Government”, an application based on state-of-the-art topic modeling to identify refugee needs bottom-up through Telegram data. We evaluate R2G with a dedicated workshop held with stakeholders from the public sector and civil society. Thus, we contribute to the ongoing discourse on how to design refugee management applications and showcase how topic modeling can be utilized for data-driven governance during refugee crises.},
booktitle = {Proceedings of the 24th Annual International Conference on Digital Government Research},
pages = {1–11},
numpages = {11},
keywords = {Design Science Research, Natural Language Processing, Refugee Management, Topic Modeling},
location = {Gda?sk, Poland},
series = {dg.o '23}
}

@proceedings{10.1145/3603163,
title = {HT '23: Proceedings of the 34th ACM Conference on Hypertext and Social Media},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@article{10.1145/3428335,
author = {Gao, Bingyu and Wang, Haoyu and Xia, Pengcheng and Wu, Siwei and Zhou, Yajin and Luo, Xiapu and Tyson, Gareth},
title = {Tracking Counterfeit Cryptocurrency End-to-end},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3428335},
doi = {10.1145/3428335},
abstract = {The production of counterfeit money has a long history. It refers to the creation of imitation currency that is produced without the legal sanction of government. With the growth of the cryptocurrency ecosystem, there is expanding evidence that counterfeit cryptocurrency has also appeared. In this paper, we empirically explore the presence of counterfeit cryptocurrencies on Ethereum and measure their impact. By analyzing over 190K ERC-20 tokens (or cryptocurrencies) on Ethereum, we have identified $2,117$ counterfeit tokens that target 94 of the 100 most popular cryptocurrencies. We perform an end-to-end characterization of the counterfeit token ecosystem, including their popularity, creators and holders, fraudulent behaviors and advertising channels. Through this, we have identified two types of scams related to counterfeit tokens and devised techniques to identify such scams. We observe that over 7,104 victims were deceived in these scams, and the overall financial loss sums to a minimum of $ 17 million (74,271.7 ETH). Our findings demonstrate the urgency to identify counterfeit cryptocurrencies and mitigate this threat.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {nov},
articleno = {50},
numpages = {28},
keywords = {blockchain, counterfeit cryptocurrency, erc-20 token, scam}
}

@inproceedings{10.1145/3636534.3649373,
author = {Johnson, Jacob and Palacios, Ashton and Arvonen, Cody and Lundrigan, Philip},
title = {Wireless Latency Shift Keying},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3649373},
doi = {10.1145/3636534.3649373},
abstract = {IEEE 802.11 (WiFi) only has two modes of trust---complete trust or complete untrust. The lack of nuance leaves no room for sensors that a user does not fully trust but wants to connect to their network, such as a WiFi sensor. Solutions exist, but they require advanced knowledge of network administration. We solve this problem by introducing a new way of modulating data in the latency of the network, called Latency Shift Keying. We use specific characteristics of the WiFi protocol to carefully control the latency of just one device on the network. We build a transmitter, receiver, and modulation scheme that is designed to encode data in the latency of a network. We develop an application, Wicket, that solves the WiFi trust issue using Latency Shift Keying to create a new security association between an untrusted WiFi sensor and a wired device on the trusted network. We evaluate its performance and show that it works in many network conditions and environments.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {452–466},
numpages = {15},
keywords = {wireless subprotocol, IoT, sensor networks, 802.11, wifi},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/3564625.3564650,
author = {Zhang, Zhechang and Yuan, Bin and Yang, Kehan and Zou, Deqing and Jin, Hai},
title = {StateDiver: Testing Deep Packet Inspection Systems with State-Discrepancy Guidance},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564625.3564650},
doi = {10.1145/3564625.3564650},
abstract = {Deep Packet Inspection (DPI) systems are essential for securing modern networks (e.g., blocking or logging abnormal network connections). However, DPI systems are known to be vulnerable in their implementations, which could be exploited for evasion attacks. Due to the critical role DPI systems play, many efforts have been made to detect vulnerabilities in the DPI systems through manual inspection, symbolic execution, and fuzzing, which suffer from either poor scalability, path explosion, or inappropriate feedback. In this paper, based on our observation that a DPI system usually reaches an abnormal internal state before a forbidden packet passes through it, we propose a fuzzing framework that prioritizes inputs/mutations which could trigger the DPI system’s abnormal internal states. Further, to avoid deep understanding of the DPI systems under inspection (e.g., to identify the abnormal states), we feed one pair of inputs to multiple DPI systems and check whether the state changes of these DPI systems are consistent — an inconsistent internal state change/transference in one of the DPI systems indicates a new abnormal state is reached in the corresponding DPI system. Naturally, inputs that trigger new abnormal states are preferentially selected for mutations to generate new inputs. Following this idea, we develop StateDiver, the first fuzzing framework that uses the state discrepancy between different DPI systems as feedback to find more bypassing strategies. We make StateDiver publicly available online. With the help of StateDiver, we tested 3 famous open-source DPI systems (Snort, Snort++, and Suricata) and discovered 16 bypass strategies (8 new and 8 previously known). We have reported all the vulnerabilities to the vendors and received one CVE by the time of paper writing. We also compared StateDiver with Geneva, the state-of-the-art fuzzing tool for detecting DPI bugs. Results showed that StateDiver outperformed Geneva at the number and speed of finding vulnerabilities, indicating the ability of StateDiver to detect strategies bypassing DPI systems effectively.},
booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
pages = {756–768},
numpages = {13},
keywords = {Deep packet inspection, StateDiver, TCP, fuzzing},
location = {Austin, TX, USA},
series = {ACSAC '22}
}

@inproceedings{10.1145/3320269.3384727,
author = {Ding, Fei and Li, Hongda and Luo, Feng and Hu, Hongxin and Cheng, Long and Xiao, Hai and Ge, Rong},
title = {DeepPower: Non-intrusive and Deep Learning-based Detection of IoT Malware Using Power Side Channels},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384727},
doi = {10.1145/3320269.3384727},
abstract = {The vulnerability of Internet of Things (IoT) devices to malware attacks poses huge challenges to current Internet security. The IoT malware attacks are usually composed of three stages: intrusion, infection and monetization. Existing approaches for IoT malware detection cannot effectively identify the executed malicious activities at intrusion and infection stages, and thus cannot help stop potential attacks timely. In this paper, we present DeepPower, a non-intrusive approach to infer malicious activities of IoT malware via analyzing power side-channel signals using deep learning. DeepPower first filters raw power signals of IoT devices to obtain suspicious signals, and then performs a fine-grained analysis on these signals to infer corresponding executed activities inside the devices. DeepPower determines whether there exists an ongoing malware infection by conducting a correlation analysis on these identified activities. We implement a prototype of DeepPower leveraging low-cost sensors and devices and evaluate the effectiveness of DeepPower against real-world IoT malware using commodity IoT devices. Our experimental results demonstrate that DeepPower is able to detect infection activities of different IoT malware with a high accuracy without any changes to the monitored devices.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {33–46},
numpages = {14},
keywords = {IoT, deep learning, malware detection, non-intrusive, power side channels},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@proceedings{10.1145/3546157,
title = {ICISDM '22: Proceedings of the 6th International Conference on Information System and Data Mining},
year = {2022},
isbn = {9781450396257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Silicon Valley, CA, USA}
}

@proceedings{10.1145/3594556,
title = {BSCI '23: Proceedings of the 5th ACM International Symposium on Blockchain and Secure Critical Infrastructure},
year = {2023},
isbn = {9798400701986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3589335.3651569,
author = {Avery, Katherine and Houmansadr, Amir and Jensen, David},
title = {The Effect of Alter Ego Accounts on A/B Tests in Social Networks},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651569},
doi = {10.1145/3589335.3651569},
abstract = {Social network users often maintain multiple active accounts, sometimes referred to as alter egos. Examples of alter egos include personal and professional accounts or named and anonymous accounts. If alter egos are common on a platform, they can affect the results of A/B testing because a user's alter egos can influence each other. For a single user, one account may be assigned treatment, while another is assigned control. Alter-ego bias is relevant when the treatment affects the individual user rather than the account. Through experimentation and theoretical analysis, we examine the worst and expected case bias for different numbers of alter egos and for a variety of network structures and peer effect strengths. We show that alter egos moderately bias the results of simulated A/B tests on several network structures, including a real-world Facebook subgraph and several types of synthetic networks: small world networks, forest fire networks, stochastic block models, and a worst-case structure. We also show that bias increases with the number of alter egos and that different network structures have different upper bounds on bias.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2024},
pages = {565–568},
numpages = {4},
keywords = {a/b testing, causal inference, social networks, user behavior},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3643991.3645078,
author = {Mohamed, Suad and Parvin, Abdullah and Parra, Esteban},
title = {Chatting with AI: Deciphering Developer Conversations with ChatGPT},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645078},
doi = {10.1145/3643991.3645078},
abstract = {Large Language Models (LLMs) have been widely adopted and are becoming ubiquitous and integral to software development. However, we have little knowledge as to how these tools are being used by software developers beyond anecdotal evidence and word-of-mouth reports. In this work, we present a study toward understanding how developers engage with and utilize LLMs by reporting the results of an empirical study identifying patterns in the conversation that developers have with LLMs. We identified a total of 19 topics describing the purpose of the developers in their conversations with LLMs. Our findings reveal that developers use LLMs to facilitate various aspects of their software development processes (e.g., information-seeking about programming languages and frameworks and soliciting high-level design recommendations) to a similar extent to which they use them for non-development purposes such as writing assistance, general purpose queries, and conducting Turing tests to assess the intrinsic capabilities of the models. This work not only sheds light on the diverse applications of LLMs in software development but also underscores their emerging role as critical tools in enhancing developer productivity and creativity as we move closer to widespread AI-assisted software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {187–191},
numpages = {5},
keywords = {large language models, LLM, ChatGPT, software development, empirical study, developer conversations},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3425174.3425209,
author = {da Silva, Henrique Neves and Endo, Andre Takeshi and Eler, Marcelo Medeiros and Vergilio, Silvia Regina and Durelli, Vinicius H. S.},
title = {On the Relation between Code Elements and Accessibility Issues in Android Apps},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425209},
doi = {10.1145/3425174.3425209},
abstract = {Mobile apps have gone mainstream and become part of our daily lives. Currently, many efforts have been made to make apps more accessible to people with disabilities. However, little is still known on how to implement more accessible apps. In the Android API, there are (code) elements that may be employed to (in)directly improve the app's accessibility. This paper aims to investigate the prevalence of accessibility code elements and their relation to potential accessibility issues. First, we identified code elements of the native Android API that may be related to accessibility features, and mapped them to principles and success criteria of the Web Content Accessibility Guidelines (WCAG) 2.1. Using a sample of 111 open source mobile apps available in Google Play, we conducted a characterization study to examine the prevalence of accessibility code elements. We also analyzed how these code elements are related to issues detected by the static analyzer Android Lint and the accessibility testing tool MATE. Our results indicate that code elements are not widely used; the ones directly related to accessibility are present in only a few apps. Additionally, our results would seem to suggest that apps that adopt accessibility code elements, tend to have less accessibility issues. By analyzing our results from the standpoint of the WCAG principles, we conclude that there is room for improvement in terms of how both the Android API and automated testing tools deal with accessibility-related issues.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {40–49},
numpages = {10},
keywords = {Mobile apps. Accessibility},
location = {Natal, Brazil},
series = {SAST '20}
}

@inbook{10.1145/3233795.3233808,
author = {Sonntag, Daniel},
title = {Medical and health systems},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233808},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {423–476},
numpages = {54}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@article{10.1145/3555552,
author = {Park, Joon Sung and Seering, Joseph and Bernstein, Michael S.},
title = {Measuring the Prevalence of Anti-Social Behavior in Online Communities},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555552},
doi = {10.1145/3555552},
abstract = {With increasing attention to online anti-social behaviors such as personal attacks and bigotry, it is critical to have an accurate accounting of how widespread anti-social behaviors are. In this paper, we empirically measure the prevalence of anti-social behavior in one of the world's most popular online community platforms. We operationalize this goal as measuring the proportion of unmoderated comments in the 97 most popular communities on Reddit that violate eight widely accepted platform norms. To achieve this goal, we contribute a human-AI pipeline for identifying these violations and a bootstrap sampling method to quantify measurement uncertainty. We find that 6.25% (95% Confidence Interval [5.36%, 7.13%]) of all comments in 2016, and 4.28% (95% CI [2.50%, 6.26%]) in 2020, are violations of these norms. Most anti-social behaviors remain unmoderated: moderators only removed one in twenty violating comments in 2016, and one in ten violating comments in 2020. Personal attacks were the most prevalent category of norm violation; pornography and bigotry were the most likely to be moderated, while politically inflammatory comments and misogyny/vulgarity were the least likely to be moderated. This paper offers a method and set of empirical results for tracking these phenomena as both the social practices (e.g., moderation) and technical practices (e.g., design) evolve.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {451},
numpages = {29},
keywords = {anti-social behavior, moderation, online communities}
}

@inproceedings{10.1145/3180155.3180192,
author = {Spadini, Davide and Aniche, Maur\'{\i}cio and Storey, Margaret-Anne and Bruntink, Magiel and Bacchelli, Alberto},
title = {When testing meets code review: why and how developers review tests},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180192},
doi = {10.1145/3180155.3180192},
abstract = {Automated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {677–687},
numpages = {11},
keywords = {automated testing, code review, gerrit, software testing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@proceedings{10.1145/3578837,
title = {ICEEL '22: Proceedings of the 2022 6th International Conference on Education and E-Learning},
year = {2022},
isbn = {9781450398428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yamanashi, Japan}
}

@proceedings{10.1145/3591569,
title = {ICIIT '23: Proceedings of the 2023 8th International Conference on Intelligent Information Technology},
year = {2023},
isbn = {9781450399616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Da Nang, Vietnam}
}

@proceedings{10.1145/3629527,
title = {ICPE '24 Companion: Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the ICPE 2024 workshops program. ICPE workshops extend the main conference by providing a forum to foster discussion on hot and emerging topics from the broad field of performance engineering. They offer a highly dynamic venue to exchange ideas, establish new collaborations, and bootstrap debates on novel techniques, methodologies, and their associated early research results. Workshops feature various presentation formats, including research paper presentations, panel discussions, and keynote talks. Through these presentations and discussions with peer researchers, ICPE workshops help shape future research and identify promising research directions for performance engineering.},
location = {London, United Kingdom}
}

@proceedings{10.1145/3563359,
title = {UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
year = {2023},
isbn = {9781450398916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@article{10.1145/3655616,
author = {Wen, Zhiyuan and Cao, Jiannong and Shen, Jiaxing and Yang, Ruosong and Liu, Shuaiqi and Sun, Maosong},
title = {Personality-affected Emotion Generation in Dialog Systems},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {1046-8188},
url = {https://doi.org/10.1145/3655616},
doi = {10.1145/3655616},
abstract = {Generating appropriate emotions for responses is essential for dialogue systems to provide human-like interaction in various application scenarios. Most previous dialogue systems tried to achieve this goal by learning empathetic manners from anonymous conversational data. However, emotional responses generated by those methods may be inconsistent, which will decrease user engagement and service quality. Psychological findings suggest that the emotional expressions of humans are rooted in personality traits. Therefore, we propose a new task, Personality-affected Emotion Generation, to generate emotion based on the personality given to the dialogue system and further investigate a solution through the personality-affected mood transition. Specifically, we first construct a daily dialogue dataset, Personality EmotionLines Dataset (PELD), with emotion and personality annotations. Subsequently, we analyze the challenges in this task, i.e., (1) heterogeneously integrating personality and emotional factors and (2) extracting multi-granularity emotional information in the dialogue context. Finally, we propose to model the personality as the transition weight by simulating the mood transition process in the dialogue system and solve the challenges above. We conduct extensive experiments on PELD for evaluation. Results suggest that by adopting our method, the emotion generation performance is improved by 13% in macro-F1 and 5% in weighted-F1 from the BERT-base model.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {134},
numpages = {27},
keywords = {dialogue systems, emotion, personality}
}

@inproceedings{10.1145/3640543.3645200,
author = {Khurana, Anjali and Subramonyam, Hariharan and Chilana, Parmit K},
title = {Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645200},
doi = {10.1145/3640543.3645200},
abstract = {Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to understand how the prompt’s text related to the LLM’s responses and often followed the LLM’s suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM’s advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM’s responses, indicating a gap between their lack of software expertise and their ability to evaluate the LLM’s assistance. With the growing push for designing domain-specific LLM assistants, we emphasize the importance of incorporating explainable, context-aware cues into LLMs to help users understand prompt-based interactions, identify biases, and maximize the utility of LLM assistants.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {288–303},
numpages = {16},
keywords = {feature-rich software, help-seeking, large language models, prompt-based interactions},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@proceedings{10.1145/3581971,
title = {ICBTA '22: Proceedings of the 2022 5th International Conference on Blockchain Technology and Applications},
year = {2022},
isbn = {9781450397575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3617733,
title = {ICCCM '23: Proceedings of the 2023 11th International Conference on Computer and Communications Management},
year = {2023},
isbn = {9798400707735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/2441776.2441891,
author = {Luther, Kurt and Fiesler, Casey and Bruckman, Amy},
title = {Redistributing leadership in online creative collaboration},
year = {2013},
isbn = {9781450313315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2441776.2441891},
doi = {10.1145/2441776.2441891},
abstract = {In this paper, we integrate theories of distributed leadership and distributed cognition to account for the roles of people and technology in online leadership. When leadership is distributed effectively, the result can be success stories like Wikipedia and Linux. However, finding a successful distribution is challenging. In the online community Newgrounds, hundreds of collaborative animation projects called "collabs" are started each year, but less than 20% are completed. We suggest that many collabs fail because leaders are overburdened and lack adequate technological support. We introduce Pipeline, a collaboration tool designed to support and transform leadership, with the goal of easing the burden on leaders of online creative projects. Through a case study of a six-week, 30-artist collaboration called Holiday Flood, we show how Pipeline supported redistributed leadership. We conclude with implications for theory and the design of social computing systems.},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work},
pages = {1007–1022},
numpages = {16},
keywords = {creativity support tools, distributed cognition, distributed leadership, online creative collaboration, social computing},
location = {San Antonio, Texas, USA},
series = {CSCW '13}
}

@article{10.5555/3122009.3242049,
author = {Achab, Massil and Bacry, Emmanuel and Ga\"{\i}ffas, St\'{e}phane and Mastromatteo, Iacopo and Muzy, Jean-Fran\c{c}ois},
title = {Uncovering causality from multivariate hawkes integrated cumulants},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {6998–7025},
numpages = {28},
keywords = {Hawkes process, causality inference, cumulants, generalized method of moments}
}

@proceedings{10.1145/3571473,
title = {SBQS '22: Proceedings of the XXI Brazilian Symposium on Software Quality},
year = {2022},
isbn = {9781450399999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Curitiba, Brazil}
}

@proceedings{10.1145/3607505,
title = {CSET '23: Proceedings of the 16th Cyber Security Experimentation and Test Workshop},
year = {2023},
isbn = {9798400707889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Marina del Rey, CA, USA}
}

@inproceedings{10.1145/3460120.3484546,
author = {Zhu, Tong and Meng, Yan and Hu, Haotian and Zhang, Xiaokuan and Xue, Minhui and Zhu, Haojin},
title = {Dissecting Click Fraud Autonomy in the Wild},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484546},
doi = {10.1145/3460120.3484546},
abstract = {Although the use of pay-per-click mechanisms stimulates the prosperity of the mobile advertisement network, fraudulent ad clicks result in huge financial losses for advertisers. Extensive studies identify click fraud according to click/traffic patterns based on dynamic analysis. However, in this study, we identify a novel click fraud, named humanoid attack, which can circumvent existing detection schemes by generating fraudulent clicks with similar patterns to normal clicks. We implement the first tool ClickScanner to detect humanoid attacks on Android apps based on static analysis and variational AutoEncoders (VAEs) with limited knowledge of fraudulent examples. We define novel features to characterize the patterns of humanoid attacks in the apps' bytecode level. ClickScanner builds a data dependency graph (DDG) based on static analysis to extract these key features and form a feature vector. We then propose a classification model only trained on benign datasets to overcome the limited knowledge of humanoid attacks.We leverage ClickScanner to conduct the first large-scale measurement on app markets (i.e., 120,000 apps from Google Play and Huawei AppGallery) and reveal several unprecedented phenomena. First, even for the top-rated 20,000 apps, ClickScanner still identifies 157 apps as fraudulent, which shows the prevalence of humanoid attacks. Second, it is observed that the ad SDK-based attack (i.e., the fraudulent codes are in the third-party ad SDKs) is now a dominant attack approach. Third, the manner of attack is notably different across apps of various categories and popularities. Finally, we notice there are several existing variants of the humanoid attack. Additionally, our measurements demonstrate the proposed ClickScanner is accurate and time-efficient (i.e., the detection overhead is only 15.35% of those of existing schemes).},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {271–286},
numpages = {16},
keywords = {click fraud, humanoid attack, static analysis, variational autoencoders},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3634737.3661137,
author = {Neef, Sebastian and Kleissner, Lorenz and Seifert, Jean-Pierre},
title = {What All the PHUZZ Is About: A Coverage-guided Fuzzer for Finding Vulnerabilities in PHP Web Applications},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3661137},
doi = {10.1145/3634737.3661137},
abstract = {Coverage-guided fuzz testing has received significant attention from the research community, with a strong focus on binary applications, greatly disregarding other targets, such as web applications. The importance of the World Wide Web in everyone's life cannot be overstated, and to this day, many web applications are developed in PHP. In this work, we address the challenges of applying coverage-guided fuzzing to PHP web applications and introduce Phuzz, a modular fuzzing framework for PHP web applications. Phuzz uses novel approaches to detect more client-side and server-side vulnerability classes than state-of-the-art related work, including SQL injections, remote command injections, insecure deserialization, path traversal, external entity injection, cross-site scripting, and open redirection. We evaluate Phuzz on a diverse set of artificial and real-world web applications with known and unknown vulnerabilities, and compare it against a variety of state-of-the-art fuzzers. In order to show Phuzz' effectiveness, we fuzz over 1,000 API endpoints of the 115 most popular WordPress plugins, resulting in over 20 security issues and 2 new CVE-IDs. Finally, we make the framework publicly available to motivate and encourage further research on web application fuzz testing.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1523–1538},
numpages = {16},
keywords = {PHUZZ, coverage-guided fuzzing, greybox fuzzing, fuzz testing, PHP, vulnerability discovery, web security, SQL injection, remote command execution, cross-site scripting},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3581641.3584079,
author = {Tabalba, Roderick S and Kirshenbaum, Nurit and Leigh, Jason and Bhattacharya, Abari and Grosso, Veronica and Di Eugenio, Barbara and Johnson, Andrew E and Zellner, Moira},
title = {An Investigation into an Always Listening Interface to Support Data Exploration},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584079},
doi = {10.1145/3581641.3584079},
abstract = {Natural Language Interfaces that facilitate data exploration tasks are rapidly gaining in interest in the research community because they enable users to focus their attention on the task of inquiry rather than the mechanics of chart construction. Yet, current systems rely solely on processing the user’s explicit commands to generate the user’s intended chart. These commands can be ambiguous due to natural language tendencies such as speech disfluency and underspecification. In this paper, we developed and studied how an always listening interface can help contextualize imprecise queries. Our study revealed that an always listening interface is able to use an on-going conversation to fill in missing properties for imprecise commands, disambiguate inaccurate commands without asking the user for clarification, as well as generate charts without being explicitly asked.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {128–141},
numpages = {14},
keywords = {Arti, Articulate+, always listening, charts, collaborative digital assistants, data exploration, data visualization, digital collaborator, visualization},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{10.1145/3375572.3375574,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3375572.3375574},
doi = {10.1145/3375572.3375574},
abstract = {Why positive train control is vulnerable to a cyber- attack (D G. Rossiter, R 31 39) Positive Train Control (PTC) is a federally-mandated replacement of traditional rail signaling on the largest railroads with a network of on- and o -train electronics to space trains and prevent collisions or runaways. Railroads are installing PTC on nearly 57,848 route miles and on 19,912 locomotives.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {6–11},
numpages = {6}
}

@inproceedings{10.1145/3522664.3528592,
author = {Borg, Markus and Bengtsson, Johan and \"{O}sterling, Harald and Hagelborn, Alexander and Gagner, Isabella and Tomaszewski, Piotr},
title = {Quality assurance of generative dialog models in an evolving conversational agent used for Swedish language practice},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528592},
doi = {10.1145/3522664.3528592},
abstract = {Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {22–32},
numpages = {11},
keywords = {AI quality, action research, conversational agent, generative dialog model, requirements engineering, software testing},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@article{10.1145/3274404,
author = {Pascarella, Luca and Spadini, Davide and Palomba, Fabio and Bruntink, Magiel and Bacchelli, Alberto},
title = {Information Needs in Contemporary Code Review},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274404},
doi = {10.1145/3274404},
abstract = {Contemporary code review is a widespread practice used by software engineers to maintain high software quality and share project knowledge. However, conducting proper code review takes time and developers often have limited time for review. In this paper, we aim at investigating the information that reviewers need to conduct a proper code review, to better understand this process and how research and tool support can make developers become more effective and efficient reviewers. Previous work has provided evidence that a successful code review process is one in which reviewers and authors actively participate and collaborate. In these cases, the threads of discussions that are saved by code review tools are a precious source of information that can be later exploited for research and practice. In this paper, we focus on this source of information as a way to gather reliable data on the aforementioned reviewers' needs. We manually analyze 900 code review comments from three large open-source projects and organize them in categories by means of a card sort. Our results highlight the presence of seven high-level information needs, such as knowing the uses of methods and variables declared/modified in the code under review. Based on these results we suggest ways in which future code review tools can better support collaboration and the reviewing task.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {135},
numpages = {27},
keywords = {code review, information needs, mining software repositories}
}

@proceedings{10.1145/3586182,
title = {UIST '23 Adjunct: Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3627508,
title = {CHIIR '24: Proceedings of the 2024 Conference on Human Information Interaction and Retrieval},
year = {2024},
isbn = {9798400704345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sheffield, United Kingdom}
}

@inproceedings{10.1145/3635636.3656184,
author = {Rayan, Jude and Kanetkar, Dhruv and Gong, Yifan and Yang, Yuewen and Palani, Srishti and Xia, Haijun and Dow, Steven P.},
title = {Exploring the Potential for Generative AI-based Conversational Cues for Real-Time Collaborative Ideation},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656184},
doi = {10.1145/3635636.3656184},
abstract = {What is the potential value and role for AI to facilitate real-time creative discussions? The paper explores principles for Generative-AI based conversational support by investigating how humans – playing the role of an AI agent – generate contextual conversational cues to guide an ideation session. We studied n=42 people (14 triads) brainstorming through a remote meeting design probe that allows a wizard facilitator to oversee the ideation and send text-based cues that appear real-time in the ideator interface. Thematic analysis of conversations, cues and post-hoc reflections by facilitators uncovered focal points, strategies and challenges. Notably, 44% of the cues sent out by the facilitators were either dismissed or ignored because they did not notice the cue update. When ideators did notice cues, certain facilitator strategies impacted the conversation more than others. Based on our analysis, we present design opportunities to improve generative AI-based systems to better support real-time creative collaborations.},
booktitle = {Proceedings of the 16th Conference on Creativity &amp; Cognition},
pages = {117–131},
numpages = {15},
keywords = {Collaboration, Creativity Support, Empirical Methods, Mixed Methods, Prototyping/Implementation},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@article{10.1145/3397334,
author = {Yin, Huanpu and Zhou, Anfu and Su, Guangyuan and Chen, Bo and Liu, Liang and Ma, Huadong},
title = {Learning to Recognize Handwriting Input with Acoustic Features},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3397334},
doi = {10.1145/3397334},
abstract = {For mobile or wearable devices with a small touchscreen, handwriting input (instead of typing on the touchscreen) is highly desirable for efficient human-computer interaction. Previous passive acoustic-based handwriting solutions mainly focus on print-style capital input, which is inconsistent with people's daily habits and thus causes inconvenience. In this paper, we propose WritingRecorder, a novel universal text entry system that enables free-style lowercase handwriting recognition. WritingRecorder leverages the built-in microphone of the smartphones to record the handwritten sound, and then designs an adaptive segmentation method to detect letter fragments in real-time from the recorded sound. Then we design a neural network named Inception-LSTM to extract the hidden and unique acoustic pattern associated with the writing trajectory of each letter and thus classify each letter. Moreover, we adopt a word selection method based on language model, so as to recognize legislate words from all possible letter combinations. We implement WritingRecorder as an APP on mobile phones and conduct the extensive experimental evaluation. The results demonstrate that WritingRecorder works in real-time and can achieve 93.2% accuracy even for new users without collecting and training on their handwriting samples, under a series of practical scenarios.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jun},
articleno = {64},
numpages = {26},
keywords = {Acoustic sensing, Handwriting recognition}
}

@inproceedings{10.1145/3607199.3607205,
author = {Spahn, Noah and Hanke, Nils and Holz, Thorsten and Kruegel, Christopher and Vigna, Giovanni},
title = {Container Orchestration Honeypot: Observing Attacks in the Wild},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607205},
doi = {10.1145/3607199.3607205},
abstract = {Containers, a mechanism to package software and its dependencies into a single artifact, have helped fuel the rapid pace of technological advancements in the last few years. However, it is not always clear what the potential security risk of moving to the cloud and container-based technologies is. In this paper, we investigate exposed container orchestration services on the Internet: how many there are, and the attacks against them. We considered three groups of container-based software: Docker, Kubernetes, and workflow tools. In a measurement study, we scanned the Internet to identify vulnerable container and container-orchestration services running on default ports. Considering the scan data, we then designed a high-interaction honeypot to reveal where attackers tend to strike and what is being done against exposed instances. The honeypot is based on container orchestration tools installed on Ubuntu servers, behind a carefully constructed gateway, and using the default ports. Our honeypot attracted attackers within minutes of launch. In total, we collected 94 days of attack data and extracted associated indicators of compromise (IOCs), which are provided to the research community to enable further insights. Our empirical study measures the risk associated with container and container orchestration systems exposed on the Internet. The assessment is performed by leveraging a novel design for a high-interaction honeypot. Using the observed data, we extract fresh insights into malicious tools, tactics, and procedures used against exposed host systems. In addition, we make available to the research community a rich dataset of unencrypted malicious traffic.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {381–396},
numpages = {16},
keywords = {Docker, Kubernetes, containers, honeypot, vulnerability},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1109/ASE51524.2021.9678923,
author = {Pan, Shengyi and Bao, Lingfeng and Ren, Xiaoxue and Xia, Xin and Lo, David and Li, Shanping},
title = {Automating developer chat mining},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678923},
doi = {10.1109/ASE51524.2021.9678923},
abstract = {Online chatrooms are gaining popularity as a communication channel between widely distributed developers of Open Source Software (OSS) projects. Most discussion threads in chatrooms follow a Q&amp;A format, with some developers (askers) raising an initial question and others (respondents) joining in to provide answers. These discussion threads are embedded with rich information that can satisfy the diverse needs of various OSS stakeholders. However, retrieving information from threads is challenging as it requires a thread-level analysis to understand the context. Moreover, the chat data is transient and unstructured, consisting of entangled informal conversations. In this paper, we address this challenge by identifying the information types available in developer chats and further introducing an automated mining technique. Through manual examination of chat data from three chatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories and create a labeled dataset with 2,959 threads. We propose a classification approach (named F2Chat) to structure the vast amount of threads based on the information type automatically, helping stakeholders quickly acquire their desired information. F2Chat effectively combines handcrafted non-textual features with deep textual features extracted by neural models. Specifically, it has two stages with the first one leveraging the siamese architecture to pretrain the textual feature encoder, and the second one facilitating an in-depth fusion of two types of features. Evaluation results suggest that our approach achieves an average F1-score of 0.628, which improves the baseline by 57%. Experiments also verify the effectiveness of our identified non-textual features under both intra-project and cross-project validations.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {854–866},
numpages = {13},
keywords = {deep learning, developer chatrooms, gitter, information mining},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1109/MSR.2019.00075,
author = {Chatterjee, Preetha and Damevski, Kostadin and Pollock, Lori and Augustine, Vinay and Kraft, Nicholas A.},
title = {Exploratory study of slack Q&amp;A chats as a mining source for software engineering tools},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00075},
doi = {10.1109/MSR.2019.00075},
abstract = {Modern software development communities are increasingly social. Popular chat platforms such as Slack host public chat communities that focus on specific development topics such as Python or Ruby-on-Rails. Conversations in these public chats often follow a Q&amp;A format, with someone seeking information and others providing answers in chat form. In this paper, we describe an exploratory study into the potential usefulness and challenges of mining developer Q&amp;A conversations for supporting software maintenance and evolution tools. We designed the study to investigate the availability of information that has been successfully mined from other developer communications, particularly Stack Overflow. We also analyze characteristics of chat conversations that might inhibit accurate automated analysis. Our results indicate the prevalence of useful information, including API mentions and code snippets with descriptions, and several hurdles that need to be overcome to automate mining that information.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {490–501},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@proceedings{10.1145/3626253,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@inproceedings{10.1145/3423337.3429437,
author = {Kogkitsidou, Eleni and Gambette, Philippe},
title = {Normalisation of 16th and 17th century texts in French and geographical named entity recognition},
year = {2020},
isbn = {9781450381635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423337.3429437},
doi = {10.1145/3423337.3429437},
abstract = {Both statistical and rule-based methods for named entity recognition are quite sensitive to the type of language used in the analysed texts. Former studies have shown for example that it was harder to detect named entities in SMS or microblog messages where words are abridged or changed to lowercase. In this article, we focus on old French texts to evaluate the impact of manual and automatic normalisation before applying five geographical named entity recognition tools, as well as an improved version of one of them, in order to help building maps displaying the locations mentioned in ancient texts. Our results show that manual normalisation leads to better results for all methods and that automatic normalisation performs differently depending on the tool used to extract geographical named entities, but with a significant improvement on most methods.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Geospatial Humanities},
pages = {28–34},
numpages = {7},
keywords = {digital humanities, geographical named entity recognition, natural language processing, text normalisation},
location = {Seattle, WA, USA},
series = {GeoHumanities '20}
}

@inproceedings{10.1145/3607199.3607208,
author = {Alam, Md Tanvirul and Bhusal, Dipkamal and Park, Youngja and Rastogi, Nidhi},
title = {Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607208},
doi = {10.1145/3607199.3607208},
abstract = {Public and commercial organizations extensively share cyberthreat intelligence (CTI) to prepare systems to defend against existing and emerging cyberattacks. However, traditional CTI has primarily focused on tracking known threat indicators such as IP addresses and domain names, which may not provide long-term value in defending against evolving attacks. To address this challenge, we propose to use more robust threat intelligence signals called attack patterns. LADDER is a knowledge extraction framework that can extract text-based attack patterns from CTI reports at scale. The framework characterizes attack patterns by capturing the phases of an attack in Android and enterprise networks and systematically maps them to the MITRE ATT&amp;CK pattern framework. LADDER can be used by security analysts to determine the presence of attack vectors related to existing and emerging threats, enabling them to prepare defenses proactively. We also present several use cases to demonstrate the application of LADDER in real-world scenarios. Finally, we provide a new, open-access benchmark malware dataset to train future cyberthreat intelligence models.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {92–108},
numpages = {17},
keywords = {Attack Patterns, Knowledge Graph, LADDER, Threat Intelligence},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1145/3635636.3656190,
author = {Palani, Srishti and Ramos, Gonzalo},
title = {Evolving Roles and Workflows of Creative Practitioners in the Age of Generative AI},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656190},
doi = {10.1145/3635636.3656190},
abstract = {Creative practitioners (like designers, software developers, and architects) have started to employ Generative AI models (GenAI) to produce text, images, and assets comparable to those made by people. While HCI research explores specific GenAI models and creativity support tools, little is known about practitioners’ evolving roles and workflows with GenAI models across a project’s stages. This knowledge is key to guide the development of the new generation of Creativity Support Tools. We contribute to this knowledge by employing a triangulated method to capture interviews, videos, and survey responses of creative practitioners reflecting on projects they completed with GenAI. Our observations let us derive a set of factors that capture practitioners’ perceived roles, challenges, benefits, and interaction patterns when creating with GenAI. From these factors, we offer insights and propose design opportunities and priorities that serve to encourage reflection from the wider community of Creativity Support Tools and GenAI stakeholders such as systems creators, researchers, and educators on how to develop systems that meet the needs of creatives in human-centered ways.},
booktitle = {Proceedings of the 16th Conference on Creativity &amp; Cognition},
pages = {170–184},
numpages = {15},
keywords = {Creative Practitioners, Creativity, Generative AI},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@proceedings{10.1145/3661167,
title = {EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salerno, Italy}
}

@proceedings{10.1145/3543758,
title = {MuC '22: Proceedings of Mensch und Computer 2022},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@article{10.1145/3359247,
author = {Wijenayake, Senuri and van Berkel, Niels and Kostakos, Vassilis and Goncalves, Jorge},
title = {Measuring the Effects of Gender on Online Social Conformity},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359247},
doi = {10.1145/3359247},
abstract = {Social conformity occurs when an individual changes their behaviour in line with the majority's expectations. Although social conformity has been investigated in small group settings, the effect of gender - of both the individual and the majority/minority - is not well understood in online settings. Here we systematically investigate the impact of groups' gender composition on social conformity in online settings. We use an online quiz in which participants submit their answers and confidence scores, both prior to and following the presentation of peer answers that are dynamically fabricated. Our results show an overall conformity rate of 39%, and a significant effect of gender that manifests in a number of ways: gender composition of the majority, the perceived nature of the question, participant gender, visual cues of the system, and final answer correctness. We conclude with a discussion on the implications of our findings in designing online group settings, accounting for the effects of gender on conformity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {145},
numpages = {24},
keywords = {avatars, gender, gender-typed questions, majority size, names, online quiz, self-reported confidence, social conformity}
}

@inproceedings{10.1145/2784731.2784748,
author = {Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton},
title = {GADTs meet their match: pattern-matching warnings that account for GADTs, guards, and laziness},
year = {2015},
isbn = {9781450336697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2784731.2784748},
doi = {10.1145/2784731.2784748},
abstract = {For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
pages = {424–436},
numpages = {13},
keywords = {Generalized Algebraic Data Types, Haskell, OutsideIn(X), pattern matching},
location = {Vancouver, BC, Canada},
series = {ICFP 2015}
}

@proceedings{10.1145/3623762,
title = {ITiCSE-WGR '23: Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In these proceedings, we present papers from the Working Groups that worked in the context of the 28th Annual Conference on Innovation &amp; Technology in Computer Science Education (ITiCSE), held in Turku Finland, and hosted by University of Turku from the 10th to the 12th of July 2023.The concept of Working Groups has been a unique feature of the ITiCSE conference series since its inception, with CompEd adopting the Working Group practice in 2019. A Working Group typically comprises 5 to 10 researchers who work together on a project related to computing education. Working Groups provide a wonderful opportunity to work intensively on a topic of interest with an international group of computing education researchers. This unique experience is one that, in our opinion, each Computer Science Educator should strive to participate in at least once.In 2023, 13 proposals for Working Groups were received and six Working Groups were selected by the Working Group chairs to recruit members and proceed for ITiCSE 2023. There were over 100 member applications to Working Groups, with 67 being accepted across the six Working Groups.},
location = {Turku, Finland}
}

@proceedings{10.1145/3603273,
title = {AAIA '23: Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications},
year = {2023},
isbn = {9798400708268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wuhan, China}
}

@inproceedings{10.1145/3298689.3346999,
author = {Rashed, Ahmed and Grabocka, Josif and Schmidt-Thieme, Lars},
title = {Attribute-aware non-linear co-embeddings of graph features},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346999},
doi = {10.1145/3298689.3346999},
abstract = {In very sparse recommender data sets, attributes of users such as age, gender and home location and attributes of items such as, in the case of movies, genre, release year, and director can improve the recommendation accuracy, especially for users and items that have few ratings. While most recommendation models can be extended to take attributes of users and items into account, their architectures usually become more complicated. While attributes for items are often easy to be provided, attributes for users are often scarce for reasons of privacy or simply because they are not relevant to the operational process at hand. In this paper, we address these two problems for attribute-aware recommender systems by proposing a simple model that co-embeds users and items into a joint latent space in a similar way as a vanilla matrix factorization, but with non-linear latent features construction that seamlessly can ingest user or item attributes or both (GraphRec). To address the second problem, scarce attributes, the proposed model treats the user-item relation as a bipartite graph and constructs generic user and item attributes via the Laplacian of the user-item co-occurrence graph that requires no further external side information but the mere rating matrix. In experiments on three recommender datasets, we show that GraphRec significantly outperforms existing state-of-the-art attribute-aware and content-aware recommender systems even without using any side information.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {314–321},
numpages = {8},
keywords = {collaborative filtering, graph embedding, non-linear factorization machines, recommender systems},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/3419394.3423631,
author = {Subramani, Karthika and Yuan, Xingzi and Setayeshfar, Omid and Vadrevu, Phani and Lee, Kyu Hyung and Perdisci, Roberto},
title = {When Push Comes to Ads: Measuring the Rise of (Malicious) Push Advertising},
year = {2020},
isbn = {9781450381383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419394.3423631},
doi = {10.1145/3419394.3423631},
abstract = {The rapid growth of online advertising has fueled the growth of ad-blocking software, such as new ad-blocking and privacy-oriented browsers or browser extensions. In response, both ad publishers and ad networks are constantly trying to pursue new strategies to keep up their revenues. To this end, ad networks have started to leverage the Web Push technology enabled by modern web browsers. As web push notifications (WPNs) are relatively new, their role in ad delivery has not yet been studied in depth. Furthermore, it is unclear to what extent WPN ads are being abused for malvertising (i.e., to deliver malicious ads). In this paper, we aim to fill this gap. Specifically, we propose a system called PushAdMiner that is dedicated to (1) automatically registering for and collecting a large number of web-based push notifications from publisher websites, (2) finding WPN-based ads among these notifications, and (3) discovering malicious WPN-based ad campaigns.Using PushAdMiner, we collected and analyzed 21,541 WPN messages by visiting thousands of different websites. Among these, our system identified 572 WPN ad campaigns, for a total of 5,143 WPN-based ads that were pushed by a variety of ad networks. Furthermore, we found that 51% of all WPN ads we collected are malicious, and that traditional ad-blockers and URL filters were mostly unable to block them, thus leaving a significant abuse vector unchecked.},
booktitle = {Proceedings of the ACM Internet Measurement Conference},
pages = {724–737},
numpages = {14},
location = {Virtual Event, USA},
series = {IMC '20}
}

@inproceedings{10.1145/3366423.3380165,
author = {Elyashar, Aviad and Uziel, Sagi and Paradise, Abigail and Puzis, Rami},
title = {The Chameleon Attack: Manipulating Content Display in Online Social Media},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380165},
doi = {10.1145/3366423.3380165},
abstract = {Online social networks (OSNs) are ubiquitous attracting millions of users all over the world. Being a popular communication media OSNs are exploited in a variety of cyber-attacks. In this article, we discuss the chameleon attack technique, a new type of OSN-based trickery where malicious posts and profiles change the way they are displayed to OSN users to conceal themselves before the attack or avoid detection. Using this technique, adversaries can, for example, avoid censorship by concealing true content when it is about to be inspected; acquire social capital to promote new content while piggybacking a trending one; cause embarrassment and serious reputation damage by tricking a victim to like, retweet, or comment a message that he wouldn’t normally do without any indication for the trickery within the OSN. An experiment performed with closed Facebook groups of sports fans shows that (1) chameleon pages can pass by the moderation filters by changing the way their posts are displayed and (2) moderators do not distinguish between regular and chameleon pages. We list the OSN weaknesses that facilitate the chameleon attack and propose a set of mitigation guidelines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {848–859},
numpages = {12},
keywords = {Chameleon Attack, Link Previews, Online Social Networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3568813,
title = {ICER '23: Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3400806.3400830,
author = {Panda, Anmol and Gonawela, A’ndre and Acharyya, Sreangsu and Mishra, Dibyendu and Mohapatra, Mugdha and Chandrasekaran, Ramgopal and Pal, Joyojeet},
title = {NivaDuck - A Scalable Pipeline to Build a Database of Political Twitter Handles for India and the United States},
year = {2020},
isbn = {9781450376884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400806.3400830},
doi = {10.1145/3400806.3400830},
abstract = {We present a scalable methodology to identify Twitter handles of politicians in a given region and test our framework in the context of Indian and US politics. The main contribution of our work is the list of the curated Twitter handles of 18500 Indian and 8000 US politicians. Our work leveraged machine learning-based classification and human verification to build a data set of Indian politicians on Twitter. We built NivaDuck, a highly precise, two-staged classification pipeline that leverages Twitter description text and tweet content to identify politicians. For India, we tested NivaDuck’s recall using Twitter handles of the members of the Indian parliament while for the US we used state and local level politicians in California state and San Diego county respectively. We found that while NivaDuck has lower recall scores, it produces large, diverse sets of politicians with precision exceeding 90 percent for the US dataset. We discuss the need for an ML-based, scalable method to compile such a dataset and its myriad use cases for the research community and its wide-ranging utilities for research in political communication on social media.},
booktitle = {International Conference on Social Media and Society},
pages = {200–209},
numpages = {10},
keywords = {archive, india, politics, twitter, united states},
location = {Toronto, ON, Canada},
series = {SMSociety'20}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00024,
author = {Alves, Isaque and Rocha, Carla},
title = {Qualifying software engineers undergraduates in DevOps - challenges of introducing technical and non-technical concepts in a project-oriented course},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00024},
doi = {10.1109/ICSE-SEET52601.2021.00024},
abstract = {The constant changes in the software industry, practices, and methodologies impose challenges to teaching and learning current software engineering concepts and skills. DevOps is particularly challenging because it covers technical concepts, such as pipeline automation, and non-technical ones, such as team roles and project management. The present study investigates a course setup to introduce these concepts to software engineering undergraduates. We designed the course by employing coding to associate DevOps concepts to Agile, Lean, and Open source practices and tools. We present the main aspects of this project-oriented DevOps course, with 240 students enrolled it since its first offering in 2016. We conducted an empirical study, with both a quantitative and qualitative analysis, to evaluate this project-oriented course setup. We collected the data from the projects repository and students' perceptions from a questionnaire. We mined 148 repositories (corresponding to 72 projects) and obtained 86 valid responses to the questionnaire. We also mapped the concepts which are more challenging to students learn from experience. The results evidence that first-hand experience facilitates the comprehension of DevOps concepts and enriches classes discussions. we present a set of lessons learned, which may help professors better design and conduct project-oriented courses to cover DevOps concepts.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {144–153},
numpages = {10},
keywords = {Agile software development, DevOps, FOSS, OSS, education, emerging domains of software, empirical software engineering, open source, tools and environments},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.1145/3143361.3143399,
author = {Aqil, Azeem and Khalil, Karim and Atya, Ahmed O.F. and Papalexakis, Evangelos E. and Krishnamurthy, Srikanth V. and Jaeger, Trent and Ramakrishnan, K. K. and Yu, Paul and Swami, Ananthram},
title = {Jaal: Towards Network Intrusion Detection at ISP Scale},
year = {2017},
isbn = {9781450354226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143361.3143399},
doi = {10.1145/3143361.3143399},
abstract = {We have recently seen an increasing number of attacks that are distributed, and span an entire wide area network (WAN). Today, typically, intrusion detection systems (IDSs) are deployed at enterprise scale and cannot handle attacks that cover a WAN. Moreover, such IDSs are implemented at a single entity that expects to look at all packets to determine an intrusion. Transferring copies of raw packets to centralized engines for analysis in a WAN can significantly impact both network performance and detection accuracy. In this paper, we propose Jaal, a framework for achieving accurate network intrusion detection at scale. The key idea in Jaal is to monitor traffic and construct in-network packet summaries. The summaries are then processed centrally to detect attacks with high accuracy. The main challenges that we address are (a) creating summaries that are concise, but sufficient to draw highly accurate inferences and (b) transforming traditional IDS rules to handle summaries instead of raw packets. We implement Jaal on a large scale SDN testbed. We show that on average Jaal yields a detection accuracy of about 98%, which is the highest reported for ISP scale network intrusion detection. At the same time, the overhead associated with transferring summaries to the central inference engine is only about 35% of what is consumed if raw packets are transferred.},
booktitle = {Proceedings of the 13th International Conference on Emerging Networking EXperiments and Technologies},
pages = {134–146},
numpages = {13},
location = {Incheon, Republic of Korea},
series = {CoNEXT '17}
}

@proceedings{10.1145/3564533,
title = {Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Evry-Courcouronnes, France}
}

@article{10.1145/3175492,
author = {Alrabaee, Saed and Shirani, Paria and Wang, Lingyu and Debbabi, Mourad},
title = {FOSSIL: A Resilient and Efficient System for Identifying FOSS Functions in Malware Binaries},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3175492},
doi = {10.1145/3175492},
abstract = {Identifying free open-source software (FOSS) packages on binaries when the source code is unavailable is important for many security applications, such as malware detection, software infringement, and digital forensics. This capability enhances both the accuracy and the efficiency of reverse engineering tasks by avoiding false correlations between irrelevant code bases. Although the FOSS package identification problem belongs to the field of software engineering, conventional approaches rely strongly on practical methods in data mining and database searching. However, various challenges in the use of these methods prevent existing function identification approaches from being effective in the absence of source code. To make matters worse, the introduction of obfuscation techniques, the use of different compilers and compilation settings, and software refactoring techniques has made the automated detection of FOSS packages increasingly difficult. With very few exceptions, the existing systems are not resilient to such techniques, and the exceptions are not sufficiently efficient.To address this issue, we propose FOSSIL, a novel resilient and efficient system that incorporates three components. The first component extracts the syntactical features of functions by considering opcode frequencies and applying a hidden Markov model statistical test. The second component applies a neighborhood hash graph kernel to random walks derived from control-flow graphs, with the goal of extracting the semantics of the functions. The third component applies z-score to the normalized instructions to extract the behavior of instructions in a function. The components are integrated using a Bayesian network model, which synthesizes the results to determine the FOSS function. The novel approach of combining these components using the Bayesian network has produced stronger resilience to code obfuscation.We evaluate our system on three datasets, including real-world projects whose use of FOSS packages is known, malware binaries for which there are security and reverse engineering reports purporting to describe their use of FOSS, and a large repository of malware binaries. We demonstrate that our system is able to identify FOSS packages in real-world projects with a mean precision of 0.95 and with a mean recall of 0.85. Furthermore, FOSSIL is able to discover FOSS packages in malware binaries that match those listed in security and reverse engineering reports. Our results show that modern malware binaries contain 0.10--0.45 of FOSS packages.},
journal = {ACM Trans. Priv. Secur.},
month = {jan},
articleno = {8},
numpages = {34},
keywords = {Binary code analysis, free software packages, function fingerprinting, malicious code analysis}
}

@article{10.1145/3617175,
author = {Golmohammadi, Amid and Zhang, Man and Arcuri, Andrea},
title = {Testing RESTful APIs: A Survey},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617175},
doi = {10.1145/3617175},
abstract = {In industry, RESTful APIs are widely used to build modern Cloud Applications. Testing them is challenging, because not only do they rely on network communications, but also they deal with external services like databases. Therefore, there has been a large amount of research sprout in recent years on how to automatically verify this kind of web services. In this article, we present a comprehensive review of the current state-of-the-art in testing RESTful APIs based on the analysis of 92 scientific articles. These articles were gathered by utilizing search queries formulated around the concept of RESTful API testing on seven popular databases. We eliminated irrelevant articles based on our predefined criteria and conducted a snowballing phase to minimize the possibility of missing any relevant paper. This survey categorizes and summarizes the existing scientific work on testing RESTful APIs and discusses the current challenges in the verification of RESTful APIs. This survey clearly shows an increasing interest among researchers in this field, from 2017 onward. However, there are still a lot of open research challenges to overcome.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {27},
numpages = {41},
keywords = {Survey, literature review, REST, API, testing, test case generation, fuzzing, web service}
}

@article{10.1145/3548772,
author = {Abdullah, Tariq and Ahmet, Ahmed},
title = {Deep Learning in Sentiment Analysis: Recent Architectures},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3548772},
doi = {10.1145/3548772},
abstract = {Humans are increasingly integrated with devices that enable the collection of vast unstructured opinionated data. Accurately analysing subjective information from this data is the task of sentiment analysis (an actively researched area in NLP). Deep learning provides a diverse selection of architectures to model sentiment analysis tasks and has surpassed other machine learning methods as the foremast approach for performing sentiment analysis tasks. Recent developments in deep learning architectures represent a shift away from Recurrent and Convolutional neural networks and the increasing adoption of Transformer language models. Utilising pre-trained Transformer language models to transfer knowledge to downstream tasks has been a breakthrough in NLP.This survey applies a task-oriented taxonomy to recent trends in architectures with a focus on the theory, design and implementation. To the best of our knowledge, this is the only survey to cover state-of-the-art Transformer-based language models and their performance on the most widely used benchmark datasets. This survey paper provides a discussion of the open challenges in NLP and sentiment analysis. The survey covers five years from 1st July 2017 to 1st July 2022.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {159},
numpages = {37},
keywords = {Deep learning, sentiment analysis, cross-lingual sentiment analysis, cross-domain sentiment analysis, transfer learning, multilingual sentiment analysis}
}

@proceedings{10.1145/3612783,
title = {Interacci\'{o}n '23: Proceedings of the XXIII International Conference on Human Computer Interaction},
year = {2023},
isbn = {9798400707902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lleida, Spain}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/3613904.3641899,
author = {Reza, Mohi and Laundry, Nathan M and Musabirov, Ilya and Dushniku, Peter and Yu, Zhi Yuan “Michael” and Mittal, Kashish and Grossman, Tovi and Liut, Michael and Kuzminykh, Anastasia and Williams, Joseph Jay},
title = {ABScribe: Rapid Exploration &amp; Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641899},
doi = {10.1145/3613904.3641899},
abstract = {Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art Large Language Models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new variations without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers’ flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration and organization of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly modify variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text fields for rapid in-place comparisons using mouse-over interactions on a popup toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions of the revision process (d = 2.41, p &lt; 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {1042},
numpages = {18},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3375547,
author = {Abulaish, Muhammad and Kamal, Ashraf and Zaki, Mohammed J.},
title = {A Survey of Figurative Language and Its Computational Detection in Online Social Networks},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3375547},
doi = {10.1145/3375547},
abstract = {The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category.},
journal = {ACM Trans. Web},
month = {feb},
articleno = {3},
numpages = {52},
keywords = {Social network analysis, figurative language, humor recognition, hyperbole detection, irony detection, metaphor detection, sarcasm detection, satire detection, simile detection}
}

@article{10.1145/3429741,
author = {Botacin, Marcus and Aghakhani, Hojjat and Ortolani, Stefano and Kruegel, Christopher and Vigna, Giovanni and Oliveira, Daniela and Geus, Paulo L\'{\i}cio De and Gr\'{e}gio, Andr\'{e}},
title = {One Size Does Not Fit All: A Longitudinal Analysis of Brazilian Financial Malware},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3429741},
doi = {10.1145/3429741},
abstract = {Malware analysis is an essential task to understand infection campaigns, the behavior of malicious codes, and possible ways to mitigate threats. Malware analysis also allows better assessment of attackers’ capabilities, techniques, and processes. Although a substantial amount of previous work provided a comprehensive analysis of the international malware ecosystem, research on regionalized, country-, and population-specific malware campaigns have been scarce. Moving towards addressing this gap, we conducted a longitudinal (2012-2020) and comprehensive (encompassing an entire population of online banking users) study of MS Windows desktop malware that actually infected Brazilian banks’ users. We found that the Brazilian financial desktop malware has been evolving quickly: it started to make use of a variety of file formats instead of typical PE binaries, relied on native system resources, and abused obfuscation techniques to bypass detection mechanisms. Our study on the threats targeting a significant population on the ecosystem of the largest and most populous country in Latin America can provide invaluable insights that may be applied to other countries’ user populations, especially those in the developing world that might face cultural peculiarities similar to Brazil’s. With this evaluation, we expect to motivate the security community/industry to seriously consider a deeper level of customization during the development of next-generation anti-malware solutions, as well as to raise awareness towards regionalized and targeted Internet threats.},
journal = {ACM Trans. Priv. Secur.},
month = {jan},
articleno = {11},
numpages = {31},
keywords = {Malware, banking, reverse engineer}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@article{10.1145/3656339,
author = {Long, Peixun and Zhao, Jianjun},
title = {Testing Multi-Subroutine Quantum Programs: From Unit Testing to Integration Testing},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3656339},
doi = {10.1145/3656339},
abstract = {Quantum computing has emerged as a promising field with the potential to revolutionize various domains by harnessing the principles of quantum mechanics. As quantum hardware and algorithms continue to advance, developing high-quality quantum software has become crucial. However, testing quantum programs poses unique challenges due to the distinctive characteristics of quantum systems and the complexity of multi-subroutine programs. This article addresses the specific testing requirements of multi-subroutine quantum programs. We begin by investigating critical properties by surveying existing quantum libraries and providing insights into the challenges of testing these programs. Building upon this understanding, we focus on testing criteria and techniques based on the whole testing process perspective, spanning from unit testing to integration testing. We delve into various aspects, including IO analysis, quantum relation checking, structural testing, behavior testing, integration of subroutine pairs, and test case generation. We also introduce novel testing principles and criteria to guide the testing process. We conduct comprehensive testing on typical quantum subroutines, including diverse mutants and randomized inputs, to evaluate our proposed approach. The analysis of failures provides valuable insights into the effectiveness of our testing methodology. Additionally, we present case studies on representative multi-subroutine quantum programs, demonstrating the practical application and effectiveness of our proposed testing principles and criteria.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {147},
numpages = {61},
keywords = {Quantum computing, software testing, unit testing, integration testing}
}

@proceedings{10.1145/3563822,
title = {FTSCS 2022: Proceedings of the 8th ACM SIGPLAN International Workshop on Formal Techniques for Safety-Critical Systems},
year = {2022},
isbn = {9781450399074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the proceedings of the Eighth ACM SIGPLAN International Workshop on Formal Techniques for Safety-Critical Systems (FTSCS 2022), held in Auckland, New Zealand, on December 7, 2022, as a satellite event of SPLASH 2022: The ACM SIGPLAN Conference on Systems, Programming, Languages, and Applications: Software for Humanity.  

The aim of this workshop is to bring together researchers and engineers who are interested in the application of formal and semi-formal methods to improve the quality of safety-critical computer systems. FTSCS strives to promote research and development of formal methods and tools for industrial applications, and is particularly interested in industrial applications of formal methods.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/3544548.3580946,
author = {Aghajari, Zhila and Baumer, Eric P. S. and DiFranzo, Dominic},
title = {What’s the Norm Around Here? Individuals’ Responses Can Mitigate the Effects of Misinformation Prevalence in Shaping Perceptions of a Community},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580946},
doi = {10.1145/3544548.3580946},
abstract = {Social norms play a significant role in how conspiratorial content and related misinformation impact online communities. However, less is understood about the mechanisms by which particular aspects of a community may drive perceptions of social norms in the community. Using anti-vaccine conspiracies as a testbed, this paper experimentally examines three such features and their relationships : prevalence of conspiratorial content, community response, and explicit community rules. Results show that prevalence of content has a significant effect on norm perceptions, while the results did not support the effects of explicit rule on norm perceptions. However, these effects can be mitigated by the way a community responds to such content. Furthermore, perceived norms also influence other expectations about the community, from escalated behaviors to belief in other conspiracy theories. The paper concludes by highlighting the implications of these findings for online platform design, for community governance, and for future research about the relationships among conspiratorial content and norm perceptions.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {550},
numpages = {27},
keywords = {Anti-vaccine, Conspiracy Theories, Misinformation, Misleading Content, Online Communities, Perceived Norms, Social Norms},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3627043.3659570,
author = {Olapade, Mayowa and Hasanli, Tarlan and Ottun, Abdul-Rasheed and Akintola, Adeyinka and Liyanage, Mohan and Flores, Huber},
title = {Pervasive Chatbots: Investigating Chatbot Interventions for Multi-Device Applications},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627043.3659570},
doi = {10.1145/3627043.3659570},
abstract = {The inherent social characteristics of humans make them prone to adopting distributed and collaborative applications easily. Although fundamental methods and technologies have been defined and developed over the years to construct these applications, their adoption in practice is uncommon because end-users may be puzzled about how to use them without much hassle. Indeed, commonly, these applications require a certain level of technical expertise and awareness to use them correctly. Fortunately, AI-chatbot interventions are envisioned to assist and support various human tasks. In this paper, we contribute pervasive chatbots as a solution that fosters a more transparent and user-friendly interconnection of devices in distributed and collaborative environments. Through two rigorous user studies, firstly, we quantify the perception of users toward distributed and collaborative applications (N = 56 participants). Secondly, we analyze the benefits of adopting pervasive chatbots when compared with the chatbot reference model designed for assistance and recommendations (N = 24 participants). Our results suggest that pervasive chatbots can significantly enhance the practicability of distributed and collaborative applications, reducing the time and effort needed for collaboration with surrounding devices by 57%. With this information, we then provide design and development implications to integrate pervasive chatbot interventions in distributed and collaborative environments. Moreover, challenges and opportunities are also provided to highlight the remaining issues that need to be addressed to realize the full vision of pervasive chatbots for any multi-device application. Our work paves the way towards the proliferation of sophisticated and highly decentralized computing environments that are easily interconnected.},
booktitle = {Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {290–300},
numpages = {11},
keywords = {Decentralized infrastructures, collaborative computing, distributed computing, opportunistic networks},
location = {Cagliari, Italy},
series = {UMAP '24}
}

@inproceedings{10.1145/3319535.3354232,
author = {Luckie, Matthew and Beverly, Robert and Koga, Ryan and Keys, Ken and Kroll, Joshua A. and claffy, k},
title = {Network Hygiene, Incentives, and Regulation: Deployment of Source Address Validation in the Internet},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354232},
doi = {10.1145/3319535.3354232},
abstract = {The Spoofer project has collected data on the deployment and characteristics of IP source address validation on the Internet since 2005. Data from the project comes from participants who install an active probing client that runs in the background. The client automatically runs tests both periodically and when it detects a new network attachment point. We analyze the rich dataset of Spoofer tests in multiple dimensions: across time, networks, autonomous systems, countries, and by Internet protocol version. In our data for the year ending August 2019, at least a quarter of tested ASes did not filter packets with spoofed source addresses leaving their networks. We show that routers performing Network Address Translation do not always filter spoofed packets, as 6.4% of IPv4/24 tested in the year ending August 2019 did not filter. Worse, at least two thirds of tested ASes did not filter packets entering their networks with source addresses claiming to be from within their network that arrived from outside their network. We explore several approaches to encouraging remediation and the challenges of evaluating their impact. While we have been able to remediate 352 IPv4/24, we have found an order of magnitude more IPv4/24 that remains unremediated, despite myriad remediation strategies, with 21% unremediated for more than six months. Our analysis provides the most complete and confident picture of the Internet's susceptibility to date of this long-standing vulnerability. Although there is no simple solution to address the remaining long-tail of unremediated networks, we conclude with a discussion of possible non-technical interventions, and demonstrate how the platform can support evaluation of the impact of such interventions over time.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {465–480},
numpages = {16},
keywords = {IP spoofing, remediation},
location = {London, United Kingdom},
series = {CCS '19}
}

@proceedings{10.1145/3607199,
title = {RAID '23: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, China}
}

@inproceedings{10.1145/3387940.3392220,
author = {Schreiber, Andreas and de Boer, Claas},
title = {Modelling Knowledge about Software Processes using Provenance Graphs and its Application to Git-based Version Control Systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392220},
doi = {10.1145/3387940.3392220},
abstract = {Using the W3C PROV data model, we present a general provenance model for software development processes and---as an example---specialized models for git services, for which we generate provenance graphs. Provenance graphs are knowledge graphs, since they have defined semantics, and can be analyzed with graph algorithms or semantic reasoning to get insights into processes.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {358–359},
numpages = {2},
keywords = {git, knowledge graphs, provenance, software development process, version control systems},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@proceedings{10.1145/3643491,
title = {MAD '24: Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation},
year = {2024},
isbn = {9798400705526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Phuket, Thailand}
}

@proceedings{10.1145/3569902,
title = {LADC '22: Proceedings of the 11th Latin-American Symposium on Dependable Computing},
year = {2022},
isbn = {9781450397377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fortaleza/CE, Brazil}
}

@article{10.1145/3610173,
author = {Ferguson, Sharon A. and Olechowski, Alison},
title = {Are We Equal Online?: An Investigation of Gendered Language Patterns and Message Engagement on Enterprise Communication Platforms},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610173},
doi = {10.1145/3610173},
abstract = {It was previously hypothesized that gender differences -- and thus gender discrimination -- would disappear if communication was no longer in person, and instead was transmitted and received in the same format for all. Yet, even online, researchers have identified gendered language styles in written communication that reveal gender cues and can lead to unequal treatment. In this work, we revisit these past findings and ask whether the same gendered patterns can be found on modern communication platforms, which present a new set of engagement features and mixed synchronous capabilities. We quantitatively analyze 335,000 Slack messages sent by 845 individuals as part of 46 teams, collected over six years of a product design capstone course. We found little evidence of traditionally gendered communication styles (characterized as elaborate, uncertain, and supportive) from the minority-gender participants. We did identify relationships between message author gender, communication style, and message engagement --- women and minority genders were more likely to have their messages engaged with, but only when using certain communication styles --- suggesting complex power dynamics exist on these platforms. We contribute the first study of gendered language styles on Enterprise Communication Platforms, adding to the community's understanding of how new settings and emerging technology relate to team collaborative dynamics, and motivating future tool development to support collaboration in diverse teams.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {324},
numpages = {29},
keywords = {enterprise communication platforms, gender, gendered linguistics, instant messaging, message engagement}
}

@proceedings{10.1145/3590777,
title = {EICC '23: Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stavanger, Norway}
}

@inproceedings{10.1145/3539618.3591883,
author = {Bernard, Nolwenn and Balog, Krisztian},
title = {MG-ShopDial: A Multi-Goal Conversational Dataset for e-Commerce},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591883},
doi = {10.1145/3539618.3591883},
abstract = {Conversational systems can be particularly effective in supporting complex information seeking scenarios with evolving information needs. Finding the right products on an e-commerce platform is one such scenario, where a conversational agent would need to be able to provide search capabilities over the item catalog, understand and make recommendations based on the user's preferences, and answer a range of questions related to items and their usage. Yet, existing conversational datasets do not fully support the idea of mixing different conversational goals (i.e., search, recommendation, and question answering) and instead focus on a single goal. To address this, we introduce MG-ShopDial: a dataset of conversations mixing different goals in the domain of e-commerce. Specifically, we make the following contributions. First, we develop a coached human-human data collection protocol where each dialogue participant is given a set of instructions, instead of a specific script or answers to choose from. Second, we implement a data collection tool to facilitate the collection of multi-goal conversations via a web chat interface, using the above protocol. Third, we create the MG-ShopDial collection, which contains 64 high-quality dialogues with a total of 2,196 utterances for e-commerce scenarios of varying complexity. The dataset is additionally annotated with both intents and goals on the utterance level. Finally, we present an analysis of this dataset and identify multi-goal conversational patterns.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2775–2785},
numpages = {11},
keywords = {conversational dataset, conversational information access, data collection methodology, multi-goal conversations},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@proceedings{10.1145/3647782,
title = {ICCMB '24: Proceedings of the 2024 7th International Conference on Computers in Management and Business},
year = {2024},
isbn = {9798400716652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3309182.3309191,
author = {El Aassal, Ayman and Verma, Rakesh},
title = {Spears Against Shields: Are Defenders Winning the Phishing War?},
year = {2019},
isbn = {9781450361781},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309182.3309191},
doi = {10.1145/3309182.3309191},
abstract = {Phishing is the act of using deceptive methods to lure users into taking harmful and dangerous actions against themselves and/or the company they work for. Hackers have been using this method to bypass various security systems, steal personal information, make sensitive data public, and all of this while raking millions of dollars. On the attack side, hackers are using phishing kits and employ all possible phishing techniques to build a successful campaign. On the defense side, we find spam/phishing filters and malicious website detection in addition to anti-phishing simulation training. In this study, we assess the current phishing landscape by testing the tools used on both sides of the war. We generated 1,000 phishing emails containing phishing links semi-automatically using natural language generation technology and tested five popular tools with anti-phishing modules. The number of undetected emails ranged from 77 to 927. We also evaluate several anti-phishing training technologies and reveal their shortcomings. Our results suggest that both anti-phishing filters and current training tools have a long way to go and thus improving these defense mechanisms is still essential.},
booktitle = {Proceedings of the ACM International Workshop on Security and Privacy Analytics},
pages = {15–24},
numpages = {10},
keywords = {anti-viruses, phishing, phishing tool comparison},
location = {Richardson, Texas, USA},
series = {IWSPA '19}
}

@article{10.1145/3386040,
author = {Laperdrix, Pierre and Bielova, Nataliia and Baudry, Benoit and Avoine, Gildas},
title = {Browser Fingerprinting: A Survey},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3386040},
doi = {10.1145/3386040},
abstract = {With this article, we survey the research performed in the domain of browser fingerprinting, while providing an accessible entry point to newcomers in the field. We explain how this technique works and where it stems from. We analyze the related work in detail to understand the composition of modern fingerprints and see how this technique is currently used online. We systematize existing defense solutions into different categories and detail the current challenges yet to overcome.},
journal = {ACM Trans. Web},
month = {apr},
articleno = {8},
numpages = {33},
keywords = {Browser fingerprinting, user privacy, web tracking}
}

@inproceedings{10.1145/3452296.3472928,
author = {Gigis, Petros and Calder, Matt and Manassakis, Lefteris and Nomikos, George and Kotronis, Vasileios and Dimitropoulos, Xenofontas and Katz-Bassett, Ethan and Smaragdakis, Georgios},
title = {Seven years in the life of Hypergiants' off-nets},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472928},
doi = {10.1145/3452296.3472928},
abstract = {Content Hypergiants deliver the vast majority of Internet traffic to end users. In recent years, some have invested heavily in deploying services and servers inside end-user networks. With several dozen Hypergiants and thousands of servers deployed inside networks, these off-net (meaning outside the Hypergiant networks) deployments change the structure of the Internet. Previous efforts to study them have relied on proprietary data or specialized per-Hypergiant measurement techniques that neither scale nor generalize, providing a limited view of content delivery on today's Internet.In this paper, we develop a generic and easy to implement methodology to measure the expansion of Hypergiants' off-nets. Our key observation is that Hypergiants increasingly encrypt their traffic to protect their customers' privacy. Thus, we can analyze publicly available Internet-wide scans of port 443 and retrieve TLS certificates to discover which IP addresses host Hypergiant certificates in order to infer the networks hosting off-nets for the corresponding Hypergiants. Our results show that the number of networks hosting Hypergiant off-nets has tripled from 2013 to 2021, reaching 4.5k networks. The largest Hypergiants dominate these deployments, with almost all of these networks hosting an off-net for at least one -- and increasingly two or more -- of Google, Netflix, Facebook, or Akamai. These four Hypergiants have off-nets within networks that provide access to a significant fraction of end user population.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {516–533},
numpages = {18},
keywords = {Hypergiants, TLS, content delivery networks, server deployment},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@proceedings{10.1145/3639856,
title = {AIMLSystems '23: Proceedings of the Third International Conference on AI-ML Systems},
year = {2023},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@proceedings{10.1145/3587259,
title = {K-CAP '23: Proceedings of the 12th Knowledge Capture Conference 2023},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 12th ACM International Conference on Knowledge Capture: K-CAP 2023, held in person on December 5th - 7th in Pensacola, Florida, US.Driven by the increasing demands for knowledge-based applications and the unprecedented availability of information from heterogeneous data sources, the study of knowledge capture is of crucial importance. Knowledge capture involves the extraction of useful knowledge from vast and diverse data sources as well as its acquisition directly from human experts.Nowadays knowledge is derived from an increasingly diverse set of data resources that differ with regard to their domain, format, quality, coverage, specificity, viewpoint, bias, and most importantly, consumers and producers of data. The heterogeneity, amount and complexity of data allow us to answer complex questions that could not be answered in isolation, requiring the interaction of different scientific fields and technologies. A goal of K-CAP is to develop such synergies using systematic and rigorous methodologies.The call for papers attracted 105 submissions from all over the world, covering a diverse range of topics spanning knowledge mining, large language models for information extraction, neuro-symbolic approaches for knowledge capture, knowledge engineering, question-answering, knowledge graphs, natural language processing, reasoning, entity linking, querying and knowledge-based applications. From a competitive set of high-quality submissions, we accepted 27 long research papers, 5 short papers, and 1 vision paper. The high-quality program is divided into 7 research sessions, in addition to 3 tutorials reflecting novel topics of interest in Knowledge Capture.We encourage everyone to attend the keynote talks that we have planned for K-CAP 2023. The highly anticipated talks by Dr. Robert R. Hoffman (Florida Institute for Human and Machine Cognition) and Dr. Jane Pinelis (Johns Hopkins University Applied Physics Laboratory) will guide us to a better understanding of the future of knowledge capture and explainable, resilient AI ecosystems, as they become commonplace in real world applications.},
location = {Pensacola, FL, USA}
}

@proceedings{10.1145/3450569,
title = {SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies},
year = {2021},
isbn = {9781450383653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the ACM Symposium on Access Control Models and Technologies (SACMAT 2021). This year's symposium continues its tradition of being the premier forum for the presentation of research results and experience reports on leading-edge issues of access control, including models, systems, applications, and theory, while also embracing a renovated focus on the general area of security.The aim of the symposium is to share novel access control and security solutions that fulfill the needs of heterogeneous applications and environments, and to identify new directions for future research and development.SACMAT provides researchers and practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and security.},
location = {Virtual Event, Spain}
}

@proceedings{10.1145/3639475,
title = {ICSE-SEIS'24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3649158,
title = {SACMAT 2024: Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 29th ACM Symposium on Access Control Models and Technologies (SACMAT 2024). This year's symposium continues its tradition of being the premier venue for presenting research results and experience reports on cutting edge advances on access control, including models, systems, applications, and theory, while also embracing an expanded focus on the general area of computer and information security and privacy. The overarching goal of the symposium is to share novel access control and computer security solutions that fulfill the needs of emerging applications and environments, and also to identify new directions for future research and development. ACM SACMAT provides researchers and also practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and computer security.},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3640544,
title = {IUI '24 Companion: Companion Proceedings of the 29th International Conference on Intelligent User Interfaces},
year = {2024},
isbn = {9798400705090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Greenville, SC, USA}
}

@inproceedings{10.1145/3131365.3131387,
author = {Iqbal, Umar and Shafiq, Zubair and Qian, Zhiyun},
title = {The ad wars: retrospective measurement and analysis of anti-adblock filter lists},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131387},
doi = {10.1145/3131365.3131387},
abstract = {The increasing popularity of adblockers has prompted online publishers to retaliate against adblock users by deploying anti-adblock scripts, which detect adblock users and bar them from accessing content unless they disable their adblocker. To circumvent anti-adblockers, adblockers rely on manually curated anti-adblock filter lists for removing anti-adblock scripts. Anti-adblock filter lists currently rely on informal crowdsourced feedback from users to add/remove filter list rules. In this paper, we present the first comprehensive study of anti-adblock filter lists to analyze their effectiveness against anti-adblockers. Specifically, we compare and contrast the evolution of two popular anti-adblock filter lists. We show that these filter lists are implemented very differently even though they currently have a comparable number of filter list rules. We then use the Internet Archive's Wayback Machine to conduct a retrospective coverage analysis of these filter lists on Alexa top-5K websites over the span of last five years. We find that the coverage of these filter lists has considerably improved since 2014 and they detect anti-adblockers on about 9% of Alexa top-5K websites. To improve filter list coverage and speedup addition of new filter rules, we also design and implement a machine learning based method to automatically detect anti-adblock scripts using static JavaScript code analysis.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {171–183},
numpages = {13},
keywords = {adblocking, anti-adblocking, javascript, machine learning, privacy, static code analysis, the wayback machine},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1145/3313831.3376654,
author = {G\'{o}mez-Zar\'{a}, Diego and Guo, Mengzi and DeChurch, Leslie A. and Contractor, Noshir},
title = {The Impact of Displaying Diversity Information on the Formation of Self-assembling Teams},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376654},
doi = {10.1145/3313831.3376654},
abstract = {Despite the benefits of team diversity, individuals often choose to work with similar others. Online team formation systems have the potential to help people assemble diverse teams. Systems can connect people to collaborators outside their networks, and features can quantify and raise the salience of diversity to users as they search for prospective teammates. But if we build a feature indicating diversity into the tool, how will people react to it? Two experiments manipulating the presence or absence of a "diversity score" feature within a teammate recommender demonstrate that, when present, individuals avoid collaborators who would increase team diversity in favor of those who lower team diversity. These results have important practical implications. Though the increased access to diverse teammates provided by recommender systems may benefit diversity, designers are cautioned against creating features that raise the salience of diversity as this information may undermine diversity.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {diversity, mixed-effect logistic regressions, social recommenders, team formation, teams},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/3635143,
author = {Paakki, Henna and Veps\"{a}l\"{a}inen, Heidi and Salovaara, Antti and Zafar, Bushra},
title = {Detecting Covert Disruptive Behavior in Online Interaction by Analyzing Conversational Features and Norm Violations},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3635143},
doi = {10.1145/3635143},
abstract = {Disruptive behavior is a prevalent threat to constructive online engagement. Covert behaviors, such as trolling, are especially challenging to detect automatically, because they utilize deceptive strategies to manipulate conversation. We illustrate a novel approach to their detection: analyzing conversational structures instead of focusing only on messages in isolation. Building on conversation analysis, we demonstrate that (1) conversational actions and their norms provide concepts for a deeper understanding of covert disruption, and that (2) machine learning, natural language processing and structural analysis of conversation can complement message-level features to create models that surpass earlier approaches to trolling detection. Our models, developed for detecting overt (aggression) as well as covert (trolling) behaviors using prior studies’ message-level features and new conversational action features, achieved high accuracies (0.90 and 0.92, respectively). The findings offer a theoretically grounded approach to computationally analyzing social media interaction and novel methods for effectively detecting covert disruptive conversations online.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jan},
articleno = {20},
numpages = {43},
keywords = {Disruptive behavior, detection, online aggression, online trolling, machine learning, natural language processing}
}

@inproceedings{10.1109/ICSE48619.2023.00066,
author = {Yin, Likang and Zhang, Xiyu and Filkov, Vladimir},
title = {On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00066},
doi = {10.1109/ICSE48619.2023.00066},
abstract = {Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability?From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {678–689},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3544548.3581548,
author = {Lashkari, Mitra and Cheng, Jinghui},
title = {“Finding the Magic Sauce”: Exploring Perspectives of Recruiters and Job Seekers on Recruitment Bias and Automated Tools},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581548},
doi = {10.1145/3544548.3581548},
abstract = {Automated recruitment tools are proliferating. While having the promise of improving efficiency, various risks, including bias, challenges the potential of these tools. An in-depth understanding of the perceived risk factors and needs from the perspective of both recruiters and job seekers is needed. We address this through an interview study in the high-tech industry to compare and contrast the concerns of these two roles. We found that the importance of clarifying position requirements and assessing candidates as “whole individuals” are commonly discussed by both recruiters and job seekers. In contrast, while recruiters tended to be more aware of cognitive bias and desired more tool support during interviews, job seekers voiced more desire towards a healthy candidate-company relationship. Additionally, both roles considered the uncertainty of the current technology capability and reduced human contact as concerns for using automated tools. Based on these results, we provided design implications for automated recruitment tools and related decision-support technologies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {868},
numpages = {16},
keywords = {Hiring, automated recruitment tools, bias, decision making, decision support system},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3184558.3186946,
author = {Krivosheev, Evgeny and Harandizadeh, Bahareh and Casati, Fabio and Benatallah, Boualem},
title = {Crowd-Machine Collaboration for Item Screening},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186946},
doi = {10.1145/3184558.3186946},
abstract = {In this paper we describe how crowd and machine classifier can be efficiently combined to screen items that satisfy a set of predicates. We show that this is a recurring problem in many domains, present machine-human (hybrid) algorithms that screen items efficiently and estimate the gain over human-only or machine-only screening in terms of performance and cost.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {95–96},
numpages = {2},
keywords = {crowdsourcing, human computation, hybrid classification, machine learning},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2998181.2998227,
author = {Yu, Bowen and Ren, Yuqing and Terveen, Loren and Zhu, Haiyi},
title = {Predicting Member Productivity and Withdrawal from Pre-Joining Attachments in Online Production Groups},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998227},
doi = {10.1145/2998181.2998227},
abstract = {Productive and dedicated members are critical to the success of online production communities like Wikipedia. Many communities organize in subgroups where members voluntarily work on projects of shared interest. In this paper, we investigate how members' pre-joining connections with the subgroup predict their productivity and withdrawal after joining. Drawing insights from attachment theories in social psychology, we examine two types of pre-joining connections: textit{identity-based} attachment (how much members' interests were aligned with the subgroup's topics) and textit{bonds-based} attachment (how much members had interacted with other members of the subgroup). Analyses of 79,704 editors in 1,341 WikiProjects show that 1) both identity-based and bonds-based attachment increased editors' post-joining productivity and reduced their likelihood of withdrawal; 2) identity-based attachment had a stronger effect on boosting direct contributions to articles while bonds-based attachment had a stronger effect on increasing article and project coordination, and reducing member withdrawal.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1775–1784},
numpages = {10},
keywords = {bonds-based attachment, identity-based attachment, online communities, online groups, peer production, pre-joining attachment, productivity, wikipedia, wikiproject, withdrawal},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3576915.3623080,
author = {Sheng, Peiyao and Wang, Xuechao and Kannan, Sreeram and Nayak, Kartik and Viswanath, Pramod},
title = {TrustBoost: Boosting Trust among Interoperable Blockchains},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623080},
doi = {10.1145/3576915.3623080},
abstract = {Currently there exist many blockchains with weak trust guarantees, limiting applications and participation. Existing solutions to boost the trust using a stronger blockchain, e.g., via checkpointing, requires the weaker blockchain to give up sovereignty. In this paper, we propose a family of protocols in which multiple blockchains interact to create a combined ledger with boosted trust. We show that even if several of the interacting blockchains cease to provide security guarantees, the combined ledger continues to be secure - our Trustboost protocols achieve the optimal threshold of tolerating the insecure blockchains. This optimality, along with the necessity of blockchain interactions, is formally shown within the classic shared memory model, tackling the long standing open challenge of solving consensus in the presence of both Byzantine objects and processes. Furthermore, our proposed construction of Trustboost simply operates via smart contracts and require no change to the underlying consensus protocols of the participating blockchains, a form of "consensus on top of consensus''. The protocols are lightweight and can be used on specific (e.g., high value) transactions; we demonstrate the practicality by implementing and deploying Trustboost as cross-chain smart contracts in the Cosmos ecosystem using approximately 3,000 lines of Rust code, made available as open source [52]. Our evaluation shows that using 10 Cosmos chains in a local testnet, Trustboost has a gas cost of roughly $2 with a latency of 2 minutes per request, which is in line with the cost on a high security chain such as Bitcoin or Ethereum.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1571–1584},
numpages = {14},
keywords = {consensus, cross-chain interoperability, smart contracts},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/2660267.2660315,
author = {Springall, Drew and Finkenauer, Travis and Durumeric, Zakir and Kitcat, Jason and Hursti, Harri and MacAlpine, Margaret and Halderman, J. Alex},
title = {Security Analysis of the Estonian Internet Voting System},
year = {2014},
isbn = {9781450329576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660267.2660315},
doi = {10.1145/2660267.2660315},
abstract = {Estonia was the first country in the world to use Internet voting nationally, and today more than 30% of its ballots are cast online. In this paper, we analyze the security of the Estonian I-voting system based on a combination of in-person election observation, code review, and adversarial testing. Adopting a threat model that considers the advanced threats faced by a national election system---including dishonest insiders and state-sponsored attacks---we find that the I-voting system has serious architectural limitations and procedural gaps that potentially jeopardize the integrity of elections. In experimental attacks on a reproduction of the system, we demonstrate how such attackers could target the election servers or voters' clients to alter election results or undermine the legitimacy of the system. Our findings illustrate the practical obstacles to Internet voting in the modern world, and they carry lessons for Estonia, for other countries considering adopting such systems, and for the security research community.},
booktitle = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security},
pages = {703–715},
numpages = {13},
keywords = {attacks, case studies, estonia, internet voting, security, security analysis, voting, vulnerabilities},
location = {Scottsdale, Arizona, USA},
series = {CCS '14}
}

@proceedings{10.1145/3555051,
title = {OpenSym '22: Proceedings of the 18th International Symposium on Open Collaboration},
year = {2022},
isbn = {9781450398459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@inproceedings{10.1109/ASE51524.2021.9678640,
author = {Gauthier, Ian X. and Lamothe, Maxime and Mussbacher, Gunter and McIntosh, Shane},
title = {Is historical data an appropriate benchmark for reviewer recommendation systems? a case study of the gerrit community},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678640},
doi = {10.1109/ASE51524.2021.9678640},
abstract = {Reviewer recommendation systems are used to suggest community members to review change requests. Like several other recommendation systems, it is customary to evaluate recommendations using held out historical data. While history-based evaluation makes pragmatic use of available data, historical records may be: (1) overly optimistic, since past assignees may have been suboptimal choices for the task at hand; or (2) overly pessimistic, since "incorrect" recommendations may have been equal (or even better) choices.In this paper, we empirically evaluate the extent to which historical data is an appropriate benchmark for reviewer recommendation systems. We replicate the cHRev and WLRRec approaches and apply them to 9,679 reviews from the Gerrit open source community. We then assess the recommendations with members of the Gerrit reviewing community using quantitative methods (personalized questionnaires about their comfort level with tasks) and qualitative methods (semi-structured interviews).We find that history-based evaluation is far more pessimistic than optimistic in the context of Gerrit review recommendations. Indeed, while 86% of those who had been assigned to a review in the past felt comfortable handling the review, 74% of those labelled as incorrect recommendations also felt that they would have been comfortable reviewing the changes. This indicates that, on the one hand, when reviewer recommendation systems recommend the past assignee, they should indeed be considered correct. Yet, on the other hand, recommendations labelled as incorrect because they do not match the past assignee may have been correct as well.Our results suggest that current reviewer recommendation evaluations do not always model the reality of software development. Future studies may benefit from looking beyond repository data to gain a clearer understanding of the practical value of proposed recommendations.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {30–41},
numpages = {12},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/2818048.2819962,
author = {Keegan, Brian C. and Lev, Shakked and Arazy, Ofer},
title = {Analyzing Organizational Routines in Online Knowledge Collaborations: A Case for Sequence Analysis in CSCW},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2819962},
doi = {10.1145/2818048.2819962},
abstract = {Research into socio-technical systems like Wikipedia has overlooked important structural patterns in the coordination of distributed work. This paper argues for a conceptual reorientation towards sequences as a fundamental unit of analysis for understanding work routines in online knowledge collaboration. We outline a research agenda for researchers in computer-supported cooperative work (CSCW) to understand the relationships, patterns, antecedents, and consequences of sequential behavior using methods already developed in fields like bio-informatics. Using a data set of 37,515 revisions from 16,616 unique editors to 96 Wikipedia articles as a case study, we analyze the prevalence and significance of different sequences of editing patterns. We illustrate the mixed method potential of sequence approaches by interpreting the frequent patterns as general classes of behavioral motifs. We conclude by discussing the methodological opportunities for using sequence analysis for expanding existing approaches to analyzing and theorizing about co-production routines in online knowledge collaboration.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing},
pages = {1065–1079},
numpages = {15},
keywords = {Wikipedia, online knowledge collaboration, organizational practice, peer production, routines, sequence analysis, socio-technical system},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@article{10.1145/3485532,
author = {Lubin, Justin and Chasins, Sarah E.},
title = {How statically-typed functional programmers write code},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485532},
doi = {10.1145/3485532},
abstract = {How working statically-typed functional programmers write code is largely understudied. And yet, a better understanding of developer practices could pave the way for the design of more useful and usable tooling, more ergonomic languages, and more effective on-ramps into programming communities. The goal of this work is to address this knowledge gap: to better understand the high-level authoring patterns that statically-typed functional programmers employ. We conducted a grounded theory analysis of 30 programming sessions of practicing statically-typed functional programmers, 15 of which also included a semi-structured interview. The theory we developed gives insight into how the specific affordances of statically-typed functional programming affect domain modeling, type construction, focusing techniques, exploratory and reasoning strategies, and expressions of intent. We conducted a set of quantitative lab experiments to validate our findings, including that statically-typed functional programmers often iterate between editing types and expressions, that they often run their compiler on code even when they know it will not successfully compile, and that they make textual program edits that reliably signal future edits that they intend to make. Lastly, we outline the implications of our findings for language and tool design. The success of this approach in revealing program authorship patterns suggests that the same methodology could be used to study other understudied programmer populations.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {155},
numpages = {30},
keywords = {functional programming, grounded theory, interviews, mixed methods, need-finding, qualitative, quantitative, randomized controlled trial, static types}
}

@inproceedings{10.1145/3368089.3418540,
author = {Costa Silva, Camila Mariane},
title = {Reusing software engineering knowledge from developer communication},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418540},
doi = {10.1145/3368089.3418540},
abstract = {Software development requires many different types of knowledge, such as knowledge about software development processes, practices and techniques, and about the domain of an application. Software, developers often share knowledge in informal communication channels (e.g., instant messaging tools, e-mails, or online forums). Considering that this informal communication contains knowledge that may be potentially relevant for other developers and given that this knowledge is not necessarily captured and formally documented for reuse, in this work we propose (a) exploring whether developer communication (via instant messaging) is a suitable source of reusable software engineering knowledge; (b) investigating how to identify that knowledge using data mining; (c) and analysing through action research how to present it to developers in a useful way for reuse. The envisioned theories and solutions approaches will analyze existing software development data captured in communication, rather than data that were captured and stored specifically to be reused.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1682–1685},
numpages = {4},
keywords = {developer communication, information needs, natural language processing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@proceedings{10.1145/3624062,
title = {SC-W '23: Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3630744,
title = {Websci Companion '24: Companion Publication of the 16th ACM Web Science Conference},
year = {2024},
isbn = {9798400704536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stuttgart, Germany}
}

@proceedings{10.1145/3563768,
title = {SPLASH Companion 2022: Companion Proceedings of the 2022 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
year = {2022},
isbn = {9781450399012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the SPLASH 2022! After two years of virtual only (SPLASH 2020), closed borders USA only (SPLASH 2021), we finally feel the reopening and going back to the pre-Covid in person vibe of the 37th OOPSLA/SPLASH. I am especially proud of having SPLASH outside of the USA/Canada region for the 3rd time in its history and the first time it is held in the Asia Pacific. We invited the Asian Symposium on Programming Languages and Systems (APLAS) to co-locate with us for the 3rd year in a row to celebrate this occasion.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3620666,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
abstract = {Welcome to the third volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is mostly dedicated to the 2024 fall cycle but also provides some statistics summarizing all three cycles.We introduced several notable changes to ASPLOS this year, most of which were discussed in our previous messages from program chairs in Volume 1 and 2, including: (1) significantly increasing the program committee size to over 220 members (more than twice the size of last year); (2) foregoing synchronous program committee (PC) meetings and instead making all decisions online; (3) overhauling the review assignment process; (4) developing an automated submission format violation identifier script that uncovers, e.g., disallowed vertical space manipulations that "squeeze" space; (5) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee; and (6) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it and highlighting how we believe that it should be handled in the future.Assuming readers have read our previous messages, here, we will only describe differences between the current cycle and the previous ones. These include: (1) Finally unifying submission and acceptance paper formatting instructions (forgoing the `jpaper' class) to rid authors of accepted papers from the need to reformat; (2) Describing the methodology we employed to select best papers, which we believe ensures quality and hope will persist; and (3) Reporting the ethical incidents we encountered and how we handled them. In the final, fourth volume, when the outcome of the ASPLOS'24 fall major revisions will become known, we plan to conduct a broader analysis of all the data we have gathered throughout the year.Following are some key statistics of the fall cycle: 340 submissions were finalized (43% more than last year's fall count and 17% less than our summer cycle) of which 111 are related to accelerators/FPGAs/GPUs, 105 to machine learning, 54 to security, 50 to datacenter/cloud and 50 to storage/memory; 183 (54%) submissions were promoted to the second review round; 39 (11.5%) papers were accepted (of which 19 were awarded artifact evaluation badges); 33 (9.7%) submissions were allowed to submit major revisions and are currently under review (these will be addressed in the fourth volume of ASPLOS'24 and will be presented in ASPLOS'25 if accepted); 1,368 reviews were uploaded; and 4,949 comments were generated during online discussions, of which 4,070 were dedicated to the submissions that made it to the second review round.This year, in the submission form, we asked authors to specify which of the three ASPLOS research areas are related to their submitted work. Analyzing this data revealed that 80%, 39%, and 29% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, generating the highest difference we have observed across the cycles between architecture and the other two. About 46% of the fall submissions are "interdisciplinary," namely, were associated with two or more of the three areas.Overall, throughout all the ASPLOS'24 cycles, we received 922 submissions, constituting a 1.54x increase compared to last year. Our reviewers submitted a total of 3,634 reviews containing more than 2.6 million words, and we also generated 12,655 online comments consisting of nearly 1.2 million words. As planned, PC members submitted an average of 15.7 reviews and a median of 15, and external review committee (ERC) members submitted an average of 4.7 and a median of 5.We accepted 170 papers thus far, written by 1100 authors, leading to an 18.4% acceptance rate, with the aforementioned 33 major revisions still under review. Assuming that the revision acceptance rate will be similar to that of previous cycles, we estimate that ASPLOS'24 will accept nearly 200 (!) papers, namely, 21%–22% of the submissions.The ASPLOS'24 program consists of 193 papers: the 170 papers we accepted thus far and, in addition, 23 major revisions from the fall cycle of ASPLOS'23, which were re-reviewed and accepted. The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@book{10.1145/3233795,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.}
}

@inproceedings{10.1145/3338906.3338935,
author = {Koyuncu, Anil and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and Le Traon, Yves},
title = {iFixR: bug report driven program repair},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338935},
doi = {10.1145/3338906.3338935},
abstract = {Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and- validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {314–325},
numpages = {12},
keywords = {Information retrieval, automatic patch generation, fault localization},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3424978.3425020,
author = {Zhong, Jie and Chen, Xiangning},
title = {Research on DDoS Attacks in IPv6},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425020},
doi = {10.1145/3424978.3425020},
abstract = {With the gradual replacement of IPv4 by IPv6, Distributed Denial-of-Service (DDoS) attacks that have plagued IPv4 appear in IPv6 more or less, and affect the normal operation of IPv6. However, the current research on how to construct a secure DDoS attack defense system under ipv6 is far away from satisfactory. This paper makes a detailed analysis of DDoS attacks from the aspect of causes, specific performance, architecture, classifies DDoS attacks according to the exploited vulnerabilities and analyzes the typical attack methods respectively. At the same time, the improvements and defects of IPv6 are further studied, along with the way to defend, summarizes how to defend against DDoS attacks in IPv6 from two aspects, and analyze the advantages and disadvantages of each scheme. Finally, how to build a more secure network security defense system in IPv6 is prospected.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {42},
numpages = {6},
keywords = {DDoS, Defense technology, DoS, IPv6, Network security},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.1145/3488560.3502190,
author = {Butler, Rhys and Duggirala, Vishnu Dutt and Banaei-Kashani, Farnoush},
title = {iLFQA: A Platform for Efficient and Accurate Long-Form Question Answering},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502190},
doi = {10.1145/3488560.3502190},
abstract = {We present an efficient and accurate long-form question-answering platform, dubbed iLFQA (i.e., short for intelligent Long-Form Question Answering). The purpose of iLFQA is to function as a platform which accepts unscripted questions and efficiently produces semantically meaningful, explanatory, and accurate long-form responses. iLFQA consists of a number of modules for zero-shot classification, text retrieval, and text generation to generate answers to questions based on an open-domain knowledge base. iLFQA is unique in the question answering space because it is an example of a deployable and efficient long-form question answering system. Question answering systems exist in many forms, but long-form question answering remains relatively unexplored, and to the best of our knowledge none of the existing long-form question answering systems are shown to be sufficiently efficient to be deployable. We have made the source code and implementation details of iLFQA available for the benefit of researchers and practitioners in this field. With this demonstration, we present iLFQA as an open-domain, deployable, and accurate open-source long-form question answering platform.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1565–1568},
numpages = {4},
keywords = {generalized language models, long-form question answering, natural language processing, text generation, text retrieval},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3487553,
title = {WWW '22: Companion Proceedings of the Web Conference 2022},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Lyon, France}
}

@inproceedings{10.1145/2934732.2934736,
author = {Pamp\'{\i}n, Humberto Jes\'{u}s Corona and Peleteiro, Ana},
title = {A Time-Aware Exploration of RecSys15 Challenge Dataset},
year = {2016},
isbn = {9781450341417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934732.2934736},
doi = {10.1145/2934732.2934736},
abstract = {E-commerce is currently one of the main applications of recommender systems, since it generates vast amounts of data that can be used to make predictions and analyse users's behaviour. In this paper we present an overview of the public dataset used for the RecSys Challenge 2015. We describe the basic statistical properties of this dataset and how events (clicks and purchases) are distributed over products (items) and users (sessions). We also present a time-aware analysis of the dataset, with the aim to better understand the change of user behaviour within time cycles, and how it affects the activity in user purchases. We further study the relation between categories, the solely type of metadata present in this completely anonymised dataset. We are interested both in how these categories are distributed and how users and items interact with them. Finally, along the paper we explain the implications that the results obtained from our analysis may have when building models for the challenge.},
booktitle = {Proceedings of the 4th Spanish Conference on Information Retrieval},
articleno = {12},
numpages = {4},
keywords = {Dataset exploration, E-commerce, Recommender Systems},
location = {Granada, Spain},
series = {CERI '16}
}

@inproceedings{10.1145/3233391.3233965,
author = {Kaffee, Lucie-Aim\'{e}e and Simperl, Elena},
title = {Analysis of Editors' Languages in Wikidata},
year = {2018},
isbn = {9781450359368},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233391.3233965},
doi = {10.1145/3233391.3233965},
abstract = {Wikidata is unique as a knowledge base as well as a community given its users contribute together to one cross-lingual project. To create a truly multilingual knowledge base, a variety of languages of contributors is needed. In this paper, we investigate the language distribution in Wikidata's editors, how it relates to Wikidata's content and the users' label editing. This gives us an insight into its community that can help supporting users working on multilingual projects.},
booktitle = {Proceedings of the 14th International Symposium on Open Collaboration},
articleno = {21},
numpages = {5},
keywords = {Community, Multilinguality, Wikidata},
location = {Paris, France},
series = {OpenSym '18}
}

@inproceedings{10.1109/ICSE43902.2021.00033,
author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
title = {An Empirical Study of Refactorings and Technical Debt in Machine Learning Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00033},
doi = {10.1109/ICSE43902.2021.00033},
abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {238–250},
numpages = {13},
keywords = {empirical studies, machine learning systems, refactoring, software repository mining, technical debt},
location = {Madrid, Spain},
series = {ICSE '21}
}

@proceedings{10.1145/3551357,
title = {PPDP '22: Proceedings of the 24th International Symposium on Principles and Practice of Declarative Programming},
year = {2022},
isbn = {9781450397032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tbilisi, Georgia}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3568444,
title = {MUM '22: Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3490100.3516458,
author = {H\"{a}m\"{a}l\"{a}inen, Perttu and Tavast, Mikke and Kunnari, Anton},
title = {Neural Language Models as What If? -Engines for HCI Research},
year = {2022},
isbn = {9781450391450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490100.3516458},
doi = {10.1145/3490100.3516458},
abstract = {Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) and user experience (UX) research. In this poster paper, we explore and critically evaluate the potential of large-scale neural language models like GPT-3 in generating synthetic research data such as participant responses to interview questions. We observe that in the best case, GPT-3 can create plausible reflections of video game experiences and emotions, and adapt its responses to given demographic information. Compared to real participants, such synthetic data can be obtained faster and at a lower cost. On the other hand, the quality of generated data has high variance, and future work is needed to rigorously quantify the human-likeness, limitations, and biases of the models in the HCI domain.},
booktitle = {Companion Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {77–80},
numpages = {4},
keywords = {GPT-3, Language models, User experience, User models},
location = {Helsinki, Finland},
series = {IUI '22 Companion}
}

@inproceedings{10.1145/3356395.3365539,
author = {Brelsford, Christa and Thakur, Gautam and Arthur, Rudy and Williams, Hywel},
title = {Using Digital Trace Data to Identify Regions and Cities},
year = {2019},
isbn = {9781450369541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356395.3365539},
doi = {10.1145/3356395.3365539},
abstract = {A greater understanding of human dynamics as they play out in both physical space and through interpersonal communication is vital for the design and development of intelligent and resilient cities. Physical context provides insight into the space-time distribution of population and their activity patterns, while interpersonal communication can now be measured at the population scale through digital interactions. In this work, we propose a novel method to discover these dynamics. We use a dataset of 72 million tweets to develop a spatially embedded network of communication, and then use community detection algorithms to explore regional and urban delineation in the United States. We compare these results to US census regions and economic and infrastructural networks. We find that the broad spatial delineation of communities and sub-communities is consistent with United States regions, states, and major metropolitan areas. We describe how these methods could be extended to generate a measure of social regions that can be consistently applied anywhere there is a sufficiently rich data source. A deeper understanding of urban social structure measured by spatially embedded communication networks can enable a better understanding of the interactions between urban social and physical contexts. This, in turn, may enable urban managers and policy makers to identify strategies for supporting urban resilience.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances on Resilient and Intelligent Cities},
pages = {5–8},
numpages = {4},
keywords = {Cities, Communities, Digital Trace Data, Networks, Twitter},
location = {Chicago, IL, USA},
series = {ARIC'19}
}

@proceedings{10.1145/3597638,
title = {ASSETS '23: Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3570945,
title = {IVA '23: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the papers presented at the 23nd International Conference on Intelligent Virtual Agents (IVA 2023) located in W\"{u}rzburg, Germany, from 19. to 22.09.2023.},
location = {W\"{u}rzburg, Germany}
}

@proceedings{10.1145/3627050,
title = {IoT '23: Proceedings of the 13th International Conference on the Internet of Things},
year = {2023},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3488932.3523262,
author = {Wyss, Elizabeth and Wittman, Alexander and Davidson, Drew and De Carli, Lorenzo},
title = {Wolf at the Door: Preventing Install-Time Attacks in npm with Latch},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3523262},
doi = {10.1145/3488932.3523262},
abstract = {The npm software ecosystem allows developers to easily import code written by others. However, manual vetting of every individual installed component is made difficult in many cases by the number of transitive dependencies brought in by installing popular packages. This has enabled attackers to propagate malicious code by hiding it deep into the dependency chains of popular packages. A particularly dangerous form of attack comes from malicious code embedded into package install scripts.We tackle the problem of preventing undesirable install-time behavior by proposing Latch, a system for mediating install-time capabilities of npm packages. Latch generates permission manifests summarizing each package's install-time behavior and checks them against user-defined policies to ensure compliance. Policies in Latch are expressed in a rich formal policy language that covers a broad range of use cases. Our key insight is that expressive Latch policies empower users to define and enforce their own individualized security needs.Evaluation of practical Latch policies on all publicly available npm packages and on a number of real-world attack packages demonstrates that our approach is effective in identifying and stopping unwanted behavior while minimizing disruption due to undesired alerts.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {1139–1153},
numpages = {15},
keywords = {install-time attack, npm, policy language, supply chain security},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@proceedings{10.1145/3610977,
title = {HRI '24: Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome one and all to the 19th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI)!We are so pleased to re-welcome the HRI community to Boulder, Colorado, where HRI 2021 would have been held, had the COVID pandemic not interfered. Following up on the successful in-person conference held last year in Sweden, this year's theme is "HRI in the Real World," and focuses on advances that aim to bring human-robot interaction out of the lab and into everyday life.One aspect of this that we are very excited about is the introduction of a robot challenge to the conference activities, where teams from around the world will showcase their research and development via actual, interactive robots in the "real world" of an academic conference. It is our hope that this feature will grow and develop over the coming years into a staple of the HRI conference.This year's HRI conference saw an impressive surge in global interest, with 352 full paper submissions from around the world, marking a significant 40% increase compared to the previous year. These papers were categorized under relevant thematic subcommittees and underwent a double-blind review process, a rebuttal phase, and selective shepherding by the HRI program committee. From this process, 87 outstanding papers (24.7%) were chosen for full presentation at the conference. Reflecting our joint sponsorship with IEEE and ACM, all accepted papers will be accessible in the ACM Digital Library and IEEE Xplore.},
location = {Boulder, CO, USA}
}

@proceedings{10.1145/3631802,
title = {Koli Calling '23: Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
year = {2023},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Koli, Finland}
}

@inproceedings{10.1145/3297280.3297323,
author = {Corsi, Pietro and Lagorio, Giovanni and Ribaudo, Marina},
title = {TickEth, a ticketing system built on ethereum},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297323},
doi = {10.1145/3297280.3297323},
abstract = {We propose TickEth, a system that aims at mitigating some of the problems encountered by the ticketing industry. The name TickEth is built by combining the words ticket, its application domain, and Ethereum, the underlying platform we adopted for the development of the prototype.Nowadays, ticketing ecosystem is wide and fragmented, and faces several problems. TickEth exploits Ethereum smart contracts to tackle the inability of checking the authenticity of tickets sold online (that can be fake or duplicates of a real ticket), the wild price range of resold tickets in the secondary market, and the unwieldy refund procedures.TickEth is an open specification, where different third-party clients can freely interact, and can be instantiated by different vendors as they like. We implemented a proof-of-concept prototype, available at: https://github.com/H221/TickEth},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {409–416},
numpages = {8},
keywords = {blockchain, dApps, ethereum, smart contract, ticketing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@proceedings{10.1145/3581754,
title = {IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3551349.3560416,
author = {Xie, Fuman and Zhang, Yanjun and Yan, Chuan and Li, Suwan and Bu, Lei and Chen, Kai and Huang, Zi and Bai, Guangdong},
title = {Scrutinizing Privacy Policy Compliance of Virtual Personal Assistant Apps},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560416},
doi = {10.1145/3551349.3560416},
abstract = {A large number of functionality-rich and easily accessible applications have become popular among various virtual personal assistant&nbsp;(VPA) services such as Amazon Alexa. VPA applications&nbsp;(or VPA apps for short) are accompanied by a privacy policy document that informs users of their data handling practices. These documents are usually lengthy and complex for users to comprehend, and developers may intentionally or unintentionally fail to comply with them. In this work, we conduct the first systematic study on the privacy policy compliance issue of VPA apps. We develop Skipper, which targets Amazon Alexa skills. It automatically depicts the skill into the declared privacy profile by analyzing their privacy policy documents with Natural Language Processing&nbsp;(NLP) and machine learning techniques, and derives the behavioral privacy profile of the skill through a black-box testing. We conduct a large-scale analysis on all skills listed on Alexa store, and find that a large number of skills suffer from the privacy policy noncompliance issues.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {90},
numpages = {13},
keywords = {Alexa skills, Virtual Personal Assistant, privacy compliance},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@proceedings{10.1145/3568812,
title = {ICER '23: Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
location = {Chicago, IL, USA}
}

@inproceedings{10.1109/ASE51524.2021.9678902,
author = {Sartaj, Hassan},
title = {Automated approach for system-level testing of unmanned aerial systems},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678902},
doi = {10.1109/ASE51524.2021.9678902},
abstract = {Unmanned aerial systems (UAS) have a large number of applications in civil and military domains. UAS rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics systems, including software systems. The current industrial practice is to manually create test scenarios, manually or automatically execute these scenarios using simulators, and manual evaluation of the outcomes. A fundamental part of system-level testing of such systems is the simulation of environmental context. The test scenarios typically consist of setting certain environment conditions and testing the system under test in these settings. The state-of-the-art approaches available for this purpose also require manual test scenario development and manual test evaluation. In this research work, we propose an approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test scenarios are generated on the fly, i.e., during test execution based on the environmental context at runtime. We develop a toolset to support automation. We perform a pilot experiment using a widely-used open-source autopilot, ArduPilot. The preliminary results show that the AITester is effective and efficient in violating autopilot expected behavior.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1069–1073},
numpages = {5},
keywords = {artificial intelligence, autopilot, model-based testing, reinforcement learning, unmanned aerial systems},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/3417564.3417575,
author = {Langdon, William B. and Weimer, Westley and Petke, Justyna and Fredericks, Erik and Lee, Seongmin and Winter, Emily and Basios, Michail and Cohen, Myra B. and Blot, Aymeric and Wagner, Markus and Bruce, Bobby R. and Yoo, Shin and Gerasimou, Simos and Krauss, Oliver and Huang, Yu and Gerten, Michael},
title = {Genetic Improvement @ ICSE 2020},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3417564.3417575},
doi = {10.1145/3417564.3417575},
abstract = {Following Prof. Mark Harman of Facebook's keynote and formal presentations (which are recorded in the proceed- ings) there was a wide ranging discussion at the eighth inter- national Genetic Improvement workshop, GI-2020 @ ICSE (held as part of the International Conference on Software En- gineering on Friday 3rd July 2020). Topics included industry take up, human factors, explainabiloity (explainability, jus- tifyability, exploitability) and GI benchmarks. We also con- trast various recent online approaches (e.g. SBST 2020) to holding virtual computer science conferences and workshops via the WWW on the Internet without face to face interac- tion. Finally we speculate on how the Coronavirus Covid-19 Pandemic will a ect research next year and into the future.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {24–30},
numpages = {7}
}

@inproceedings{10.1145/3556223.3556253,
author = {Hidayat, Taufik Safar and Fadillah, Yoga and Lubis, Muharman and Lubis, Arif Ridho},
title = {Distributed Denial of Service (DDoS) Mitigation: Security Awareness Domain and Resources (SADAR)},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556223.3556253},
doi = {10.1145/3556223.3556253},
abstract = {Information security is a constantly evolving field to safeguard confidentiality, integrity and assets with defensive measures should be developed and planned to improve information security. Therefore, it is very important for a company to update its security system and defense techniques from attacks that occur. Furthermore, Distributed Denial of Service (DDoS) attack is a major threat that can disrupt and hinder legitimate service requests on a network. They were first reported in 1996 and until now the complexity and sophistication of these attacks is increasing, which are usually used to force a site or service to go offline, that is, by passing high data traffic until the server cannot work. Meanwhile, In August 2021, Microsoft claimed to have succeeded in overcoming the largest DDoS attack reaching 2 TBps. Moreover, with the increasing number of DDoS attacks, a mitigation scenario is needed to deal with the complexity and technical development of this DDoS. In the network world, detecting a DDoS somehow, is extremely difficult because the attempt should distinguish between normal traffic and DDoS attacks. Thus, some recommended security solutions will be compiled in this paper to deliver solution involving the mitigation.},
booktitle = {Proceedings of the 10th International Conference on Computer and Communications Management},
pages = {200–206},
numpages = {7},
keywords = {DDoS, Information Security, Mitigation, Network},
location = {Okayama, Japan},
series = {ICCCM '22}
}

@inproceedings{10.1145/3488560.3502182,
author = {Soprano, Michael and Roitero, Kevin and Bombassei De Bona, Francesco and Mizzaro, Stefano},
title = {Crowd_Frame: A Simple and Complete Framework to Deploy Complex Crowdsourcing Tasks Off-the-shelf},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502182},
doi = {10.1145/3488560.3502182},
abstract = {Due to their relatively low cost and ability to scale, crowdsourcing based approaches are widely used to collect a large amount of human annotated data. To this aim, multiple crowdsourcing platforms exist, where requesters can upload tasks and workers can carry them out and obtain payment in return. Such platforms share a task design and deploy workflow that is often counter-intuitive and cumbersome. To address this issue, we propose Crowd_Frame, a simple and complete framework which allows to develop and deploy diverse types of complex crowdsourcing tasks in an easy and customizable way. We show the abilities of the proposed framework and we make it available to researchers and practitioners.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1605–1608},
numpages = {4},
keywords = {crowdsourcing, framework, user behavior},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@proceedings{10.1145/3579856,
title = {ASIA CCS '23: Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3573051.3596191,
author = {Smolansky, Adele and Cram, Andrew and Raduescu, Corina and Zeivots, Sandris and Huber, Elaine and Kizilcec, Rene F.},
title = {Educator and Student Perspectives on the Impact of Generative AI on Assessments in Higher Education},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596191},
doi = {10.1145/3573051.3596191},
abstract = {The sudden popularity and availability of generative AI tools, such as ChatGPT that can write compelling essays on any topic, code in various programming languages, and ace standardized tests across domains, raises questions about the sustainability of traditional assessment practices. To seize this opportunity for innovation in assessment practice, we conducted a survey to understand both the educators' and students' perspectives on the issue. We measure and compare attitudes of both stakeholders across various assessment scenarios, building on an established framework for examining the quality of online assessments along six dimensions. Responses from 389 students and 36 educators across two universities indicate moderate usage of generative AI, consensus for which types of assessments are most impacted, and concerns about academic integrity. Educators prefer adapted assessments that assume AI will be used and encourage critical thinking, but students' reaction is mixed, in part due to concerns about a loss of creativity. The findings show the importance of engaging educators and students in assessment reform efforts to focus on the process of learning over its outputs, higher-order thinking, and authentic applications.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {378–382},
numpages = {5},
keywords = {ChatGPT, assessment, educators, generative AI, students, survey},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/2911451.2914757,
author = {Pham, Kien and Santos, A\'{e}cio and Freire, Juliana},
title = {Understanding Website Behavior based on User Agent},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914757},
doi = {10.1145/2911451.2914757},
abstract = {Web sites have adopted a variety of adversarial techniques to prevent web crawlers from retrieving their content. While it is possible to simulate users behavior using a browser to crawl such sites, this approach is not scalable. Therefore, understanding existing adversarial techniques is important to design crawling strategies that can adapt to retrieve the content as efficiently as possible. Ideally, a web crawler should detect the nature of the adversarial policies and select the most cost-effective means to defeat them.In this paper, we discuss the results of a large-scale study of web site behavior based on their responses to different user-agents. We issued over 9 million HTTP GET requests to 1.3 million unique web sites from DMOZ using six different user-agents and the TOR network as an anonymous proxy. We observed that web sites do change their responses depending on user-agents and IP addresses. This suggests that probing sites for these features can be an effective means to detect adversarial techniques.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1053–1056},
numpages = {4},
keywords = {adversarial crawling, focused crawler, stealth crawling, user agent, web cloaking, web crawler detection},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1145/3347092,
author = {Hertzmann, Aaron},
title = {Computers do not make art, people do},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3347092},
doi = {10.1145/3347092},
abstract = {The continually evolving relationship between artistic technologies and artists.},
journal = {Commun. ACM},
month = {apr},
pages = {45–48},
numpages = {4}
}

@book{10.1145/3617448,
author = {Myers, Brad A.},
title = {Pick, Click, Flick! The Story of Interaction Techniques},
year = {2024},
isbn = {9798400709494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {57},
abstract = {This book provides a comprehensive study of the many ways to interact with computers and computerized devices. An “interaction technique” starts when the user performs an action that causes an electronic device to respond, and includes the direct feedback from the device to the user. Examples include physical buttons and switches, on-screen menus and scrollbars operated by a mouse, touchscreen widgets, gestures such as flick-to-scroll, text entry on computers and touchscreens, input for virtual reality systems, interactions with conversational agents such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, and adaptations of all of these for people with disabilities. Pick, Click, Flick! is written for anyone interested in interaction techniques, including computer scientists and designers working on human-computer interaction, as well as implementers and consumers who want to understand and get the most out of their digital devices.REVIEWS“Pick, Click, Flick! is an impressive reference manual of the many years of interaction design development. It is a reference book, invaluable when questions arise, whether while you are busy designing something, or learning, or teaching, where assigning sections of the reference will be a valuable resource and learning tool for students. Brad Myers has provided a great service to the interaction community.” ‐ Don Norman, Distinguished Prof. Emeritus, Design Lab, University of California, San Diego“Every UX professional should immerse themselves in this book. Not only does it unravel the fascinating and complex history of GUI widgets that will captivate any user interface nerd, but it also stands as the definitive guide to an incredibly diverse array of interaction techniques. This is not just an engaging read; it’s an essential toolkit.” ‐ Jakob Nielsen, Principal, Nielsen Design Group}
}

@inproceedings{10.1145/3290480.3290491,
author = {Jiang, Yikun and Xie, Wei and Tang, Yong},
title = {Detecting Authentication-Bypass Flaws in a Large Scale of IoT Embedded Web Servers},
year = {2018},
isbn = {9781450365673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290480.3290491},
doi = {10.1145/3290480.3290491},
abstract = {With the rapid development of network and communication technologies, everything is able to be connected to the Internet. IoT devices, which include home routers, IP cameras, wireless printers and so on, are crucial parts facilitating to build pervasive and ubiquitous networks. As the number of IoT devices around the world increases, the security issues become more and more serious.To handle with the security issues and protect the IoT devices from being compromised, the firmware of devices needs to be strengthened by discovering and repairing vulnerabilities. Current vulnerability detection tools can only help strengthening traditional software, nevertheless these tools are not practical enough for IoT device firmware, because of the peculiarity in firmware's structure and embedded device's architecture. Therefore, new vulnerability detection framework is required for analyzing IoT device firmware.This paper reviews related works on vulnerability detection in IoT firmware, proposes and implements a framework to automatically detect authentication-bypass flaws in a large scale of Linux-based firmware. The proposed framework is evaluated with a data set of 2351 firmware images from several target vendors, which is proved to be capable of performing large-scale and automated analysis on firmware, and 1 known and 10 unknown authentication-bypass flaws are found by the analysis.},
booktitle = {Proceedings of the 8th International Conference on Communication and Network Security},
pages = {56–63},
numpages = {8},
keywords = {IoT firmware, Vulnerability detection, automated, large scale},
location = {Qingdao, China},
series = {ICCNS '18}
}

@proceedings{10.1145/3547522,
title = {NordiCHI '22: Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450394482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3584871,
title = {ICSIM '23: Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Palmerston North, New Zealand}
}

@article{10.1145/3529507,
author = {Monniaux, David and Six, Cyril},
title = {Formally Verified Loop-Invariant Code Motion and Assorted Optimizations},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3529507},
doi = {10.1145/3529507},
abstract = {We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination. This approach is lightweight: each pass comes with a simple and independent proof of correctness. Experiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers. Our static analysis employs an efficient yet verified hashed set structure, resulting in the fast compilation.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {dec},
articleno = {3},
numpages = {27},
keywords = {Verified compilation, common subexpression elimination, CompCert}
}

@proceedings{10.1145/3604915,
title = {RecSys '23: Proceedings of the 17th ACM Conference on Recommender Systems},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1109/ASONAM55673.2022.10068665,
author = {Elmas, Tu\u{g}rulcan and Ibanez, Thomas Romain and Hutter, Alexandre and Overdorf, Rebekah and Aberer, Karl},
title = {WayPop Machine: A Wayback Machine to Investigate Popularity and Root Out Trolls},
year = {2023},
isbn = {9781665456616},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM55673.2022.10068665},
doi = {10.1109/ASONAM55673.2022.10068665},
abstract = {Contrary to celebrities who owe their popularity online to their activity offline, malicious users such as trolls have to gain fame on social media through the social media itself. The exact reasons that a certain user has become popular are often obscure especially when the popularity was gained illicitly through means such as fake amplification of content. In this paper, we develop a methodology for uncovering why an account has become popular and present an open source tool that encapsulates this methodology. This tool aims to aid others in uncovering malicious accounts which have artificially gained many followers and to distinguish such accounts from those which gained followers and popularity honestly.},
booktitle = {Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {391–395},
numpages = {5},
keywords = {trolls, osint, tool, data visualization, twitter, social media, social cybersecurity, online social networks},
location = {Istanbul, Turkey},
series = {ASONAM '22}
}

@inproceedings{10.1145/3195836.3195842,
author = {Kovalenko, Vladimir and Bacchelli, Alberto},
title = {Code review for newcomers: is it different?},
year = {2018},
isbn = {9781450357258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195836.3195842},
doi = {10.1145/3195836.3195842},
abstract = {Onboarding is a critical stage in the tenure of software developers with a project, because meaningful contribution requires familiarity with the codebase. Some software teams employ practices, such as mentoring, to help new developers get accustomed faster. Code review, i.e., the manual inspection of code changes, is an opportunity for sharing knowledge and helping with onboarding.In this study, we investigate whether and how contributions from developers with low experience in a project do receive a different treatment during code review. We compare reviewers' experience, metrics of reviewers' attention, and change merge rate between changes from newcomers and from more experienced authors in 60 active open source projects. We find that the only phenomenon that is consistent across the vast majority of projects is a lower merge rate for newcomers' changes.},
booktitle = {Proceedings of the 11th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {29–32},
numpages = {4},
location = {Gothenburg, Sweden},
series = {CHASE '18}
}

@inproceedings{10.1145/3415959.3415998,
author = {Felicioni, Nicol\`{o} and Donati, Andrea and Conterio, Luca and Bartoccioni, Luca and Hu, Davide Yi Xian and Bernardis, Cesare and Ferrari Dacrema, Maurizio},
title = {Multi-Objective Blended Ensemble For Highly Imbalanced Sequence Aware Tweet Engagement Prediction},
year = {2020},
isbn = {9781450388351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415959.3415998},
doi = {10.1145/3415959.3415998},
abstract = {In this paper we provide a description of the methods we used as team BanaNeverAlone for the ACM RecSys Challenge 2020, organized by Twitter. The challenge addresses the problem of user engagement prediction: the goal is to predict the probability of a user engagement (Like, Reply, Retweet or Retweet with comment), based on a series of past interactions on the Twitter platform. Our proposed solution relies on several features that we extracted from the original dataset, as well as on consolidated models, such as gradient boosting for decision trees and neural networks. The ensemble model, built using blending, and a multi-objective optimization allowed our team to rank in position 4.},
booktitle = {Proceedings of the Recommender Systems Challenge 2020},
pages = {29–33},
numpages = {5},
keywords = {ACM RecSys Challenge 2020, Blending, Gradient Boosting for Decision Trees, Neural Networks, Recommender Systems},
location = {Virtual Event, Brazil},
series = {RecSysChallenge '20}
}

@proceedings{10.1145/2983990,
title = {OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3546155,
title = {NordiCHI '22: Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3581641,
title = {IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@article{10.1145/3075645,
author = {Christy, Matthew and Gupta, Anshul and Grumbach, Elizabeth and Mandell, Laura and Furuta, Richard and Gutierrez-Osuna, Ricardo},
title = {Mass Digitization of Early Modern Texts With Optical Character Recognition},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3075645},
doi = {10.1145/3075645},
abstract = {Optical character recognition (OCR) engines work poorly on texts published with premodern printing technologies. Engaging the key technological contributors from the IMPACT project, an earlier project attempting to solve the OCR problem for early modern and modern texts, the Early Modern OCR Project (eMOP) of Texas A8M received funding from the Andrew W. Mellon Foundation to improve OCR outputs for early modern texts from the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) proprietary database products—or some 45 million pages. Added to print problems are the poor quality of the page images in these collections, which would be too time consuming and expensive to reimage. This article describes eMOP's attempts to OCR 307,000 documents digitized from microfilm to make our cultural heritage available for current and future researchers. We describe the reasoning behind our choices as we undertook the project based on other relevant studies; discoveries we made; the data and the system we developed for processing it; the software, algorithms, training procedures, and tools that we developed; and future directions that should be taken for further work in developing OCR engines for cultural heritage materials.},
journal = {J. Comput. Cult. Herit.},
month = {dec},
articleno = {6},
numpages = {25},
keywords = {Machine learning, digital humanities}
}

@proceedings{10.1145/3643833,
title = {WiSec '24: Proceedings of the 17th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
year = {2024},
isbn = {9798400705823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2024 ACM Conference on Security and Privacy in Wireless and Mobile Networks (ACM WiSec)!Now in its 17th year, WiSec continues to be the premier venue for research on all aspects of security and privacy in wireless and mobile networks, their systems, and their applications. We are hosted by the Korea Institute of Information Security &amp; Cryptology, located in the city center of Seoul, Korea - a city known for its dynamic mix of 600-year-old palaces and the contemporary urban landscape characterized by towering skyscrapers.We begin our exciting three-day main conference program on May 27th with single-track technical paper sessions, a poster and demo session, two excellent keynotes from telecommunication security expert Prof. Jean-Pierre Seifert (TU Berlin) and wireless security expert Mathy Vanhoef (KU Leuven), and a panel on wireless security and AI. Three invited talks named "Vision Talk" discuss the future of wireless and mobile security issues. The WiseML Workshop follows the main program on May 30th. We invite participants to attend the exciting paper presentations and keynotes, interact with the presenters during the Q&amp;A sessions after each talk, network during the coffee breaks and lunches each day, and socialize during the banquet dinner.},
location = {Seoul, Republic of Korea}
}

@inproceedings{10.1145/3544548.3580683,
author = {Mcnutt, Andrew M and Outkine, Anton and Chugh, Ravi},
title = {A Study of Editor Features in a Creative Coding Classroom},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580683},
doi = {10.1145/3544548.3580683},
abstract = {Creative coding is a rapidly expanding domain for both artistic expression and computational education. Numerous libraries and IDEs support creative coding, however there has been little consideration of how the environments themselves might be designed to serve these twin goals. To investigate this gap, we implemented and used an experimental editor to teach a sequence of college and high-school creative coding courses. In the first year, we conducted a log analysis of student work (n=39) and surveys regarding prospective features (n=25). These guided our implementation of common enhancements (e.g. color pickers) as well as uncommon ones (e.g. bidirectional shape editing). In the second year, we studied the effects of these features through logging (n=39+) and survey (n=23) studies. Reflecting on the results, we identify opportunities to improve creativity- and novice-focused IDEs and highlight tensions in their design—as in tools that augment artistry or efficiency but may be perceived as hindering learning.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {121},
numpages = {15},
keywords = {Code editors, Creative coding, Introductory programming, p5},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3569219,
title = {Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3625078,
title = {BIOTC '23: Proceedings of the 2023 5th Blockchain and Internet of Things Conference},
year = {2023},
isbn = {9798400708213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Osaka, Japan}
}

@inproceedings{10.5555/3237383.3237878,
author = {Papapanagiotou, Petros and Davoust, Alan and Murray-Rust, Dave and Manataki, Areti and Van Kleek, Max and Shadbolt, Nigel and Robertson, Dave},
title = {Social Machines for All},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In today's interconnected world, people interact to a unprecedented degree through the use of digital platforms and services, forming complex 'social machines'. These are now homes to autonomous agents as well as people, providing an open space where human and computational intelligence can mingle---a new frontier for distributed agent systems. However, participants typically have limited autonomy to define and shape the machines they are part of. In this paper, we envision a future where individuals are able to develop their own Social Machines, enabling them to interact in a trustworthy, decentralized way. To make this possible, development methods and tools must see their barriers-to-entry dramatically lowered. People should be able to specify the agent roles and interaction patterns in an intuitive, visual way, analyse and test their designs and deploy them as easy to use systems. We argue that this is a challenging but realistic goal, which should be tackled by navigating the trade-off between the accessibility of the design methods --primarily the modelling formalisms-- and their expressive power. We support our arguments by drawing ideas from different research areas including electronic institutions, agent-based simulation, process modelling, formal verification, and model-driven engineering.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1208–1212},
numpages = {5},
keywords = {analysis, design, model-driven development, modelling, social machines},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@proceedings{10.1145/3582515,
title = {GoodIT '23: Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3517804.3526066,
author = {Martens, Wim},
title = {Towards Theory for Real-World Data},
year = {2022},
isbn = {9781450392600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517804.3526066},
doi = {10.1145/3517804.3526066},
abstract = {Fundamental research on data manipulation languages is often motivated by the search for balance between desirable properties, such as expressiveness, robustness, compositionality, the existence of efficient algorithms, etc. Real-world data can be helpful for this search in many different respects. Data sets may exhibit common structures that efficient algorithms can exploit. Query logs and schemas can give us an idea of single features that are used very often, or groups of features that are frequently used together. In this sense, they can guide us towards features or fragments of data manipulation languages that are common in practice and may therefore be worthy of deeper study. In other cases, we may even get a glimpse on features that are not well-understood by users, which may inspire us to redesign them or develop tools that increase their ease-of-use. This tutorial aims to provide, first of all, an overview on several practical studies that have been conducted in the areas of tree-structured and graph-structured data, with a focus on cases with strong interaction between analysis of the data and fundamental research. Second, it aims to provide a set of lessons learned after the investigation of some large-scale logs consisting of more than 850 million queries.},
booktitle = {Proceedings of the 41st ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {261–276},
numpages = {16},
keywords = {GQL, JSON, RDF, SPARQL, XML, cypher, graph databases, query languages, regular languages, regular path queries, schema languages, tree-structured data},
location = {Philadelphia, PA, USA},
series = {PODS '22}
}

@proceedings{10.1145/3657054,
title = {dg.o '24: Proceedings of the 25th Annual International Conference on Digital Government Research},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system, many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state-of-the-art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs; and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr},
keywords = {intentions, software evolution, change management, version control}
}

@inproceedings{10.1145/3583780.3614662,
author = {Cazzolato, Mirela and Vijayakumar, Saranya and Lee, Meng-Chieh and Vajiac, Catalina and Park, Namyong and Fidalgo, Pedro and Traina, Agma J.M. and Faloutsos, Christos},
title = {CallMine: Fraud Detection and Visualization of Million-Scale Call Graphs},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614662},
doi = {10.1145/3583780.3614662},
abstract = {Given a million-scale dataset of who-calls-whom data containing imperfect labels, how can we detect existing and new fraud patterns? We propose CallMine, with carefully designed features and visualizations. Our CallMine method has the following properties: (a) Scalable, being linear on the input size, handling about 35 million records in around one hour on a stock laptop; (b) Effective, allowing natural interaction with human analysts; (c) Flexible, being applicable in both supervised and unsupervised settings; (d) Automatic, requiring no user-defined parameters.In the real world, in a multi-million-scale dataset, CallMine was able to detect fraudsters 7,000x faster, namely in a matter of hours, while expert humans took over 10 months to detect them.CIKM-ARP Categories: Application; Analytics and machine learning; Data presentation.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4509–4515},
numpages = {7},
keywords = {fraud detection, graph mining, phone call network, visualization},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3243274.3243275,
author = {Collins, Karen and Dockwray, Ruth},
title = {Tamaglitchi: A Pilot Study of Anthropomorphism and Non-Verbal Sound},
year = {2018},
isbn = {9781450366090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243274.3243275},
doi = {10.1145/3243274.3243275},
abstract = {In this paper, we present an overview of the current state of research in anthropomorphism as it relates specifically to product design, and then present a short pilot study of nonverbal sound's influence on anthropomorphism, through two short experiments, one qualitative and one quantitative. These experiments use an online variation of a virtual pet similar to the Tamagotchi, which we have called "Tamaglitchi". Results show that non-verbal sound increased the tendency to anthropomorphize a virtual pet.},
booktitle = {Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion},
articleno = {19},
numpages = {6},
keywords = {Interface design, anthropomorphism, audio, games, sound},
location = {Wrexham, United Kingdom},
series = {AM '18}
}

@inproceedings{10.1145/3368691.3368733,
author = {Alazzam, Hadeel and Alsmady, Abdulsalam and Shorman, Amaal Al},
title = {Supervised detection of IoT botnet attacks},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368733},
doi = {10.1145/3368691.3368733},
abstract = {Nowadays, more and more people start using IoT devices, which raise the threats of compromising these devices, since it's easily manipulated and hacked than desktop devices. This fact increased the number of cyberattacks that relay on IoT-based Botnet attacks. In this paper, we investigate the using of a supervised technique for detecting anomalies in IoT networks. The proposed model used a random forest classifier, the training data consider only 4 types of attacks while testing considers 10 types of attacks. The proposed model was effective in detection the new attacks and achieved 99% in terms of TPR, 100% in terms of TNR, and near-zero false alarms.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {42},
numpages = {6},
keywords = {IoT botnet attacks, anomaly detection, attacks detection, supervised learning},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@proceedings{10.1145/3649921,
title = {FDG '24: Proceedings of the 19th International Conference on the Foundations of Digital Games},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Worcester, MA, USA}
}

@proceedings{10.1145/3636243,
title = {ACE '24: Proceedings of the 26th Australasian Computing Education Conference},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@article{10.1145/3573074.3573077,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3573074.3573077},
doi = {10.1145/3573074.3573077},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {7–12},
numpages = {6}
}

@inproceedings{10.1145/3613904.3642716,
author = {Das Swain, Vedant and Gao, Lan and Mondal, Abhirup and Abowd, Gregory D. and De Choudhury, Munmun},
title = {Sensible and Sensitive AI for Worker Wellbeing: Factors that Inform Adoption and Resistance for Information Workers},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642716},
doi = {10.1145/3613904.3642716},
abstract = {Algorithmic estimations of worker behavior are gaining popularity. Passive Sensing–enabled AI (PSAI) systems leverage behavioral traces from workers’ digital tools to infer their experience. Despite their conceptual promise, the practical designs of these systems elicit tensions that lead to workers resisting adoption. This paper teases apart the monolithic representation of PSAI by investigating system components that maximize value and mitigate concerns. We conducted an interactive online survey using the Experimental Vignette Method. Using Linear Mixed-effects Models we found that PSAI systems were more acceptable when sensing digital time use or physical activity, instead of visual modes. Inferences using language were only acceptable in work-restricted contexts. Compared to insights into performance, workers preferred insights into mental wellbeing. However, they resisted systems that automatically forwarded these insights to others. Our findings provide a template to reflect on existing systems and plan future implementations of PSAI to be more worker-centered.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {104},
numpages = {30},
keywords = {digital phenotyping, future of work, harms, human resource management, impacts, information work, mental health, passive sensing, technology adoption, worker wellbeing},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3457784.3457788,
author = {Quy Tran, Ban and Van Nguyen, Thai and Duc Phung, Thang and Tan Nguyen, Viet and Duy Tran, Dat and Tung Ngo, Son},
title = {FU Covid-19 AI Agent built on Attention algorithm using a combination of Transformer, ALBERT model, and RASA framework},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457788},
doi = {10.1145/3457784.3457788},
abstract = {Potentialized by Natural Language Processing (NLP) technology, we can build a chatbot or an AI Agent to automatically address the need to automatically get credible and timely information, especially in the fight against epidemics. However, Vietnamese understanding is still a big challenge for NLP. This paper introduces an AI Agent using the Attention algorithm and Albert model to implement the question/answering task in the Covid-19 field for the Vietnamese language. In the end, we also built two other modules, one for Vietnamese diacritic auto-correction and another for updating Covid-19 statistics (using RASA framework), to deploy a Covid-19 chatbot application on mobile devices.},
booktitle = {Proceedings of the 2021 10th International Conference on Software and Computer Applications},
pages = {22–31},
numpages = {10},
keywords = {AI Agent, Albert, Application, Attention, Covid-19, Natural Language Processing, Question Answering, Rasa, Transformer},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA '21}
}

@proceedings{10.5555/3643142,
title = {WSC '23: Proceedings of the Winter Simulation Conference},
year = {2023},
isbn = {9798350369663},
publisher = {IEEE Press},
location = {San Antonio, Texas, USA}
}

@article{10.1145/3565884,
author = {Wu, Feijie and Yuen, Ho Yin and Chan, Henry and Leung, Victor C. M. and Cai, Wei},
title = {Facilitating Serverless Match-based Online Games with Novel Blockchain Technologies},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3565884},
doi = {10.1145/3565884},
abstract = {Applying peer-to-peer (P2P) architecture to online video games has already attracted both academic and industrial interests, since it removes the need for expensive server maintenance. However, there are two major issues preventing the use of a P2P architecture, namely how to provide an effective distributed data storage solution, and how to tackle potential cheating behaviors. Inspired by emerging blockchain techniques, we propose a novel consensus model called Proof-of-Play (PoP) to provide a decentralized data storage system that incorporates an anti-cheating mechanism for P2P games, by rewarding players that interact with the game as intended, along with consideration of security measures to address the Nothing-at-stake Problem and the Long-range Attack. To validate our design, we utilize a game-theory model to show that under certain assumptions, the integrity of the PoP system would not be undermined due to the best interests of any user. Then, as a proof-of-concept, we developed a P2P game (Infinity Battle) to demonstrate how a game can be integrated with PoP in practice. Finally, experiments were conducted to study PoP in comparison with Proof-of-Work (PoW) to show its advantages in various aspects.},
journal = {ACM Trans. Internet Technol.},
month = {feb},
articleno = {10},
numpages = {26},
keywords = {Peer-to-peer game, blockchain, consensus model}
}

@proceedings{10.1145/3545948,
title = {RAID '22: Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2022},
isbn = {9781450397049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3564625,
title = {ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@article{10.1145/3479502,
author = {Pathak, Arjunil and Madani, Navid and Joseph, Kenneth},
title = {A Method to Analyze Multiple Social Identities in Twitter Bios},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479502},
doi = {10.1145/3479502},
abstract = {Twitter users signal social identity in their profile descriptions, or bios, in a number of important but complex ways that are not well-captured by existing characterizations of how identity is expressed in language. Better ways of defining and measuring these expressions may therefore be useful both in understanding how social identity is expressed in text, and how the self is presented on Twitter. To this end, the present work makes three contributions. First, using qualitative methods, we identify and define the concept of a personal identifier, which is more representative of the ways in which identity is signaled in Twitter bios. Second, we propose a method to extract all personal identifiers expressed in a given bio. Finally, we present a series of validation analyses that explore the strengths and limitations of our proposed method. Our work opens up exciting new opportunities at the intersection between the social psychological study of social identity and the study of how we compose the self through markers of identity on Twitter and in social media more generally.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {358},
numpages = {35},
keywords = {computational social science, self-presentation, social identity, twitter}
}

@proceedings{10.1145/3580507,
title = {EC '23: Proceedings of the 24th ACM Conference on Economics and Computation},
year = {2023},
isbn = {9798400701047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the course of two decades, EC has established itself as one of the few truly successful interdisciplinary conferences, attracting papers and participants with a broad range of interests in economics and computer science, and fostering work in the intersection.},
location = {London, United Kingdom}
}

@proceedings{10.1145/3594671,
title = {Programming '23: Companion Proceedings of the 7th International Conference on the Art, Science, and Engineering of Programming},
year = {2023},
isbn = {9798400707551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3630106.3658909,
author = {Thach, Hibby and Mayworm, Samuel and Thomas, Michaelanne and Haimson, Oliver L.},
title = {Trans-centered moderation: Trans technology creators and centering transness in platform and community governance},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658909},
doi = {10.1145/3630106.3658909},
abstract = {Mainstream platforms’ content moderation systems typically employ generalized “one-size-fits-all” approaches, intended to serve both general and marginalized users. Thus, transgender people must often create their own technologies and moderation systems to meet their specific needs. In our interview study of transgender technology creators (n=115), we found that creators face issues of transphobic abuse and disproportionate content moderation. Trans tech creators address these issues by carefully moderating and vetting their userbases, centering trans contexts in content moderation systems, and employing collective governance and community models. Based on these findings, we argue that trans tech creators’ approaches to moderation offer important insights into how to better design for trans users, and ultimately, marginalized users in the larger platform ecology. We introduce the concept of trans-centered moderation – content moderation that reviews and successfully vets transphobic users, appoints trans moderators to effectively moderate trans contexts, considers the limitations and constraints of technology for addressing social challenges, and employs collective governance and community models. Trans-centered moderation can help to improve platform design for trans users while reducing the harm faced by trans people and marginalized users more broadly.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {326–336},
numpages = {11},
keywords = {content moderation, governance, online communities, platforms, trans technologies, transgender},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@proceedings{10.1145/3526114,
title = {UIST '22 Adjunct: Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bend, OR, USA}
}

@proceedings{10.1145/3630590,
title = {AINTEC '23: Proceedings of the 18th Asian Internet Engineering Conference},
year = {2023},
isbn = {9798400709395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/3173574.3174182,
author = {Ford, Denae and Lustig, Kristina and Banks, Jeremy and Parnin, Chris},
title = {"We Don't Do That Here": How Collaborative Editing with Mentors Improves Engagement in Social Q&amp;A Communities},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174182},
doi = {10.1145/3173574.3174182},
abstract = {Online question-and-answer (Q&amp;A) communities like Stack Overflow have norms that are not obvious to novice users. Novices create and post programming questions without feedback, and the community enforces site norms through public downvoting and commenting. This can leave novices discouraged from further participation. We deployed a month long, just-in-time mentorship program to Stack Overflow in which we redirected novices in the process of asking a question to an on-site Help Room. There, novices received feedback on their question drafts from experienced Stack Overflow mentors. We present examples and discussion of various question improvements including: question context, code formatting, and wording that adheres to on-site cultural norms. We find that mentored questions are substantially improved over non-mentored questions, with average scores increasing by 50%. We provide design implications that challenge how socio-technical communities onboard novices across domains.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {collaborative editing, e-mentoring, programming, social q&amp;a},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@proceedings{10.1145/3603287,
title = {ACM SE '24: Proceedings of the 2024 ACM Southeast Conference},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.},
location = {Marietta, GA, USA}
}

@proceedings{10.1145/3652032,
title = {LCTES 2024: Proceedings of the 25th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
year = {2024},
isbn = {9798400706165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES 2024), the 25th edition of this longstanding conference! This year’s conference is co-located with PLDI 2024, bringing together affiliated research conferences and workshops into a week-long joint meeting in Copenhagen, Denmark. The mission of LCTES is to provide a link between languages, compilers, and tools for embedded systems, bringing together scientists and engineers from these communities. LCTES offers a forum for researchers and developers from either area to come together, interact, share insights, and collaborate on developing novel solutions.},
location = {Copenhagen, Denmark}
}

@proceedings{10.1145/3620665,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
abstract = {Welcome to the second volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is dedicated to the 2024 summer review cycle.We introduced several notable changes to ASPLOS this year, many of which were discussed in the previous message from program chairs in Volume 1. Here, to avoid repetition, we assume that readers have already read the latter message and will only describe differences between the current cycle and the previous one. These include: (1) developing and utilizing an automated format violation identifier script focused on uncovering disallowed vertical space manipulations that "squeeze" space; (2) incorporating authors-declared best-matching topics into our review assignment process; (3) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee (PC) meetings, which necessitated additional managerial involvement in online dissensions; and (4) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it, and highlighting how we believe that it should be handled in the future.Key statistics of the ASPLOS'24 summer cycle include: 409 submissions were finalized (about 1.5x more than last year's summer count and nearly 2.4x more than our spring cycle), with 107 related to accelerators/FPGAs/GPUs, 97 to machine learning, 88 to storage/memory, 80 to security, and 69 to datacenter/cloud; 179 (44%) submissions were promoted to the second review round; 54 (13.2%) papers were accepted (with 20 awarded one or more artifact evaluation badges); 33 (8.1%) submissions were allowed to submit major revisions, of which 27 were subsequently accepted during the fall cycle (with 13 awarded one or more artifact evaluation badges); 1,499 reviews were uploaded; and 5,557 comments were generated during online discussions.Analyzing the per-submission most-related broader areas of research, which we asked authors to associate with their work in the submission form, revealed that 71%, 47%, and 28% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, with about 45% being "interdisciplinary" submissions (associated with more than one area). The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@article{10.1145/3325642.3325643,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/3325642.3325643},
doi = {10.1145/3325642.3325643},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle:; http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {aug},
pages = {6–12},
numpages = {7}
}

@article{10.1145/3362077.3362088,
author = {Hilton, Alexander D.},
title = {Artificial intelligence: the societal responsibility to inform, educate, and regulate},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3362077.3362088},
doi = {10.1145/3362077.3362088},
abstract = {Artificial Intelligence (AI) is a rapidly growing field; one that is mysterious to the general public. The mention of the word AI fills the imaginations of many with thoughts of talking robots, jobs being replaced, and possibly even the destruction of mankind. Perhaps imaginations are running wild due to, perhaps driven by the loose definition of AI as systems able to perform tasks that normally require human intelligence that allows Hollywood to take some creative license. The experts in the field tend to work directly with AI and often for large companies, allowing for the imagination and news headlines to be where the public gets their information. Many wonder if this new technology is going to be an overall benefit to society or if it will bring unmitigated disaster. When the imagination runs wild, instead of understanding, news stories can perpetuate concerns and anxieties rather than hope and optimism.},
journal = {AI Matters},
month = {dec},
pages = {70–76},
numpages = {7}
}

@proceedings{10.1145/3617072,
title = {EuroUSEC '23: Proceedings of the 2023 European Symposium on Usable Security},
year = {2023},
isbn = {9798400708145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3650105.3652302,
author = {Blyth, Scott and Treude, Christoph and Wagner, Markus},
title = {Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652302},
doi = {10.1145/3650105.3652302},
abstract = {AI foundation models have the capability to produce a wide array of responses to a single prompt, a feature that is highly beneficial in software engineering to generate diverse code solutions. However, this advantage introduces a significant trade-off between diversity and correctness. In software engineering tasks, diversity is key to exploring design spaces and fostering creativity, but the practical value of these solutions is heavily dependent on their correctness. Our study systematically investigates this trade-off using experiments with HumanEval tasks, exploring various parameter settings and prompting strategies. We assess the diversity of code solutions using similarity metrics from the code clone community. The study identifies combinations of parameters and strategies that strike an optimal balance between diversity and correctness, situated on the Pareto front of this trade-off space. These findings offer valuable insights for software engineers on how to effectively use AI foundation models to generate code solutions that are diverse and accurate.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {119–123},
numpages = {5},
keywords = {foundation models, correctness, creativity},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3640543.3645172,
author = {Weidele, Daniel Karl I. and Martino, Mauro and Valente, Abel N. and Rossiello, Gaetano and Strobelt, Hendrik and Franke, Loraine and Alvero, Kathryn and Misko, Shayenna and Auer, Robin and Bagchi, Sugato and Mihindukulasooriya, Nandana and Chowdhury, Faisal and Bramble, Gregory and Samulowitz, Horst and Gliozzo, Alfio and Amini, Lisa},
title = {Empirical Evidence on Conversational Control of GUI in Semantic Automation},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645172},
doi = {10.1145/3640543.3645172},
abstract = {This research explores integration of a Large Language Model (LLM) fine-tuned to conversationally control the user interface (UI) for a Semantic Automation Layer (SAL). We condense SAL capabilities from prior work and prioritize with business analysts and data engineers via a Kano model, before implementing a prototypical UI. We augment the UI with our conversational engine and propose In-situ Prompt Engineering and learn from Human Feedback to smoothen the interaction and manipulation of UI through natural language commands. To evaluate the efficacy and usability of conversational control in various use-case scenarios, we conduct and report on an empirical interaction design user study. Our findings provide evidence supporting enhanced user engagement and satisfaction. We also observe significant increase of trust in AI after working with our conversational UI. This work generates areas for further refinement and research towards more intelligent, highly-integrated conversational UIs even beyond our application within Semantic Automation. We discuss our findings and point out next steps paving the way for future research and development in creating more intuitive and adaptive user interfaces.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {869–885},
numpages = {17},
keywords = {Conversational Graphical User Interface, Empirical Interaction Design User Study, Fine-tuned large language model, In-situ Prompt Engineering for UI Control, Semantic Automation Layer},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3379337.3415858,
author = {Zhang, Amy X. and Hugh, Grant and Bernstein, Michael S.},
title = {PolicyKit: Building Governance in Online Communities},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415858},
doi = {10.1145/3379337.3415858},
abstract = {The software behind online community platforms encodes a governance model that represents a strikingly narrow set of governance possibilities focused on moderators and administrators. When online communities desire other forms of government, such as ones that take many members? opinions into account or that distribute power in non-trivial ways, communities must resort to laborious manual effort. In this paper, we present PolicyKit, a software infrastructure that empowers online community members to concisely author a wide range of governance procedures and automatically carry out those procedures on their home platforms. We draw on political science theory to encode community governance into policies, or short imperative functions that specify a procedure for determining whether a user-initiated action can execute. Actions that can be governed by policies encompass everyday activities such as posting or moderating a message, but actions can also encompass changes to the policies themselves, enabling the evolution of governance over time. We demonstrate the expressivity of PolicyKit through implementations of governance models such as a random jury deliberation, a multi-stage caucus, a reputation system, and a promotion procedure inspired by Wikipedia's Request for Adminship (RfA) process.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {365–378},
numpages = {14},
keywords = {governance, moderation, online communities, policy, toolkit},
location = {Virtual Event, USA},
series = {UIST '20}
}

@proceedings{10.1145/3617023,
title = {WebMedia '23: Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ribeir\~{a}o Preto, Brazil}
}

@proceedings{10.1145/3635636,
title = {C&amp;C '24: Proceedings of the 16th Conference on Creativity &amp; Cognition},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@inproceedings{10.1109/ASONAM49781.2020.9381323,
author = {Zaeem, Razieh Nokhbeh and Li, Chengjing and Barber, K. Suzanne},
title = {On sentiment of online fake news},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381323},
doi = {10.1109/ASONAM49781.2020.9381323},
abstract = {The presence of disinformation and fake news on the Internet and especially social media has become a major concern. Prime examples of such fake news surged in the 2016 U.S. presidential election cycle and the COVID-19 pandemic. We quantify sentiment differences between true and fake news on social media using a diverse body of datasets from the literature that contains about 100K previously labeled true and fake news. We also experiment with a variety of sentiment analysis tools. We model the association between sentiment and veracity as conditional probability and also leverage statistical hypothesis testing to uncover the relationship between sentiment and veracity. With a significance level of 99.999%, we observe a statistically significant relationship between negative sentiment and fake news and between positive sentiment and true news. The degree of association, as measured by Goodman and Kruskal's gamma, ranges between .037 to .475. Finally, we make our data and code publicly available to support reproducibility. Our results assist in the development of automatic fake news detectors.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {760–767},
numpages = {8},
keywords = {disinformation, fake news, misinformation, sentiment analysis, social networks, veracity},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3532106.3533541,
author = {Alves-Oliveira, Patr\'{\i}cia and Bavier, Matthew and Malandkar, Samrudha and Eldridge, Ryan and Sayigh, Julie and Bj\"{o}rling, Elin A. and Cakmak, Maya},
title = {FLEXI: A Robust and Flexible Social Robot Embodiment Kit},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533541},
doi = {10.1145/3532106.3533541},
abstract = {The social robotics market is appealing yet challenging. Though social robots are built few remain on the market for long. Many reasons account for their short lifespan with costs and context-specificity ranking high amount them. In this work, we designed, fabricated, and developed FLEXI, a social robot embodiment kit that enabled unlimited customization, making it applicable for a broad range of use cases. The hardware and software of FLEXI were entirely developed by this research team from scratch. FLEXI includes a rich set of materials and attachment pieces to allow for a diverse range of hardware customizations that ensure the embodiment is appropriate for specific customer/researcher projects. It also includes an open-source end-user programming interface to lower the barrier of robotics access to interdisciplinary teams that populate the field of Human-Robot Interaction. We present an iterative development of this cost-effective kit through the lenses of case studies, conceptual research, and soft deployment of FLEXI in three application scenarios: community-support, mental health, and education. Additionally, we provide in open-access the full list of materials and a tutorial to fabricate FLEXI, making it accessible to any maker space, research lab, or workshop space interested in working with or learning about social robots.},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {1177–1191},
numpages = {15},
keywords = {Authors Keywords - Social robotics, fabrication, open access, robot kit},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@inproceedings{10.1145/2661829.2662049,
author = {Walk, Simon and Singer, Philipp and Strohmaier, Markus},
title = {Sequential Action Patterns in Collaborative Ontology-Engineering Projects: A Case-Study in the Biomedical Domain},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2662049},
doi = {10.1145/2661829.2662049},
abstract = {Within the last few years the importance of collaborative ontology-engineering projects, especially in the biomedical domain, has drastically increased. This recent trend is a direct consequence of the growing complexity of these structured data representations, which no single individual is able to handle anymore. For example, the World Health Organization is currently actively developing the next revision of the International Classification of Diseases (ICD), using an OWL-based core for data representation and Web 2.0 technologies to augment collaboration. This new revision of ICD consists of roughly 50,000 diseases and causes of death and is used in many countries around the world to encode patient history, to compile health-related statistics and spendings. Hence, it is crucial for practitioners to better understand and steer the underlying processes of how users collaboratively edit an ontology. Particularly, generating predictive models is a pressing issue as these models may be leveraged for generating recommendations in collaborative ontology-engineering projects and to determine the implications of potential actions on the ontology and community. In this paper we approach this task by (i) exploring whether regularities and common patterns in user action sequences, derived from change-logs of five different collaborative ontology-engineering projects from the biomedical domain, exist. Based on this information we (ii) model the data using Markov chains of varying order, which are then used to (iii) predict user actions in the sequences at hand.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1349–1358},
numpages = {10},
keywords = {collaborative ontology-engineering, markov chain, sequential pattern, state prediction},
location = {Shanghai, China},
series = {CIKM '14}
}

@proceedings{10.1145/3663384,
title = {CHIWORK '24: Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Newcastle upon Tyne, United Kingdom}
}

@inproceedings{10.1145/3555776.3577663,
author = {Gniewkowski, Mateusz and Maciejewski, Henryk and Surmacz, Tomasz and Walentynowicz, Wiktor},
title = {Sec2vec: Anomaly Detection in HTTP Traffic and Malicious URLs},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577663},
doi = {10.1145/3555776.3577663},
abstract = {In this paper, we show how methods known from Natural Language Processing (NLP) can be used to detect anomalies in HTTP requests and malicious URLs. Most of the current solutions focusing on a similar problem are either rule-based or trained using manually selected features. Modern NLP methods, however, have great potential in capturing a deep understanding of samples and therefore improving the classification results. Other methods, which rely on a similar idea, often ignore the interpretability of the results, which is so important in machine learning. We are trying to fill this gap. In addition, we show to what extent the proposed solutions are resistant to concept drift. In our work, we compare three different vectorization methods: simple BoW, fastText, and the current state-of-the-art language model RoBERTa. The obtained vectors are later used in the classification task. In order to explain our results, we utilize the SHAP method. We evaluate the feasibility of our methods on four different datasets: CSIC2010, UNSW-NB15, MALICIOUSURL, and ISCX-URL2016. The first two are related to HTTP traffic, the other two contain malicious URLs. The results we show are comparable to others or better, and most importantly - interpretable.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1154–1162},
numpages = {9},
keywords = {cybersecurity, explainability, NLP, semi-supervised},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@proceedings{10.1145/3638067,
title = {IHC '23: Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
year = {2023},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3622758,
title = {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2023), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet fully proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.  

Onward! 2023 is co-located with SPLASH 2023, running from Sunday 22nd of October till Friday 27th of October, in Cascais, Portugal. We are delighted to have Felienne Hermans giving the Onward! keynote, on Wednesday 25th of October, on "Creating a learnable and inclusive programming language".  

All papers and essays that lie here before you received at least three reviews, leading to a decision of accept, reject, or conditional accept. Authors of conditionally accepted papers were provided with explicit requirements for acceptance, and were carefully re-reviewed in the second phase. The essays track received six submissions, out of which four were accepted. The papers track accepted nine out of nineteen submissions.  

We hope that the papers and essays in these proceedings will stimulate and challenge your thinking about programming and software engineering, and we are looking forward to many discussions at the conference.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3472163.3472169,
author = {Comito, Carmela},
title = {COVID-19 Concerns in US: Topic Detection in Twitter},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472169},
doi = {10.1145/3472163.3472169},
abstract = {COVID-19 pandemic is affecting the lives of the citizens worldwide. Epidemiologists, policy makers and clinicians need to understand public concerns and sentiment to make informed decisions and adopt preventive and corrective measures to avoid critical situations. In the last few years, social media become a tool for spreading the news, discussing ideas and comments on world events. In this context, social media plays a key role since represents one of the main source to extract insight into public opinion and sentiment. In particular, Twitter has been already recognized as an important source of health-related information, given the amount of news, opinions and information that is shared by both citizens and official sources. However, it is a challenging issue identifying interesting and useful content from large and noisy text-streams. The study proposed in the paper aims to extract insight from Twitter by detecting the most discussed topics regarding COVID-19. The proposed approach combines peak detection and clustering techniques. Tweets features are first modeled as time series. After that, peaks are detected from the time series, and peaks of textual features are clustered based on the co-occurrence in the tweets. Results, performed over real-world datasets of tweets related to COVID-19 in US, show that the proposed approach is able to accurately detect several relevant topics of interest, spanning from health status and symptoms, to government policy, economic crisis, COVID-19-related updates, prevention, vaccines and treatments.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {103–110},
numpages = {8},
keywords = {COVID-19, Social Media Data, Topic Modeling},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3366424.3383571,
author = {Liubonko, Kateryna and S\'{a}ez-Trumper, Diego},
title = {Matching Ukrainian Wikipedia Red Links with English Wikipedia’s Articles},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383571},
doi = {10.1145/3366424.3383571},
abstract = {This work tackles the problem of matching Wikipedia red links with existing articles. Links in Wikipedia pages are considered red when lead to nonexistent articles. In other Wikipedia editions could exist articles that correspond to such red links. In our work, we propose a way to match red links in one Wikipedia edition to existent pages in another edition. We define the task as a Named Entity Linking problem because red link titles are mostly named entities. We solve it in a context of Ukrainian red links and English existing pages. We created a dataset of 3171 most frequent Ukrainian red links and a dataset of almost 3 million pairs of red links and the most probable candidates for the correspondent pages in English Wikipedia. This dataset is publicly released1. In this work we define conceptual characteristics of the data — word and graph properties — based on its analysis and exploit these properties in entity resolution. BabelNet knowledge base was applied to this task and was regarded as a baseline for our approach (F1 score = 32 %). To improve the result we introduced several similarity metrics based on mentioned red links characteristics. Combined in a linear model they resulted in F1 score = 85 %. To the best of our knowledge, we are the first to state the problem and propose a solution for red links in Ukrainian Wikipedia edition.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {819–826},
numpages = {8},
keywords = {Entity linking, Red links, Wikipedia},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@proceedings{10.1145/2997364,
title = {SLE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
year = {2016},
isbn = {9781450344470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3594806,
title = {PETRA '23: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@proceedings{10.1145/3593013,
title = {FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@proceedings{10.1145/3558535,
title = {AFT '22: Proceedings of the 4th ACM Conference on Advances in Financial Technologies},
year = {2022},
isbn = {9781450398619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cambridge, MA, USA}
}

@inproceedings{10.1145/2568225.2568324,
author = {Monperrus, Martin},
title = {A critical review of "automatic patch generation learned from human-written patches": essay on the problem statement and the evaluation of automatic software repair},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568324},
doi = {10.1145/2568225.2568324},
abstract = {At ICSE'2013, there was the first session ever dedicated to automatic program repair. In this session, Kim et al. presented PAR, a novel template-based approach for fixing Java bugs. We strongly disagree with key points of this paper. Our critical review has two goals. First, we aim at explaining why we disagree with Kim and colleagues and why the reasons behind this disagreement are important for research on automatic software repair in general. Second, we aim at contributing to the field with a clarification of the essential ideas behind automatic software repair. In particular we discuss the main evaluation criteria of automatic software repair: understandability, correctness and completeness. We show that depending on how one sets up the repair scenario, the evaluation goals may be contradictory. Eventually, we discuss the nature of fix acceptability and its relation to the notion of software correctness.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {234–242},
numpages = {9},
keywords = {Bugs, automatic patch generation, automatic program fixing, automatic software repair, error recovery, faults},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3125486.3125491,
author = {Ruocco, Massimiliano and Skrede, Ole Steinar Lillest\o{}l and Langseth, Helge},
title = {Inter-Session Modeling for Session-Based Recommendation},
year = {2017},
isbn = {9781450353533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125486.3125491},
doi = {10.1145/3125486.3125491},
abstract = {In recent years, research has been done on applying Recurrent Neural Networks (RNNs) as recommender systems. Results have been promising, especially in the session-based setting where RNNs have been shown to outperform state-of-the-art models. In many of these experiments, the RNN could potentially improve the recommendations by utilizing information about the user's past sessions, in addition to its own interactions in the current session. A problem for session-based recommendation, is how to produce accurate recommendations at the start of a session, before the system has learned much about the user's current interests. We propose a novel approach that extends an RNN recommender to be able to process the user's recent sessions, in order to improve recommendations. This is done by using a second RNN to learn from recent sessions, and predict the user's interest in the current session. By feeding this information to the original RNN, it is able to improve its recommendations. Our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions, compared to a single RNN working only on the current session. The proposed model especially improves recommendations at the start of sessions, and is therefore able to deal with the cold start problem within sessions.},
booktitle = {Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems},
pages = {24–31},
numpages = {8},
keywords = {Recommender Systems, Recurrent Neural Network, Session-Based Recommendation},
location = {Como, Italy},
series = {DLRS 2017}
}

@proceedings{10.1145/3582437,
title = {FDG '23: Proceedings of the 18th International Conference on the Foundations of Digital Games},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3588015,
title = {ETRA '23: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},
year = {2023},
isbn = {9798400701504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tubingen, Germany}
}

@inproceedings{10.1145/3442442.3452348,
author = {Scharpf, Philipp and Schubotz, Moritz and Gipp, Bela},
title = {Fast Linking of Mathematical Wikidata Entities in Wikipedia Articles Using Annotation Recommendation},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452348},
doi = {10.1145/3442442.3452348},
abstract = {Mathematical information retrieval (MathIR) applications such as semantic formula search and question answering systems rely on knowledge-bases that link mathematical expressions to their natural language names. For database population, mathematical formulae need to be annotated and linked to semantic concepts, which is very time-consuming. In this paper, we present our approach to structure and speed up this process by using an application-driven strategy and AI-aided system. We evaluate the quality and time-savings of AI-generated formula and identifier annotation recommendations on a test selection of Wikipedia articles from the physics domain. Moreover, we evaluate the community acceptance of Wikipedia formula entity links and Wikidata item creation and population to ground the formula semantics. Our evaluation shows that the AI guidance was able to significantly speed up the annotation process by a factor of 1.4 for formulae and 2.4 for identifiers. Our contributions were accepted in 88% of the edited Wikipedia articles and 67% of the Wikidata items. The &gt;&gt;AnnoMathTeX&lt;&lt; annotation recommender system is hosted by Wikimedia at annomathtex.wmflabs.org. In the future, our data refinement pipeline will be integrated seamlessly into the Wikimedia user interfaces.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {602–609},
numpages = {8},
keywords = {Entity Linking, Recommender Systems, Wikidata, Wikipedia},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3637543,
title = {CF '24 Companion: Proceedings of the 21st ACM International Conference on Computing Frontiers: Workshops and Special Sessions},
year = {2024},
isbn = {9798400704925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This is the companion proceedings of the 21st ACM International Conference on Computing Frontiers (Volume 2) collecting the papers from co-located workshops and invited papers from special sessions. For papers from the main track, as well as keynote abstract and poster abstracts, see Volume 1.},
location = {Ischia, Italy}
}

@proceedings{10.1145/3555858,
title = {FDG '22: Proceedings of the 17th International Conference on the Foundations of Digital Games},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3636501,
title = {CPP 2024: Proceedings of the 13th ACM SIGPLAN International Conference on Certified Programs and Proofs},
year = {2024},
isbn = {9798400704888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 13th ACM SIGPLAN International Conference on Certified 
Programs and Proofs (CPP 2024). CPP covers the practical and 
theoretical topics in all areas that consider formal verification and 
certification as an essential paradigm for their work. CPP spans 
topics in computer science, mathematics, logic, and education. CPP 
2024 will be held on 15-16 January 2024 in London, UK. The conference is co-located with POPL 2024, and is sponsored by ACM SIGPLAN in cooperation with ACM SIGLOG.},
location = {London, UK}
}

@inproceedings{10.1145/3491102.3517722,
author = {Zhang, Mingrui Ray and Lukoff, Kai and Rao, Raveena and Baughan, Amanda and Hiniker, Alexis},
title = {Monitoring Screen Time or Redesigning It? Two Approaches to Supporting Intentional Social Media Use},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517722},
doi = {10.1145/3491102.3517722},
abstract = {Existing designs helping people manage their social media use include: 1) external supports that monitor and limit use; 2) internal supports that change the interface itself. Here, we design and deploy Chirp, a mobile Twitter client, to independently examine how users experience external and internal supports. To develop Chirp, we identified 16 features that influence users’ sense of agency on Twitter through a survey of 129 participants and a design workshop. We then conducted a four-week within-subjects deployment with 31 participants. Our internal supports (including features to filter tweets and inform users when they have exhausted new content) significantly increased users’ sense of agency, while our external supports (a usage dashboard and nudges to close the app) did not. Participants valued our internal supports and said that our external supports were for “other people.” Our findings suggest that design patterns promoting agency may serve users better than screen time tools.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {60},
numpages = {19},
keywords = {digital wellbeing, mobile phone, sense of agency, social media, twitter},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1109/TASLP.2023.3328289,
author = {Jiang, Congcong and Qian, Tieyun and Liu, Bing},
title = {One General Teacher for Multi-Data Multi-Task: A New Knowledge Distillation Framework for Discourse Relation Analysis},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3328289},
doi = {10.1109/TASLP.2023.3328289},
abstract = {Automatically identifying the discourse relations can help many downstream NLP tasks such as reading comprehension and machine translation. It can be categorized into explicit and implicit discourse relation recognition (EDRR and IDRR). Due to the lack of connectives, IDRR remains to be a big challenge. A good number of methods have been developed to combine explicit data with implicit ones under the multi-task learning framework. However, the difference in linguistic property and class distribution makes it hard to directly optimize EDRR and IDRR with multi-task learning. In this paper, we take the first step to exploit the knowledge distillation (KD) technique for discourse relation analysis. Our target is to train &lt;italic&gt;a focused single-data single-task student&lt;/italic&gt; with the help of &lt;italic&gt;a general multi-data multi-task teacher&lt;/italic&gt;. Specifically, we first train one teacher for both the top and second level relation classification tasks with explicit and implicit data. We then transfer the feature embeddings and soft labels from the teacher network to the student network. Moreover, we develop an adaptive knowledge distillation module to reduce the number of hyper-parameters and also to stimulate the potential of the student on autonomous learning. Extensive experimental results on the popular PDTB dataset proves that our model achieves a new state-of-the-art performance. We also show the effectiveness of our proposed KD architecture through detailed analysis.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {239–249},
numpages = {11}
}

@inproceedings{10.1145/3618257.3624824,
author = {Heimbach, Lioba and Kiffer, Lucianna and Ferreira Torres, Christof and Wattenhofer, Roger},
title = {Ethereum's Proposer-Builder Separation: Promises and Realities},
year = {2023},
isbn = {9798400703829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618257.3624824},
doi = {10.1145/3618257.3624824},
abstract = {With Ethereum's transition from Proof-of-Work to Proof-of-Stake in September 2022 came another paradigm shift, the Proposer-Builder Separation (PBS) scheme. PBS was introduced to decouple the roles of selecting and ordering transactions in a block (i.e., the builder), from those validating its contents and proposing the block to the network as the new head of the blockchain (i.e., the proposer). In this landscape, proposers are the validators in the Proof-of-Stake consensus protocol, while now relying on specialized block builders for creating blocks with the highest value for the proposer. Additionally, relays act as mediators between builders and proposers. We study PBS adoption and show that the current landscape exhibits significant centralization amongst the builders and relays. Further, we explore whether PBS effectively achieves its intended objectives of enabling hobbyist validators to maximize block profitability and preventing censorship. Our findings reveal that although PBS grants validators the opportunity to access optimized and competitive blocks, it tends to stimulate censorship rather than reduce it. Additionally, we demonstrate that relays do not consistently uphold their commitments and may prove unreliable. Specifically, proposers do not always receive the complete promised value, and the censorship or filtering capabilities pledged by relays exhibit significant gaps.},
booktitle = {Proceedings of the 2023 ACM on Internet Measurement Conference},
pages = {406–420},
numpages = {15},
keywords = {blockchain, ethereum, proposer-builder seperation},
location = {Montreal QC, Canada},
series = {IMC '23}
}

@proceedings{10.1145/3549555,
title = {CBMI '22: Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Graz, Austria}
}

@inproceedings{10.5555/3242181.3242211,
author = {Fujimoto, Richard M. and Carothers, Christopher and Ferscha, Alois and Jefferson, David and Loper, Margaret and Marathe, Madhav and Taylor, Simon J. E.},
title = {Computational challenges in modeling &amp; simulation of complex systems},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Modeling and simulation faces many new computational challenges in the design of complex engineered systems. The systems that need to be modeled are increasingly interconnected and interdependent, achieving unprecedented levels of complexity. The computational platforms upon which simulations execute have undergone dramatic changes in recent years. Position statements by leading researchers are presented concerning important computational challenges and opportunities facing the M&amp;S community.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {28},
numpages = {15},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@proceedings{10.1145/3535735,
title = {ICIEI '22: Proceedings of the 7th International Conference on Information and Education Innovations},
year = {2022},
isbn = {9781450396196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Belgrade, Serbia}
}

@inproceedings{10.1145/3630590.3630599,
author = {Pham, Tien-Huy and Vo, Quoc-Huy and Dao, Ha and Fukuda, Kensuke},
title = {SSOLogin: A framework for automated web privacy measurement with SSO logins},
year = {2023},
isbn = {9798400709395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630590.3630599},
doi = {10.1145/3630590.3630599},
abstract = {Single Sign-On (SSO) enables users to access multiple websites and applications using a single set of login credentials. Undoubtedly, SSO makes it easy for users to log in to multiple websites without remembering credentials. However, it is also important to consider the potential risk of users being unaware of how the identity of their account will be utilized, the development of online tracking techniques, and any potential exchange of information with third parties without the user’s knowledge. In this paper, we propose SSOLogin, which enables us to perform large-scale automation of website login through an SSO account. We confirm that SSOLogin automatically logins to 91.8% of SSO account available websites in Tranco Top 500 sites. Next, by crawling Tranco Top 10K websites with SSOLogin, we show that 1,420 sites (14.2%) contain SSO login (Google and Facebook) as of July 2023, primarily on Information Technology and News/Media websites in the United States and the United Kingdom. We then shed light on the characteristics of privacy leakage of websites with SSO logins by setting up measurements in Japan. We find that 99% of websites have third-party online tracking activities, which may pose risks to user privacy. After SSO login, the target website shows an increase in third-party tracking domains. Logging in with Google adds 81 new tracking domains, while Facebook adds 33 new domains. Despite the convenience of logging in with an SSO account, it is important to be aware of the potential privacy risks associated with this practice.},
booktitle = {Proceedings of the 18th Asian Internet Engineering Conference},
pages = {69–77},
numpages = {9},
keywords = {SSO login, Single Sign-On, Web tracking, measurement},
location = {Hanoi, Vietnam},
series = {AINTEC '23}
}

@inproceedings{10.1145/3387940.3391497,
author = {Machado, Leticia S. and Steinmacher, Igor and Marczak, Sabrina and de Souza, Cleidson R. B.},
title = {How Online Forums Complement Task Documentation in Software Crowdsourcing},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391497},
doi = {10.1145/3387940.3391497},
abstract = {An issue in software crowdsourcing is the quality of the task documentation and the high number of registered crowd workers to solve tasks but few submitted solutions only. This happens because uncommunicated or misunderstood requirements can lead crowd workers to deliver a solution that does not meet the customers' requirements or, worse, to give up submitting a solution. In this paper, we present an empirical study in which we analyzed task documentation and online forums messages associated with 25 Software Crowdsourcing (SW CS) challenges. The findings corroborate that weak documentation is a challenge in SW CS. Meanwhile, online forums allow crowd workers to gather additional technical and operational information that is not present in the official task documentation. We provide a stepping stone towards understanding the interplay between requirements and communication, to make it possible to improve SW CS development processes, practices, and tools.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {101–108},
numpages = {8},
keywords = {Software crowdsourcing, communication, documentation tasks},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@proceedings{10.1145/3593743,
title = {C&amp;T '23: Proceedings of the 11th International Conference on Communities and Technologies},
year = {2023},
isbn = {9798400707582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lahti, Finland}
}

@proceedings{10.1145/3626232,
title = {CODASPY '24: Proceedings of the Fourteenth ACM Conference on Data and Application Security and Privacy},
year = {2024},
isbn = {9798400704215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the fourteenth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2024), for the first time held outside United States of America. This conference series has been founded to foster novel and exciting research in the data and application security and privacy arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with several fellow data security and privacy researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference. CODASPY has become a leading forum for presentation of research results and experience reports on hardware and software security. The conference gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of data and applications security and privacy.},
location = {Porto, Portugal}
}

@proceedings{10.1145/3634814,
title = {ASSE '23: Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference},
year = {2023},
isbn = {9798400708534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aizu-Wakamatsu City, Japan}
}

@proceedings{10.1145/3592538,
title = {CPSS '23: Proceedings of the 9th ACM Cyber-Physical System Security Workshop},
year = {2023},
isbn = {9798400700903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3607199.3607211,
author = {Anwar, Md Sakib and Zuo, Chaoshun and Yagemann, Carter and Lin, Zhiqiang},
title = {Extracting Threat Intelligence From Cheat Binaries For Anti-Cheating},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607211},
doi = {10.1145/3607199.3607211},
abstract = {Rampant cheating remains a serious concern for game developers who fear losing loyal customers and revenue. While numerous anti-cheating techniques have been proposed, cheating persists in a vibrant (and profitable) illicit market. Inspired by novel insights into the economics behind cheat development and recent techniques for defending against advanced persistent threats (APTs), we propose a fully automated methodology for extracting “cheat intelligence” from widely distributed cheat binaries to produce a “memory access graph” that guides selective data randomization to yield immune game clients. We have implemented a prototype system for Android and Windows games, CheatFighter, and evaluated it on 86 cheats collected from a variety of real-world sources, including Telegram channels and online forums. CheatFighter successfully counteracts 80 of the real-world cheats in under a minute, demonstrating practical end-to-end protection against widespread cheating.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {17–31},
numpages = {15},
keywords = {Anti-cheating, Automated client hardening, Program analysis},
location = {Hong Kong, China},
series = {RAID '23}
}

@proceedings{10.1145/3587716,
title = {ICMLC '23: Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhuhai, China}
}

@proceedings{10.1145/3643488,
title = {ICDAR '24: Proceedings of the 5th ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
year = {2024},
isbn = {9798400705496},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Phuket, Thailand}
}

@inproceedings{10.1145/3459637.3482481,
author = {Petrescu, Alexandru and Truic\u{a}, Ciprian-Octavian and Apostol, Elena-Simona and Karras, Panagiotis},
title = {Sparse Shield: Social Network Immunization vs. Harmful Speech},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482481},
doi = {10.1145/3459637.3482481},
abstract = {With the rise of social media users and the general shift of communication from traditional media to online platforms, the spread of harmful content (e.g., hate speech, misinformation, fake news) has been exacerbated. Harmful content in the form of hate speech causes a person distress or harm, having a negative impact on the individual mental health, with even more detrimental effects on the psychology of children and teenagers. In this paper, we propose an end-to-end solution with real-time capabilities to detect harmful content in real-time and mitigate its spread over the network. Our main contribution is Sparse Shield, a novel method that out-scales existing state-of-the-art methods for network immunization. We also propose a novel architecture for harmful speech mitigation that maximizes the impact of immunization. Our solution aims to identify a set of users for which to move harmful content at the bottom of the user feed, rather than censoring users. By immunizing certain network nodes in this manner, we minimize the negative impact on the network and minimize the interference with and limitation of individual freedoms: the information is not hidden but rather not as easy to reach without an explicit search. Our analysis is based on graphs built on real-world data collected from Twitter; these graphs reflect real user behavior. We perform extensive scalability experiments to prove the superiority of our method over existing state-of-the-art network immunization techniques. We also perform extensive experiments to showcase that Sparse Shield outperforms existing techniques on the task of harmful speech mitigation on a real-world dataset.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1426–1436},
numpages = {11},
keywords = {counteractive immunization, harmful speech detection, network immunization, preventive immunization},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3384943.3409420,
author = {Zhao, Juntao and Chi, Yuanfang and Wang, Zehua and Leung, Victor C.M. and Cai, Wei},
title = {CloudArcade: A Blockchain Empowered Cloud Gaming System},
year = {2020},
isbn = {9781450376105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384943.3409420},
doi = {10.1145/3384943.3409420},
abstract = {By rendering game scenes on the remote cloud and delivering real-time video to end devices via the Internet, cloud gaming enables players to access game services anytime anywhere despite the hardware capacity of their terminals. However, as a commercial service, the state-of-the-art payment models for cloud gaming are still in their preliminary stages. In this paper, we reveal the shortages of existing cloud gaming pricing models and propose CloudArcade, a token-based cloud gaming system that employs blockchain-empowered cryptocurrency as a payment method for the players using the cloud gaming services. By using cryptocurrency, CloudArcade provides a transparent and resource-aware pricing method, it also enables a time irrelevant silent payment on the floating price to protects users' payment. These features eliminate the quality of experience degradation caused by the spot price in the traditional dynamic pricing model on the QoE-aware service pricing. We also employ the payment channel in CloudArcade to improve the system performance. Discussions on service pricing criteria are put forward, open issues about token issuing and malicious resource speculation are also reviewed. We believe the design of CloudArcade can show a good generality on other QoE-aware and human-centered applications.},
booktitle = {Proceedings of the 2nd ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {31–40},
numpages = {10},
keywords = {blockchain, cloud gaming, cryptocurrency, human-centered computing, pricing},
location = {Taipei, Taiwan},
series = {BSCI '20}
}

