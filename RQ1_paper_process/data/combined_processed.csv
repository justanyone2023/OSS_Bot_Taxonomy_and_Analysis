title,author,abstract,year,id
An exploratory study of reactions to bot comments on GitHub,"Farah, Juan Carlos and Spaenlehauer, Basile and Lu, Xinyang and Ingram, Sandy and Gillet, Denis","The widespread use of bots to support software development makes social coding platforms such as GitHub a particularly rich source of data for the study of human-bot interaction. Software development bots are used to automate repetitive tasks, interacting with their human counterparts via comments posted on the various discussion interfaces available on such platforms. One type of interaction supported by GitHub involves reacting to comments using predefined emoji. To investigate how users react to bot comments, we conducted an observational study comprising 54 million GitHub comments, with a particular focus on comments that elicited the laugh reaction. The results from our analysis suggest that some reaction types are not equally distributed across human and bot comments and that a bot's design and purpose influence the types of reactions it receives. Furthermore, while the laugh reaction is not exclusively used to express laughter, it can be used to convey humor when a bot behaves unexpectedly. These insights could inform the way bots are designed and help developers equip them with the ability to recognize and recover from unanticipated situations. In turn, bots could better support the communication, collaboration, and productivity of teams using social coding platforms.",2022,1
RABBIT: A tool for identifying bot accounts based on their recent GitHub event history,"Chidambaram, Natarajan and Mens, Tom and Decan, Alexandre","Collaborative software development through GitHub repositories frequently relies on bot accounts to automate repetitive and error-prone tasks. This highlights the need to have accurate and efficient bot identification tools. Several such tools have been proposed in the past, but they tend to rely on a substantial amount of historical data, or they limit themselves to a reduced subset of activity types, making them difficult to use at large scale. To overcome these limitations, we developed RABBIT, an open source command-line tool that queries the GitHub Events API to retrieve the recent events of a given GitHub account and predicts whether the account is a human or a bot. RABBIT is based on an XGBoost classification model that relies on six features related to account activities and achieves high performance, with an AUC, F1 score, precision and recall of 0.92. Compared to the state-of-the-art in bot identification, RABBIT exhibits a similar performance in terms of precision, recall and F1 score, while being more than an order of magnitude faster and requiring considerably less data. This makes RABBIT usable on a large scale, capable of processing several thousand accounts per hour efficiently.",2024,2
On the adoption of a TODO bot on GitHub: a preliminary study,"Mohayeji, Hamid and Ebert, Felipe and Arts, Eric and Constantinou, Eleni and Serebrenik, Alexander","Bots support different software maintenance and evolution activities, such as code review or executing tests. Recently, several bots have been proposed to help developers to keep track of postponed activities, expressed by means of TODO comments: e.g., TODO Bot automatically creates a GitHub issue when a TODO comment is added to a repository, increasing visibility of TODO comments. In this work, we perform a preliminary evaluation of the impact of the TODO Bot on software development practice. We conjecture that the introduction of the TODO Bot would facilitate keeping track of the TODO comments, and hence encourage developers to use more TODO comments in their code changes.To evaluate this conjecture, we analyze all the 2,208 repositories which have at least one GitHub issue created by the TODO Bot. Firstly, we investigate to what extent the bot is being used and describe the repositories using the bot. We observe that the majority (54%) of the repositories which adopted the TODO Bot are new, i.e., were created within less than one month of first issue created by the bot, and from those, more than 60% have the issue created within three days. We observe a statistically significant increase in the number of the TODO comments after the adoption of the bot, however with a small effect size. Our results suggest that the adoption of the TODO Bot encourages developers to introduce TODO comments rendering the postponed decisions more visible. Nevertheless, it does not speed up the process of addressing TODO comments or corresponding GitHub issues.",2022,3
Bot detection in GitHub repositories,"Chidambaram, Natarajan and Mazrae, Pooya Rostami","Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the GrimoireLab software development analytics platform. In this work, we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using Kibana.",2022,4
GFI-bot: automated good first issue recommendation on GitHub,"He, Hao and Su, Haonan and Xiao, Wenxin and He, Runzhi and Zhou, Minghui","To facilitate newcomer onboarding, GitHub recommends the use of ""good first issue"" (GFI) labels to signal issues suitable for newcomers to resolve. However, previous research shows that manually labeled GFIs are scarce and inappropriate, showing a need for automated recommendations. In this paper, we present GFI-Bot (accessible at https://gfibot.io), a proof-of-concept machine learning powered bot for automated GFI recommendation in practice. Project maintainers can configure GFI-Bot to discover and label possible GFIs so that newcomers can easily locate issues for making their first contributions. GFI-Bot also provides a high-quality, up-to-date dataset for advancing GFI recommendation research.",2022,5
Bot or not? Detecting bots in GitHub pull request activity based on comment similarity,"Golzadeh, Mehdi and Legay, Damien and Decan, Alexandre and Mens, Tom","Many empirical studies focus on socio-technical activity in social coding platforms such as GitHub, for example to study the onboarding, abandonment, productivity and collaboration among team members. Such studies face the difficulty that GitHub activity can also be generated automatically by bots of a different nature. It therefore becomes imperative to distinguish such bots from human users. We propose an automated approach to detect bots in GitHub pull request (PR) activity. Relying on the assumption that bots contain repetitive message patterns in their PR comments, we analyse the similarity between multiple messages from the same GitHub identity, using a clustering method that combines the Jaccard and Levenshtein distance. We empirically evaluate our approach by analysing 20,090 PR comments of 250 users and 42 bots in 1,262 GitHub repositories. Our results show that the method is able to clearly separate bots from human users.",2020,6
Exploring How Software Developers Work with Mention Bot in GitHub,"Peng, Zhenhui and Yoo, Jeehoon and Xia, Meng and Kim, Sunghun and Ma, Xiaojuan","Recently, major software development platforms have started to provide automatic reviewer recommendation (ARR) services for pull requests, to improve the collaborative coding review process. However, the user experience of ARR is under-investigated. In this paper, we use a two-stage mixed-methods approach to study how software developers perceive and work with the Facebook mention bot, one of the most popular ARR bots in GitHub. Specifically, in Stage I, we conduct archival analysis on projects employing mention bot and a user survey to investigate the bot's performance. A year later, in Stage II, we revisit these projects and conduct additional surveys and interviews with three user groups: project owners, contributors and reviewers. Results show that developers appreciate mention bot saving their effort, but are bothered by its unstable setting and unbalanced workload allocation. We conclude with design considerations for improving ARR services.",2018,7
On the accuracy of bot detection techniques,"Golzadeh, Mehdi and Decan, Alexandre and Chidambaram, Natarajan","Development bots are often used to automate a wide variety of repetitive tasks in collaborative software development. Such bots are commonly among the most active project contributors in terms of commit activity. As such, tools that analyse contributor activity (e.g., for recognizing and giving credit to project members for their contributions) need to take into account the bots and exclude their activity. While there are a few techniques to detect bots in software repositories, these techniques are not perfect and may miss some bots or may wrongly identify some human accounts as bots. In this paper, we present an exploratory study on the accuracy of bot detection techniques on a set of 540 accounts from 27 GitHub projects. We show that none of the bot detection techniques are accurate enough to detect bots among the 20 most active contributors of each project. We show that combining these techniques drastically increases the accuracy and recall of bot detection. We also highlight the importance of considering bots when attributing contributions to humans, since bots are prevalent among the top contributors and responsible for large proportions of commits.",2022,8
Autonomy Is an Acquired Taste: Exploring Developer Preferences for GitHub Bots,"Ghorbani, Amir and Cassee, Nathan and Robinson, Derek and Alami, Adam and Ernst, Neil A. and Serebrenik, Alexander and W\k{a}sowski, Andrzej","Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",2023,9
Use Bots to Improve GitHub Pull-Request Feedback,"Hu, Zhewei and Gehringer, Edward","Rising enrollments make it difficult for instructors and teaching assistants to give adequate feedback on each student's work. In our software engineering course, we have 50-120 students each semester. Our course projects require students to submit GitHub pull requests as deliverables for their open-source software (OSS) projects. We have set up a static code analyzer and a continuous integration service on GitHub to help students check code style and functionality. However, these tools cannot enforce system-specific customized guidelines and do not explicitly display detailed information. In this study, we discuss how we bypass the limitations of existing tools by implementing three Internet bots. The Expertiza Bot can help detect violations of more than 35 system-specific guidelines. The Travis CI Bot can explicitly display instant test execution results on the GitHub pull-request page. The Code Climate Bot can insert pull-request comments to remind students to fix issues detected by the static code analyzer. These bots are either open source or free for OSS projects, and can be easily integrated with GitHub repositories. Our survey results show that more than 70% of students think the advice given by the bots is useful. We tallied the amount of feedback given by the bots and the teaching staff for each GitHub pull request. Results show that bots can provide significantly more feedback (six times more on average) than teaching staff. Bots can also offer more timely feedback than teaching staff and help student contributions avoid more than 33% system-specific guideline violations.",2019,10
Cataloging GitHub Repositories,"Sharma, Abhishek and Thung, Ferdian and Kochhar, Pavneet Singh and Sulistya, Agus and Lo, David","GitHub is one of the largest and most popular repository hosting service today, having about 14 million users and more than 54 million repositories as of March 2017. This makes it an excellent platform to find projects that developers are interested in exploring. GitHub showcases its most popular projects by cataloging them manually into categories such as DevOps tools, web application frameworks, and game engines. We propose that such cataloging should not be limited only to popular projects. We explore the possibility of developing such cataloging system by automatically extracting functionality descriptive text segments from readme files of GitHub repositories. These descriptions are then input to LDA-GA, a state-of-the-art topic modeling algorithm, to identify categories. Our preliminary experiments demonstrate that additional meaningful categories which complement existing GitHub categories can be inferred. Moreover, for inferred categories that match GitHub categories, our approach can identify additional projects belonging to them. Our experimental results establish a promising direction in realizing automatic cataloging system for GitHub.",2017,11
Towards an autonomous bot for automatic source code refactoring,"Wyrich, Marvin and Bogner, Justus","Continuous refactoring is necessary to maintain source code quality and to cope with technical debt. Since manual refactoring is inefficient and error-prone, various solutions for automated refactoring have been proposed in the past. However, empirical studies have shown that these solutions are not widely accepted by software developers and most refactorings are still performed manually. For example, developers reported that refactoring tools should support functionality for reviewing changes. They also criticized that introducing such tools would require substantial effort for configuration and integration into the current development environment.In this paper, we present our work towards the Refactoring-Bot, an autonomous bot that integrates into the team like a human developer via the existing version control platform. The bot automatically performs refactorings to resolve code smells and presents the changes to a developer for asynchronous review via pull requests. This way, developers are not interrupted in their workflow and can review the changes at any time with familiar tools. Proposed refactorings can then be integrated into the code base via the push of a button. We elaborate on our vision, discuss design decisions, describe the current state of development, and give an outlook on planned development and research activities.",2019,12
What to Expect from Code Review Bots on GitHub? A Survey with OSS Maintainers,"Wessel, Mairieli and Serebrenik, Alexander and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco A.","Software bots are used by Open Source Software (OSS) projects to streamline the code review process. Interfacing between developers and automated services, code review bots report continuous integration failures, code quality checks, and code coverage. However, the impact of such bots on maintenance tasks is still neglected. In this paper, we study how project maintainers experience code review bots. We surveyed 127 maintainers and asked about their expectations and perception of changes incurred by code review bots. Our findings reveal that the most frequent expectations include enhancing the feedback bots provide to developers, reducing the maintenance burden for developers, and enforcing code coverage. While maintainers report that bots satisfied their expectations, they also perceived unexpected effects, such as communication noise and newcomers' dropout. Based on these results, we provide a series of implications for bot developers, as well as insights for future research.",2020,13
The promises and perils of mining GitHub,"Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela","With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.",2014,14
Towards s/engineer/bot: principles for program repair bots,"van Tonder, Rijnard and Goues, Claire Le","Of the hundreds of billions of dollars spent on developer wages, up to 25% accounts for fixing bugs. Companies like Google save significant human effort and engineering costs with automatic bug detection tools, yet automatically fixing them is still a nascent endeavour. Very recent work (including our own) demonstrates the feasibility of automatic program repair in practice. As automated repair technology matures, it presents great appeal for integration into developer workflows. We believe software bots are a promising vehicle for realizing this integration, as they bridge the gap between human software development and automated processes. We envision repair bots orchestrating automated refactoring and bug fixing. To this end, we explore what building a repair bot entails. We draw on our understanding of patch generation, validation, and real world software development interactions to identify six principles that bear on engineering repair bots and discuss related design challenges for integrating human workflows. Ultimately, this work aims to foster critical focus and interest for making repair bots a reality.",2019,15
Experiences Building an Answer Bot for Gitter,"Romero, Ricardo and Parra, Esteban and Haiduc, Sonia","Software developers use modern chat platforms to communicate about the status of a project and to coordinate development and release efforts, among other things. Developers also use chat platforms to ask technical questions to other developers. While some questions are project-specific and require an experienced developer familiar with the system to answer, many questions are rather general and may have been already answered by other developers on platforms such as the Q&amp;A site StackOverflow.In this paper, we present GitterAns, a bot that can automatically detect when a developer asks a technical question in a chat and leverages the information present in Q&amp;A forums to provide the developer with possible answers to their question. The results of a preliminary study indicate promising results, with GitterAns achieving an accuracy of 0.78 in identifying technical questions.",2020,16
Understanding Promotion-as-a-Service on GitHub,"Du, Kun and Yang, Hao and Zhang, Yubao and Duan, Haixin and Wang, Haining and Hao, Shuang and Li, Zhou and Yang, Min","As the world’s leading software development platform, GitHub has become a social networking site for programmers and recruiters who leverage its social features, such as star and fork, for career and business development. However, in this paper, we found a group of GitHub accounts that conducted promotion services in GitHub, called “promoters”, by performing paid star and fork operations on specified repositories. We also uncovered a stealthy way of tampering with historical commits, through which these promoters are able to fake commits retroactively. By exploiting such a promotion service, any GitHub user can pretend to be a skillful developer with high influence. To understand promotion services in GitHub, we first investigated the underground promotion market of GitHub and identified 1,023 suspected promotion accounts from the market. Then, we developed an SVM (Support Vector Machine) classifier to detect promotion accounts from all active users extracted from GH Archive ranging from 2015 to 2019. In total, we detected 63,872 suspected promotion accounts. We further analyzed these suspected promotion accounts, showing that (1) a hidden functionality in GitHub is abused to boost the reputation of an account by forging historical commits and (2) a group of small businesses exploit GitHub promotion services to promote their products. We estimated that suspicious promoters could have made a profit of $3.41 million and $4.37 million in 2018 and 2019, respectively.",2020,17
An Empirical Study of Blockchain Repositories in GitHub,"Das, Ajoy and Uddin, Gias and Ruhe, Guenther","Blockchain is a distributed ledger technique that guarantees the traceability of transactions. Blockchain is adopted in multiple domains like finance (e.g., cryptocurrency), healthcare, security, and supply chain. In the open-source software (OSS) portal GitHub, we observe a growing adoption of Blockchain-based solutions. Given the rapid emergence of Blockchain-based solutions in our daily life and the evolving cryptocurrency market, it is important to know the status quo, how developers generally interact in those repos, and how much freedom they have in applying code changes. We report an empirical study of 3,664 Blockchain software repositories from GitHub. We divide the Blockchain repositories into two categories: Tool (e.g., SDKs) and Applications (e.g., service/solutions developed using SDKs). The Application category is further divided into two sub-categories: Crypto and Non-Crypto applications. In all Blockchain repository categories, the contribution interactions on commits are the most common interaction type. We found that more organizations contributing to the Blockchain repos than individual users. The median numbers of internal and external users in tools are higher than the application repos. We observed a higher degree of collaboration (e.g., for maintenance efforts) among users in Blockchain tools than those in the application repos. Among the artifacts, issues have a greater number of interactions than commits and pull requests. Related to autonomy we found that less than half of total project contributions are autonomous. Our findings offer implications to Blockchain stakeholders, like developers to stay aware of OSS practices around Blockchain software.",2022,18
Investigating the effects of gender bias on GitHub,"Imtiaz, Nasif and Middleton, Justin and Chakraborty, Joymallya and Robson, Neill and Bai, Gina and Murphy-Hill, Emerson","Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.",2019,19
CORRECT: code reviewer recommendation at GitHub for Vendasta technologies,"Rahman, Mohammad Masudur and Roy, Chanchal K. and Redl, Jesse and Collins, Jason A.","Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (e.g., external library experience) of a developer but also her experience in certain specialized technologies (e.g., Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the request, mines two relevant histories, and then returns a ranked list of appropriate code reviewers for the request within the browser's context. Demo: https://www.youtube.com/watch?v=rXU1wTD6QQ0",2016,20
GitHub sponsors: exploring a new way to contribute to open source,"Shimada, Naomichi and Xiao, Tao and Hata, Hideaki and Treude, Christoph and Matsumoto, Kenichi","GitHub Sponsors, launched in 2019, enables donations to individual open source software (OSS) developers. Financial support for OSS maintainers and developers is a major issue in terms of sustaining OSS projects, and the ability to donate to individuals is expected to support the sustainability of developers, projects, and community. In this work, we conducted a mixed-methods study of GitHub Sponsors, including quantitative and qualitative analyses, to understand the characteristics of developers who are likely to receive donations and what developers think about donations to individuals. We found that: (1) sponsored developers are more active than non-sponsored developers, (2) the possibility to receive donations is related to whether there is someone in their community who is donating, and (3) developers are sponsoring as a new way to contribute to OSS. Our findings are the first step towards data-informed guidance for using GitHub Sponsors, opening up avenues for future work on this new way of financially sustaining the OSS community.",2022,21
Conversational Bot for Newcomers Onboarding to Open Source Projects,"Dominic, James and Houser, Jada and Steinmacher, Igor and Ritter, Charles and Rodeghero, Paige","This paper targets the problems newcomers face when onboarding to open source projects and the low retention rate of newcomers. Open source software projects are becoming increasingly more popular. Many major companies have started building open source software. Unfortunately, many newcomers only commit once to an open source project before moving on to another project. Even worse, many novices struggle with joining open source communities and end up leaving quickly, sometimes before their first successful contribution. In this paper, we propose a conversational bot that would recommend projects to newcomers and assist in the onboarding to the open source community. The bot would be able to provide helpful resources, such as Stack Overflow related content. It would also be able to recommend human mentors. We believe that this bot would improve newcomers' experience by providing support not only during their first contribution, but by acting as an agent to engage them to the project.",2020,22
Painting the landscape of automotive software in GitHub,"Kochanthara, Sangeeth and Dajsuren, Yanja and Cleophas, Loek and van den Brand, Mark","The automotive industry has transitioned from being an electromechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook's front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing a first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing its characteristics and development styles.The landscape is defined by 15,000+ users contributing to ≈600 actively-developed automotive software projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics-related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe shifts along multiple dimensions, including preferred language from MATLAB to Python and prevalence of perception and decision-related software over traditional automotive software. This study witnesses open source automotive software boom in its infancy with many implications for future research and practice.",2022,23
"Combining GitHub, Chat, and Peer Evaluation Data to Assess Individual Contributions to Team Software Development Projects","Hundhausen, Christopher and Conrad, Phill and Adesope, Olusola and Tariq, Ahsun","Assessing team software development projects is notoriously difficult and typically based on subjective metrics. To help make assessments more rigorous, we conducted an empirical study to explore relationships between subjective metrics based on peer and instructor assessments, and objective metrics based on GitHub and chat data. We studied 23 undergraduate software teams (n = 117 students) from two undergraduate computing courses at two North American research universities. We collected data on teams’ (a) commits and issues from their GitHub code repositories, (b) chat messages from their Slack and Microsoft Teams channels, (c) peer evaluation ratings from the CATME peer evaluation system, and (d) individual assignment grades from the courses. We derived metrics from (a) and (b) to measure both individual team members’ contributions to the team, and the equality of team members’ contributions. We then performed Pearson analyses to identify correlations among the metrics, peer evaluation ratings, and individual grades. We found significant positive correlations between team members’ GitHub contributions, chat contributions, and peer evaluation ratings. In addition, the equality of teams’ GitHub contributions was positively correlated with teams’ average peer evaluation ratings and negatively correlated with the variance in those ratings. However, no such positive correlations were detected between the equality of teams’ chat contributions and their peer evaluation ratings. Our study extends previous research results by providing evidence that (a) team members’ chat contributions, like their GitHub contributions, are positively correlated with their peer evaluation ratings; (b) team members’ chat contributions are positively correlated with their GitHub contributions; and (c) the equality of team’ GitHub contributions is positively correlated with their peer evaluation ratings. These results lend further support to the idea that combining objective and subjective metrics can make the assessment of team software projects more comprehensive and rigorous.",2023,24
Understanding the impact of GitHub suggested changes on recommendations between developers,"Brown, Chris and Parnin, Chris","Recommendations between colleagues are effective for encouraging developers to adopt better practices. Research shows these peer interactions are useful for improving developer behaviors, or the adoption of activities to help software engineers complete programming tasks. However, in-person recommendations between developers in the workplace are declining. One form of online recommendations between developers are pull requests, which allow users to propose code changes and provide feedback on contributions. GitHub, a popular code hosting platform, recently introduced the suggested changes feature, which allows users to recommend improvements for pull requests. To better understand this feature and its impact on recommendations between developers, we report an empirical study of this system, measuring usage, effectiveness, and perception. Our results show that suggested changes support code review activities and significantly impact the timing and communication between developers on pull requests. This work provides insight into the suggested changes feature and implications for improving future systems for automated developer recommendations, such as providing situated, concise, and actionable feedback.",2020,25
Predicting open source contributor turnover from value-related discussions: An analysis of GitHub issues,"Jamieson, Jack and Yamashita, Naomi and Foong, Eureka","Discussions about project values are important for engineering software that meets diverse human needs and positively impacts society. Because value-related discussions involve deeply held beliefs, they can lead to conflicts or other outcomes that may affect motivations to continue contributing to open source projects. However, it is unclear what kind of value-related discussions are associated with significant changes in turnover. We address this gap by identifying discussions related to important project values and investigating the extent to which those discussions predict project turnover in the following months. We collected logs of GitHub issues and commits from 52 projects that share similar ethical commitments and were identified as part of the DWeb (Decentralized Web) community. We identify issues related to DWeb's core values of respectfulness, freedom, broadmindedness, opposing centralized social power, equity &amp; equality, and protecting the environment. We then use Granger causality analysis to examine how changes in the proportion of discussions related to those values might predict changes in incoming and outgoing turnover. We found multiple significant relationships between value-related discussions and turnover, including that discussions about respectfulness predict an increase in contributors leaving and a decrease in new contributors, while discussions about social power predicted better contributor retention. Understanding these antecedents of contributor turnover is important for managing open source projects that incorporate human-centric issues. Based on the results, we discuss implications for open source maintainers and for future research.",2024,26
Leveraging predictions from multiple repositories to improve bot detection,"Chidambaram, Natarajan and Decan, Alexandre and Golzadeh, Mehdi","Contemporary social coding platforms such as GitHub facilitate collaborative distributed software development. Developers engaged in these platforms often use machine accounts (bots) for automating effort-intensive or repetitive activities. Determining whether a contributor corresponds to a bot or a human account is important in socio-technical studies, for example to assess the positive and negative impact of using bots, analyse the evolution of bots and their usage, identify top human contributors, and so on. BoDeGHa is one of the bot detection tools that have been proposed in the literature. It relies on comment activity within a single repository to predict whether an account is driven by a bot or by a human. This paper presents preliminary results on how the effectiveness of BoDeGHa can be improved by combining the predictions obtained from many repositories at once. We found that doing this not only increases the number of cases for which a prediction can be made, but that many diverging predictions can be fixed this way. These promising, albeit preliminary, results suggest that the ""wisdom of the crowd"" principle can improve the effectiveness of bot detection tools.",2022,27
Herding a Deluge of Good Samaritans: How GitHub Projects Respond to Increased Attention,"Maldeniya, Danaja and Budak, Ceren and Robert Jr., Lionel P. and Romero, Daniel M.","Collaborative crowdsourcing is a well-established model of work, especially in the case of open source software development. The structure and operation of these virtual and loosely-knit teams differ from traditional organizations. As such, little is known about how their behavior may change in response to an increase in external attention. To understand these dynamics, we analyze millions of actions of thousands of contributors in over 1100 open source software projects that topped the GitHub Trending Projects page and thus experienced a large increase in attention, in comparison to a control group of projects identified through propensity score matching. In carrying out our research, we use the lens of organizational change, which considers the challenges teams face during rapid growth and how they adapt their work routines, organizational structure, and management style. We show that trending results in an explosive growth in the effective team size. However, most newcomers make only shallow and transient contributions. In response, the original team transitions towards administrative roles, responding to requests and reviewing work done by newcomers. Projects evolve towards a more distributed coordination model with newcomers becoming more central, albeit in limited ways. Additionally, teams become more modular with subgroups specializing in different aspects of the project. We discuss broader implications for collaborative crowdsourcing teams that face attention shocks.",2020,28
Explainable software bot contributions: case study of automated bug fixes,"Monperrus, Martin","In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans.Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers.We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of ""dry patches"": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an ""explainable bug fix"": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples.In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation.",2019,29
The promises and perils of open source software release and usage by government – evidence from GitHub and literature,"Eibl, Gregor and Thurnay, L\H{o}rinc","Abstract: Open Source Software (OSS) is extensively utilized in industry and government because it allows for open access to the source code and allows for external involvement in the software development process. There are several factors driving this movement in a government setting, making it difficult to assess the adoption's success. Through a study of billions of rows of GitHub activity data, this research analyzes the production of OSS by administrations in German-speaking countries in detail and analyses the motivating factors and challenges to OSS adoption through a literature review. Similar studies have been conducted in other nations, with somewhat different approaches, foci, and different ways to identify public GitHub users as well as insiders and outsiders of OSS projects. 16 consequences of OSS usage and development are listed in the paper. On GitHub, we found 1021 OSS projects run by public agencies in largly German-speaking nations. We then compiled a list of the most popular projects based on commits and the most active public agencies in terms of projects. The research also finds automatic contributions by bots, which have not been taken into account in the literature so far, and demonstrates highly substantial positive correlations between commits, forks, and stars as proxy for the popularity of these projects. This research introduces a new method for identifying government organizations in OSS platforms and illuminates the possible positive and negative effects of the public sector's release and adoption of open source software.",2023,30
CORMS: a GitHub and Gerrit based hybrid code reviewer recommendation approach for modern code review,"Pandya, Prahar and Tiwari, Saurabh","Modern Code review (MCR) techniques are widely adopted in both open-source software platforms and organizations to ensure the quality of their software products. However, the selection of reviewers for code review is cumbersome with the increasing size of development teams. The recommendation of inappropriate reviewers for code review can take more time and effort to complete the task effectively. In this paper, we extended the baseline of reviewers' recommendation framework - RevFinder - to handle issues with newly created files, retired reviewers, the external validity of results, and the accuracies of the state-of-the-art RevFinder. Our proposed hybrid approach, CORMS, works on similarity analysis to compute similarities among file paths, projects/sub-projects, author information, and prediction models to recommend reviewers based on the subject of the change. We conducted a detailed analysis on the widely used 20 projects of both Gerrit and GitHub to compare our results with RevFinder. Our results reveal that on average, CORMS, can achieve top-1, top-3, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 45.1%, 67.5%, 74.6%, 79.9% and 0.58 for the 20 projects, consequently improves the RevFinder approach by 44.9%, 34.4%, 20.8%, 12.3% and 18.4%, respectively.",2022,31
Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects,"Khatoonabadi, Sayedhassan and Costa, Diego Elias and Mujahid, Suhaib and Shihab, Emad","Pull Requests (PRs) that are neither progressed nor resolved clutter the list of PRs, making it difficult for the maintainers to manage and prioritize unresolved PRs. To automatically track, follow up, and close such inactive PRs, Stale bot was introduced by GitHub. Despite its increasing adoption, there are ongoing debates on whether using Stale bot alleviates or exacerbates the problem of inactive PRs. To better understand if and how Stale bot helps projects in their pull-based development workflow, we perform an empirical study of 20 large and popular open source projects. We find that Stale bot can help deal with a backlog of unresolved PRs, as the projects closed more PRs within the first few months of adoption. Moreover, Stale bot can help improve the efficiency of the PR review process as the projects reviewed PRs that ended up merged and resolved PRs that ended up closed faster after the adoption. However, Stale bot can also negatively affect the contributors, as the projects experienced a considerable decrease in their number of active contributors after the adoption. Therefore, relying solely on Stale bot to deal with inactive PRs may lead to decreased community engagement and an increased probability of contributor abandonment.",2023,32
An Exploratory Study of Bot Commits,"Dey, Tapajit and Vasilescu, Bogdan and Mockus, Audris","Background: Bots help automate many of the tasks performed by software developers and are widely used to commit code in various social coding platforms. At present, it is not clear what types of activities these bots perform and understanding it may help design better bots, and find application areas which might benefit from bot adoption. Aim: We aim to categorize the Bot Commits by the type of change (files added, deleted, or modified), find the more commonly changed file types, and identify the groups of file types that tend to get updated together. Method: 12,326,137 commits made by 461 popular bots (that made at least 1000 commits) were examined to identify the frequency and the type of files added/ deleted/ modified by the commits, and association rule mining was used to identify the types of files modified together. Result: Majority of the bot commits modify an existing file, a few of them add new files, while deletion of a file is very rare. Commits involving more than one type of operation are even rarer. Files containing data, configuration, and documentation are most frequently updated, while HTML is the most common type in terms of the number of files added, deleted, and modified. Files of the type ""Markdown"",""Ignore List"", ""YAML"", ""JSON"" were the types that are updated together with other types of files most frequently. Conclusion: We observe that majority of bot commits involve single file modifications, and bots primarily work with data, configuration, and documentation files. A better understanding if this is a limitation of the bots and, if overcome, would lead to different kinds of bots remains an open question.",2020,33
"""Could You Define That in Bot Terms""? Requesting, Creating and Using Bots on Reddit","Long, Kiel and Vines, John and Sutton, Selina and Brooker, Phillip and Feltwell, Tom and Kirman, Ben and Barnett, Julie and Lawson, Shaun","Bots are estimated to account for well over half of all web traffic, yet they remain an understudied topic in HCI. In this paper we present the findings of an analysis of 2284 submissions across three discussion groups dedicated to the request, creation and discussion of bots on Reddit. We set out to examine the qualities and functionalities of bots and the practical and social challenges surrounding their creation and use. Our findings highlight the prevalence of misunderstandings around the capabilities of bots, misalignments in discourse between novices who request and more expert members who create them, and the prevalence of requests that are deemed to be inappropriate for the Reddit community. In discussing our findings, we suggest future directions for the design and development of tools that support more carefully guided and reflective approaches to bot development for novices, and tools to support exploring the consequences of contextually-inappropriate bot ideas.",2017,34
TwiBot-20: A Comprehensive Twitter Bot Detection Benchmark,"Feng, Shangbin and Wan, Herun and Wang, Ningnan and Li, Jundong and Luo, Minnan","Twitter has become a vital social media platform while an ample amount of malicious Twitter bots exist and induce undesirable social effects. Successful Twitter bot detection proposals are generally supervised, which rely heavily on large-scale datasets. However, existing benchmarks generally suffer from low levels of user diversity, limited user information and data scarcity. Therefore, these datasets are not sufficient to train and stably benchmark bot detection measures. To alleviate these problems, we present TwiBot-20, a massive Twitter bot detection benchmark, which contains 229,573 users, 33,488,192 tweets, 8,723,736 user property items and 455,958 follow relationships. TwiBot-20 covers diversified bots and genuine users to better represent the real-world Twittersphere. TwiBot-20 also includes three modals of user information to support both binary classification of single users and community-aware approaches. To the best of our knowledge, TwiBot-20 is the largest Twitter bot detection benchmark to date. We reproduce competitive bot detection methods and conduct a thorough evaluation on TwiBot-20 and two other public datasets. Experiment results demonstrate that existing bot detection measures fail to match their previously claimed performance on TwiBot-20, which suggests that Twitter bot detection remains a challenging task and requires further research efforts.",2021,35
Between JIRA and GitHub: ASFBot and its influence on human comments in issue trackers,"Moharil, Ambarish and Orlov, Dmitrii and Jameel, Samar and Trouwen, Tristan and Cassee, Nathan and Serebrenik, Alexander","Open-Source Software (OSS) projects have adopted various automations for repetitive tasks in recent years. One common type of automation in OSS is bots. In this exploratory case study, we seek to understand how the adoption of one particular bot (ASFBot) by the Apache Software Foundation (ASF) impacts the discussions in the issue-trackers of these projects. We use the SmartShark dataset to investigate whether the ASFBot affects (i) human comments mentioning pull requests and fixes in issue comments and (ii) the general human comment rate on issues. We apply a regression discontinuity design (RDD) on nine ASF projects that have been active both before and after the ASFBot adoption. Our results indicate (i) an immediate decrease in the number of median comments mentioning pull requests and fixes after the bot adoption, but the trend of a monthly decrease in this comment count is reversed, and (ii) no effect in the number of human comments after the bot adoption. We make an effort to gather first insights in understanding the impact of adopting the ASFBot on the commenting behavior of developers who are working on ASF projects.",2022,36
Towards Understanding and Characterizing the Arbitrage Bot Scam In the Wild,"Li, Kai and Guan, Shixuan and Lee, Darren","This paper presents the first comprehensive analysis of an emerging cryptocurrency scam named ""arbitrage bot"" disseminated on online social networks. The scam revolves around Decentralized Exchanges (DEX) arbitrage and aims to lure victims into executing a so-called ""bot contract"" to steal funds from them. To entice victims and convince them of this scheme, we found that scammers have flocked to publish YouTube videos to demonstrate plausible profits and provide detailed instructions and links to the bot contract.To collect the scam at a large scale, we developed a fully automated scam detection system namedCryptoScamHunter, which continuously collects YouTube videos and automatically detects scams. Meanwhile,CryptoScamHunter can download the source code of the bot contract from the provided links and extract the associated scam cryptocurrency address. Through deployingCryptoScamHunter from Jun. 2022 to Jun. 2023, we have detected 10,442 arbitrage bot scam videos published from thousands of YouTube accounts. Our analysis reveals that different strategies have been utilized in spreading the scam, including crafting popular accounts, registering spam accounts, and using obfuscation tricks to hide the real scam address in the bot contracts. Moreover, from the scam videos we have collected over 800 malicious bot contracts with source code and extracted 354 scam addresses. By further expanding the scam addresses with a similar contract matching technique, we have obtained a total of 1,697 scam addresses. Through tracing the transactions of all scam addresses on the Ethereum mainnet and Binance Smart Chain, we reveal that over 25,000 victims have fallen prey to this scam, resulting in a financial loss of up to 15 million USD.Overall, our work sheds light on the dissemination tactics and censorship evasion strategies adopted in the arbitrage bot scam, as well as on the scale and impact of such a scam on online social networks and blockchain platforms, emphasizing the urgent need for effective detection and prevention mechanisms against such fraudulent activity.",2023,37
GitHub-OSS fixit,"Tan, Shin Hwei and Hu, Chunfeng and Li, Ziqiang and Zhang, Xiaowen and Zhou, Ying","Many studies have shown the benefits of introducing open-source projects into teaching Software Engineering (SE) courses. However, there are several limitations of existing studies that limit the wide adaptation of open-source projects in a classroom setting, including (1) the selected project is limited to one particular project, (2) most studies only investigated on its effect on teaching a specific SE concept, and (3) students may make mistakes in their contribution which leads to poor quality code. Meanwhile, software companies have successfully launched programs like Google Summer of Code (GSoC) and FindBugs ""fixit"" to contribute to open-source projects. Inspired by the success of these programs, we propose GitHub-OSS Fixit, a team-based course project where students are taught to contribute to open-source Java projects by fixing bugs reported in GitHub. We described our course outline to teach students SE concepts by encouraging the usages of several automated program analysis tools. We also included the carefully designed instructions that we gave to students for participating in GitHub-OSS Fixit. As all lectures and labs are conducted online, we think that our course design could help in guiding future online SE courses. Overall, our survey results show that students think that GitHub-OSS Fixit could help them to improve many skills and apply the knowledge taught in class. In total, 154 students have submitted 214 pull requests to 24 different Java projects, in which 93 of them have been merged, and 46 have been closed by developers.",2021,38
Automatic Core-Developer Identification on GitHub: A Validation Study,"Bock, Thomas and Alznauer, Nils and Joblin, Mitchell and Apel, Sven","Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods.",2023,39
Matching GitHub developer profiles to job advertisements,"Hauff, Claudia and Gousios, Georgios","GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally ""be seen"" by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.",2015,40
FixMe: a GitHub bot for detecting and monitoring on-hold self-admitted technical debt,"Phaithoon, Saranphon and Wongnil, Supakarn and Pussawong, Patiphol and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee and Maipradit, Rungroj and Hata, Hideaki and Matsumoto, Kenichi","Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related ""On-hold SATD"", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN_YQ.",2022,41
An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub,"S\""{u}l\""{u}n, Emre and Sa\c{c}ak\c{c}\i{}, Metehan and T\""{u}z\""{u}n, Eray","GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates’ extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18&nbsp;days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates’ positive impact on large-scale open-source projects, offering recommendations for improved effectiveness.",2024,42
How to design a program repair bot? insights from the repairnator project,"Urli, Simon and Yu, Zhongxing and Seinturier, Lionel and Monperrus, Martin","Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a ""program repair bot"" and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 523 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 15 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.",2018,43
"Mentions of Security Vulnerabilities on Reddit, Twitter and GitHub","Horawalavithana, Sameera and Bhattacharjee, Abhishek and Liu, Renhao and Choudhury, Nazim and O. Hall, Lawrence and Iamnitchi, Adriana","Activity on social media is seen as a relevant sensor for different aspects of the society. In a heavily digitized society, security vulnerabilities pose a significant threat that is publicly discussed on social media. This study presents a comparison of user-generated content related to security vulnerabilities on three digital platforms: two social media conversation channels (Reddit and Twitter) and a collaborative software development platform (GitHub). Our data analysis shows that while more security vulnerabilities are discussed on Twitter, relevant conversations go viral earlier on Reddit. We show that the two social media platforms can be used to accurately predict activity on GitHub.",2019,44
Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection,"Hays, Chris and Schutzman, Zachary and Raghavan, Manish and Walk, Erin and Zimmer, Philipp","Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near-perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules — shallow decision trees trained on a small number of features — achieve near-state-of-the-art performance on most available datasets and that bot detection datasets, even when combined together, do not generalize well to out-of-sample datasets. Our findings reveal that predictions are highly dependent on each dataset’s collection and labeling procedures rather than fundamental differences between bots and humans. These results have important implications for both transparency in sampling and labeling procedures and potential biases in research using existing bot detection tools for pre-processing.",2023,45
Understanding User-Bot Interactions for Small-Scale Automation in Open-Source Development,"Liu, Dongyu and Smith, Micah J. and Veeramachaneni, Kalyan","Small-scale automation tools, or ""bots,"" have been widely deployed in open-source software development to support manual project maintenance tasks. Though interactions between these bots and human developers can have significant effects on user experience, previous research has instead mostly focused on project outcomes. We reviewed existing small-scale bots in wide use on GitHub. After an in-depth qualitative and quantitative evaluation, we compiled several important design principles for human-bot interaction in this context. Following the requirements, we further propose a workflow to support bot developers.",2020,46
Exploring ChatGPT for Toxicity Detection in GitHub,"Mishra, Shyamal and Chatterjee, Preetha","Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses significant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models effectively, we need large software engineering-specific toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training effective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.",2024,47
How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions,"Bernardo, Jo\~{a}o Helis and Da Costa, Daniel Alencar and Medeiros, S\'{e}rgio Queiroz de and Kulesza, Uir\'{a}","Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build duration, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively.",2024,48
Promises and Perils of Inferring Personality on GitHub,"van Mil, Frenk C.J. and Rastogi, Ayushi and Zaidman, Andy","Background: Personality plays a pivotal role in our understanding of human actions and behavior. Today, the applications of personality are widespread, built on the solutions from psychology to infer personality. Aim: In software engineering, for instance, one widely used solution to infer personality uses textual communication data. As studies on personality in software engineering continue to grow, it is imperative to understand the performance of these solutions. Method: This paper compares the inferential ability of three widely studied text-based personality tests against each other and the ground truth on GitHub. We explore the challenges and potential solutions to improve the inferential ability of personality tests. Results: Our study shows that solutions for inferring personality are far from being perfect. Software engineering communications data can infer individual developer personality with an average error rate of 41%. In the best case, the error rate can be reduced up to 36% by following our recommendations1.",2021,49
Which contributions predict whether developers are accepted into github teams,"Middleton, Justin and Murphy-Hill, Emerson and Green, Demetrius and Meade, Adam and Mayer, Roger and White, David and McDonald, Steve","Open-source software (OSS) often evolves from volunteer contributions, so OSS development teams must cooperate with their communities to attract new developers. However, in view of the myriad ways that developers interact over platforms for OSS development, observers of these communities may have trouble discerning, and thus learning from, the successful patterns of developer-to-team interactions that lead to eventual team acceptance. In this work, we study project communities on GitHub to discover which forms of software contribution characterize developers who begin as development team outsiders and eventually join the team, in contrast to developers who remain team outsiders. From this, we identify and compare the forms of contribution, such as pull requests and several forms of discussion comments, that influence whether new developers join OSS teams, and we discuss the implications that these behavioral patterns have for the focus of designers and educators.",2018,50
Community formation and detection on GitHub collaboration networks,"Moradi-Jamei, Behnaz and Kramer, Brandon L. and Calder\'{o}n, J. Bayo\'{a}n Santiago and Korkmaz, Gizem","This paper studies community formation in OSS collaboration networks. While most current work examines the emergence of small-scale OSS projects, our approach draws on a large-scale historical dataset of 1.8 million GitHub users and their repository contributions. OSS collaborations are characterized by small groups of users that work closely together, leading to the presence of communities defined by short cycles in the underlying network structure. To understand the impact of this phenomenon, we apply a pre-processing step that accounts for the cyclic network structure by using Renewal-Nonbacktracking Random Walks (RNBRW) and the strength of pairwise collaborations before implementing the Louvain method to identify communities within the network. Equipping Louvain with RNBRW and the contribution strength provides a more assertive approach for detecting small-scale teams and reveals nontrivial differences in community detection such as users' tendencies toward preferential attachment to more established collaboration communities. Using this method, we also identify key factors that affect community formation, including the effect of users' location and primary programming language, which was determined using a comparative method of contribution activities. Overall, this paper offers several promising methodological insights for both open-source software experts and network scholars interested in studying team formation.",2022,51
A Preliminary Study of Bots Usage in Open Source Community,"Wu, Xiaojun and Gao, Anze and Zhang, Yang and Wang, Tao and Tang, Yi","Bots are seen as a promising approach in software development, which help to deal with the ever-increasing complexity of modern software engineering and development. The number of bots in open source community, such as GitHub, has expanded substantially over the last three years. Due to its increasing popularity, it is essential to characterize the current usage of bots in practices. In this paper, we present an empirical study of bots usage in GitHub community. By analyzing 7,399 projects from GitHub, we find that 4,148 (56%) projects have used bots. Through automatic identification and manual detection, we collect a total of 196 bots. We then analyze and classify them into 4 categories and 14 topics. Finally, we discuss some raised implications for bots in current GitHub community.",2022,52
Recommending good first issues in GitHub OSS projects,"Xiao, Wenxin and He, Hao and Xu, Weiwei and Tan, Xin and Dong, Jinhao and Zhou, Minghui","Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for newcomers to locate suitable development tasks, while existing ""Good First Issues"" (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RecGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RecGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RecGFI, we collect 53,510 resolved issues among 100 GitHub projects and carefully restore their historical states to build ground truth datasets. Our evaluation shows that RecGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals interesting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer.",2022,53
Disrupting developer productivity one bot at a time,"Storey, Margaret-Anne and Zagalsky, Alexey","Bots are used to support different software development activities, from automating repetitive tasks to bridging knowledge and communication gaps in software teams. We anticipate the use of Bots will increase and lead to improvements in software quality and developer and team productivity, but what if the disruptive effect is not what we expect?  Our goal in this paper is to provoke and inspire researchers to study the impact (positive and negative) of Bots on software development. We outline the modern Bot landscape and use examples to describe the common roles Bots occupy in software teams. We propose a preliminary cognitive support framework that can be used to understand these roles and to reflect on the impact of Bots in software development on productivity. Finally, we consider challenges that Bots may bring and propose some directions for future research.",2016,54
A decade of social bot detection,"Cresci, Stefano",Bots increasingly tamper with political elections and economic discussions. Tracing trends in detection strategies and key suggestions on how to win the fight.,2020,55
An empirical study on the survival rate of GitHub projects,"Ait, Adem and Izquierdo, Javier Luis C\'{a}novas and Cabot, Jordi","The number of Open Source projects hosted in social coding platforms such as GitHub is constantly growing. However, many of these projects are not regularly maintained and some are even abandoned shortly after they were created. In this paper we analyze early project development dynamics in software projects hosted on GitHub, including their survival rate. To this aim, we collected all 1,127 GitHub repositories from four different ecosystems (i.e., NPM packages, R packages, WordPress plugins and Laravel packages) created in 2016. We stored their activity in a time series database and analyzed their activity evolution along their lifespan, from 2016 to now. Our results reveal that the prototypical development process consists of intensive coding-driven active periods followed by long periods of inactivity. More importantly, we have found that a significant number of projects die in the first year of existence with the survival rate decreasing year after year. In fact, the probability of surviving longer than five years is less than 50% though some types of projects have better chances of survival.",2022,56
Towards Human-Bot Collaborative Software Architecting with ChatGPT,"Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fahmideh, Mahdi and Aktar, Mst Shamima and Mikkonen, Tommi","Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders’ perspectives, designers’ intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects’ knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects’ productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE.",2023,57
Bots By Topic: Exploring Differences in Bot Activity by Conversation Topic,"Wirth, Kurt and Menchen-Trevino, Ericka and Moore, Ryan T.","This study introduces a new tool to compare bot levels in real-time across conversation topics or hashtags. With the data collected, we measured higher levels of bot activity in some topics of conversation as compared to others and propose a novel application of bot detection analysis to advance research in this fast-changing field.",2019,58
Code of Conduct Conversations in Open Source Software Projects on Github,"Li, Renee and Pandurangan, Pavitthra and Frluckaj, Hana and Dabbish, Laura","The rapid growth of open source software necessitates a deeper understanding of moderation and governance methods currently used within these projects. The code of conduct, a set of rules articulating standard behavior and responsibilities for participation within a community, is becoming an increasingly common policy document in open source software projects for setting project norms of behavior and discouraging negative or harassing comments and conversation. This study describes the conversations around adopting and crafting a code of conduct as well as those utilizing code of conduct for community governance. We conduct a qualitative analysis of a random sample of GitHub issues that involve the code of conduct. We find that codes of conduct are used both proactively and reactively to govern community behavior in project issues. Oftentimes, the initial addition of a code of conduct does not involve much community participation and input. However, a controversial moderation act is capable of inciting mass community feedback and backlash. Project maintainers balance the tension between disciplining potentially offensive forms of speech and encouraging broad and inclusive participation. These results have implications for the design of inclusive and effective governance practices for open source software communities.",2021,59
GitHubInclusifier: Finding and fixing non-inclusive language in GitHub Repositories,"Todd, Liam and Grundy, John and Treude, Christoph","Non-inclusive language in software artefacts has been recognised as a serious problem. We describe a tool to find and fix non-inclusive language in a variety of GitHub repository artefacts. These include various README files, PDFs, code comments, and code. A wide variety of non-inclusive language including racist, ageist, ableist, violent and others are located and issues created, tagging the artefacts for checking. Suggested fixes can be generated using third-party LLM APIs, and approved changes made to documents, including code refactorings, and committed to the repository.The tool and evaluation data are available from: https://github.com/LiamTodd/github-inclusifierThe demo video is available at: https://www.youtube.com/watch?v=1z1QKdQg-nM",2024,60
A Tool for the Definition and Deployment of Platform-Independent Bots on Open Source Projects,"Ait, Adem and C\'{a}novas Izquierdo, Javier Luis and Cabot, Jordi","The development of Open Source Software (OSS) projects is a collaborative process that heavily relies on active contributions by passionate developers. Creating, retaining and nurturing an active community of developers is a challenging task; and finding the appropriate expertise to drive the development process is not always easy. To alleviate this situation, many OSS projects try to use bots to automate some development tasks, thus helping community developers to cope with the daily workload of their projects. However, the techniques and support for developing bots is specific to the code hosting platform where the project is being developed (e.g., GitHub or GitLab). Furthermore, there is no support for orchestrating bots deployed in different platforms nor for building bots that go beyond pure development activities. In this paper, we propose a tool to define and deploy bots for OSS projects, which besides automation tasks they offer a more social facet, improving community interactions. The tool includes a Domain-Specific Language (DSL) which allows defining bots that can be deployed on top of several platforms and that can be triggered by different events (e.g., creation of a new issue or a pull request). We describe the design and the implementation of the tool, and illustrate its use with examples.",2023,61
There’s no Such Thing as a Free Lunch: Lessons Learned from Exploring the Overhead Introduced by the Greenkeeper Dependency Bot in Npm,"Rombaut, Benjamin and Cogo, Filipe R. and Adams, Bram and Hassan, Ahmed E.","Dependency management bots are increasingly being used to support the software development process, for example, to automatically update a dependency when a new version is available. Yet, human intervention is often required to either accept or reject any action or recommendation the bot creates. In this article, our objective is to study the extent to which dependency management bots create additional, and sometimes unnecessary, work for their users. To accomplish this, we analyze 93,196 issue reports opened by Greenkeeper, a popular dependency management bot used in open source software projects in the npm ecosystem. We find that Greenkeeper is responsible for half of all issues reported in client projects, inducing a significant amount of overhead that must be addressed by clients, since many of these issues were created as a result of Greenkeeper taking incorrect action on a dependency update (i.e., false alarms). Reverting a broken dependency update to an older version, which is a potential solution that requires the least overhead and is automatically attempted by Greenkeeper, turns out to not be an effective mechanism. Finally, we observe that 56% of the commits referenced by Greenkeeper issue reports only change the client’s dependency specification file to resolve the issue. Based on our findings, we argue that dependency management bots should (i) be configurable to allow clients to reduce the amount of generated activity by the bots, (ii) take into consideration more sources of information than only the pass/fail status of the client’s build pipeline to help eliminate false alarms, and (iii) provide more effective incentives to encourage clients to resolve dependency issues.",2023,62
BotHunter: an approach to detect software bots in GitHub,"Abdellatif, Ahmad and Wessel, Mairieli and Steinmacher, Igor and Gerosa, Marco A. and Shihab, Emad","Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level.",2022,63
Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues,"Ehsani, Ramtin and Rezapour, Rezvaneh and Chatterjee, Preetha","To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.",2023,64
Coding together in a social network: collaboration among GitHub users,"Celi\'{n}ska, Dorota","In this article we investigate developers involved in the creation of Open Source software to identify which characteristics favor innovation in the Open Source community. The results of the analysis show that higher reputation in the community improves the probability of gaining collaborators to a certain degree, but developers are also driven by reciprocity. This is consistent with the concept of gift economy. A significant network effect exists and emerges from standardization, showing that developers using the most popular programming languages in the service are likely to have more collaborators. Providing additional information (valid URL to developer's homepage) improves the chances of finding coworkers. The results can be generalized for the population of mature users of GitHub.",2018,65
Should I stale or should I close? an analysis of a bot that closes abandoned issues and pull requests,"Wessel, Mairieli and Steinmacher, Igor and Wiese, Igor and Gerosa, Marco A.","On GitHub, projects use bots to automate predefined and repetitive tasks related to issues and pull requests. Our research investigates the adoption of the stale bot, which helps maintainers triaging abandoned issues and pull requests. We analyzed the bots' configuration settings and their modifications over time. These settings define the time for tagging issues and pull request as stale and closing them. We collected data from 765 OSS projects hosted on GitHub. Our results indicate that most of the studied projects made no more than three modifications in the configurations file, issues tagged as bug reports are exempt from being considered stale, while the same occurs with pull requests that need some input to be processed.",2019,66
How Gamification Affects Software Developers: Cautionary Evidence from a Natural Experiment on GitHub,"Moldon, Lukas and Strohmaier, Markus and Wachs, Johannes","We examine how the behavior of software developers changes in response to removing gamification elements from GitHub, an online platform for collaborative programming and software development. We find that the unannounced removal of daily activity streak counters from the user interface (from user profile pages) was followed by significant changes in behavior. Long-running streaks of activity were abandoned and became less common. Weekend activity decreased and days in which developers made a single contribution became less common. Synchronization of streaking behavior in the platform's social network also decreased, suggesting that gamification is a powerful channel for social influence. Focusing on a set of software developers that were publicly pursuing a goal to make contributions for 100 days in a row, we find that some of these developers abandon this quest following the removal of the public streak counter. Our findings provide evidence for the significant impact of gamification on the behavior of developers on large collaborative programming and software development platforms. They urge caution: gamification can steer the behavior of software developers in unexpected and unwanted directions.",2021,67
Enhancing developers’ support on pull requests activities with software bots,"Wessel, Mairieli","Software bots are employed to support developers' activities, serving as conduits between developers and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save development cost, time, and effort, the bots' presence can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers enhance existing bots. Toward this end, we are interviewing maintainers, contributors, and bot developers to understand the problems in the human-bot interaction and how they affect the collaboration in a project. Afterward, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.",2020,68
RefBot: intelligent software refactoring bot,"Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem","The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost.In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any ""open"" or ""merge"" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.",2020,69
Build-A-Bot: Developing A Software Platform For A Modular Mobile Robot,"Kassem, Khaled and Pavlova, Galya and Schlund, Sebastian and Michahelles, Florian","This work aims to create a software platform that can manage dynamic changes in the configuration and functionalities of a modular robot that is currently in development, simulate its final appearance and behavior in augmented reality, and evaluate its usability through a user study. We developed a software platform for a modular collaborative robot where users can add, remove, and swap modules without code changes. The software platform displays possible functions based on the current configuration of modules. To enable 3D interaction with the robot’s digital twin in real-time, we utilized an AR environment with the HoloLens. We conducted a user study with 28 participants without prior knowledge of robotics to evaluate the software’s usability and user experience for non-experts. The study results indicate that the software platform was well-received and user-friendly, with the digital twin in an AR environment providing a realistic robot simulation. Participants’ subjective feedback on usability, user experience, and cognitive workload of different software components was collected, and the analysis showed that the platform is suitable for non-experts. The results showed that our design of the platform and its UI is well-accepted and easy to use, shown by a SUS score of 74.29. We show how we built our software platform, as well as the simulation in AR. Moreover, we propose the practice of using AR simulation to design the software platform before future physical prototype development, in order to test different scenarios, and possibly inform the physical design. Finally, we recommend that future work explores usability with a more diverse set of non-expert users, as well as different tasks.",2024,70
Leveraging Software Bots to Enhance Developers' Collaboration in Online Programming Communities,"Wessel, Mairieli","Software bots are applications that are integrated into human communication channels, serving as an interface between users and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save developers' costs, time, and effort, the interaction of these bots can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers to enhance existing bots, thereby improving the partnership with contributors and maintainers. Toward this end, we are interviewing developers to understand what are the problems on the human-bot interaction and how they affect human collaboration. Afterwards, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.",2020,71
"Bots for pull requests: the good, the bad, and the promising","Wessel, Mairieli and Abdellatif, Ahmad and Wiese, Igor and Conte, Tayana and Shihab, Emad and Gerosa, Marco A. and Steinmacher, Igor","Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (""the good""). However, their interactions can be disruptive and noisy and lead to information overload (""the bad""). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (""the promising""). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",2022,72
Bob or Bot: Exploring ChatGPT's Answers to University Computer Science Assessment,"Richards, Mike and Waugh, Kevin and Slaymaker, Mark and Petre, Marian and Woodthorpe, John and Gooch, Daniel","Cheating has been a long-standing issue in university assessments. However, the release of ChatGPT and other free-to-use generative AI tools has provided a new and distinct method for cheating. Students can run many assessment questions through the tool and generate a superficially compelling answer, which may or may not be accurate.&nbsp;We ran a dual-anonymous “quality assurance” marking exercise across four end-of-module assessments across a distance university computer science (CS) curriculum. Each marker received five ChatGPT-generated scripts alongside 10 student scripts. A total of 90 scripts were marked; every ChatGPT-generated script for the undergraduate modules received at least a passing grade (&gt;40%), with all of the introductory module CS1 scripts receiving a distinction (&gt;85%). None of the ChatGPT-taught postgraduate scripts received a passing grade (&gt;50%). We also present the results of interviewing the markers and of running our sample scripts through a GPT-2 detector and the TurnItIn AI detector, which both identified every ChatGPT-generated script but differed in the number of false positives. As such, we contribute a baseline understanding of how the public release of generative AI is likely to significantly impact quality assurance processes. Our analysis demonstrates that in most cases, across a range of question formats, topics, and study levels, ChatGPT is at least capable of producing adequate answers for undergraduate assessment.",2024,73
Don't Disturb Me: Challenges of Interacting with Software Bots on Open Source Software Projects,"Wessel, Mairieli and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco Aurelio","Software bots are used to streamline tasks in Open Source Software (OSS) projects' pull requests, saving development cost, time, and effort. However, their presence can be disruptive to the community. We identified several challenges caused by bots in pull request interactions by interviewing 21 practitioners, including project maintainers, contributors, and bot developers. In particular, our findings indicate noise as a recurrent and central problem. Noise affects both human communication and development workflow by overwhelming and distracting developers. Our main contribution is a theory of how human developers perceive annoying bot behaviors as noise on social coding platforms. This contribution may help practitioners understand the effects of adopting a bot, and researchers and tool designers may leverage our results to better support human-bot interaction on social coding platforms.",2021,74
The Power of Bots: Characterizing and Understanding Bots in OSS Projects,"Wessel, Mairieli and de Souza, Bruno Mendes and Steinmacher, Igor and Wiese, Igor S. and Polato, Ivanilton and Chaves, Ana Paula and Gerosa, Marco A.","Leveraging the pull request model of social coding platforms, Open Source Software (OSS) integrators review developers' contributions, checking aspects like license, code quality, and testability. Some projects use bots to automate predefined, sometimes repetitive tasks, thereby assisting integrators' and contributors' work. Our research investigates the usage and impact of such bots. We sampled 351 popular projects from GitHub and found that 93 (26%) use bots. We classified the bots, collected metrics from before and after bot adoption, and surveyed 228 developers and integrators. Our results indicate that bots perform numerous tasks. Although integrators reported that bots are useful for maintenance tasks, we did not find a consistent, statistically significant difference between before and after bot adoption across the analyzed projects in terms of number of comments, commits, changed files, and time to close pull requests. Our survey respondents deem the current bots as not smart enough and provided insights into the bots' relevance for specific tasks, challenges, and potential new features. We discuss some of the raised suggestions and challenges in light of the literature in order to help GitHub bot designers reuse and test ideas and technologies already investigated in other contexts.",2018,75
The Inconvenient Side of Software Bots on Pull Requests,"Wessel, Mairieli and Steinmacher, Igor","Software bots are applications that integrate their work with humans' tasks, serving as conduits between users and other tools. Due to their ability to automate tasks, bots have been widely adopted by Open Source Software (OSS) projects hosted on GitHub. Commonly, OSS projects use bots to automate a variety of routine tasks to save time from maintainers and contributors. Although bots can be useful for supporting maintainers' work, sometimes their comments are seen as spams, and are quickly ignored by contributors. In fact, the way that these bots interact on pull requests can be disruptive and perceived as unwelcoming. In this paper, we propose the concept of a meta-bot to deal with current problems on the human-bot interaction on pull requests. Besides providing additional value to this interaction, meta-bot will reduce interruptions and help maintainers and contributors stay aware of important information.",2020,76
Software bots in software engineering: benefits and challenges,"Wessel, Mairieli and Gerosa, Marco A. and Shihab, Emad","Software bots are becoming increasingly popular in software engineering (SE). In this tutorial, we define what a bot is and present several examples. We also discuss the many benefits bots provide to the SE community, including helping in development tasks (such as pull request review and integration) and onboarding newcomers to a project. Finally, we discuss the challenges related to interacting with and developing software bots.",2022,77
Detecting and Characterizing Bots that Commit Code,"Dey, Tapajit and Mousavi, Sara and Ponce, Eduardo and Fry, Tanner and Vasilescu, Bogdan and Filippova, Anna and Mockus, Audris","Background: Some developer activity traditionally performed manually, such as making code commits, opening, managing, or closing issues is increasingly subject to automation in many OSS projects. Specifically, such activity is often performed by tools that react to events or run at specific times. We refer to such automation tools as bots and, in many software mining scenarios related to developer productivity or code quality, it is desirable to identify bots in order to separate their actions from actions of individuals. Aim: Find an automated way of identifying bots and code committed by these bots, and to characterize the types of bots based on their activity patterns. Method and Result: We propose BIMAN, a systematic approach to detect bots using author names, commit messages, files modified by the commit, and projects associated with the commits. For our test data, the value for AUC-ROC was 0.9. We also characterized these bots based on the time patterns of their code commits and the types of files modified, and found that they primarily work with documentation files and web pages, and these files are most prevalent in HTML and JavaScript ecosystems. We have compiled a shareable dataset containing detailed information about 461 bots we found (all of which have more than 1000 commits) and 13,762,430 commits they created.",2020,78
Defining and classifying software bots: a faceted taxonomy,"Lebeuf, Carlene and Zagalsky, Alexey and Foucault, Matthieu and Storey, Margaret-Anne","While bots have been around for many decades, recent technological advancements and the increasing adoption of language-based communication platforms have led to a surge of new software bots, which have become increasingly pervasive in our everyday lives. Although many novel bots are being designed and deployed, the terms used to describe them and their properties are vast, diverse, and often inconsistent. Even the concept of what is or is not a bot is unclear. This hinders our ability to study, understand, design, and classify bots.In this paper, we present a taxonomy of software bots, which focuses on the observable properties and behaviours of software bots, as well as the environments where bots are deployed and designed. We see this taxonomy as a focal point for a discussion in our community so that together we can deeply consider how to evaluate and understand existing bots, as well as how we may design more innovative and productive bots.",2019,79
Summary of the Third International Workshop on Bots in Software Engineering (BotSE 2021),"Wagner, Stefan and Gerosa, Marco A. and Wessel, Mairieli","Bots automate tasks in software engineering projects and interact with software developers. Bots have been proposed, for example, for testing, maintenance, and automating bug fixes. The research community has been discussing these bots in the International Workshop on Bots in Software Engineering (BotSE), collocated with ICSE (the International Conference on Software Engineering). The workshop participants share experiences and challenges, discuss new usages of bots, and map out future directions. In this paper, we present a summary of the 3rd edition of the workshop, which comprised nine papers, one journal-first presentation, and two keynotes, followed by extensive discussion. More details can be found at http://botse.org/",2022,80
Among the Machines: Human-Bot Interaction on Social Q&amp;A Websites,"Murgia, Alessandro and Janssens, Daan and Demeyer, Serge and Vasilescu, Bogdan","With the rise of social media and advancements in AI technology, human-bot interaction will soon be commonplace. In this paper we explore human-bot interaction in STACK OVERFLOW, a question and answer website for developers. For this purpose, we built a bot emulating an ordinary user answering questions concerning the resolution of git error messages. In a first run this bot impersonated a human, while in a second run the same bot revealed its machine identity. Despite being functionally identical, the two bot variants elicited quite different reactions.",2016,81
Approving automation: analyzing requests for permissions of bots in wikidata,"Farda-Sarbas, Mariam and Zhu, Hong and Nest, Marisa Frizzi and M\""{u}ller-Birn, Claudia","Wikidata, initially developed to serve as a central structured knowledge base for Wikipedia, is now a melting point for structured data for companies, research projects and other peer production communities. Wikidata's community consists of humans and bots, and most edits in Wikidata come from these bots. Prior research has raised concerns regarding the challenges for editors to ensure the quality of bot-generated data, such as the lack of quality control and knowledge diversity. In this research work, we provide one way of tackling these challenges by taking a closer look at the approval process of bot activity on Wikidata. We collected all bot requests, i.e. requests for permissions (RfP) from October 2012 to July 2018. We analyzed these 683 bot requests by classifying them regarding activity focus, activity type, and source mentioned. Our results show that the majority of task requests deal with data additions to Wikidata from internal sources, especially from Wikipedia. However, we can also show the existing diversity of external sources used so far. Furthermore, we examined the reasons which caused the unsuccessful closing of RfPs. In some cases, the Wikidata community is reluctant to implement specific bots, even if they are urgently needed because there is still no agreement in the community regarding the technical implementation. This study can serve as a foundation for studies that connect the approved tasks with the editing behavior of bots on Wikidata to understand the role of bots better for quality control and knowledge diversity.",2019,82
"Expecting the unexpected: distilling bot development, challenges, and motivations","Pinheiro, Andr\'{e} M. and Rabello, Caio S. and Furtado, Leonardo B. and Pinto, Gustavo and de Souza, Cleidson R. B.","Software bots are becoming an increasingly popular tool in the software development landscape, which is particularly due to their potential of use in several different contexts. More importantly, software developers interested in transitioning to bot development may have to face challenges intrinsic related to bot software development. However, so far, it is still unclear what is the profile of bot developers, what motivate them, or what challenges do they face when dealing with bot development. To shed an initial light on this direction, we conducted a survey with 43 Github users who have been involved (showing their interest or actively contributing to) in bot software projects.",2019,83
Current and future bots in software development,"Erlenhov, Linda and de Oliveira Neto, Francisco Gomes and Scandariato, Riccardo and Leitner, Philipp","Bots that support software development (""DevBots"") are seen as a promising approach to deal with the ever-increasing complexity of modern software engineering and development. Existing DevBots are already able to relieve developers from routine tasks such as building project images or keeping dependencies up-to-date. However, advances in machine learning and artificial intelligence hold the promise of future, significantly more advanced, DevBots. In this paper, we introduce the terminology of contemporary and ideal DevBots. Contemporary DevBots represent the current state of practice, which we characterise using a facet-based taxonomy. We exemplify this taxonomy using 11 existing, industrial-strength bots. We further provide a vision and definition of future (ideal) DevBots, which are not only autonomous, but also adaptive, as well as technically and socially competent. These properties may allow ideal DevBots to act more akin to artificial team mates than simple development tools.",2019,84
An empirical study of bots in software development: characteristics and challenges from a practitioner’s perspective,"Erlenhov, Linda and Neto, Francisco Gomes de Oliveira and Leitner, Philipp","Software engineering bots – automated tools that handle tedious tasks – are increasingly used by industrial and open source projects to improve developer productivity. Current research in this area is held back by a lack of consensus of what software engineering bots (DevBots) actually are, what characteristics distinguish them from other tools, and what benefits and challenges are associated with DevBot usage. In this paper we report on a mixed-method empirical study of DevBot usage in industrial practice. We report on findings from interviewing 21 and surveying a total of 111 developers. We identify three different personas among DevBot users (focusing on autonomy, chat interfaces, and “smartness”), each with different definitions of what a DevBot is, why developers use them, and what they struggle with.We conclude that future DevBot research should situate their work within our framework, to clearly identify what type of bot the work targets, and what advantages practitioners can expect. Further, we find that there currently is a lack of general purpose “smart” bots that go beyond simple automation tools or chat interfaces. This is problematic, as we have seen that such bots, if available, can have a transformative effect on the projects that use them.",2020,85
TutorBot: contextual learning guide for software engineers,"Subramanian, Venkatesh and Ramachandra, Nisha and Dubash, Neville","This document is poster submission on using conversational chat bot to guide a software engineer in their learning journey and keeping pace with the technology changes. We describe the motivation, technical approach, and experience of building, piloting such a Bot in a controlled setting and capturing the user feedback. The document also discusses future opportunities to extend and enhance the functionality.",2019,86
"Ready, Aim, Snipe! Analysis of Sniper Bots and their Impact on the DeFi Ecosystem","Cernera, Federico and La Morgia, Massimo and Mei, Alessandro and Mongardini, Alberto Maria and Sassi, Francesco","In the world of cryptocurrencies, public listing of a new token often generates significant hype, in many cases causing its price to skyrocket in a few seconds. In this scenario, timing is crucial to determine the success or failure of an investment opportunity. In this work, we present an in-depth analysis of sniper bots, automated tools designed to buy tokens as soon as they are listed on the market. We leverage GitHub open-source repositories of sniper bots to analyze their features and how they are implemented. Then, we build a dataset of Ethereum and BNB Smart Chain (BSC) liquidity pools to identify addresses that serially take advantage of sniper bots. Our findings reveal 14,029 sniping operations on Ethereum and 1,395,042 in BSC that bought tokens for a total of $10,144,808 dollars and $18,720,447, respectively. We find that Ethereum operations have a higher success rate but require a larger investment. Finally, we analyze token smart contracts to identify mechanisms that can hinder sniper bots.",2023,87
Bots in Social and Interaction Networks: Detection and Impact Estimation,"Mendoza, Marcelo and Tesconi, Maurizio and Cresci, Stefano","The rise of bots and their influence on social networks is a hot topic that has aroused the interest of many researchers. Despite the efforts to detect social bots, it is still difficult to distinguish them from legitimate users. Here, we propose a simple yet effective semi-supervised method that allows distinguishing between bots and legitimate users with high accuracy. The method learns a joint representation of social connections and interactions between users by leveraging graph-based representation learning. Then, on the proximity graph derived from user embeddings, a sample of bots is used as seeds for a label propagation algorithm. We demonstrate that when the label propagation is done according to pairwise account proximity, our method achieves F1 = 0.93, whereas other state-of-the-art techniques achieve F1 ≤ 0.87. By applying our method to a large dataset of retweets, we uncover the presence of different clusters of bots in the network of Twitter interactions. Interestingly, such clusters feature different degrees of integration with legitimate users. By analyzing the interactions produced by the different clusters of bots, our results suggest that a significant group of users was systematically exposed to content produced by bots and to interactions with bots, indicating the presence of a selective exposure phenomenon.",2020,88
BotWalk: Efficient Adaptive Exploration of Twitter Bot Networks,"Minnich, Amanda and Chavoshi, Nikan and Koutra, Danai and Mueen, Abdullah","We propose BotWalk, a near-real time adaptive Twitter exploration algorithm to identify bots exhibiting novel behavior. Due to suspension pressure, Twitter bots are constantly changing their behavior to evade detection. Traditional supervised approaches to bot detection are non-adaptive and thus cannot identify novel bot behaviors. We therefore devise an unsupervised approach, which allows us to identify bots as they evolve. We characterize users with a behavioral feature vector which consists of (well-studied in isolation) metadata-, content-, temporal-, and network-based features. We identify a random bot from our seed bank, populated initially by previously-labeled bots, gather this user's followers' features from Twitter in real time, and employ an unsupervised ensemble anomaly detection method in the multi-dimensional behavioral space. These potential bots are folded into the seed bank and the process is then repeated, with the new seeds' features allowing us to adaptively identify novel bot behavior. BotWalk allows for the identification of on average 6,000 potential bots a day. Our method allowed us to detect 7,995 previously undiscovered bots from a sample of 15 seed bots with a precision of 90%.",2017,89
Detecting Financial Bots on the Ethereum Blockchain,"Niedermayer, Thomas and Saggese, Pietro and Haslhofer, Bernhard","The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%. Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.",2024,90
The Roles Bots Play in Wikipedia,"Zheng, Lei (Nico) and Albano, Christopher M. and Vora, Neev M. and Mai, Feng and Nickerson, Jeffrey V.","Bots are playing an increasingly important role in the creation of knowledge in Wikipedia. In many cases, editors and bots form tightly knit teams. Humans develop bots, argue for their approval, and maintain them, performing tasks such as monitoring activity, merging similar bots, splitting complex bots, and turning off malfunctioning bots. Yet this is not the entire picture. Bots are designed to perform certain functions and can acquire new functionality over time. They play particular roles in the editing process. Understanding these roles is an important step towards understanding the ecosystem, and designing better bots and interfaces between bots and humans. This is important for understanding Wikipedia along with other kinds of work in which autonomous machines affect tasks performed by humans. In this study, we use unsupervised learning to build a nine category taxonomy of bots based on their functions in English Wikipedia. We then build a multi-class classifier to classify 1,601 bots based on labeled data. We discuss different bot activities, including their edit frequency, their working spaces, and their software evolution. We use a model to investigate how bots playing certain roles will have differential effects on human editors. In particular, we build on previous research on newcomers by studying the relationship between the roles bots play, the interactions they have with newcomers, and the ensuing survival rate of the newcomers.",2019,91
Collaborating with Bots and Automation on OpenStreetMap,"van Berkel, Niels and Pohl, Henning","OpenStreetMap (OSM) is a large online community where users collaborate to map the world. In addition to manual edits, the OSM mapping database is regularly modified by bots and automated edits. In this paper, we seek to better understand how people and bots interact and conflict with each other. We start by analysing over 15 years of mailing list discussions related to bots and automated edits. From this data, we uncover five themes, including how automation results in power differentials between users and how community ideals of consensus clash with the realities of bot use. Subsequently, we surveyed OSM contributors on their experiences with bots and automated edits. We present findings about the current escalation and review mechanisms, as well as the lack of appropriate tools for evaluating and discussing bots. We discuss how OSM and similar communities could use these findings to better support collaboration between humans and bots.",2024,92
JITBot: an explainable just-in-time defect prediction bot,"Khanan, Chaiyakarn and Luewichana, Worawit and Pruktharathikoon, Krissakorn and Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee","Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explainability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/",2021,93
Learning from Machines? Social Bots Influence on COVID-19 Vaccination-Related Discussions: 2021 in Review,"Javed, Muhammad and Dimaguila, Gerardo Luis and Habibabadi, Sedigh Khademi and Palmer, Chris and Buttery, Jim","The World Health Organization defines vaccine hesitancy as a delay in acceptance or refusal of vaccination despite the availability of vaccination services. Vaccine hesitancy contributes to lower rates of vaccination in a population and delayed vaccine coverage. A large number of COVID-19 vaccines have been administered worldwide against COVID-19. Due to concerns people have about COVID-19 vaccine adverse events, a significant proportion of people exhibit hesitancy towards the vaccines. These are often prompted by information and misinformation spread through social media conversation, which is not driven exclusively by genuine human-run accounts. Social bots have been shown to be very active during the pandemic participating in discussions about vaccines, including the spread of conflicting and misleading information.Using a novel ensemble technique, we sought to identify and describe the involvement of social bots in COVID-19 vaccination-related discussions on Twitter and how this could have influenced sentiments and hesitancies about COVID-19 vaccines. We included tweets from January to December 2021 to present a whole year's analysis in relation to the vaccines. Unique usernames from these posts were passed to Botometer and Tweetbotornot, programs that review Twitter accounts, to detect a broad range of social bots using a scoring system. A domain-oriented transfer learning technique is applied by finetuning the CT-BERT V2 model to detect the influence of social bots on COVID-19 vaccine sentiments. We computed the ratio of sentiment transmission from bots-to-human, human-to-human, human-to-bots, and bots-to-bots. BERTopic was used to extract the topics of discussion to identify the amplified or transferred hesitancies.Social bots’ participation in online discussions noticeably influenced human sentiments and hesitancies about COVID-19 vaccination. A major portion of sentiments transferred from bot to human during the period of study appeared to amplify or transfer hesitancies regarding COVID-19 vaccination.",2023,94
Building sankie: an AI platform for DevOps,"Kumar, Rahul and Bansal, Chetan and Maddila, Chandra and Sharma, Nitin and Martelock, Shawn and Bhargava, Ravi","There has been a fundamental shift amongst software developers and engineers in the past few years. The software development life cycle (SDLC) for a developer has increased in complexity and scale. Changes that were developed and deployed over a matter of days or weeks are now deployed in a matter of hours. Due to greater availability of compute, storage, better tooling, and the necessity to react, developers are constantly looking to increase their velocity and throughput of developing and deploying changes. Consequently, there is a great need for more intelligent and context sensitive DevOps tools and services that help developers increase their efficiency while developing and debugging. Given the vast amounts of heterogeneous data available from the SDLC, such intelligent tools and services can now be built and deployed at a large scale to help developers achieve their goals and be more productive. In this paper, we present Sankie, a scalable and general service that has been developed to assist and impact all stages of the modern SDLC. Sankie provides all the necessary infrastructure (back-end and front-end bots) to ingest data from repositories and services, train models based on the data, and eventually perform decorations or provide information to engineers to help increase the velocity and throughput of changes, bug fixes etc. This paper discusses the architecture as well as some of the key observations we have made from wide scale deployment of Sankie within Microsoft.",2019,95
"A Case Study of Developer Bots: Motivations, Perceptions, and Challenges","Asthana, Sumit and Sajnani, Hitesh and Voyloshnikova, Elena and Acharya, Birendra and Herzig, Kim","Continuous integration and deployment (CI/CD) is now a widely adopted development model in practice as it reduces the time from ideas to customers. This adoption has also revived the idea of ""shifting left"" during software development -- a practice intended to find and prevent defects early in the software delivery process. To assist with that, engineering systems integrate developer bots in the development workflow to improve developer productivity and help them identify issues early in the software delivery process.  

In this paper, we present a case study of developer bots in Microsoft. We identify and analyze 23 developer bots that are deployed across 13,000 repositories and assist about 6,000 developers daily in their CI/CD software development workflows. We classify these bots across five major categories: Config Violation, Security, Data-privacy, Developer Productivity, and Code Quality. By conducting interviews and surveys with bot developers and bot users and by analyzing about half a million historical bot actions spanning over one and a half years, we present software workflows that motivate bot instrumentation, factors impacting their usefulness as perceived by bot users, and challenges associated with their use. Our findings echo existing issues with bots, such as noise, and illustrate new benefits (e.g., cross-team communication) and challenges (e.g., too many bots) for large software teams.",2023,96
"SoK: False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations","Akhtar, Mohammad Majid and Masood, Rahat and Ikram, Muhammad and Kanhere, Salil S","The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics concerning false information detection have been overlooked in previous research.To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM element. Through a systematic investigation of prior research, we identify commonalities, highlight existing gaps, and extract valuable insights in the field.Our findings underscore the urgent need for interdisciplinary research to effectively combat social media manipulations, and our systematization can guide future research efforts and assist OSN providers in ensuring the safety and integrity of their platforms.",2024,97
Detecting IMAP Credential Stuffing Bots Using Behavioural Biometrics,"Barkworth, Ashley and Tabassum, Rehnuma and Habibi Lashkari, Arash","Credential stuffing has seen a great uptick in use and is now one of the most common types of cyberattacks. Legacy email protocols like the Internet Mail Access Protocol (IMAP) are particularly vulnerable to this kind of attack as they do not support multi-factor authentication (MFA). We propose a supervised learning system that detects credential stuffing bots using two kinds of behavioural biometrics: mouse and keystroke dynamics. The system records a user’s mouse and keystroke events while they complete three tasks in a graphical user interface (GUI) application. We also developed two types of bots: a simple bot and an advanced bot, the latter of which uses techniques to simulate human-like mouse and keyboard motions. We evaluated our system using the Random Forest (RF), decision tree (DT), support vector machine (SVM), and k-nearest neighbors (KNN) algorithms and compared them against two data sets: Simple containing human and simple bot data, and Advanced containing human and advanced bot data. The highest accuracy against the Simple and Advanced data sets were both 96.95% achieved by the KNN and RF classifiers, respectively. The RF classifier showed the best overall performance, achieving the highest precision and mean AUC against the Simple data set and the highest scores across all metrics against the Advanced data set.",2023,98
Sorry to bother you: designing bots for effective recommendations,"Brown, Chris and Parnin, Chris","Bots have been proposed as a way to encourage developer actions and support software development activities. Many bots make recommendations to users, however humans may find these recommendations ineffective or problematic. In this paper, we argue that while bots can help automate many tasks, ultimately bots still need to find ways to interact with humans and handle all of the associated social and cognitive problems entailed. To illustrate this problem, we performed a small study where we generated 52 pull requests making tool recommendation to developers. As a result, we only convinced two developers to accept the pull request, while receiving several forms of feedback on why the pull request was ineffective. We summarize this feedback and suggest design principles for bot recommendations, including how psychology frameworks, such as nudge theory, can be used to improve human-bot interactions.",2019,99
How has forking changed in the last 20 years? a study of hard forks on GitHub,"Zhou, Shurui and Vasilescu, Bogdan and K\""{a}stner, Christian","The notion of forking has changed with the rise of distributed version control systems and social coding environments, like GitHub. Traditionally forking refers to splitting off an independent development branch (which we call hard forks); research on hard forks, conducted mostly in pre-GitHub days showed that hard forks were often seen critical as they may fragment a community Today, in social coding environments, open-source developers are encouraged to fork a project in order to contribute to the community (which we call social forks), which may have also influenced perceptions and practices around hard forks. To revisit hard forks, we identify, study, and classify 15,306 hard forks on GitHub and interview 18 owners of hard forks or forked repositories. We find that, among others, hard forks often evolve out of social forks rather than being planned deliberately and that perception about hard forks have indeed changed dramatically, seeing them often as a positive noncompetitive alternative to the original project.",2020,100
BotSpot: A Hybrid Learning Framework to Uncover Bot Install Fraud in Mobile Advertising,"Yao, Tianjun and Li, Qing and Liang, Shangsong and Zhu, Yadong","Mobile advertising has become inarguably one of the fastest growing industries all over the world. The influx of capital attracts increasing fraudsters to defraud money from advertisers. There are many tricks a fraudster can leverage, among which bot install fraud is undoubtedly the most insidious one due to its ability to implement sophisticated behavioral patterns and emulate normal users, so as to evade from detection rules defined by human experts. In this work, we propose an anti-fraud method based on heterogeneous graph that incorporates both local context and global context via graph neural networks (GNN) and gradient boosting classifier to detect bot fraud installs at Mobvista, a leading global mobile advertising company. Offline evaluations in two datasets show the proposed method outperforms all the competitive baseline methods by at least 2.2% in the first dataset and 5.75% in the second dataset given the evaluation metric Recall@90% Precision. Furthermore, we deploy our method to tackle million-scale data daily at Mobvista. The online performance also shows that the proposed methods consistently detect more bots than other baseline methods.",2020,101
"Model bots, not humans on social media","Chavoshi, Nikan and Mueen, Abdullah","The Posting schedule reveals characteristic patterns of users on social media. Motivated by this knowledge, several researchers have modeled posting schedules and argued that deviation from the model indicates bot or spammer characteristics. It is true that circadian rhythms induce regularity in human posting behavior; however, in this paper, we show that this regularity is an individual trait and insufficient to develop a generic model. More surprisingly, we show that bots are more structured in their posting behaviors compared to humans by using a Convolutional Neural Network (CNN).More precisely, we demonstrate using Class Activation Maps that bots contain less entropy than humans. Thus, we conclude that bots are more amenable to generic models than humans. We evaluate the hypothesis on more than 32 million posts from 12 thousand Twitter users with 97% accuracy.",2020,102
Accelerating software engineering research adoption with analysis bots,"Beschastnikh, Ivan and Lungu, Mircea F. and Zhuang, Yanyan","An important part of software engineering (SE) research is to develop new analysis techniques and to integrate these techniques into software development practice. However, since access to developers is non-trivial and research tool adoption is slow, new analyses are typically evaluated as follows: a prototype tool that embeds the analysis is implemented, a set of projects is identified, their revisions are selected, and the tool is run in a controlled environment, rarely involving the developers of the software. As a result, research artifacts are brittle and it is unclear if an analysis tool would actually be adopted.In this paper, we envision harnessing the rich interfaces provided by popular social coding platforms for automated deployment and evaluation of SE research analysis. We propose that SE analyses can be deployed as analysis bots. We focus on two specific benefits of such an approach: (1) analysis bots can help evaluate analysis techniques in a less controlled, and more realistic context, and (2) analysis bots provide an interface for developers to ""subscribe"" to new research techniques without needing to trust the implementation, the developer of the new tool, or to install the analysis tool locally. We outline basic requirements for an analysis bots platform, and present research challenges that would need to be resolved for bots to flourish.",2017,103
Summary of the 2nd International Workshop on Bots in Software Engineering (BotSE 2020),"Shihab, Emad and Wagner, Stefan and Aur\'{e}lio Gerosa, Marco","Bots automate many tasks in software engineering projects often in the form of chatbots. Bots have been proposed, for example, for testing, maintenance, or automating bug fixes. Following the success of the first BotSE workshop, we organized this second edition collocated with ICSE 2020 to bring together the research community that investigates bots for software engineering. Specifically, the workshop's goal was to share experiences and challenges, discuss new types of bots, and map out future directions. The workshop program comprised the presentation of 8 papers and 2 keynotes, followed by extensive discussion. Overall, the community matured by discussing how to design, build, and evaluate bots. The community aims to organise a 3rd edition of the workshop. Website: http://botse.org/",2021,104
"""Nip it in the Bud"": Moderation Strategies in Open Source Software Projects and the Role of Bots","Hsieh, Jane and Kim, Joselyn and Dabbish, Laura and Zhu, Haiyi","Much of our modern digital infrastructure relies critically upon open sourced software. The communities responsible for building this cyberinfrastructure require maintenance and moderation, which is often supported by volunteer efforts. Moderation, as a non-technical form of labor, is a necessary but often overlooked task that maintainers undertake to sustain the community around an OSS project. This study examines the various structures and norms that support community moderation, describes the strategies moderators use to mitigate conflicts, and assesses how bots can play a role in assisting these processes. We interviewed 14 practitioners to uncover existing moderation practices and ways that automation can provide assistance. Our main contributions include a characterization of moderated content in OSS projects, moderation techniques, as well as perceptions of and recommendations for improving the automation of moderation tasks. We hope that these findings will inform the implementation of more effective moderation practices in open source communities.",2023,105
Tool choice matters: JavaScript quality assurance tools and usage outcomes in GitHub projects,"Kavaler, David and Trockman, Asher and Vasilescu, Bogdan and Filkov, Vladimir","Quality assurance automation is essential in modern software development. In practice, this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context. Data and analytics of the pros and cons can inform these decisions. Yet, in most cases, there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices.We propose a general methodology to model the time-dependent effect of automation tool choice on four outcomes of interest: prevalence of issues, code churn, number of pull requests, and number of contributors, all with a multitude of controls. On a large data set of npm JavaScript projects, we extract the adoption events for popular tools in three task classes: linters, dependency managers, and coverage reporters. Using mixed methods approaches, we study the reasons for the adoptions and compare the adoption effects within each class, and sequential tool adoptions across classes. We find that some tools within each group are associated with more beneficial outcomes than others, providing an empirical perspective for the benefits of each. We also find that the order in which some tools are implemented is associated with varying outcomes.",2019,106
Can Generative AI Bots Be Trusted?,"Denning, Peter J.",It will be a long road to learning how to use generative AI wisely.,2023,107
Operationalizing Conflict and Cooperation between Automated Software Agents in Wikipedia: A Replication and Expansion of 'Even Good Bots Fight',"Geiger, R. Stuart and Halfaker, Aaron","This paper replicates, extends, and refutes conclusions made in a study published in PLoS ONE (""Even Good Bots Fight""), which claimed to identify substantial levels of conflict between automated software agents (or bots) in Wikipedia using purely quantitative methods. By applying an integrative mixed-methods approach drawing on trace ethnography, we place these alleged cases of bot-bot conflict into context and arrive at a better understanding of these interactions. We found that overwhelmingly, the interactions previously characterized as problematic instances of conflict are typically better characterized as routine, productive, even collaborative work. These results challenge past work and show the importance of qualitative/quantitative collaboration. In our paper, we present quantitative metrics and qualitative heuristics for operationalizing bot-bot conflict. We give thick descriptions of kinds of events that present as bot-bot reverts, helping distinguish conflict from non-conflict. We computationally classify these kinds of events through patterns in edit summaries. By interpreting found/trace data in the socio-technical contexts in which people give that data meaning, we gain more from quantitative measurements, drawing deeper understandings about the governance of algorithmic systems in Wikipedia. We have also released our data collection, processing, and analysis pipeline, to facilitate computational reproducibility of our findings and to help other researchers interested in conducting similar mixed-method scholarship in other platforms and contexts.",2017,108
C2Store: C2 Server Profiles at Your Fingertips,"Jain, Vivek and Alam, S M Maksudul and Krishnamurthy, Srikanth V. and Faloutsos, Michalis","How can we build a definitive capability for tracking C2 servers? Having a large-scale continuously updating capability would be essential for understanding the spatiotemporal behaviors of C2 servers and, ultimately, for helping contain botnet activities. Unfortunately, existing information from threat intelligence feeds and previous works is often limited to a specific set of botnet families or short-term data collections. Responding to this need, we present C2Store, an initiative to provide the most comprehensive information on C2 servers. Our work makes the following contributions: (a) we develop techniques to collect, verify, and combine C2 server addresses from five types of sources, including uncommon platforms, such as GitHub and Twitter; (b) we create an open-access annotated database of 335,967 C2 servers across 133 malware families, which supports semantically-rich and smart queries; (c) we identify surprising behaviors of C2 servers with respect to their spatiotemporal patterns and behaviors. First, we successfully mine Twitter and GitHub and identify C2 servers with a precision of 97% and 94%, respectively. Furthermore, we find that the threat feeds identify only 24% of the servers in our database, with Twitter and GitHub providing 32%. A surprising observation is the identification of 250 IP addresses, each of which hosts more than 5 C2 servers for different botnet families at the same time. Overall, we envision C2Store as an ongoing effort that will facilitate research by providing timely, historical, and comprehensive C2 server information by critically combining multiple sources of information.",2023,109
Public Software Development Activity During the Pandemic,"Klotzman, Vanessa and Farmahinifarahani, Farima and Lopes, Cristina","Background The emergence of the COVID-19 pandemic has impacted all human activity, including software development. Early reports seem to indicate that the pandemic may have had a negative effect on software developers, socially and personally, but that their software development productivity may not have been negatively impacted. Aims: Early reports about the effects of the pandemic on software development focused on software developers' well-being and on their productivity as employees. We are interested in a different aspect of software development: the developers' public contributions, as seen in GitHub and Stack Overflow activities. Did the pandemic affect the developers' public contributions and, of so, in what way? Method: Considering the data from between 2017 and till 2020, we study the trends within GitHub's push, create, pull request, and release events, and within Stack Overflow's new users, posts, votes, and comments. We performed linear regressions, correlation analyses, outlier analyses, hypothesis testing, and we also contacted individual developers in order to gather qualitative insights about their unusual public contributions. Results: Our study shows that within GitHub and Stack Overflow, the onset of the pandemic (March/April 2020) is reflected in a set of outliers in developers' contributions that point to an increase in activity. The distributions of contributions during the entire year of 2020 were, in some aspects, different, but, in other aspects, similar from the recent past. Additionally, we found one noticeably disrupted pattern of contribution in Stack Overflow, namely the ratio Questions/Answers, which was much higher in 2020 than before. Testimonials from the developers we contacted were mixed: while some developers reported that their increase in activity was due to the pandemic, others reported that it was not. Conclusion: In Github, there was a noticeable increase in public software development activity in 2020, as well as more abrupt changes in daily activities; in Stack Overflow, there was a noticeable increase in new users and new questions at the onset of the pandemic, and in the ratio of Questions/Answers during 2020. The results may be attributed to the pandemic, but other factors could have come into play.",2021,110
The Signals that Potential Contributors Look for When Choosing Open-source Projects,"Qiu, Huilian Sophie and Li, Yucen Lily and Padala, Susmita and Sarma, Anita and Vasilescu, Bogdan","While open-source software has become ubiquitous, its sustainability is in question: without a constant supply of contributor effort, open-source projects are at risk. While prior work has extensively studied the motivations of open-source contributors in general, relatively little is known about how people choose which project to contribute to, beyond personal interest. This question is especially relevant in transparent social coding environments like GitHub, where visible cues on personal profile and repository pages, known as signals, are known to impact impression formation and decision making. In this paper, we report on a mixed-methods empirical study of the signals that influence the contributors' decision to join a GitHub project. We first interviewed 15 GitHub contributors about their project evaluation processes and identified the important signals they used, including the structure of the README and the amount of recent activity. Then, we proceeded quantitatively to test out the impact of each signal based on the data of 9,977 GitHub projects. We reveal that many important pieces of information lack easily observable signals, and that some signals may be both attractive and unattractive. Our findings have direct implications for open-source maintainers and the design of social coding environments, e.g., features to be added to facilitate better project searching experience.",2019,111
"Backports: change types, challenges and strategies","Chakroborti, Debasish and Schneider, Kevin A. and Roy, Chanchal K.","Source code repositories allow developers to manage multiple versions (or branches) of a software system. Pull-requests are used to modify a branch, and backporting is a regular activity used to port changes from a current development branch to other versions. In open-source software, backports are common and often need to be adapted by hand, which motivates us to explore backports and backporting challenges and strategies. In our exploration of 68,424 backports from 10 GitHub projects, we found that bug, test, document, and feature changes are commonly backported. We identified a number of backporting challenges, including that backports were inconsistently linked to their original pull-request (49%), that backports had incompatible code (13%), that backports failed to be accepted (10%), and that there were backporting delays (16 days to create, 5 days to merge). We identified some general strategies for addressing backporting issues. We also noted that backporting strategies depend on the project type and that further investigation is needed to determine their suitability. Furthermore, we created the first-ever backports dataset that can be used by other researchers and practitioners for investigating backports and backporting.",2022,112
Collaboration Drives Individual Productivity,"Muri\'{c}, Goran and Abeliuk, Andres and Lerman, Kristina and Ferrara, Emilio","How does the number of collaborators affect individual productivity? Results of prior research have been conflicting, with some studies reporting an increase in individual productivity as the number of collaborators grows, while other studies showing that the free-rider effect skews the effort invested by individuals, making larger groups less productive. The difference between these schools of thought is substantial: if a super-scaling effect exists, as suggested by former studies, then as groups grow, their productivity will increase even faster than their size, super-linearly improving their efficiency. We address this question by studying two planetary-scale collaborative systems: GitHub and Wikipedia. By analyzing the activity of over 2 million users on these platforms, we discover that the interplay between group size and productivity exhibits complex, previously-unobserved dynamics: the productivity of smaller groups scales super-linearly with group size, but saturates at larger sizes. This effect is not an artifact of the heterogeneity of productivity: the relation between group size and productivity holds at the individual level. People tend to do more when collaborating with more people. We propose a generative model of individual productivity that captures the non-linearity in collaboration effort. The proposed model is able to explain and predict group work dynamics in GitHub and Wikipedia by capturing their maximally informative behavioral features, and it paves the way for a principled, data-driven science of collaboration.",2019,113
Tension between GDPR and Public Blockchains: A Data-Driven Analysis of Online Discussions,"Chousein, Zeynep and Tetik, Haci Yakup and Sa\u{g}lam, Rahime Belen and B\""{u}lb\""{u}l, Abdullah and Li, Shujun","Since coming into effect in May 2018, the EU General Data Protection Regulation (GDPR) has raised serious concerns among users of public (permissionless) blockchain systems. Such concerns are triggered by a tension between some unique characteristics of public blockchain systems and some new data subject rights introduced in the GDPR, e.g., the data immutability and the “right to erasure” (a.k.a.&nbsp;“the right to be forgotten”). The aim of this work is to understand how service providers and developers behind public blockchain systems have communicated about such GDPR-related challenges to their users and how the users have perceived such GDPR-related issues. To this end, for 50 public blockchain systems whose corresponding cryptocurrency had a capital market size over $150 million, we analyzed relevant communications and discussions on the following three online channels: blog and forums posts, GitHub repositories, and discussions on Twitter. Our results show that service providers and developers of the selected public blockchain systems did not play an active role in GDPR-related online discussions on Twitter. They also did not communicate with their users about GDPR on their forums and blogs frequently, where we could identify only 56 posts out of 17,821 posts for the period we studied. Our study also reveals that only an extreme minority of the studied systems (4) mentioned GDPR in their GitHub repositories. Our work adds new evidence on the lack of transparency and active communications of the public blockchain sector on the challenging GDPR compliance issue of public blockchain systems.",2021,114
Sorry to Bother You Again: Developer Recommendation Choice Architectures for Designing Effective Bots,"Brown, Chris and Parnin, Chris","Software robots, or bots, are useful for automating a wide variety of programming and software development tasks. Despite the advantages of using bots throughout the software engineering process, research shows that developers often face challenges interacting with these systems. To improve automated developer recommendations from bots, this work introduces developer recommendation choice architectures. Choice architecture is a behavioral science concept that suggests the presentation of options impacts the decisions humans make. To evaluate the impact of framing recommendations for software engineers, we examine the impact of one choice architecture, actionability, for improving the design of bot recommendations. We present the results of a preliminary study evaluating this choice architecture in a bot and provide implications for integrating choice architecture into the design of future software engineering bots.",2020,115
An Empirical Study of Reviewer Recommendation in Pull-based Development Model,"Yang, Cheng and Zhang, Xunhui and Zeng, Lingbin and Fan, Qiang and Yin, Gang and Wang, Huaimin","Code review is an important process to reduce code defects and improve software quality. However, in social coding communities using the pull-based model, everyone can submit code changes, which increases the required code review efforts. Therefore, there is a great need of knowing the process of code review and analyzing the pre-existing reviewer recommendation algorithms. In this paper, we do an empirical study about the PRs and their reviewers in Rails project. Moreover, we reproduce a popular and effective IR-based code reviewer recommendation algorithm and validate it on our dataset which contains 16,049 PRs. We find that the inactive reviewers are very important to code reviewing process, however, the pre-existing method's recommendation result strongly depends on the activeness of reviewers.",2017,116
Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study,"Rahman, Akond and Shamim, Shazibul Islam and Bose, Dibyendu Brinto and Pandita, Rahul","Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters.Objective: The goal of this paper is to help practitioners secure their Kubernetes clusters by identifying security misconfigurations that occur in Kubernetes manifests.Methodology: We conduct an empirical study with 2,039 Kubernetes manifests mined from 92 open-source software repositories to systematically characterize security misconfigurations in Kubernetes manifests. We also construct a static analysis tool called Security Linter for Kubernetes Manifests (SLI-KUBE) to quantify the frequency of the identified security misconfigurations.Results: In all, we identify 11 categories of security misconfigurations, such as absent resource limit, absent securityContext, and activation of hostIPC. Specifically, we identify 1,051 security misconfigurations in 2,039 manifests. We also observe the identified security misconfigurations affect entities that perform mesh-related load balancing, as well as provision pods and stateful applications. Furthermore, practitioners agreed to fix 60% of 10 misconfigurations reported by us.Conclusion: Our empirical study shows Kubernetes manifests to include security misconfigurations, which necessitates security-focused code reviews and application of static analysis when Kubernetes manifests are developed.",2023,117
Classifying issues into custom labels in GitBot,"Park, Doje and Cho, Heetae and Lee, Seonah","GitBots are bots in Git repositories to automate repetitive tasks that occur in software development, testing and maintenance. GitBots are expected to perform the repetitive tasks that are normally done by humans, such as feedback on issue reports and answers to questions. However, studies on GitBots for labeling issue reports fall short of replacing developers' labeling tasks. Developers still manually attach labels to issues. In this paper, we introduce an issue labeling bot classifying issue reports into custom labels that developers define by themselves so that our bot could attach labels in a similar way to human behavior.",2022,118
The Lives and Deaths of Open Source Code Forges,"Squire, Megan","Code forges are third party software repositories that also provide various tools and facilities for distributed software development teams to use, including source code control systems, mailing lists and communication forums, bug tracking systems, web hosting space, and so on. The main contributions of this paper are to present some new data sets relating to the technology adoption lifecycles of a group of six free, libre, and open source software (FLOSS) code forges, and to compare the lifecycles of the forges to each other and to the model presented by classical Diffusion of Innovation (DoI) theory. We find that the observed adoption patterns of code forges rarely follow the DoI model, especially as larger code forges are beset by spam and abuse. The only forge exhibiting a DoI-like lifecycle was a smaller, community-managed, special-purpose forge whose demise was planned in advance. The results of this study will be useful in explaining adoption trajectories, both to practitioners building collaborative FLOSS ecosystems and to researchers who study the evolution and adoption of socio-technical systems.",2017,119
The Robots Are Here: Navigating the Generative AI Revolution in Computing Education,"Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir","Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.",2023,120
Understanding automated code review process and developer experience in industry,"Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu","Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality.",2022,121
 'Welcome' Changes? Descriptive and Injunctive Norms in a Wikipedia Sub-Community,"Morgan, Jonathan T. and Filippova, Anna","Open online communities rely on social norms for behavior regulation, group cohesion, and sustainability. Research on the role of social norms online has mainly focused on one source of influence at a time, making it difficult to separate different normative influences and understand their interactions. In this study, we use the Focus Theory to examine interactions between several sources of normative influence in a Wikipedia sub-community: local descriptive norms, local injunctive norms, and norms imported from similar sub-communities. We find that exposure to injunctive norms has a stronger effect than descriptive norms, that the likelihood of performing a behavior is higher when both injunctive and descriptive norms are congruent, and that conflicting social norms may negatively impact pro-normative behavior. We contextualize these findings through member interviews, and discuss their implications for both future research on normative influence in online groups and the design of systems that support open collaboration.",2018,122
A unified code review automation for large-scale industry with diverse development environments,"Kim, Hyungjin and Kwon, Yonghwi and Kwon, Hyukin and Ryou, Yeonhee and Joh, Sangwoo and Kim, Taeksu and Kim, Chul-Joo","Code Review is an essential activity to ensure the quality of the software in the development process. Code Review Automation with various analyses can reduce human efforts of code review activities. However, it is a challenge to automate the code review process for large-scale companies such as Samsung Electronics due to their complex development environments: many kinds of products, various sizes of software, different version control systems, and diverse code review systems. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments. Our findings provide practical evidence that our system motivates developers in Samsung Electronics to improve code quality.",2022,123
Understanding (Mis)Behavior on the EOSIO Blockchain,"Huang, Yuheng and Wang, Haoyu and Wu, Lei and Tyson, Gareth and Luo, Xiapu and Zhang, Run and Liu, Xuanzhe and Huang, Gang and Jiang, Xuxian","EOSIO has become one of the most popular blockchain platforms since its mainnet launch in June 2018. In contrast to the traditional PoW-based systems (e.g., Bitcoin and Ethereum), which are limited by low throughput, EOSIO is the first high throughput Delegated Proof of Stake system that has been widely adopted by many decentralized applications. Although EOSIO has millions of accounts and billions of transactions, little is known about its ecosystem, especially related to security and fraud. In this paper, we perform a large-scale measurement study of the EOSIO blockchain and its associated DApps. We gather a large-scale dataset of EOSIO and characterize activities including money transfers, account creation and contract invocation. Using our insights, we then develop techniques to automatically detect bots and fraudulent activity. We discover thousands of bot accounts (over 30% of the accounts in the platform) and a number of real-world attacks (301 attack accounts). By the time of our study, 80 attack accounts we identified have been confirmed by DApp teams, causing 828,824 EOS tokens losses (roughly $2.6 million) in total.",2020,124
A Fintech Workshop Course Applies Project-Based Learning in the Intelligent Query of the Money Laundering Control Act,"Lo, Wen-Chi","This article considers the application of project-based learning in the intelligent query of the money laundering control act through the course implementation method. This course is designed and implemented to provide practical applications of financial technology for college students in the departments and classes of the Financial Technology College. The teaching goal of this workshop course is to develop students' knowledge and skills and apply them to practical cases, such as using LINE BOT as the most popular human-machine interface and underlying technologies such as Django website building tools SQLite3 database, and Python Crawler technology. The workshop involves completing tasks as a class utilizing students' critical thinking, communication skills, teamwork, and entrepreneurial spirit. From the perspective of the economic environment, people focus on the practical significance of teamwork teaching methods. The evaluation results show that project-based learning has performed well in teaching financial technology workshops. The experience of this course and student reviews show that students like the practical application aspect of the system and are interested in learning. We believe that other business schools that offer similar Fintech workshop courses will also benefit from this approach.",2023,125
BreakBot: analyzing the impact of breaking changes to assist library evolution,"Ochoa, Lina and Degueule, Thomas and Falleri, Jean-R\'{e}my","""If we make this change to our code, how will it impact our clients?"" It is difficult for library maintainers to answer this simple---yet essential!---question when evolving their libraries. Library maintainers are constantly balancing between two opposing positions: make changes at the risk of breaking some of their clients, or avoid changes and maintain compatibility at the cost of immobility and growing technical debt. We argue that the lack of objective usage data and tool support leaves maintainers with their own subjective perception of their community to make these decisions.We introduce BreakBot, a bot that analyses the pull requests of Java libraries on GitHub to identify the breaking changes they introduce and their impact on client projects. Through static analysis of libraries and clients, it extracts and summarizes objective data that enrich the code review process by providing maintainers with the appropriate information to decide whether---and how---changes should be accepted, directly in the pull requests.",2022,126
Running a Red Light: An Investigation into Why Software Engineers (Occasionally) Ignore Coverage Checks,"Sterk, Alexander and Wessel, Mairieli and Hooten, Eli and Zaidman, Andy","Many modern code coverage tools track and report code coverage data generated from running tests during continuous integration. They report code coverage data through a variety of channels, including email, Slack, Mattermost, or through the web interface of social coding platforms such as GitHub. In fact, this ensemble of tools can be configured in such a way that the software engineer gets a failing status check when code coverage drops below a certain threshold. In this study, we broadly investigate the opinions and experience with code coverage tools through a survey among 279 software engineers whose projects use the Codecov coverage tool and bot. In particular, we are investigating why software engineers would ignore a failing status check caused by drop in code coverage. We observe that &gt;80% of software engineers --- at least sometimes --- ignore these failing status checks, and we get insights into the main reasons why software engineers ignore these checks.",2024,127
Multimodal recommendation of messenger channels,"Koshchenko, Ekaterina and Klimov, Egor and Kovalenko, Vladimir","Collaboration platforms, such as GitHub and Slack, are a vital instrument in the day-to-day routine of software engineering teams. The data stored in these platforms has a significant value for data-driven methods that assist with decision-making and help improve software quality. However, the distribution of this data across different platforms leads to the fact that combining it is a very time-consuming process. Most existing algorithms for socio-technical assistance, such as recommendation systems, are based only on data directly related to the purpose of the algorithms, often originating from a single system.In this work, we explore the capabilities of a multimodal recommendation system in the context of software engineering. Using records of interaction between employees in a software company in messenger channels and repositories, as well as the organizational structure, we build several channel recommendation models for a software engineering collaboration platform, and compare them on historical data. In addition, we implement a channel recommendation bot and assess the quality of recommendations from the best models with a user study.We find that the multimodal recommender yields better recommendations than unimodal baselines, allows to mitigate the overfitting problem, and helps to deal with cold start. Our findings suggest that the multimodal approach is promising for other recommendation problems in software engineering.",2022,128
"Mitigating turnover with code review recommendation: balancing expertise, workload, and knowledge distribution","Mirsaeedi, Ehsan and Rigby, Peter C.","Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover with minimal impacton the development process. We evaluate review recommenders in the context of ensuring expertise during review, Expertise, reducing the review workload of the core team, CoreWorkload, and reducing the Files at Risk to turnover, FaR. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing risk of knowledge loss from turnover by up to 65%. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover by -29% but they unacceptably reduce the overall expertise during reviews by -26%. We develop the Sofia recommender that suggests experts when none of the files under review are hoarded by developers, but distributes knowledge when files are at risk. In this way, we are able to simultaneously increase expertise during review with a ΔExpertise of 6%, with a negligible impact on workload of ΔCoreWorkload of 0.09%, and reduce the files at risk by ΔFaR -28%. Sofia is integrated into GitHub pull requests allowing developers to select an appropriate expert or ""learner"" based on the context of the review. We release the Sofia bot as well as the code and data for replication purposes.",2020,129
Toward Mitigating Misinformation and Social Media Manipulation in LLM Era,"Zhang, Yizhou and Sharma, Karishma and Du, Lun and Liu, Yan","The pervasive abuse of misinformation to influence public opinion on social media has become increasingly evident in various domains, encompassing politics, as seen in presidential elections, and healthcare, most notably during the recent COVID-19 pandemic. This threat has grown in severity as the development of Large Language Models (LLMs) empowers manipulators to generate highly convincing deceptive content with greater efficiency. Furthermore, the recent strides in chatbots integrated with LLMs, such as ChatGPT, have enabled the creation of human-like interactive social bots, posing a significant challenge to both human users and the social-bot-detection systems of social media platforms.These challenges motivate researchers to develop algorithms to mitigate misinformation and social media manipulations. This tutorial introduces the advanced machine learning researches that are helpful for this goal, including (1) detection of social manipulators, (2) learning causal models of misinformation and social manipulation, and (3) LLM-generated misinformation detection. In addition, we also present possible future directions.",2024,130
Unveiling Quality in Chatbot Conversations: Quantitative Analysis of Chatbot Requirements,"Silva, Geovana Ramos Sousa and Canedo, Edna Dias","As conversational assistants and natural language interfaces proliferate, the demand for a precise understanding of quality software requirements for chatbots becomes increasingly critical. In this work, we adopted a quantitative methodology, scrutinizing a dataset composed of conversational requirements from a diverse range of agile projects for chatbot development, and identified meaningful patterns in the language and structure utilized. Our investigation led to significant findings, revealing the importance of structured documentation, conversation flow, and user interaction in the development of chatbots, with the most desired quality attributes being capability, naturalness, straightforwardness, and clarity. In addition, a significant emphasis was placed on feature development and meeting acceptance criteria. The research also illuminated the iterative nature of chatbot development, with a recurrent presence of verbs related to improvement or refactoring. While less pronounced, the roles of documentation and testing in ensuring chatbot quality and effectiveness were also noted. This work provides valuable insights into chatbot requirements management and the significance of quality attributes in chatbot development.",2023,131
Designing for Workplace Reflection: A Chat and Voice-Based Conversational Agent,"Kocielnik, Rafal and Avrahami, Daniel and Marlow, Jennifer and Lu, Di and Hsieh, Gary","Conversational agents stand to play an important role in supporting behavior change and well-being in many domains. With users able to interact with conversational agents through both text and voice, understanding how designing for these channels supports behavior change is important. To begin answering this question, we designed a conversational agent for the workplace that supports workers' activity journaling and self-learning through reflection. Our agent, named Robota, combines chat-based communication as a Slack Bot and voice interaction through a personal device using a custom Amazon Alexa Skill. Through a 3-week controlled deployment, we examine how voice-based and chat-based interaction affect workers' reflection and support self-learning. We demonstrate that, while many current technical limitations exist, adding dedicated mobile voice interaction separate from the already busy chat modality may further enable users to step back and reflect on their work. We conclude with discussion of the implications of our findings to design of workplace self-tracking systems specifically and to behavior-change systems in general.",2018,132
Why Developers Are Slacking Off: Understanding How Software Teams Use Slack,"Lin, Bin and Zagalsky, Alexey and Storey, Margaret-Anne and Serebrenik, Alexander","Slack is a modern communication platform for teams that is seeing wide and rapid adoption by software develop-ment teams. Slack not only facilitates team messaging and archiving, but it also supports a wide plethora of inte-grations to external services and bots. We have found that Slack and its integrations (i.e., bots) are playing an increas-ingly significant role in software development, replacing email in some cases and disrupting software development processes. To understand how Slack impacts development team dynamics, we designed an exploratory study to inves-tigate how developers use Slack and how they benefit from it. We find that developers use Slack for personal, team-wide and community-wide purposes. Our research also reveals that developers use and create diverse integrations (called bots) to support their work. This study serves as the first step towards understanding the role of Slack in sup-porting software engineering.",2016,133
Third-Party Developers and Tool Development For Community Management on Live Streaming Platform Twitch,"Cai, Jie and Lin, Ya-Fang and Zhang, He and Carroll, John M.","Community management is critical for stakeholders to collaboratively build and sustain communities with socio-technical support. However, most of the existing research has mainly focused on the community members and the platform, with little attention given to the developers who act as intermediaries between the platform and community members and develop tools to support community management. This study focuses on third-party developers (TPDs) for the live streaming platform Twitch and explores their tool development practices. Using a mixed method with in-depth qualitative analysis, we found that TPDs maintain complex relationships with different stakeholders (streamers, viewers, platform, professional developers), and the multi-layered policy restricts their agency regarding idea innovation and tool development. We argue that HCI research should shift its focus from tool users to tool developers with regard to community management. We propose designs to support closer collaboration between TPDS and the platform and professional developers and streamline TPDs’ development process with unified toolkits and policy documentation.",2024,134
Translating motivational interviewing for the HPV vaccine into a computable ontology model for automated AI conversational interaction,"Moore, Nicole and Amith, Muhammad and Neumann, Ana and Hamilton, Jane and Tang, Lu and Savas, Lara and Tao, Cui","Human papillomavirus (HPV) vaccinations are lower than expected. To protect the onset of head and neck cancers, innovative strategies to improve the rates are needed. Artificial intelligence may offer some solutions, specifically conversational agents to perform counseling methods. We present our efforts in developing a dialogue model for automating motivational interviewing (MI) to encourage HPV vaccination. We developed a formalized dialogue model for MI using an existing ontology-based framework to manifest a computable representation using OWL2. New utterance classifications were identified along with the ontology that encodes the dialogue model. Our work is available on GitHub under the GPL v.3. We discuss how an ontology-based model of MI can help standardize/formalize MI counseling for HPV vaccine uptake. Our future steps will involve assessing MI fidelity of the ontology model, operationalization, and testing the dialogue model in a simulation with live participants.",2024,135
Technological Frames and User Innovation: Exploring Technological Change in Community Moderation Teams,"Kiene, Charles and Jiang, Jialun Aaron and Hill, Benjamin Mako",Management of technological change in organizations is one of the most enduring topics in the literature on computer-supported cooperative work. The successful navigation of technological change is both more challenging and more critical in online communities that are entirely mediated by technology than it is in traditional organizations. This paper presents an analysis of 14 in-depth interviews with moderators of subcommunities of one technological platform (Reddit) that added communities on a new technological platform (Discord). Moderation teams experienced several problems related to moderating content at scale as well as a disconnect between the affordances of Discord and their assumptions based on their experiences on Reddit. We found that moderation teams used Discord's API to create scripts and bots that augmented Discord to make the platform work more like tools on Reddit. These tools were particularly important in communities struggling with scale. Our findings suggest that increasingly widespread end user programming allow users of social computing systems to innovate and deploy solutions to unanticipated design problems by transforming new technological platforms to align with their past expectations.,2019,136
Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?,"Savelka, Jaromir and Agarwal, Arav and Bogart, Christopher and Song, Yifan and Sakr, Majd","We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (&lt;70% on even entry-level modules). Yet, it is clear that a straightforward application of these easily accessible models could enable a learner to obtain a non-trivial portion of the overall available score (&gt;55%) in introductory and intermediate courses alike. While the models exhibit remarkable capabilities, including correcting solutions based on auto-grader's feedback, some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps). These findings can be leveraged by instructors wishing to adapt their assessments so that GPT becomes a valuable assistant for a learner as opposed to an end-to-end solution.",2023,137
Repairnator patches programs automatically,"Monperrus, Martin and Urli, Simon and Durieux, Thomas and Martinez, Matias and Baudry, Benoit and Seinturier, Lionel","Repairnator is a bot. It constantly monitors software bugs discovered during continuous integration of open-source software and tries to fix them automatically. If it succeeds in synthesizing a valid patch, Repairnator proposes the patch to the human developers, disguised under a fake human identity. To date, Repairnator has been able to produce patches that were accepted by the human developers and permanently merged into the code base. This is a milestone for human-competitiveness in software engineering research on automatic program repair.",2019,138
Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses,"Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd","This paper studies recent developments in large language models’ (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class’ assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4’s handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments.",2023,139
On the Use of Tests for Software Supply Chain Threats,"Hejderup, Joseph","Development teams are increasingly investing in automating the updating of third-party libraries to limit the patch time of zero-day exploits such as the Equifax breach. GitHub bots such as Dependabot and Renovate build such functionality by leveraging existing test infrastructure in repositories to test and evaluate new library updates. However, two recent studies suggest that test suites in projects lack effectiveness and coverage to reliably find regressions in third-party libraries. Adequate test coverage and effectiveness are critical in discovering new vulnerabilities and weaknesses from third-party libraries. The recent Log4Shell incident exemplifies this, as projects will likely not have adequate tests for logging libraries. This position paper discusses the weaknesses and challenges of current testing practices and techniques from a supply chain security perspective. We highlight two key challenges that researchers and practitioners need to address: (1) the lack of resources and best practices for testing the uses of third-party libraries and (2) enhancing the reliability of automating library updates.",2022,140
Towards Automated Detection of Unethical Behavior in Open-Source Software Projects,"Win, Hsu Myat and Wang, Haibo and Tan, Shin Hwei","Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholders’ perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8% average true positive rate (up to 100% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.",2023,141
Wide-spectrum characterization of long-running political phenomena on social media: the brexit case,"Calisir, Emre and Brambilla, Marco","In this study, we propose a wide-spectrum analysis of long-running political events on social media, with reference to an interesting real-world international case: the so-called Brexit, the process through which the United Kingdom activated the option of leaving the European Union. In this study, we model the users participating in 33 months of Twitter debate, covering their behaviour and demographics. By using publicly shared tweets, we developed a stance classification model to evaluate the change of stance over time. We also extracted the key topics of the long-running debate, studying which political side have discussed them most and what is the general sentiment on each. We also revealed the participation of bot accounts, and we found that the higher the bot score, the more likely the account is in a pro-Leave position. We conclude our study with a temporal and comparative analysis of politicians' social media accounts.",2020,142
Knowledge Transfer in Modern Code Review,"Caulo, Maria and Lin, Bin and Bavota, Gabriele and Scanniello, Giuseppe and Lanza, Michele","Knowledge transfer is one of the main goals of modern code review, as shown by several studies that surveyed and interviewed developers. While knowledge transfer is a clear expectation of the code review process, there are no analytical studies using data mined from software repositories to assess the effectiveness of code review in ""training"" developers and improve their skills over time. We present a mining-based study investigating how and whether the code review process helps developers to improve their contributions to open source projects over time. We analyze 32,062 peer-reviewed pull requests (PRs) made across 4,981 GitHub repositories by 728 developers who created their GitHub account in 2015. We assume that PRs performed in the past by a developer D that have been subject to a code review process have ""transferred knowledge"" to D. Then, we verify if over time (i.e., when more and more reviewed PRs are made by D), the quality of the contributions made by D to open source projects increases (as assessed by proxies we defined, such as the acceptance of PRs, or the polarity of the sentiment in the review comments left for the submitted PRs). With the above measures, we were unable to capture the positive impact played by the code review process on the quality of developers' contributions. This might be due to several factors, including the choices we made in our experimental design.Additional investigations are needed to confirm or contradict such a negative result.",2020,143
"""This is damn slick!"": estimating the impact of tweets on open source project popularity and new contributors","Fang, Hongbo and Lamba, Hemank and Herbsleb, James and Vasilescu, Bogdan","Twitter is widely used by software developers. But how effective are tweets at promoting open source projects? How could one use Twitter to increase a project's popularity or attract new contributors? In this paper we report on a mixed-methods empirical study of 44,544 tweets containing links to 2,370 open-source GitHub repositories, looking for evidence of causal effects of these tweets on the projects attracting new GitHub stars and contributors, as well as characterizing the high-impact tweets, the people likely being attracted by them, and how they differ from contributors attracted otherwise. Among others, we find that tweets have a statistically significant and practically sizable effect on obtaining new stars and a small average effect on attracting new contributors. The popularity, content of the tweet, as well as the identity of tweet authors all affect the scale of the attraction effect. In addition, our qualitative analysis suggests that forming an active Twitter community for an open source project plays an important role in attracting new committers via tweets. We also report that developers who are new to GitHub or have a long history of Twitter usage but few tweets posted are most likely to be attracted as contributors to the repositories mentioned by tweets. Our work contributes to the literature on open source sustainability.",2022,144
Contribution-Based Firing of Developers?,"Orrei, Vincenzo and Raglianti, Marco and Nagy, Csaba and Lanza, Michele","There has been some recent clamor about the developer layoff and turnover policies enacted by high-profile corporate executives. Precisely defining the contributions in software development has always been a thorny issue, as it is difficult to establish a developer’s “performance” without recurring to guesswork, due to how software development works and how Git persists history.  
Taking inspiration from a seemingly informal notion, the pony factor, we present an approach to identify the key developers in a software project. We present an analysis of 1,011 GitHub repositories, providing fact-based reflections on development contributions.",2023,145
Exploit those code reviews! bigger data for deeper learning,"Heum\""{u}ller, Robert and Nielebock, Sebastian and Ortmeier, Frank","Modern code review (MCR) processes are prevalent in most organizations that develop software due to benefits in quality assurance and knowledge transfer. With the rise of collaborative software development platforms like GitHub and Bitbucket, today, millions of projects share not only their code but also their review data. Although researchers have tried to exploit this data for more than a decade, most of that knowledge remains a buried treasure. A crucial catalyst for many advances in deep learning, however, is the accessibility of large-scale standard datasets for different learning tasks. This paper presents the ETCR (Exploit Those Code Reviews!) infrastructure for mining MCR datasets from any GitHub project practicing pull-request-based development. We demonstrate its effectiveness with ETCR-Elasticsearch, a dataset of &gt;231𝑘 review comments for &gt;47𝑘 Java file revisions in &gt;40𝑘 pull-requests from the Elasticsearch project. ETCR is designed with the challenge of deep learning in mind. Compared to previous datasets, ETCR datasets include all information for linking review comments to nodes in the respective program’s Abstract Syntax Tree.",2021,146
Mitigating Browser-based DDoS Attacks using CORP,"Agrawall, Akash and Chaitanya, Krishna and Agrawal, Arnav Kumar and Choppella, Venkatesh","On March 27, 2015, Github witnessed a massive DDoS attack, the largest in Github's history till date. In this incident, browsers and users were used as vectors to launch the attack. In this paper, we analyse such browser-based DDoS attacks and simulate them in a lab environment. Existing browser security policies like Same Origin Policy (SOP), Content Security Policy (CSP) do not mitigate these attacks by design. In this paper we observe that CORP (Cross Origin Request Policy), a browser security policy, can be used to mitigate these attacks. CORP enables a server to control cross-origin interactions initiated by a browser. The browser intercepts the cross-origin requests and blocks unwanted requests by the server. This takes the load off the server to mitigate the attack.",2017,147
GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation,"Zhang, Jing and Zhang, Xiaokang and Zhang-Li, Daniel and Yu, Jifan and Yao, Zijun and Ma, Zeyao and Xu, Yiqi and Wang, Haohua and Zhang, Xiaohan and Lin, Nianyi and Lu, Sunrui and Li, Juanzi and Tang, Jie","We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics. Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. All the source code is available on Github.",2023,148
Toward an empirical theory of feedback-driven development,"Beller, Moritz","Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are pre- rather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial framework on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.",2018,149
Deciphering Crypto Twitter,"Kang, Inwon and Mridul, Maruf Ahmed and Sanders, Abraham and Ma, Yao and Munasinghe, Thilanka and Gupta, Aparna and Seneviratne, Oshani","Cryptocurrency is a fast-moving space, with a continuous influx of new projects every year. However, an increasing number of incidents in the space, such as hacks and security breaches, threaten the growth of the community and the development of technology. This dynamic and often tumultuous landscape is vividly mirrored and shaped by discussions within “Crypto Twitter,” a key digital arena where investors, enthusiasts, and skeptics converge, revealing real-time sentiments and trends through social media interactions. We present our analysis on a Twitter dataset collected during a formative period of the cryptocurrency landscape. We collected 40 million tweets using keywords related to cryptocurrency and performed a nuanced analysis that involved grouping the tweets by semantic similarity and constructing a tweet and user network. We used sentence-level embeddings and autoencoders to create K-means clusters of tweets. We identified six groups of tweets and their topics to examine different cryptocurrency-related interests and the change in sentiment over time. For example, we identified different groups of tweets demonstrating coordinated behavior in the market or expressing distrust in centralized cryptocurrency exchanges. Moreover, we discovered sentiment indicators that point to real-life incidents in the crypto world, such as the FTX incident of November 2022. We also constructed and analyzed different networks of tweets and users in our dataset by considering the reply and quote relationships and analyzed the largest components of each network. Our networks reveal a structure of bot activity in Crypto Twitter and suggest that they can be detected and handled using a network-based approach. Our work sheds light on the potential of social media signals to detect and understand crypto events, benefiting investors, regulators, and curious observers alike, as well as the potential for bot detection in Crypto Twitter using a network-based approach.",2024,150
A quality analysis of facebook messenger's most popular chatbots,"Pereira, Juanan and D\'{\i}az, Oscar","This work introduces a set of quality attributes for chatbots. The selection is grounded on scholarly but also reputed blog references from 2016 and 2017. In addition, attributes should be amenable to be extracted (semi) automatically. On these premises, we consider four attributes: ""support of a minimal set of common commands"", ""foresee language variations in both inputs and ouput"", ""human-assistance provision"" and ""timeliness"". These attributes are worked out for the 100 most popular chatbots in Facebook Messager. The aim is to look for correlations between these attributes and chatbot popularity in terms of number of ""likes"". Results show that there is no significance correlation with any of the attributes. However, the experiment come up with two main insights. First, the lack of common communication paterns that would permit users to move their experiences and expectations from one chatbot to another. Second, the existence of many programming errors that reflect that bot programming is still a nascent area.",2018,151
First come first served: the impact of file position on code review,"Fregnan, Enrico and Braz, Larissa and D'Ambros, Marco and \c{C}al\i{}kl\i{}, G\""{u}l and Bacchelli, Alberto","The most popular code review tools (e.g., Gerrit and GitHub) present the files to review sorted in alphabetical order. Could this choice or, more generally, the relative position in which a file is presented bias the outcome of code reviews? We investigate this hypothesis by triangulating complementary evidence in a two-step study.  
First, we observe developers’ code review activity. We analyze the review comments pertaining to 219,476 Pull Requests (PRs) from 138 popular Java projects on GitHub. We found files shown earlier in a PR to receive more comments than files shown later, also when controlling for possible confounding factors: e.g., the presence of discussion threads or the lines added in a file. Second, we measure the impact of file position on defect finding in code review. Recruit- ing 106 participants, we conduct an online controlled experiment in which we measure participants’ performance in detecting two unrelated defects seeded into two different files. Participants are assigned to one of two treatments in which the position of the defective files is switched. For one type of defect, participants are not affected by its file’s position; for the other, they have 64% lower odds to identify it when its file is last as opposed to first. Overall, our findings provide evidence that the relative position in which files are presented has an impact on code reviews’ outcome; we discuss these results and implications for tool design and code review.",2022,152
Towards offensive language detection and reduction in four Software Engineering communities,"Cheriyan, Jithin and Savarimuthu, Bastin Tony Roy and Cranefield, Stephen","Software Engineering (SE) communities such as Stack Overflow have become unwelcoming, particularly through members’ use of offensive language. Research has shown that offensive language drives users away from active engagement within these platforms. This work aims to explore this issue more broadly by investigating the nature of offensive language in comments posted by users in four prominent SE platforms – GitHub, Gitter, Slack and Stack Overflow (SO). It proposes an approach to detect and classify offensive language in SE communities by adopting natural language processing and deep learning techniques. Further, a Conflict Reduction System (CRS), which identifies offence and then suggests what changes could be made to minimize offence has been proposed. Beyond showing the prevalence of offensive language in over 1 million comments from four different communities which ranges from 0.07% to 0.43%, our results show promise in successful detection and classification of such language. The CRS system has the potential to drastically reduce manual moderation efforts to detect and reduce offence in SE communities.",2021,153
Conversational DevBots for Secure Programming: An Empirical Study on SKF Chatbot,"Tony, Catherine and Balasubramanian, Mohana and D\'{\i}az Ferreyra, Nicol\'{a}s E. and Scandariato, Riccardo","Conversational agents or chatbots are widely investigated and used across different fields including healthcare, education, and marketing. Still, the development of chatbots for assisting secure coding practices is in its infancy. In this paper, we present the results of an empirical study on SKF chatbot, a software-development bot (DevBot) designed to answer queries about software security. To the best of our knowledge, SKF chatbot is one of the very few of its kind, thus a representative instance of conversational DevBots aiding secure software development. In this study, we collect and analyse empirical evidence on the effectiveness of SKF chatbot, while assessing the needs and expectations of its users (i.e., software developers). Furthermore, we explore the factors that may hinder the elaboration of more sophisticated conversational security DevBots and identify features for improving the efficiency of state-of-the-art solutions. All in all, our findings provide valuable insights pointing towards the design of more context-aware and personalized conversational DevBots for security engineering.",2022,154
SOTorrent: reconstructing and analyzing the evolution of stack overflow posts,"Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan","Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.",2018,155
The forgotten case of the dependency bugs: on the example of the robot operating system,"Fischer-Nielsen, Anders and Fu, Zhoulai and Su, Ting and W\k{a}sowski, Andrzej","A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.The paper presents a definition and a taxonomy of dependency bugs extracted from data. It describes multiple facets of these bugs and estimates that as many as 15% (!) of all reported bugs are dependency bugs. We show that lightweight tools can find dependency bugs efficiently, although it is challenging to decide which tools to build and difficult to build general tools. We present the research problem to the community, and posit that it should be feasible to eradicate it from software development practice.",2020,156
Detecting Malicious Accounts in Online Developer Communities Using Deep Learning,"Gong, Qingyuan and Zhang, Jiayun and Chen, Yang and Li, Qi and Xiao, Yu and Wang, Xin and Hui, Pan","Online developer communities like GitHub provide services such as distributed version control and task management, which allow a massive number of developers to collaborate online. However, the openness of the communities makes themselves vulnerable to different types of malicious attacks, since the attackers can easily join and interact with legitimate users. In this work, we formulate the malicious account detection problem in online developer communities, and propose GitSec, a deep learning-based solution to detect malicious accounts. GitSec distinguishes malicious accounts from legitimate ones based on the account profiles as well as dynamic activity characteristics. On one hand, GitSec makes use of users' descriptive features from the profiles. On the other hand, GitSec processes users' dynamic behavioral data by constructing two user activity sequences and applying a parallel neural network design to deal with each of them, respectively. An attention mechanism is used to integrate the information generated by the parallel neural networks. The final judgement is made by a decision maker implemented by a supervised machine learning-based classifier. Based on the real-world data of GitHub users, our extensive evaluations show that GitSec is an accurate detection system, with an F1-score of 0.922 and an AUC value of 0.940.",2019,157
Socio-technical work-rate increase associates with changes in work patterns in online projects,"Sarker, Farhana and Vasilescu, Bogdan and Blincoe, Kelly and Filkov, Vladimir","Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance.Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked.",2019,158
Effectiveness of code contribution: from patch-based to pull-request-based tools,"Zhu, Jiaxin and Zhou, Minghui and Mockus, Audris","Code contributions in Free/Libre and Open Source Software projects are controlled to maintain high-quality of software. Alternatives to patch-based code contribution tools such as mailing lists and issue trackers have been developed with the pull request systems being the most visible and widely available on GitHub. Is the code contribution process more effective with pull request systems? To answer that, we quantify the effectiveness via the rates contributions are accepted and ignored, via the time until the first response and final resolution and via the numbers of contributions. To control for the latent variables, our study includes a project that migrated from an issue tracker to the GitHub pull request system and a comparison between projects using mailing lists and pull request systems. Our results show pull request systems to be associated with reduced review times and larger numbers of contributions. However, not all the comparisons indicate substantially better accept or ignore rates in pull request systems. These variations may be most simply explained by the differences in contribution practices the projects employ and may be less affected by the type of tool. Our results clarify the importance of understanding the role of tools in effective management of the broad network of potential contributors and may lead to strategies and practices making the code contribution more satisfying and efficient from both contributors' and maintainers' perspectives.",2016,159
"""@alex, this fixes #9"": Analysis of Referencing Patterns in Pull Request Discussions","Chopra, Ashish and Mo, Morgan and Dodson, Samuel and Beschastnikh, Ivan and Fels, Sidney S. and Yoon, Dongwook","Pull Requests (PRs) are a frequently used method for proposing changes to source code repositories. When discussing proposed changes in a PR discussion, stakeholders often reference a wide variety of information objects for establishing shared awareness and common ground. Previous work has not considered how the referential behavior impacts collaborative software development via PRs. This knowledge gap is the major barrier in evaluating the current support for referencing in PRs and improving them. We conducted an explorative analysis of textasciitilde7K references, collected from 450 public PRs on GitHub, and constructed taxonomies of referent types and expressions. Using our annotated dataset, we identified several patterns in the use of references. Referencing source code elements was prevalent but the authoring interface lacks support for it. Three classes of contextual factors influence referencing behaviors: referent type, discussion thread, and project attributes. Referencing patterns may indicate PR outcomes (e.g., merged PRs frequently reference issues, users, and tests). We conclude with design implications to support more effective referencing in PR discussion interfaces.",2021,160
Code notes: designing a low-cost tangible coding tool for/with children,"Sabuncuo\u{g}lu, Alpay and Erkaya, Merve and Buruk, O\u{g}uz Turan and G\""{o}ksun, Tilbe","Programming has become an essential subject for today's education curriculum and as a result, the importance of creating the right environments to teach is increasing. For such environments, featuring tangible tools enhances creativity and collaboration. However, due to their high prices, current tangible tools are not reachable by most of the students. We developed Code Notes as a low-cost, attainable and tangible tool aimed to motivate children to support programming education. Code Notes is comprised of an Android app and code-cardboards to teach the basic concepts in programming. We continue to develop the platform with insights gained from children. This paper shares the design phases of Code Notes and observations from our two-month programming project. We also presented some future concepts of Code Notes that offer an active and embodied interaction with the teaching material.",2018,161
SortingHat: wizardry on software project members,"Moreno, David and Due\~{n}as, Santiago and Cosentino, Valerio and Fernandez, Miguel Angel and Zerouali, Ahmed and Robles, Gregorio and Gonzalez-Barahona, Jesus M.","Nowadays, software projects and in particular open source ones heavily rely on a plethora of tools (e.g., Git, GitHub) to support and coordinate development activities. Despite their paramount value, they foster to fragment members' contribution, since members can access them with different identities (e.g., email, username). Thus, researchers and practitioners willing to evaluate individual members contributions are often forced to develop ad-hoc scripts or perform manual work to merge identities. This comes at the risk of obtaining wrong results and hindering replication of their work. In this demo we present SortingHat, which helps to track unique identities of project members and their related information such as gender, country and organization enrollments. It allows to manipulate identities interactively as well as to load bulks of identities via batch files (useful for projects with large communities). SortingHat is a component of GrimoireLab, an industry strong free platform developed by Bitergia, which offers commercial software analytics and is part of the CHAOSS project of the Linux Foundation. A video showing SortingHat is available at https://youtu.be/724I1XcQV6c.",2019,162
Comparing Different Developer Behavior Recommendation Styles,"Brown, Chris and Parnin, Chris","Research shows that one of the most effective ways software engineers discover useful developer behaviors, or tools and practices designed to help developers complete programming tasks, is through human-to-human recommendations from coworkers during work activities. However, due to the increasingly distributed nature of the software industry and development teams, opportunities for these peer interactions are in decline. To overcome the deprecation of peer interactions in software engineering, we explore the impact of several system-to-human recommendation systems, including the recently introduced suggested changes feature on GitHub which allows users to propose code changes to developers on contributions to repositories, to discover their impact on developer recommendations. In this work, we aim to study the effectiveness of suggested changes for recommending developer behaviors by performing a user study with professional software developers to compare static analysis tool recommendations from emails, pull requests, issues, and suggested changes. Our results provide insight into creating systems for recommendations between developers and design implications for improving automated recommendations to software engineers.",2020,163
Katti: An Extensive and Scalable Tool for Website Analyses,"Nettersheim, Florian and Arlt, Stephan and Rademacher, Michael and Dehling, Florian","Research on web security and privacy frequently relies on tools that analyze a set of websites. One major obstacle to the judicious analysis is the employment of a rock-solid and feature-rich web crawler. For example, the automated analysis of ad-malware campaigns on websites requests crawling a vast set of domains on multiple real web browsers, while simultaneously mitigating bot detections and applying user interactions on websites. Further, the ability to attach various threat analysis frameworks lacks current tooling efforts in web crawling and analyses. In this paper we introduce Katti, which overcomes several of today’s technical hurdles in web crawling. Our tool employs a distributed task queue that efficiently and reliably handles both large crawling and threat analyses requests. Katti&nbsp; extensively collects all available web data through an integrated person-in-the-middle proxy. Moreover, Katti&nbsp; is not limited to a specific use case, allowing users to easily customize our tool to their individual research intends.",2023,164
Let's See Your Digits: Anomalous-State Detection using Benford's Law,"Maurus, Samuel and Plant, Claudia","Benford's Law explains a curious phenomenon in which the leading digits of ""naturally-occurring"" numerical data are distributed in a precise fashion. In this paper we begin by showing that system metrics generated by many modern information systems like Twitter, Wikipedia, YouTube and GitHub obey this law. We then propose a novel unsupervised approach called BenFound that exploits this property to detect anomalous system events. BenFound tracks the ""Benfordness"" of key system metrics, like the follower counts of tweeting Twitter users or the change deltas in Wikipedia page edits. It then applies a novel Benford-conformity test in real-time to identify ""non-Benford events"". We investigate a variety of such events, showing that they correspond to unnatural and often undesirable system interactions like spamming, hashtag-hijacking and denial-of-service attacks. The result is a technically-uncomplicated and effective ""red flagging"" technique that can be used to complement existing anomaly-detection approaches. Although not without its limitations, it is highly efficient and requires neither obscure parameters, nor text streams, nor natural-language processing.",2017,165
Wikipedia Tools for Google Spreadsheets,"Steiner, Thomas","In this paper, we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free, Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online, while collaborating with other users in realtime. Wikipedia is a free-access, free-content Internet encyclopedia, whose content and data is available, among other means, through an API. With the Wikipedia Tools for Google Spreadsheets, we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released under the permissive Apache 2.0 license.",2016,166
An Experience Report on Technical Debt in Pull Requests: Challenges and Lessons Learned,"Karmakar, Shubhashis and Codabux, Zadia and Vidoni, Melina","Background: GitHub is a collaborative platform for global software development, where Pull Requests (PRs) are essential to bridge code changes with version control. However, developers often trade software quality for faster implementation, incurring Technical Debt (TD). When developers undertake reviewers’ roles and evaluate PRs, they can often detect TD instances, leading to either PR rejection or discussions. Aims: We investigated whether Pull Request Comments (PRCs) indicate TD by assessing three large-scale repositories: Spark, Kafka, and React. Method: We combined manual classification with automated detection using machine learning and deep learning models. Results: We classified two datasets and found that 37.7 and 38.7% of PRCs indicate TD, respectively. Our best model achieved F1 = 0.85 when classifying TD during the validation phase. Conclusions: We faced several challenges during this process, which may hint that TD in PRCs is discussed differently from other software artifacts (e.g., code comments, commits, issues, or discussion forums). Thus, we present challenges and lessons learned to assist researchers in pursuing this area of research.",2022,167
Motivation Research Using Labeling Functions,"Amit, Idan and Feitelson, Dror G.","Motivation is an important factor in software development. However, it is a subjective concept that is hard to quantify and study empirically. In order to use the wealth of data available about real software development projects in GitHub, we represent the motivation of developers using labeling functions. These are validated heuristics that need only be better than a guess, computable on a dataset. We define four labeling functions for motivation based on behavioral cues like working in diverse hours of the day. We validated the functions by agreement with respect to a developers survey, per person behavior, and temporal changes. We then apply them to 150 thousand developers working on GitHub projects. Using the identification of motivated developers, we measure developer performance gaps. We show that motivated developers have up to 70% longer activity period, produce up to 300% more commits, and invest up to 44% more time per commit.",2024,168
PyTy: Repairing Static Type Errors in Python,"Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael","Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.",2024,169
User-Centric Deployment of Automated Program Repair at Bloomberg,"Williams, David and Callan, James and Kirbas, Serkan and Mechtaev, Sergey and Petke, Justyna and Prideaux-Ghee, Thomas and Sarro, Federica","Automated program repair (APR) tools have unlocked the potential for the rapid rectification of codebase issues. However, to encourage wider adoption of program repair in practice, it is necessary to address the usability concerns related to generating irrelevant or out-of-context patches. When software engineers are presented with patches they deem uninteresting or unhelpful, they are burdened with more ""noise"" in their workflows and become less likely to engage with APR tools in future. This paper presents a novel approach to optimally time, target, and present auto-generated patches to software engineers. To achieve this, we designed, developed, and deployed a new tool dubbed B-Assist, which leverages GitHub's Suggested Changes interface to seamlessly integrate automated suggestions into active pull requests (PRs), as opposed to creating new, potentially distracting PRs. This strategy ensures that suggestions are not only timely, but also contextually relevant and delivered to engineers most familiar with the affected code. Evaluation among Bloomberg software engineers demonstrated their preference for this approach. From our user study, B-Assist's efficacy is evident, with the acceptance rate of patch suggestions being as high as 74.56%; engineers also found the suggestions valuable, giving usefulness ratings of at least 4 out of 5 in 78.2% of cases. Further, this paper sheds light on persisting usability challenges in APR and lays the groundwork for enhancing the user experience in future APR tools.",2024,170
Heard it through the Gitvine: an empirical study of tool diffusion across the npm ecosystem,"Lamba, Hemank and Trockman, Asher and Armanios, Daniel and K\""{a}stner, Christian and Miller, Heather and Vasilescu, Bogdan","Automation tools like continuous integration services, code coverage reporters, style checkers, dependency managers, etc. are all known to provide significant improvements in developer productivity and software quality. Some of these tools are widespread, others are not. How do these automation ""best practices"" spread? And how might we facilitate the diffusion process for those that have seen slower adoption? In this paper, we rely on a recent innovation in transparency on code hosting platforms like GitHub---the use of repository badges---to track how automation tools spread in open-source ecosystems through different social and technical mechanisms over time. Using a large longitudinal data set, multivariate network science techniques, and survival analysis, we study which socio-technical factors can best explain the observed diffusion process of a number of popular automation tools. Our results show that factors such as social exposure, competition, and observability affect the adoption of tools significantly, and they provide a roadmap for software engineers and researchers seeking to propagate best practices and tools.",2020,171
The Emerging Artifacts of Centralized Open-Code,"Choksi, Madiha Zahrah and Mandel, Ilan and Widder, David and Shvartzshnaider, Yan","In 2022, generative model based coding assistants became widely available with the public release of GitHub Copilot. Approaches to generative coding are often critiqued within the context of advances in machine learning. We argue that tools such as Copilot are better understood when contextualized against technologies derived from the same communities and datasets. Our work traces the historical and ideological origins of free and open source code and characterizes the process of centralization. We examine three case studies —Dependabot, Crater, and Copilot— to compare the engineering, social, and legal qualities of technical artifacts derived from shared community-based labor. Our analysis focuses on the implications these artifacts create for infrastructural dependencies, community adoption, and intellectual property. Reframing generative coding assistants through a set of peer technologies broadens considerations for academics and policymakers beyond machine learning, to include the ways technical artifacts are derived from communities.",2024,172
Activity-based analysis of open source software contributors: roles and dynamics,"Cheng, Jinghui and Guo, Jin L. C.","Contributors to open source software (OSS) communities assume diverse roles to take different responsibilities. One major limitation of the current OSS tools and platforms is that they provide a uniform user interface regardless of the activities performed by the various types of contributors. This paper serves as a non-trivial first step towards resolving this challenge by demonstrating a methodology and establishing knowledge to understand how the contributors' roles and their dynamics, reflected in the activities contributors perform, are exhibited in OSS communities. Based on an analysis of user action data from 29 GitHub projects, we extracted six activities that distinguished four Active roles and five Supporting roles of OSS contributors, as well as patterns in role changes. Through the lens of the Activity Theory, these findings provided rich design guidelines for OSS tools to support diverse contributor roles.",2019,173
An Industry Case Study on Adoption of AI-based Programming Assistants,"Davila, Nicole and Wiese, Igor and Steinmacher, Igor and Lucio da Silva, Lucas and Kawamoto, Andre and Favaro, Gilson Jose Peres and Nunes, Ingrid","Programming assistants based on artificial intelligence (AI), such as ChatGPT and GitHub Copilot, have gained worldwide popularity recently. Studies in software development have explored the adoption of these tools, investigating their characteristics and impacts and how practitioners interact and perceive them. To contribute to this growing body of knowledge, in this study, we aim to explore the adoption of AI-based programming assistants in the Brazilian industry. More specifically, we aim to understand how practitioners of a particular Brazilian agroindustry-related company perceive and use AI-based tools to develop software. Using an online survey, we collected and analyzed 72 responses from employees of the studied company. Our findings suggest that practitioners mainly adopt ChatGPT and GitHub Copilot, interacting with these tools to accelerate online searching, typing, and syntax recall. A recurrent difficulty is the lack of context in the suggestions provided by these tools, but participants work on detailed descriptions to contextualize and cope with this challenge. Among the reasons for not using AI-based tools, the most influential is that participants use a commercial programming language, i.e., Uniface, which these tools lack examples. Our results provide insights into the state of the practice related to AI-based programming assistants and discuss implications for practitioners and researchers.",2024,174
On Wasted Contributions: Understanding the Dynamics of Contributor-Abandoned Pull Requests–A Mixed-Methods Study of 10 Large Open-Source Projects,"Khatoonabadi, Sayedhassan and Costa, Diego Elias and Abdalkareem, Rabe and Shihab, Emad","Pull-based development has enabled numerous volunteers to contribute to open-source projects with fewer barriers. Nevertheless, a considerable amount of pull requests (PRs) with valid contributions are abandoned by their contributors, wasting the effort and time put in by both the contributors and maintainers. To better understand the underlying dynamics of contributor-abandoned PRs, we conduct a mixed-methods study using both quantitative and qualitative methods. We curate a dataset consisting of 265,325 PRs including 4,450 abandoned ones from ten popular and mature GitHub projects and measure 16 features characterizing PRs, contributors, review processes, and projects. Using statistical and machine learning techniques, we find that complex PRs, novice contributors, and lengthy reviews have a higher probability of abandonment and the rate of PR abandonment fluctuates alongside the projects’ maturity or workload. To identify why contributors abandon their PRs, we also manually examine a random sample of 354 abandoned PRs. We observe that the most frequent abandonment reasons are related to the obstacles faced by contributors, followed by the hurdles imposed by maintainers during the review process. Finally, we survey the top core maintainers of the studied projects to understand their perspectives on dealing with PR abandonment and on our findings.",2023,175
Self-Service Performance Testing Platform for Autonomous Development Teams,"Vasilevskii, Aleksei and Kachur, Oleksandr","In the modern fast paced and highly autonomous software development teams, it's crucial to maintain a sustainable approach to all performance engineering activites, including performance testing. The high degree of autonomy often results in teams building their own frameworks that are not used consistently and may be abandoned due to lack of support or integration with existing infrastructure, processes and tools.To address these challenges, we present a self-service performance testing platform based on open-source software, that supports distributed load generation, historical results storage and a notification system to trigger alerts in Slack messenger. In addition, it integrates with GitHub Actions to enable developers running load tests as part of their CI/CD pipelines.We'd like to share some of the technical solutions and the details of the decision-making process behind the performance testing platform in a scale-up environment, our experience in building this platform and, most importantly, rolling it out to autonomous development teams and onboarding them into the continuous performance improvement process.",2024,176
MegaVul: A C/C++ Vulnerability Dataset with Comprehensive Code Representations,"Ni, Chao and Shen, Liyu and Yang, Xiaohu and Zhu, Yan and Wang, Shaohua","We constructed a newly large-scale and comprehensive C/C++ vulnerability dataset named MegaVul by crawling the Common Vulnerabilities and Exposures (CVE) database and CVE-related open-source projects. Specifically, we collected all crawlable descriptive information of the vulnerabilities from the CVE database and extracted all vulnerability-related code changes from 28 Git-based websites. We adopt advanced tools to ensure the extracted code integrality and enrich the code with four different transformed representations. Totally, MegaVul contains 17,380 vulnerabilities collected from 992 open-source repositories spanning 169 different vulnerability types disclosed from January 2006 to October 2023. Thus, MegaVul can be used for a variety of software security-related tasks including detecting vulnerabilities and assessing vulnerability severity. All information is stored in the JSON format for easy usage. MegaVul is publicly available on GitHub and will be continuously updated. It can be easily extended to other programming languages.",2024,177
Dialoging Resonance in Human-Chatbot Conversation: How Users Perceive and Reciprocate Recommendation Chatbot's Self-Disclosure Strategy,"Liang, Kai-Hui and Shi, Weiyan and Oh, Yoo Jung and Wang, Hao-Chuan and Zhang, Jingwen and Yu, Zhou","Using chatbots to make recommendations is increasingly popular. The design of recommendation chatbots has mainly been taking an information-centric approach by focusing on the recommended content per se. Limited attention is on how social connection and relational strategies, such as self-disclosure from a chatbot, may influence users' perception and acceptance of the recommendation. In this work, we designed, implemented, and evaluated a social chatbot capable of performing three different levels of self-disclosure: factual information (low), cognitive opinions (medium), and emotions (high). In the evaluation, we recruited 372 participants to converse with the chatbot on two topics: movies and COVID-19 experiences. In each topic, the chatbot conducted small talks and made relevant recommendations to the topic. Participants were randomly assigned to four experimental conditions where the chatbot used factual, cognitive, emotional, and adaptive strategies to perform self-disclosures. By training a text classifier to identify users' level of self-disclosure in real-time, the adaptive chatbot can dynamically match its self-disclosure language to the level of disclosure exhibited by the users. Our results show that users reciprocate with higher-level self-disclosure when a recommendation chatbot displays emotions throughout the conversation. The utilization of emotional disclosure by the chatbot resulted in enhanced enjoyment during interactions and a more favorable perception of the bot. This, in turn, led to greater effectiveness in making recommendations, including a higher likelihood of accepting the recommendation. We discuss the understandings obtained and implications to future design.",2024,178
Effect of Technical and Social Factors on Pull Request Quality for the NPM Ecosystem,"Dey, Tapajit and Mockus, Audris","Background: Pull request (PR) based development, which is a norm for the social coding platforms, entails the challenge of evaluating the contributions of, often unfamiliar, developers from across the open source ecosystem and, conversely, submitting a contribution to a project with unfamiliar maintainers. Previous studies suggest that the decision of accepting or rejecting a PR may be influenced by a diverging set of technical and social factors, but often focus on relatively few projects, do not consider ecosystem-wide measures, or the possible non-monotonic relationships between the predictors and PR acceptance probability. Aim: We aim to shed light on this important decision making process by testing which measures significantly affect the probability of PR acceptance on a significant fraction of a large ecosystem, rank them by their relative importance in predicting PR acceptance, and determine the shape of the functions that map each predictor to PR acceptance. Method: We proposed seven hypotheses regarding which technical and social factors might affect PR acceptance and created 17 measures based on them. Our dataset consisted of 470,925 PRs from 3349 popular NPM packages and 79,128 GitHub users who created those. We tested which of the measures affect PR acceptance and ranked the significant measures by their importance in a predictive model. Results: Our predictive model had and AUC of 0.94, and 15 of the 17 measures were found to matter, including five novel ecosystem-wide measures. Measures describing the number of PRs submitted to a repository and what fraction of those get accepted, and signals about the PR review phase were most significant. We also discovered that only four predictors have a linear influence on the PR acceptance probability while others showed a more complicated response. Conclusion: Our findings should be helpful for PR creators, integrators, as well as tool designers to focus on the important factors affecting PR acceptance.",2020,179
AdAttester: Secure Online Mobile Advertisement Attestation Using TrustZone,"Li, Wenhao and Li, Haibo and Chen, Haibo and Xia, Yubin","Mobile advertisement (ad for short) is a major financial pillar for developers to provide free mobile apps. However, it is frequently thwarted by ad fraud, where rogue code tricks ad providers by forging ad display or user clicks, or both. With the mobile ad market growing drastically (e.g., from $8.76 billion in 2012 to $17.96 billion in 2013), it is vitally important to provide a verifiable mobile ad framework to detect and prevent ad frauds. Unfortunately, this is notoriously hard as mobile ads usually run in an execution environment with a huge TCB.This paper proposes a verifiable mobile ad framework called AdAttester, based on ARM?s TrustZone technology. AdAttester provides two novel security primitives, namely unforgeable clicks and verifiable display. The two primitives attest that ad-related operations (e.g., user clicks) are initiated by the end user (instead of a bot) and that the ad is displayed intact and timely. AdAttester leverages the secure world of TrustZone to implement these two primitives to collect proofs, which are piggybacked on ad requests to ad providers for attestation. AdAttester is non-intrusive to mobile users and can be incrementally deployed in existing ad ecosystem. A prototype of AdAttester is implemented for Android running on a Samsung Exynos 4412 board. Evaluation using 182 typical mobile apps with ad frauds shows that AdAttester can accurately distinguish ad fraud from legitimate ad operations, yet incurs small performance overhead and little impact on user experience.",2015,180
"""Looks Good To Me ;-)"": Assessing Sentiment Analysis Tools for Pull Request Discussions","Coutinho, Daniel and Cito, Luisa and Lima, Maria Vit\'{o}ria and Arantes, Beatriz and Alves Pereira, Juliana and Arriel, Johny and Godinho, Jo\~{a}o and Martins, Vinicius and Lib\'{o}rio, Paulo V\'{\i}tor C. F. and Leite, Leonardo and Garcia, Alessandro and Assun\c{c}\~{a}o, Wesley K. G. and Steinmacher, Igor and Baffa, Augusto and Fonseca, Baldoino","Modern software development relies on cloud-based collaborative platforms (e.g., GitHub and GitLab). In these platforms, developers often employ a pull-based development approach, proposing changes via pull requests and engaging in communication via asynchronous message exchanges. Since communication is key for software development, studies have linked different types of sentiments embedded in the communication to their effects on software projects, such as bug-inducing commits or the non-acceptance of pull requests. In this context, sentiment analysis tools are paramount to detect the sentiment of developers’ messages and prevent potentially harmful impact. Unfortunately, existing state-of-the-art tools vary in terms of the nature of their data collection and labeling processes. Yet, there is no comprehensive study comparing the performance and generalizability of existing tools utilizing a dataset that was designed and systematically curated to this end, and in this specific context. Therefore, in this study, we design a methodology to assess the effectiveness of existing sentiment analysis tools in the context of pull request discussions. For that, we created a dataset that contains ≈ 1.8K manually labeled messages from 36 software projects. The messages were labeled by 19 experts (neuroscientists and software engineers), using a novel and systematic manual classification process designed to reduce subjectivity. By applying these existing tools to the dataset, we observed that while some tools ]perform acceptably, their performance is far from ideal, especially when classifying negative messages. This is interesting since negative sentiment is often related to a critical or unfavorable opinion. We also observed that some messages have characteristics that can make them harder to classify, causing disagreements between the experts and possible misclassifications by the tools, requiring more attention from researchers. Our contributions include valuable resources to pave the way to develop robust and mature sentiment analysis tools that capture/anticipate potential problems during software development.",2024,181
What Makes a Good TODO Comment?,"Wang, Haoye and Gao, Zhipeng and Bi, Tingting and Grundy, John and Wang, Xinyu and Wu, Minghui and Yang, Xiaohu","Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. To assist developers, we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality.",2024,182
The OCEAN mailing list data set: network analysis spanning mailing lists and code repositories,"Warrick, Melanie and Rosenblatt, Samuel F. and Young, Jean-Gabriel and Casari, Amanda and H\'{e}bert-Dufresne, Laurent and Bagrow, James","Communication surrounding the development of an open source project largely occurs outside the software repository itself. Historically, large communities often used a collection of mailing lists to discuss the different aspects of their projects. Multimodal tool use, with software development and communication happening on different channels, complicates the study of open source projects as a sociotechnical system. Here, we combine and standardize mailing lists of the Python community, resulting in 954,287 messages from 1995 to the present. We share all scraping and cleaning code to facilitate reproduction of this work, as well as smaller datasets for the Golang (122,721 messages), Angular (20,041 messages) and Node.js (12,514 messages) communities. To showcase the usefulness of these data, we focus on the CPython repository and merge the technical layer (which GitHub account works on what file and with whom) with the social layer (messages from unique email addresses) by identifying 33% of GitHub contributors in the mailing list data. We then explore correlations between the valence of social messaging and the structure of the collaboration network. We discuss how these data provide a laboratory to test theories from standard organizational science in large open source projects.",2022,183
Characterizing Commits in Open-Source Software,"Ferreira, M\'{\i}vian and Gon\c{c}alves, Diego and Bigonha, Mariza and Ferreira, Kecia","Mining software repositories has been the basis of many studies on software engineering. Many of these works rely on commits’ data extracted since commit is the basic unit of information about activities performed on the projects. However, not knowing the characteristics of commits may introduce biases and threats in studies that consider commits’ data. This work presents an empirical study to characterize commits in terms of four aspects: the size of commits in the total number of files; the size of commits in the number of source-code files, the size of commits by category; and the time interval of commits performed by contributors. We analyzed 1M commits from the 24 most popular and active Java-based projects hosted on GitHub. The main findings of this work show that: the size of commits follows a heavy-tailed distribution; most commits involve one to 10 files; most commits affect one to four source-code files; the commits involving hundreds of files not only refer to merge or management activities; the distribution of the time intervals is approximately a Normal distribution, i.e., the distribution tends to be symmetric, and the mean is representative; in the average, a developer proceed a commit every eight hours. The results of this study should be considered by researchers in empirical works to avoid biases when analyzing commits’ data. Besides, the results provide information that practitioners may apply to improve the management and the planning of software activities.",2023,184
An efficient algorithm for type-safe structural diffing,"Miraldo, Victor Cacciari and Swierstra, Wouter","Effectively computing the difference between two version of a source file has become an indispensable part of software development. The de facto standard tool used by most version control systems is the UNIX diff utility, that compares two files on a line-by-line basis without any regard for the structure of the data stored in these files. This paper presents an alternative datatype generic algorithm for computing the difference between two values of any algebraic datatype. This algorithm maximizes sharing between the source and target trees, while still running in linear time. Finally, this paper demonstrates that by instantiating this algorithm to the Lua abstract syntax tree and mining the commit history of repositories found on GitHub, the resulting patches can often be merged automatically, even when existing technology has failed.",2019,185
Understanding Regular Expression Denial of Service (ReDoS): Insights from LLM-Generated Regexes and Developer Forums,"Siddiq, Mohammed Latif and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva","Regular expression Denial of Service (ReDoS) represents an algorithmic complexity attack that exploits the processing of regular expressions (regexes) to produce a denial-of-service attack. This attack occurs when a regex's evaluation time scales polynomially or exponentially with input length, posing significant challenges for software developers. The advent of Large Language Models (LLMs) has revolutionized the generation of regexes from natural language prompts, but not without its risks. Prior works showed that LLMs can generate code with vulnerabilities and security smells. In this paper, we examined the correctness and security of regexes generated by LLMs as well as the characteristics of LLM-generated vulnerable regexes. Our study also examined ReDoS patterns in actual software projects, aligning them with corresponding regex equivalence classes and algorithmic complexity. Moreover, we analyzed developer discussions on GitHub and StackOverflow, constructing a taxonomy to investigate their experiences and perspectives on ReDoS. In this study, we found that GPT-3.5 was the best LLM to generate regexes that are both correct and secure. We also observed that LLM-generated regexes mainly have polynomial ReDoS vulnerability patterns, and it is consistent with vulnerable regexes found in open source projects. We also found that developers' main discussions around insecure regexes is related to mitigation strategies to remove vulnerable regexes.",2024,186
Unveiling Elite Developers’ Activities in Open Source Projects,"Wang, Zhendong and Feng, Yang and Wang, Yi and Jones, James A. and Redmiles, David","Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB. We investigate elite developers’ contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers.",2020,187
Using Large Language Models to Generate JUnit Tests: An Empirical Study,"Siddiq, Mohammed Latif and Da Silva Santos, Joanna Cecilia and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Al Rifat, Fahmid and Carvalho Lopes, Vin\'{\i}cius","A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.",2024,188
Struggling to Keep Tabs on Capstone Projects: A Chatbot to Tackle Student Procrastination,"Pereira, Juanan and D\'{\i}az, \'{O}scar","Capstone projects usually represent the most significant academic endeavor with which students have been involved. Time management tends to be one of the hurdles. On top, University students are prone to procrastinatory behavior. Inexperience and procrastination team up for students failing to meet deadlines. Supervisors strive to help. Yet heavy workloads frequently prevent tutors from continuous involvement. This article looks into the extent to which conversational agents (a.k.a. chatbots) can tackle procrastination in single-student capstone projects. Specifically, chatbot enablers put in play include (1) alerts, (2) advice, (3) automatic rescheduling, (4) motivational messages, and (5) reference to previous capstone projects. Informed by Cognitive Behavioural Theory, these enablers are framed within the three phases involved in self-regulation misalignment: pre-actional, actional, and post-actional. To motivate this research, we first analyzed 77 capstone-project reports. We found that students’ Gantt charts (1) fail to acknowledge review meetings (70%) and milestones (100%) and (2) suffer deviations from the initial planned effort (16.28%). On these grounds, we develop GanttBot, a Telegram chatbot that is configured from the student’s Gantt diagram. GanttBot reminds students about close landmarks, it informs tutors when intervention might be required, and it learns from previous projects about common pitfalls, advising students accordingly. For evaluation purposes, course 17/18 acts as the control group ( N=28 ) while course 18/19 acts as the treatment group ( N=25  students). Using “overdue days” as the proxy for procrastination, results indicate that course 17/18 accounted for an average of 19 days of delay (SD = 5), whereas these days go down to 10 for the intervention group in course 18/19 (SD = 4). GanttBot is available for public usage as a Telegram chatbot.",2021,189
How to not get rich: an empirical study of donations in open source,"Overney, Cassandra and Meinicke, Jens and K\""{a}stner, Christian and Vasilescu, Bogdan","Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.",2020,190
Massive Cross-Platform Simulations of Online Social Networks,"Muri\'{c}, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\'{e}s and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio","As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.",2020,191
Jack-in-the-box: An Empirical Study of JavaScript Bundling on the Web and its Security Implications,"Rack, Jeremy and Staicu, Cristian-Alexandru","In recent years, we have seen an increased interest in studying the software supply chain of user-facing applications to uncover problematic third-party dependencies. Prior work shows that web applications often rely on outdated or vulnerable third-party code. Moreover, real-world supply chain attacks show that dependencies can also be used to deliver malicious code, e.g., for carrying cryptomining operations. Nonetheless, existing measurement studies in this domain neglect an important software engineering practice: developers often merge together third-party code into a single file called bundle, which they then deliver from their own servers, making it appear as first-party code. Bundlers like Webpack or Rollup are popular open-source projects with tens of thousand of GitHub stars, suggesting that this technology is widely-used by developers. Ignoring bundling may result in underestimating the complexity of modern software supply chains.In this work, we aim to address these methodological shortcomings of prior work. To this end, we propose a novel methodology for automatically detecting bundles, and partially reverse engineer them. Using this methodology, we conduct the first large-scale empirical study of bundled code on the web and examine its security implications. We provide evidence about the high prevalence of bundles, which are contained in 40% of all websites, and the average website includes more than one bundle. Following our methodology, we reidentify 1 051 vulnerabilities originating from 33 vulnerable npm packages, included in bundled code. Among the vulnerabilities, we find 17 critical and 59 high severity ones, which might enable malicious actors to execute attacks such as arbitrary code execution. Analyzing the low-rated libraries included in bundles, we discover 10 security holding packages, which suggest that supply-chain attacks affecting bundles are not only possible, but they are already happening.",2023,192
Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks,"Weiss, Michael and Tonella, Paolo","Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remote-only model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.",2023,193
"Understanding the Usages, Lifecycle, and Opportunities of Screen Readers’ Plugins","Momotaz, Farhani and Ehtesham-Ul-Haque, Md and Billah, Syed Masum","Screen reader plugins are small pieces of code that blind users can download and install to enhance the capabilities of their screen readers. This article aims to understand why blind users use these plugins, as well as how these plugins are developed, deployed, and maintained. To this end, we conducted an interview study with 14 blind users to gain individual perspectives and analyzed 2,000 online posts scraped from three plugin-related forums to gain the community perspective. Our study revealed that screen reader users rely on plugins for various reasons, such as to improve the usability of screen readers and application software, to make partially accessible applications accessible, and to receive custom auditory feedback. Furthermore, installing plugins is easy; uninstalling them is unlikely; and finding them online is ad hoc, challenging, and sometimes poses security threats. In addition, developing screen reader plugins is technically demanding; only a handful of people develop plugins. Unfortunately, most plugins do not receive updates once distributed and become obsolete. The lack of financial incentives plays in the slow growth of the plugin ecosystem. Further, we outlined the complex, tripartite collaboration among individual blind users, their online communities, and developer communities in creating a plugin. Additionally, we reported several phenomena within and between these communities that are likely to influence a plugin’s development. Based on our findings, we recommend creating a community-driven repository for all plugins hosted on a peer-to-peer infrastructure, engaging third-party developers, and raising general awareness about the benefits and dangers of plugins. We believe our findings will inspire HCI researchers to embrace the plugin-based distribution model as an effective way to combat accessibility and usability problems in non-visual interaction and to investigate potential ways to improve the collaboration between blind users and developer communities.",2023,194
Towards Understanding Emotions in Informal Developer Interactions: A Gitter Chat Study,"Sajadi, Amirali and Damevski, Kostadin and Chatterjee, Preetha","Emotions play a significant role in teamwork and collaborative activities like software development. While researchers have analyzed developer emotions in various software artifacts (e.g., issues, pull requests), few studies have focused on understanding the broad spectrum of emotions expressed in chats. As one of the most widely used means of communication, chats contain valuable information in the form of informal conversations, such as negative perspectives about adopting a tool. In this paper, we present a dataset of developer chat messages manually annotated with a wide range of emotion labels (and sub-labels), and analyze the type of information present in those messages. We also investigate the unique signals of emotions specific to chats and distinguish them from other forms of software communication. Our findings suggest that chats have fewer expressions of Approval and Fear but more expressions of Curiosity compared to GitHub comments. We also notice that Confusion is frequently observed when discussing programming-related information such as unexpected software behavior. Overall, our study highlights the potential of mining emotions in developer chats for supporting software maintenance and evolution tools.",2023,195
Generating and Validating Contextually Relevant Justifications for Conversational Recommendation,"Volokhin, Sergey and Collins, Marcus and Rokhlenko, Oleg and Agichtein, Eugene","Providing a justification or explanation for a recommendation has been shown to improve the users’ experience with recommender systems, in particular by increasing confidence in the recommendations. However, in order to be effective in a conversational setting, the justifications have to be appropriate for the conversation so far. Previous approaches rely on a user history of reviews and ratings of related items to personalize the recommendation, but this information is not generally available when conversing with a new user, and as such a cold-start problem imposes a challenge in generating suitable justifications. To address this problem, we propose and validate a new method, CONJURE (CONversational JUstificatons for REcommendations) to generate contextually relevant justifications for conversational recommendations. Specifically, we investigate whether the conversation itself can be used effectively to model the user, identify relevant review content from other users, and generate a justification that boosts the user’s confidence in and understanding of the recommendation. To implement CONJURE, we test several novel extensions to prior algorithms, by exploiting an auxiliary corpus of movie reviews to construct the justifications from extracted pieces of those reviews. In particular, we explore different conversation representations and ranking approaches. To evaluate CONJURE, we developed a pairwise crowd task to compare justifications. Our results show large, significant improvements in Efficiency and Transparency metrics over the previous non-contextualized template-based methods. We plan to release our code and an augmented conversation corpus on Github.",2022,196
Software engineering at the speed of light: how developers stay current using twitter,"Singer, Leif and Figueira Filho, Fernando and Storey, Margaret-Anne","The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents.",2014,197
"Novelty Begets Popularity, But Curbs Participation - A Macroscopic View of the Python Open-Source Ecosystem","Fang, Hongbo and Herbsleb, James and Vasilescu, Bogdan","Who creates the most innovative open-source software projects? And what fate do these projects tend to have? Building on a long history of research to understand innovation in business and other domains, as well as recent advances towards modeling innovation in scientific research from the science of science field, in this paper we adopt the analogy of innovation as emerging from the novel recombination of existing bits of knowledge. As such, we consider as innovative the software projects that recombine existing software libraries in novel ways, i.e., those built on top of atypical combinations of packages as extracted from import statements. We then report on a large-scale quantitative study of innovation in the Python open-source software ecosystem. Our results show that higher levels of innovativeness are statistically associated with higher GitHub star counts, i.e., novelty begets popularity. At the same time, we find that controlling for project size, the more innovative projects tend to involve smaller teams of contributors, as well as be at higher risk of becoming abandoned in the long term. We conclude that innovation and open source sustainability are closely related and, to some extent, antagonistic.",2024,198
DIRE: a neural approach to decompiled identifier naming,"Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Goues, Claire Le and Neubig, Graham and Vasilescu, Bogdan","The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.",2020,199
The Human Side of Fuzzing: Challenges Faced by Developers during Fuzzing Activities,"Nourry, Olivier and Kashiwa, Yutaro and Lin, Bin and Bavota, Gabriele and Lanza, Michele and Kamei, Yasutaka","Fuzz testing, also known as fuzzing, is a software testing technique aimed at identifying software vulnerabilities. In recent decades, fuzzing has gained increasing popularity in the research community. However, existing studies led by fuzzing experts mainly focus on improving the coverage and performance of fuzzing techniques. That is, there is still a gap in empirical knowledge regarding fuzzing, especially about the challenges developers face when they adopt fuzzing. Understanding these challenges can provide valuable insights to both practitioners and researchers on how to further improve fuzzing processes and techniques.We conducted a study to understand the challenges encountered by developers during fuzzing. More specifically, we first manually analyzed 829 randomly sampled fuzzing-related GitHub issues and constructed a taxonomy consisting of 39 types of challenges (22 related to the fuzzing process itself, 17 related to using external fuzzing providers). We then surveyed 106 fuzzing practitioners to verify the validity of our taxonomy and collected feedback on how the fuzzing process can be improved. Our taxonomy, accompanied with representative examples and highlighted implications, can serve as a reference point on how to better adopt fuzzing techniques for practitioners, and indicates potential directions researchers can work on toward better fuzzing approaches and practices.",2023,200
Sentiment in software engineering: detection and application,"Cassee, Nathan","In software engineering the role of human aspects is an important one, especially as developers indicate that they experience a wide range of emotions while developing software. Within software engineering researchers have sought to understand the role emotions and sentiment play in the  
development of software by studying issues, pull-requests and commit messages.  
To detect sentiment, automated tools are used, and in this doctoral thesis we plan to study the use of these sentiment analysis tools, their applications, best practices for their usage and the effect of non-natural language on their performance. In addition to studying the application of sentiment analysis tools, we also aim to study self-admitted technical debt and bots in software engineering, to understand why developers express sentiment and what they signal when they express sentiment. Through studying both the application of sentiment analysis tools and the role of sentiment in software engineering, we hope to provide practical recommendations for both researchers and developers.",2022,201
An Empirical Study on Refactoring-Inducing Pull Requests,"Coelho, Fl\'{a}via and Tsantalis, Nikolaos and Massoni, Tiago and Alves, Everton L. G.","Background: Pull-based development has shaped the practice of Modern Code Review (MCR), in which reviewers can contribute code improvements, such as refactorings, through comments and commits in Pull Requests (PRs). Past MCR studies uniformly treat all PRs, regardless of whether they induce refactoring or not. We define a PR as refactoring-inducing, when refactoring edits are performed after the initial commit(s), as either a result of discussion among reviewers or spontaneous actions carried out by the PR developer. Aims: This mixed study (quantitative and qualitative) explores code reviewing-related aspects intending to characterize refactoring-inducing PRs. Method: We hypothesize that refactoring-inducing PRs have distinct characteristics than non-refactoring-inducing ones and thus deserve special attention and treatment from researchers, practitioners, and tool builders. To investigate our hypothesis, we mined a sample of 1,845 Apache's merged PRs from GitHub, mined refactoring edits in these PRs, and ran a comparative study between refactoring-inducing and non-refactoring-inducing PRs. We also manually examined 2,096 review comments and 1,891 detected refactorings from 228 refactoring-inducing PRs. Results: We found 30.2% of refactoring-inducing PRs in our sample and that they significantly differ from non-refactoring-inducing ones in terms of number of commits, code churn, number of file changes, number of review comments, length of discussion, and time to merge. However, we found no statistical evidence that the number of reviewers is related to refactoring-inducement. Our qualitative analysis revealed that at least one refactoring edit was induced by review in 133 (58.3%) of the refactoring-inducing PRs examined. Conclusions: Our findings suggest directions for researchers, practitioners, and tool builders to improve practices around pull-based code review.",2021,202
Fighting for Their Voice: Understanding Indian Muslim Women's Responses to Networked Harassment,"Bhimdiwala, Ayesha and Adavi, Krishna Akhil Kumar and Arif, Ahmer","Social computing researchers have described how online harassment is taking on more sophisticated forms where communities rather than lone individuals carry out attacks. At the same time, those who are targeted are coming together with members of the online crowd to confront this networked harassment. However, we currently lack empirical studies of how such self-organizing efforts take place, especially in non-western contexts like India. To address this gap, this paper examines two case studies of gendered Islamophobia, Sulli Deals and Bulli Bai where a group of Indian Muslim women were targeted via fake auctions hosted on GitHub, that were subsequently amplified over social media. We conducted 12 in-depth interviews with the women targeted in these incidents to understand how these women collectively made sense of these incidents, and the social and spiritual resources they drew upon as they constructed a response together. Our findings describe how our participants understood these incidents (e.g. with families and each other); the work they did to combat the harassment (e.g. coordinating a unified media response); and how religious beliefs and spiritual practices played into this (e.g. values around courage, patience, flexibility, and humility). We conclude by discussing how these findings can refresh our understanding of designing community-driven responses to networked harassment.",2024,203
Teaching CS50 with AI: Leveraging Generative Artificial Intelligence in Computer Science Education,"Liu, Rongxin and Zenke, Carter and Liu, Charlie and Holmes, Andrew and Thornton, Patrick and Malan, David J.","In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had ""a personal tutor.'' Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.",2024,204
A Bridging Centrality Plugin for GEPHI and a Case Study for &lt;italic&gt;Mycobacterium Tuberculosis&lt;/italic&gt; H37Rv,"Pereira, Getulio and Ghosh, Preetam and Santos, Anderson","Bridging Centrality (BriCe) is a popular measure that combines the Betweenness centrality and Bridging coefficient metrics to characterize nodes acting as a bridge among clusters. However, there were no implementations of the BriCe plugin that can be readily used in the GEPHI software or any other software dedicated to graph-based studies. In this paper, we present the BriCe plugin for GEPHI. It is available as a third-party functionality from the native GEPHI interface as a handy plugin to add; hence, no additional download and installation process is necessary. The BriCe plugin for GEPHI is open-source, and one can access the code through the GEPHI GitHub repository. As a use case of the BriCe plugin, we analyzed the genome of Mycobacterium tuberculosis H37Rv to identify biological explanations on &lt;italic&gt;why some proteins were ranked with top BriCe values?&lt;/italic&gt; For instance, we were able to formulate a new hypothesis combining the predicted sub cellular localization and high BriCe values concerning lipopolysaccharides (LPS) exportation. Our hypothesis provides a possible link among proteins of a glycosyltransferase group and the type VII Secretion System. The Bridging Centrality plugin for GEPHI is an easy to use tool for analyzing complex graphs and draw novel insights from graphical data.",2021,205
The software challenges of building smart chatbots,"Daniel, Gwendal and Cabot, Jordi",Chatbots are becoming complex software artifacts that require a high-level of expertise in a variety of technical domains. This technical briefing will cover the software engineering challenges of developing high-quality chatbots. Attendees will be able to create their own bots leveraging the open source chatbot development platform Xatkit.,2021,206
Assessing the Quality of Sources in Wikidata Across Languages: A Hybrid Approach,"Amaral, Gabriel and Piscopo, Alessandro and Kaffee, Lucie-aim\'{e}e and Rodrigues, Odinaldo and Simperl, Elena","Wikidata is one of the most important sources of structured data on the web, built by a worldwide community of volunteers. As a secondary source, its contents must be backed by credible references; this is particularly important, as Wikidata explicitly encourages editors to add claims for which there is no broad consensus, as long as they are corroborated by references. Nevertheless, despite this essential link between content and references, Wikidata's ability to systematically assess and assure the quality of its references remains limited. To this end, we carry out a mixed-methods study to determine the relevance, ease of access, and authoritativeness of Wikidata references, at scale and in different languages, using online crowdsourcing, descriptive statistics, and machine learning. Building on previous work of ours, we run a series of microtasks experiments to evaluate a large corpus of references, sampled from Wikidata triples with labels in several languages. We use a consolidated, curated version of the crowdsourced assessments to train several machine learning models to scale up the analysis to the whole of Wikidata. The findings help us ascertain the quality of references in Wikidata and identify common challenges in defining and capturing the quality of user-generated multilingual structured data on the web. We also discuss ongoing editorial practices, which could encourage the use of higher-quality references in a more immediate way. All data and code used in the study are available on GitHub for feedback and further improvement and deployment by the research community.",2021,207
On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers,"Cabra-Acela, Laura and Mojica-Hanke, Anamaria and Linares-V\'{a}squez, Mario and Herbold, Steffen","Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support users’ daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&amp;A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user’s context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at  GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM",2023,208
Conversations around organizational risk and insider threat,"Osterritter, Luke and Carley, Kathleen M.","Organizational risk and resilience as well as insider threat have been studied through the lenses of socio-psychological studies and information and computer sciences. As with all disciplines, it is an area in which practitioners, enthusiasts, and experts discuss the theory, issues, and solutions of the field in various online public forums. Such conversations, despite their public nature, can be difficult to understand and to study, even by those deeply involved in the communities themselves. Who are the key actors? How can we understand and characterize the culture around such communities, the problems they face, and the solutions favored by the experts in the field? Which narratives are being created and propagated, and by whom - and are these actors truly people, or are they autonomous agents, or ""bots""?In this paper, we demonstrate the value in applying dynamic network analysis and social network analysis to gain situational awareness of the public conversation around insider threat, nation-state espionage, and industrial espionage. Characterizing public discourse around a topic can reveal individuals and organizations attempting to push or shape narratives in ways that might not be obvious to casual observation. Such techniques have been used to great effect in the study of elections, the COVID-19 pandemic, and the study of misinformation and disinformation, and we hope to show that their use in this area is a powerful way to build a foundation of understanding around the conversations in the online public forum, provide data and analysis for use in further research, and equip counter insider threat practitioners with new insights.",2022,209
Analyzing Social Media Activities at Bellingcat,"B\""{a}r, Dominik and Calderon, Fausto and Lawlor, Michael and Licklederer, Sophia and Totzauer, Manuel and Feuerriegel, Stefan","Open-source journalism emerged as a new phenomenon in the media ecosystem, which uses crowdsourcing to fact-check and generate investigative reports for world events using open sources (e.g., social media). A particularly prominent example is Bellingcat. Bellingcat is known for its investigations on the illegal use of chemical weapons during the Syrian war, the Russian responsibility for downing flight MH17, the identification of the perpetrators in the attempted murder of Alexei Navalny, and war crimes in the Russo-Ukraine war. Crucial for this is social media in order to disseminate findings and crowdsource fact-checks. In this work, we characterize the social media activities at Bellingcat on Twitter. For this, we built a comprehensive dataset of all N = &nbsp;24,682 tweets posted by Bellingcat on Twitter since its inception in July 2014. Our analysis is three-fold: (1)&nbsp;We analyze how Bellingcat uses Twitter to disseminate information and collect information from its follower base. Here, we find a steady increase in both posts and replies over time, particularly during the Russo-Ukrainian war, which is in line with the growing importance of Bellingcat for the traditional media ecosystem. (2)&nbsp;We identify characteristics of posts that are successful in eliciting user engagement. User engagement is particularly large for posts embedding additional media items and with a more negative sentiment. (3)&nbsp;We examine how the follower base has responded to the Russian invasion of Ukraine. Here, we find that the sentiment has become more polarized and negative. We attribute this to a ∼ 13-fold increase in bots interacting with the Bellingcat account. Overall, our findings provide recommendations for how open-source journalism such as Bellingcat can successfully operate on social media.",2023,210
Username Squatting on Online Social Networks: A Study on X,"Lepipas, Anastasios and Borovykh, Anastasia and Demetriou, Soteris","Adversaries have been targeting unique identifiers to launch typo-squatting, mobile app squatting and even voice squatting attacks. Anecdotal evidence suggest that online social networks (OSNs) are also plagued with accounts that use similar usernames. This can be confusing to users but can also be exploited by adversaries. However, to date no study characterizes this problem on OSNs. In this work, we define the username squatting problem and design the first multi-faceted measurement study to characterize it on X. We develop a username generation tool (UsernameCrazy) to help us analyze hundreds of thousands of username variants derived from celebrity accounts. Our study reveals that thousands of squatted usernames have been suspended by X, while tens of thousands that still exist on the network are likely bots. Out of these, a large number share similar profile pictures and profile names to the original account signalling impersonation attempts. We found that squatted accounts are being mentioned by mistake in tweets hundreds of thousands of times and are even being prioritized in searches by the network's search recommendation algorithm exacerbating the negative impact squatted accounts can have in OSNs. We use our insights and take the first step to address this issue by designing a framework (SQUAD) that combines UsernameCrazy with a new classifier to efficiently detect suspicious squatted accounts. Our evaluation of SQUAD's prototype implementation shows that it can achieve 94% F1-score when trained on a small dataset.",2024,211
Graph Neural Networks in IoT: A Survey,"Dong, Guimin and Tang, Mingyue and Wang, Zhiyuan and Gao, Jiechao and Guo, Sikun and Cai, Lihua and Gutierrez, Robert and Campbel, Bradford and Barnes, Laura E. and Boukhechba, Mehdi","The Internet of Things (IoT) boom has revolutionized almost every corner of people’s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.",2023,212
Duplicate Bug Report Detection: How Far Are We?,"Zhang, Ting and Han, Donggyun and Vinayakarao, Venkatesh and Irsan, Ivana Clairine and Xu, Bowen and Thung, Ferdian and Lo, David and Jiang, Lingxiao","Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in the research literature. The industry uses some other techniques. Unfortunately, there is insufficient comparison among them, and it is unclear how far we have been. This work fills this gap by comparing the aforementioned techniques. To compare them, we first need a benchmark that can estimate how a tool would perform if applied in a realistic setting today. Thus, we first investigated potential biases that affect the fair comparison of the accuracy of DBRD techniques. Our experiments suggest that data age and issue tracking system (ITS) choice cause a significant difference. Based on these findings, we prepared a new benchmark. We then used it to evaluate DBRD techniques to estimate better how far we have been. Surprisingly, a simpler technique outperforms recently proposed sophisticated techniques on most projects in our benchmark. In addition, we compared the DBRD techniques proposed in research with those used in Mozilla and VSCode. Surprisingly, we observe that a simple technique already adopted in practice can achieve comparable results as a recently proposed research tool. Our study gives reflections on the current state of DBRD, and we share our insights to benefit future DBRD research.",2023,213
What Ignites a Reply? Characterizing Conversations in Microblogs,"Torres, Johnny and Vaca, Carmen and Abad, Cristina L.","Nowadays, microblog platforms provide a medium to share content and interact with other users. With the large-scale data generated on these platforms, the origin and reasons of users engagement in conversations has attracted the attention of the research community. In this paper, we analyze the factors that might spark conversations in Twitter, for the English and Spanish languages. Using a corpus of 2.7 million tweets, we reconstruct existing conversations, then extract several contextual and content features. Based on the features extracted, we train and evaluate several predictive models to identify tweets that will spark a conversation. Our findings show that conversations are more likely to be initiated by users with high activity level and popularity. For less popular users, the type of content generated is a more important factor. Experimental results shows that the best predictive model is able obtain an average score $F1=0.80$. We made available the dataset scripts and code used in this paper to the research community via Github.",2017,214
Scale-Invariant Reinforcement Learning in Real-Time Strategy Games,"Lemos, Marcelo Luiz Harry Diniz and Vieira, Ronaldo E Silva and Tavares, Anderson Rocha and Marcolino, Leandro Soriano and Chaimowicz, Luiz","Real-time strategy games present a significant challenge for artificial game-playing agents by combining several fundamental AI problems. Despite the difficulties, attempts to create autonomous agents using Deep Reinforcement Learning have been successful, with bots like AlphaStar beating even expert human players. Many RTS games include several distinct world maps with different dimensions, which may affect the agent’s observation and the representation of game states. However, most current architectures suffer from fixed input sizes or require extensive and complex training. In this paper, we overcome these limitations by combining Grid-Wise Control with Spatial Pyramid Pooling (SPP). Specifically, we employ the encoder-decoder framework provided by the GridNet architecture and enhance the critic component of PPO by adding an SPP layer to it. The new layer generates a standardized representation of any game state regardless of the initial observation dimensions, allowing the agent to act on any map. Our evaluation demonstrates that our proposed method improves the models’ flexibility and provides a more effective and efficient solution for training autonomous agents in multiple RTS game scenarios.",2024,215
Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump,"Hu, Sihao and Zhang, Zhen and Lu, Shengliang and He, Bingsheng and Li, Zhao","With the proliferation of pump-and-dump schemes (P&amp;Ds) in the cryptocurrency market, it becomes imperative to detect such fraudulent activities in advance to alert potentially susceptible investors. In this paper, we focus on predicting the pump probability of all coins listed in the target exchange before a scheduled pump time, which we refer to as the target coin prediction task. Firstly, we conduct a comprehensive study of the latest 709 P&amp;D events organized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis reveals some interesting patterns of P&amp;Ds, such as that pumped coins exhibit intra-channel homogeneity and inter-channel heterogeneity. Here channel refers a form of group in Telegram that is frequently used to coordinate P&amp;D events. This observation inspires us to develop a novel sequence-based neural network, dubbed SNN, which encodes a channel's P&amp;D event history into a sequence representation via the positional attention mechanism to enhance the prediction accuracy. Positional attention helps to extract useful information and alleviates noise, especially when the sequence length is long. Extensive experiments verify the effectiveness and generalizability of proposed methods. Additionally, we release the code and P&amp;D dataset on GitHub https://github.com/Bayi-Hu/Pump-and-Dump-Detection-on-Cryptocurrency, and regularly update the dataset.",2023,216
PhishinWebView: Analysis of Anti-Phishing Entities in Mobile Apps with WebView Targeted Phishing,"Choi, Yoonjung and Lee, Woonghee and Hur, Junbeom","Despite the relentless efforts on developing anti-phishing techniques, phishing attacks continue to proliferate, often incorporating evasion techniques to bypass detection. While recent studies have continuously enhanced our understanding of their evasion techniques in desktop environments, few studies have been conducted to explore how the phishing attack is being handled in mobile environments, specifically WebView.In this study, we systematically evaluate the blocking processes of anti-phishing entities in individual apps in the real world by designing the phishing attack tailored to WebView. Specifically, we select eight well-known apps using WebView, and report 80 typical phishing sites (without evasion techniques) and 130 user-agent-specific phishing sites (accessible exclusively via each app's WebView). For scalable analysis, we develop an autonomous evaluation framework and investigate accessibility of both apps and Safe Browsing entities. As a result, we find that user-agent-specific (UA-specific) phishing sites successfully evade blocking across all of the eight Android apps. We also investigate accessing strategies of anti-phishing crawlers of both the apps and Safe Browsing entities; and find that only two apps' crawlers can access UA-specific phishing sites without any subsequent actions such as blocking the link. Based on our experiment results, we present security recommendations to take proactive phishing cautions using link preview bots. To the best of our knowledge, this is the first study that explores how the WebView environments handle phishing attacks and disclose their limitation in the real world.",2024,217
Who Controls the Internet? Analyzing Global Threats using Property Graph Traversals,"Simeonovski, Milivoj and Pellegrino, Giancarlo and Rossow, Christian and Backes, Michael","The Internet is built on top of intertwined network services, e.g., email, DNS, and content distribution networks operated by private or governmental organizations. Recent events have shown that these organizations may, knowingly or unknowingly, be part of global-scale security incidents including state-sponsored mass surveillance programs and large-scale DDoS attacks. For example, in March 2015 the Great Cannon attack has shown that an Internet service provider can weaponize millions of Web browsers and turn them into DDoS bots by injecting malicious JavaScript code into transiting TCP connections.While attack techniques and root cause vulnerabilities are routinely studied, we still lack models and algorithms to study the intricate dependencies between services and providers, reason on their abuse, and assess the attack impact. To close this gap, we present a technique that models services, providers, and dependencies as a property graph. Moreover, we present a taint-style propagation-based technique to query the model, and present an evaluation of our framework on the top 100k Alexa domains.",2017,218
DISCO: a dataset of discord chat conversations for software engineering research,"Subash, Keerthana Muthu and Kumar, Lakshmi Prasanna and Vadlamani, Sri Lakshmi and Chatterjee, Preetha and Baysal, Olga","Today, software developers work on complex and fast-moving projects that often require instant assistance from other domain and subject matter experts. Chat servers such as Discord facilitate live communication and collaboration among developers all over the world. With numerous topics discussed in parallel, mining and analyzing the chat data of these platforms would offer researchers and tool makers opportunities to develop software tools and services such as automated virtual assistants, chat bots, chat summarization techniques, Q&amp;A thesaurus, and more.In this paper, we propose a dataset called DISCO consisting of the one-year public DIScord chat COnversations of four software development communities. We have collected the chat data of the channels containing general programming Q&amp;A discussions from the four Discord servers, applied a disentanglement technique [13] to extract conversations from the chat transcripts, and performed a manual validation of conversations on a random sample (500 conversations). Our dataset consists of 28, 712 conversations, 1, 508, 093 messages posted by 323, 562 users. As a case study on the dataset, we applied a topic modelling technique for extracting the top five general topics that are most discussed in each Discord channel.",2022,219
Digital nudges for encouraging developer actions,"Brown, Chris","Researchers have examined a wide variety of practices to help software engineers complete different programming tasks. Despite the fact that studies show software engineering practices and tools created to improve the software development process are useful for preventing bugs, decreasing debugging costs, reducing debugging time, and providing additional benefits, software engineers rarely use them in practice. To persuade humans to alter and adopt new behaviors, psychologists have studied the concept of nudges. My research aims to investigate how digital nudges, or the process of using technology to automatically create nudges, can be beneficial in helping software developers and teams adopt software engineering activities and integrate them into their normal workflow.",2019,220
Reviewer Recommendation using Software Artifact Traceability Graphs,"S\""{u}l\""{u}n, Emre and T\""{u}z\""{u}n, Eray and Do\u{g}rus\""{o}z, U\u{g}ur","Various types of artifacts (requirements, source code, test cases, documents, etc.) are produced throughout the lifecycle of a software. These artifacts are often related with each other via traceability links that are stored in modern application lifecycle management repositories. Throughout the lifecycle of a software, various types of changes can arise in any one of these artifacts. It is important to review such changes to minimize their potential negative impacts. To maximize benefits of the review process, the reviewer(s) should be chosen appropriately.In this study, we reformulate the reviewer suggestion problem using software artifact traceability graphs. We introduce a novel approach, named RSTrace, to automatically recommend reviewers that are best suited based on their familiarity with a given artifact. The proposed approach, in theory, could be applied to all types of artifacts. For the purpose of this study, we focused on the source code artifact and conducted an experiment on finding the appropriate code reviewer(s). We initially tested RSTrace on an open source project and achieved top-3 recall of 0.85 with an MRR (mean reciprocal ranking) of 0.73. In a further empirical evaluation of 37 open source projects, we confirmed that the proposed reviewer recommendation approach yields promising top-k and MRR scores on the average compared to the existing reviewer recommendation approaches.",2019,221
Hierarchical and Hybrid Organizational Structures in Open-source Software Projects: A Longitudinal Study,"Joblin, Mitchell and Eckl, Barbara and Bock, Thomas and Schmid, Angelika and Siegmund, Janet and Apel, Sven","Despite the absence of a formal process and a central command-and-control structure, developer organization in open-source software (OSS) projects are far from being a purely random process. Prior work indicates that, over time, highly successful OSS projects develop a hybrid organizational structure that comprises a hierarchical part and a non-hierarchical part. This suggests that hierarchical organization is not necessarily a global organizing principle and that a fundamentally different principle is at play below the lowest positions in the hierarchy. Given the vast proportion of developers are in the non-hierarchical part, we seek to understand the interplay between these two fundamentally differently organized groups, how this hybrid structure evolves, and the trajectory individual developers take through these structures over the course of their participation. We conducted a longitudinal study of the full histories of 20&nbsp;popular OSS projects, modeling their organizational structures as networks of developers connected by communication ties and characterizing developers’ positions in terms of hierarchical (sub)structures in these networks. We observed a number of notable trends and patterns in the subject projects: (1)&nbsp;hierarchy is a pervasive structural feature of developer networks of OSS projects; (2)&nbsp;OSS projects tend to form hybrid organizational structures, consisting of a hierarchical and a non-hierarchical part; and (3)&nbsp;the positional trajectory of a developer starts loosely connected in the non-hierarchical part and then tightly integrate into the hierarchical part, which is associated with the acquisition of experience (tenure), in addition to coordination and coding activities. Our study (a)&nbsp;provides a methodological basis for further investigations of hierarchy formation, (b)&nbsp;suggests a number of hypotheses on prevalent organizational patterns and trends in OSS projects to be addressed in further work, and (c)&nbsp;may ultimately guide the governance of organizational structures.",2023,222
Properties of Fairness Measures in the Context of Varying Class Imbalance and Protected Group Ratios,"Brzezinski, Dariusz and Stachowiak, Julia and Stefanowski, Jerzy and Szczech, Izabela and Susmaga, Robert and Aksenyuk, Sofya and Ivashka, Uladzimir and Yasinskyi, Oleksandr","Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.",2024,223
Example Driven Code Review Explanation,"Rahman, Shadikur and Koana, Umme Ayman and Nayebi, Maleknaz","Background: Code reviewing is an essential part of software development to ensure software quality. However, the abundance of review tasks and the intensity of the workload for reviewers negatively impact the quality of the reviews. The short review text is often unactionable. Aims: We propose the Example Driven Review Explanation (EDRE) method to facilitate the code review process by adding additional explanations through examples. EDRE recommends similar code reviews as examples to further explain a review and help a developer to understand the received reviews with less communication overhead. Method: Through an empirical study in an industrial setting and by analyzing 3,722 Code reviews across three open-source projects, we compared five methods of data retrieval, text classification, and text recommendation. Results: EDRE using TF-IDF word embedding along with an SVM classifier can provide practical examples for each code review with 92% F-score and 90% Accuracy. Conclusions: The example-based explanation is an established method for assisting experts in explaining decisions. EDRE can accurately provide a set of context-specific examples to facilitate the code review process in software teams.",2022,224
Improving collaboration efficiency in fork-based development,"Zhou, Shurui","Fork-based development is a lightweight mechanism that allows developers to collaborate with or without explicit coordination. Although it is easy to use and popular, when developers each create their own fork and develop independently, their contributions are usually not easily visible to others. When the number of forks grows, it becomes very difficult to maintain an overview of what happens in individual forks, which would lead to additional problems and inefficient practices: lost contributions, redundant development, fragmented communities, and so on. Facing the problems mentioned above, we developed two complementary strategies: (1) Identifying existing best practices and suggesting evidence-based interventions for projects that are inefficient; (2) designing new interventions that could improve the awareness of a community using fork-based development, and help developers to detect redundant development to reduce unnecessary effort.",2020,225
RefBERT: A Two-Stage Pre-trained Framework for Automatic Rename Refactoring,"Liu, Hao and Wang, Yanlin and Wei, Zhao and Xu, Yong and Wang, Juhong and Li, Hui and Ji, Rongrong","Refactoring is an indispensable practice of improving the quality and maintainability of source code in software evolution. Rename refactoring is the most frequently performed refactoring that suggests a new name for an identifier to enhance readability when the identifier is poorly named. However, most existing works only identify renaming activities between two versions of source code, while few works express concern about how to suggest a new name. In this paper, we study automatic rename refactoring on variable names, which is considered more challenging than other rename refactoring activities. We first point out the connections between rename refactoring and various prevalent learning paradigms and the difference between rename refactoring and general text generation in natural language processing. Based on our observations, we propose RefBERT, a two-stage pre-trained framework for rename refactoring on variable names. RefBERT first predicts the number of sub-tokens in the new name and then generates sub-tokens accordingly. Several techniques, including constrained masked language modeling, contrastive learning, and the bag-of-tokens loss, are incorporated into RefBERT to tailor it for automatic rename refactoring on variable names. Through extensive experiments on our constructed refactoring datasets, we show that the generated variable names of RefBERT are more accurate and meaningful than those produced by the existing method. Our implementation and data are available at https://github.com/KDEGroup/RefBERT.",2023,226
Exploring the security and privacy risks of chatbots in messaging services,"Edu, Jide and Mulligan, Cliona and Pierazzi, Fabio and Polakis, Jason and Suarez-Tangil, Guillermo and Such, Jose","The unprecedented adoption of messaging platforms for work and recreation has made it an attractive target for malicious actors. In this context, third-party apps (so-called chatbots) offer a variety of attractive functionalities that support the experience in large channels. Unfortunately, under the current permission and deployment models, chatbots in messaging systems could steal information from channels without the victim's awareness. In this paper, we propose a methodology that incorporates static and dynamic analysis for automatically assessing security and privacy issues in messaging platform chatbots. We also provide preliminary findings from the popular Discord platform that highlight the risks that chatbots pose to users. Unlike other popular platforms like Slack or MS Teams, Discord does not implement user-permission checks---a task entrusted to third-party developers. Among others, we find that 55% of chatbots from a leading Discord repository request the ""administrator"" permission, and only 4.35% of chatbots with permissions actually provide a privacy policy.",2022,227
Neural-machine-translation-based commit message generation: how far are we?,"Liu, Zhongxin and Xia, Xin and Hassan, Ahmed E. and Lo, David and Xing, Zhenchang and Wang, Xinyu","Commit messages can be regarded as the documentation of software changes. These messages describe the content and purposes of changes, hence are useful for program comprehension and software maintenance. However, due to the lack of time and direct motivation, commit messages sometimes are neglected by developers. To address this problem, Jiang et al. proposed an approach (we refer to it as NMT), which leverages a neural machine translation algorithm to automatically generate short commit messages from code. The reported performance of their approach is promising, however, they did not explore why their approach performs well. Thus, in this paper, we first perform an in-depth analysis of their experimental results. We find that (1) Most of the test &lt;pre&gt;diffs&lt;/pre&gt; from which NMT can generate high-quality messages are similar to one or more training &lt;pre&gt;diffs&lt;/pre&gt; at the token level. (2) About 16% of the commit messages in Jiang et al.’s dataset are noisy due to being automatically generated or due to them describing repetitive trivial changes. (3) The performance of NMT declines by a large amount after removing such noisy commit messages. In addition, NMT is complicated and time-consuming. Inspired by our first finding, we proposed a simpler and faster approach, named NNGen (Nearest Neighbor Generator), to generate concise commit messages using the nearest neighbor algorithm. Our experimental results show that NNGen is over 2,600 times faster than NMT, and outperforms NMT in terms of BLEU (an accuracy measure that is widely used to evaluate machine translation systems) by 21%. Finally, we also discuss some observations for the road ahead for automated commit message generation to inspire other researchers.",2018,228
Automating Patch Set Generation from Code Reviews Using Large Language Models,"Rahman, Md Tajmilur and Singh, Rahul and Sultan, Mir Yousuf","The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs' feedback, thereby evaluating their readiness to support developers in responding to code-review comments. Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.",2024,229
V-Achilles: An Interactive Visualization of Transitive Security Vulnerabilities,"Jarukitpipat, Vipawan and Chhun, Klinton and Wanprasert, Wachirayana and Ragkhitwetsagul, Chaiyong and Choetkiertikul, Morakot and Sunetnanta, Thanwadee and Kula, Raula Gaikovina and Chinthanet, Bodin and Ishio, Takashi and Matsumoto, Kenichi","A key threat to the usage of third-party dependencies has been the threat of security vulnerabilities, which risks unwanted access to a user application. As part of an ecosystem of dependencies, users of a library are prone to both the direct and transitive dependencies adopted into their applications. Recent work involves tool supports for vulnerable dependency updates, rarely showing the complexity of the transitive updates. In this paper, we introduce our solution to support vulnerability updating in npm. V-Achilles is a prototype that shows a visualization (i.e., using dependency graphs) affected by vulnerability attacks. In addition to the tool overview, we highlight three use cases to demonstrate the usefulness and application of our prototype with real-world npm packages. The prototype is available at https://github.com/MUICT-SERU/V-Achilles, with an accompanying video demonstration at https://www.youtube.com/watch?v=tspiZfhMNcs.",2023,230
Group Chat Ecology in Enterprise Instant Messaging: How Employees Collaborate Through Multi-User Chat Channels on Slack,"Wang, Dakuo and Wang, Haoyu and Yu, Mo and Ashktorab, Zahra and Tan, Ming","Despite the long history of studying instant messaging usage, we know very little about how today's people participate in group chat channels and interact with others inside a real-world organization. In this short paper, we aim to update the existing knowledge on how group chat is used in the context of today's organizations. The knowledge is particularly important for the new norm of remote works under the COVID-19 pandemic. We have the privilege of collecting two valuable datasets: a total of 4,300 group chat channels in Slack from an R&amp;D department in a multinational IT company; and a total of 117 groups' performance data. Through qualitative coding of 100 randomly sampled group channels from the 4,300 channels dataset, we identified and reported 9 categories such as Project channels, IT-Support channels, and Event channels. We further defined a feature metric with 21 meta-features (and their derived features) without looking at the message content to depict the group communication style for these group chat channels, with which we successfully trained a machine learning model that can automatically classify a given group channel into one of the 9 categories. In addition to the descriptive data analysis, we illustrated how these communication metrics can be used to analyze team performance. We cross-referenced 117 project teams and their team-based Slack channels and identified 57 teams that appeared in both datasets, then we built a regression model to reveal the relationship between these group communication styles and the project team performance. This work contributes an updated empirical understanding of human-human communication practices within the enterprise setting, and suggests design opportunities for the future of human-AI communication experience.",2022,231
Auditing YouTube’s Recommendation Algorithm&nbsp;for Misinformation Filter Bubbles,"Srba, Ivan and Moro, Robert and Tomlein, Matus and Pecher, Branislav and Simko, Jakub and Stefancova, Elena and Kompan, Michal and Hrckova, Andrea and Podrouzek, Juraj and Gavornik, Adrian and Bielikova, Maria","In this article, we present results of an auditing study performed over YouTube aimed at investigating how fast a user can get into a misinformation filter bubble, but also what it takes to “burst the bubble,” i.e., revert the bubble enclosure. We employ a sock puppet audit methodology, in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation-promoting content. Then they try to burst the bubbles and reach more balanced recommendations by watching misinformation-debunking content. We record search results, home page results, and recommendations for the watched videos. Overall, we recorded 17,405 unique videos, out of which we manually annotated 2,914 for the presence of misinformation. The labeled data was used to train a machine learning model classifying videos into three classes (promoting, debunking, neutral) with the accuracy of 0.82. We use the trained model to classify the remaining videos that would not be feasible to annotate manually.Using both the manually and automatically annotated data, we observe the misinformation bubble dynamics for a range of audited topics. Our key finding is that even though filter bubbles do not appear in some situations, when they do, it is possible to burst them by watching misinformation-debunking content (albeit it manifests differently from topic to topic). We also observe a sudden decrease of misinformation filter bubble effect when misinformation-debunking videos are watched after misinformation-promoting videos, suggesting a strong contextuality of recommendations. Finally, when comparing our results with a previous similar study, we do not observe significant improvements in the overall quantity of recommended misinformation content.",2023,232
Automating the measurement of heterogeneous chatbot designs,"Ca\~{n}izares, Pablo C. and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan","Chatbots are being increasingly used to provide a natural language interface to all kinds of software services. However, while there are many platforms and tools for chatbot development, they typically lack support to statically measure properties of the designed chatbots, as indicators of their size, complexity, quality or usability, and facilitating comparison.To attack this problem, in this paper we propose a suite of 20 metrics for chatbot designs. The metrics are defined on a neutral chatbot design language, becoming independent of the implementation platform. We have developed a tool, called Asymob, which supports the translation of chatbots defined in several platforms into this neutral format to perform the measurements. As a proof-of-concept, we evaluate the metrics over a collection of Dialogflow and Rasa chatbots from several sources and open-source repositories. Our metrics helped detecting quality issues statically, and served as a basis for comparing chatbots from different origins and built using different technologies.",2022,233
Improving Automated Code Reviews: Learning From Experience,"Lin, Hong Yi and Thongtanunam, Patanamon and Treude, Christoph and Charoenwet, Wachiraphan","Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.",2024,234
The Prevalence of Single Sign-On on the Web: Towards the Next Generation of Web Content Measurement,"Ardi, Calvin and Calder, Matt","Much of the content and structure of the Web remains inaccessible to evaluate at scale because it is gated by user authentication. This limitation restricts researchers to examining only a superficial layer of a website: the landing page or public, search-indexable pages. Since it is infeasible to create individual accounts across thousands of webpages, we examine the prevalence of Single Sign-On (SSO) on the web to explore the feasibility of using a few accounts to authenticate to many sites. We find that 58% of the top 10K websites with logins are accessible with popular 3rd-party SSO providers, such as Google, Facebook, and Apple, indicating that leveraging SSO offers a scalable solution to access a large volume of user-gated content.",2023,235
V2: fast detection of configuration drift in Python,"Horton, Eric and Parnin, Chris","Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions.We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.",2020,236
Enabling Collaborative Data Science Development with the Ballet Framework,"Smith, Micah J. and Cito, J\""{u}rgen and Lu, Kelvin and Veeramachaneni, Kalyan","While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects.",2021,237
Evaluating Human-AI Partnership for LLM-based Code Migration,"Omidvar Tehrani, Behrooz and M, Ishaani and Anubhai, Anmol","The potential of Generative AI, especially Large Language Models (LLMs), to transform software development is remarkable. In this paper, we focus on one area in software development called “code migration”. We define code migration as the process of transitioning the language version of a code repository by converting both the source code and its dependencies. Carefully designing an effective human-AI partnership is essential for boosting developer productivity and faster migrations when performing code migrations. Though human-AI partnerships have been generally explored in the literature, their application to code migrations remains largely unexamined. In this work, we leverage an LLM-based code migration tool called Amazon Q Code Transformation to conduct semi-structured interviews with 11 participants undertaking code migrations. We discuss human’s role in the human-AI partnership (human as a director and a reviewer) and define a trust framework based on various model outcomes to earn trust with LLMs. The guidelines presented in this paper offer a vital starting point for designing human-AI partnerships that effectively augment and complement human capabilities in software development with Generative AI.",2024,238
A longitudinal analysis of bloated Java dependencies,"Soto-Valero, C\'{e}sar and Durieux, Thomas and Baudry, Benoit","We study the evolution and impact of bloated dependencies in a single software ecosystem: Java/Maven. Bloated dependencies are third-party libraries that are packaged in the application binary but are not needed to run the application. We analyze the history of 435 Java projects. This historical data includes 48,469 distinct dependencies, which we study across a total of 31,515 versions of Maven dependency trees. Bloated dependencies steadily increase over time, and 89.2% of the direct dependencies that are bloated remain bloated in all subsequent versions of the studied projects. This empirical evidence suggests that developers can safely remove a bloated dependency. We further report novel insights regarding the unnecessary maintenance efforts induced by bloat. We find that 22% of dependency updates performed by developers are made on bloated dependencies, and that Dependabot suggests a similar ratio of updates on bloated dependencies.",2021,239
A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19,"Oniani, David and Wang, Yanshan","COVID-19 (2019 Novel Coronavirus) has resulted in an ongoing pandemic and as of 26 July 2020, has caused more than 15.7 million cases and over 640,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf (Term Frequency - Inverse Document Frequency), Bidirectional Encoder Representations from Transformers (BERT), Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT), and Universal Sentence Encoder (USE) to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online and made its source code available free of charge to anyone interested in running it locally, online, or just for experimental purposes. Overall, our work has yielded significant results in both designing a chatbot that produces high-quality responses to COVID-19-related questions and comparing several embedding generation techniques.",2020,240
Mining code review data to understand waiting times between acceptance and merging: an empirical analysis,"Kudrjavets, Gunnar and Kumar, Aditya and Nagappan, Nachiappan and Rastogi, Ayushi","Increasing code velocity (or the speed with which code changes are reviewed and merged) is integral to speeding up development and contributes to the work satisfaction of engineers. While factors affecting code change acceptance have been investigated in the past, solutions to decrease the code review lifetime are less understood. This study investigates the code review process to quantify delays and investigate opportunities to potentially increase code velocity. We study the temporal characteristics of half a million code reviews hosted on Gerrit and Phabricator, starting from the first response, to a decision to accept or reject the changes, and until the changes are merged into a target branch. We identified two types of time delays: (a) the wait time from the proposal of code changes until first response, and (b) the wait time between acceptance and merging. Our study indicates that reducing the time between acceptance and merging has the potential to speed up Phabricator code reviews by 29--63%. Small code changes and changes made by authors with a large number of previously accepted code reviews have a higher chance of being immediately accepted, without code review iterations. Our analysis suggests that switching from manual to automatic merges can help increase code velocity.",2022,241
A Comparative Evaluation of Chatbot Development Platforms,"Dagkoulis, Ioannis and Moussiades, Lefteris","Chatbots and virtual assistants have become part of people's everyday life. The need for mass production of these services rapidly and efficiently has created an explosion of software-related services focused on developing chatbots. Big companies like Google, Microsoft, Amazon, and IBM offer complete Chatbot Development Platforms and compete with each other. Our effort is to help people interested in using these platforms decide which is the best CDP for their case. Similar attempts have happened but are now outdated as CDPs have introduced breaking changes. We study each CDP, define criteria and calculate scores based on requirement assumptions. In parallel, we observe how innovations in NLP are presented in the market through CDPs.",2023,242
Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study,"Tufano, Rosalia and Mastropaolo, Antonio and Pepe, Federica and Dabic, Ozren and Di Penta, Massimiliano and Bavota, Gabriele","Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.",2024,243
A Review on IoT Botnet,"Zhao, Hao and Shu, Hui and Xing, Ying","The rapid development of the Internet of Things (IoT) also generates security threats that cannot be ignored. This paper lists the development history of botnets, introduces representative IoT botnets, and reveals their new characteristics. The botnet mechanism and characteristics of IoT botnets are analyzed in depth from three aspects: composition structure, life cycle, and attack behavior, and are compared with traditional botnets. Networks for comparison. The analysis discusses the details of IoT botnet technologies such as scanning discovery, covert communication and survival residency. Summarizes the research status of IoT botnet detection technologies and gives future directions. Finally, the importance of preventing IoT botnets and solving IoT security problems is discussed.",2021,244
Efficient Bi-objective SQL Optimization for Enclaved Cloud Databases with Differentially Private Padding,"Chen, Yaxing and Zheng, Qinghua and Yan, Zheng","Hardware-enabled enclaves have been applied to efficiently enforce data security and privacy protection in cloud database services. Such enclaved systems, however, are reported to suffer from I/O-size (also referred to as communication-volume)-based side-channel attacks. Albeit differentially private padding has been exploited to defend against these attacks as a principle method, it introduces a challenging bi-objective parametric query optimization (BPQO) problem and current solutions are still not satisfactory. Concretely, the goal in BPQO is to find a Pareto-optimal plan that makes a tradeoff between query performance and privacy loss; existing solutions are subjected to poor computational efficiency and high cloud resource waste. In this article, we propose a two-phase optimization algorithm called TPOA to solve the BPQO problem. TPOA incorporates two novel ideas: divide-and-conquer to separately handle parameters according to their types in optimization for dimensionality reduction; on-demand-optimization to progressively build a set of necessary Pareto-optimal plans instead of seeking a complete set for saving resources. Besides, we introduce an acceleration mechanism in TPOA to improve its efficiency, which prunes the non-optimal candidate plans in advance. We theoretically prove the correctness of TPOA, numerically analyze its complexity, and formally give an end-to-end privacy analysis. Through a comprehensive evaluation on its efficiency by running baseline algorithms over synthetic and test-bed benchmarks, we can conclude that TPOA outperforms all benchmarked methods with an overall efficiency improvement of roughly two orders of magnitude; moreover, the acceleration mechanism speeds up TPOA by 10-200\texttimes{}.",2023,245
What makes a good commit message?,"Tian, Yingchen and Zhang, Yuxia and Stol, Klaas-Jan and Jiang, Lin and Liu, Hui","A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an ""audit trail"" by which developers can understand how the source code of a project has changed---and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a ""good"" commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether ""good"" commit messages can be automatically identified; such automation could prompt developers to write better commit messages.",2022,246
An Empathic Agent that Alleviates Stress by Providing Support via Social Media,"Medeiros, Lenin and Bosse, Tibor","This paper describes the development of an 'artificial friend', i.e., an intelligent agent that provides support via text messages in social media in order to alleviate the stress that users experience as a result of everyday problems. The agent consists of three main components: 1) a module that processes text messages based on text mining and classifies them into categories of problems, 2) a module that selects appropriate support strategies based on a validated psychological model of emotion regulation, and 3) a module that generates appropriate responses based on the output of the first two modules. The application is able to interact with users via the social network Telegram.",2017,247
QButterfly: Lightweight Survey Extension for Online User Interaction Studies for Non-Tech-Savvy Researchers,"Ebert, Nico and Scheppler, Bj\""{o}rn and Ackermann, Kurt Alexander and Geppert, Tim","We provide a user-friendly, flexible, and lightweight open-source HCI toolkit (github.com/QButterfly) that allows non-tech-savvy researchers to conduct online user interaction studies using the widespread Qualtrics and LimeSurvey platforms. These platforms already provide rich functionality (e.g., for experiments or usability tests) and therefore lend themselves to an extension to display stimulus web pages and record clickstreams. The toolkit consists of a survey template with embedded JavaScript, a JavaScript library embedded in the HTML web pages, and scripts to analyze the collected data. No special programming skills are required to set up a study or match survey data and user interaction data after data collection. We empirically validated the software in a laboratory and a field study. We conclude that this extension, even in its preliminary version, has the potential to make online user interaction studies (e.g., with crowdsourced participants) accessible to a broader range of researchers.",2023,248
Gender Representation Among Contributors to Open-Source Infrastructure: An Analysis of 20 Package Manager Ecosystems,"Qiu, Huilian Sophie and Zhao, Zihe H and Yu, Tielin Katy and Wang, Justin and Ma, Alexander and Fang, Hongbo and Dabbish, Laura and Vasilescu, Bogdan","While the severe underrepresentation of women and non-binary people in open source is widely recognized, there is little empirical data on how the situation has changed over time and which subcommunities have been more effectively reducing the gender imbalance. To obtain a clearer image of gender representation in open source, we compiled and synthesized existing empirical data from the literature, and computed historical trends in the representation of women across 20 open source ecosystems. While inherently limited by the ability of automatic name-based gender inference to capture true gender identities at an individual level, our census still provides valuable populationlevel insights. Across all and in most ecosystems, we observed a promising upward trend in the percentage of women among code contributors over time, but also high variation in the percentage of women contributors across ecosystems. We also found that, in most ecosystems, women withdraw earlier from open-source participation than men.The representation of women and non-binary people has been extremely low in the open-source software community. Most of the statistics reported by prior studies are below 10%. However, the majority of the prior works were based on subsamples instead of the entire population. Our work started with a review of the gender distributions reported in the literature. Then we provided an overview of the gender distribution in 20 of the largest open-source ecosystem, i.e., grouped by package managers such as npm and PyPI, and investigated its change over time. Moreover, we analyzed the turnover rate between men and women contributors. Across all and in most ecosystems, we observed a promising upward trend in the percentage of women among code contributors over time, but also high variation in the percentage of women contributors across ecosystems. We also found that, in most ecosystems, women withdraw earlier from open-source participation than men.",2023,249
Measuring and Clustering Heterogeneous Chatbot Designs,"Ca\~{n}izares, Pablo C. and L\'{o}pez-Morales, Jose Mar\'{\i}a and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan","Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies.To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.",2024,250
Chatbot as Islamic Finance Expert (CaIFE): When Finance Meets Artificial Intelligence,"Khan, Shahnawaz and Rabbani, Mustafa Raza","Artificial intelligence (AI) is the key technology in the new disruptive technological innovation and industrial transformation. AI has very wide application in finance and banking. The financial institutions not only answer the queries of the customers, but they should also clarify the complaints the customer face and provide the solution. For this purpose, many banks and financial institutions are using Chatbot to provide solution to customer complaints and queries. Chatbots are very efficient in providing solution to customers queries and are available 24 hours to give solution to customer's complaints. Finally, we propose an artificial Intelligence based interactive Chatbot called 'Chatbot as Islamic Finance Expert' (CaIFE). Our interactive Chatbot CaIFE receives automatic robot support related to Islamic finance and banking by having users communicate with a robot having knowledge accumulated by machine learning. It answers any query related to Islamic finance and banking on real time basis. It then presents a case study of CaIFE and explains its characteristics and limitations.",2021,251
Automatic Generation of Conversational Interfaces for Tabular Data Analysis,"Gomez-Vazquez, Marcos and Cabot, Jordi and Claris\'{o}, Robert","Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources, including support for data analytics questions that are responded via charts rendered by the chatbot. Moreover, our chatbots are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns matched to the chatbot intents and entities.",2024,252
Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains,"Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua","The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.",2024,253
The Impact of Code Ownership of DevOps Artefacts on the Outcome of DevOps CI Builds,"Kola-Olawuyi, Ajiromola and Weeraddana, Nimmi Rashinika and Nagappan, Meiyappan","DevOps is a key element in sustaining the quality and efficiency of software development. Yet, the effectiveness of DevOps methodologies extends beyond just technological expertise. It is greatly affected by the manner in which teams handle and engage with DevOps artefacts. Grasping the intricacies of code ownership and contribution patterns within DevOps artefacts is vital for refining strategies and ensuring they deliver their full potential.There are two main strategies to manage DevOps artefacts as suggested in prior work: (1) all project developers need to contribute to DevOps artefacts, and (2) a dedicated group of developers needs to be authoring DevOps artefacts. To analyze which strategy works best for Open-Source Software (OSS) projects, we conduct an empirical analysis on a dataset of 892,193 CircleCI builds spanning 1,689 OSS projects. We employ a two-pronged approach to our study. First, we investigate the impact of chronological code ownership of DevOps artefacts on the outcome of a CI build on a build level. Second, we study the impact of the Skewness of DevOps contributions on the success rate of CI builds at the project level.Our findings reveal that, in general, larger chronological ownership and higher Skewness values of DevOps contributions are related to more successful build outcomes and higher rates of successful build outcomes, respectively. We further find that projects with low Skewness values could have high build success rates when the number of developers in the project is relatively small. Thus, our results suggest that while larger software organizations are better off having dedicated DevOps developers, smaller organizations would benefit from having all developers involved in DevOps.",2024,254
Blackmarket-Driven Collusion on Online Media: A Survey,"Dutta, Hridoy Sankar and Chakraborty, Tanmoy","Online media platforms have enabled users to connect with individuals and organizations, and share their thoughts. Other than connectivity, these platforms also serve multiple purposes, such as education, promotion, updates, and awareness. Increasing, the reputation of individuals in online media (aka social reputation) is thus essential these days, particularly for business owners and event managers who are looking to improve their publicity and sales. The natural way of gaining social reputation is a tedious task, which leads to the creation of unfair ways to boost the reputation of individuals artificially. Several online blackmarket services have developed a thriving ecosystem with lucrative offers to attract content promoters for publicizing their content online. These services are operated in such a way that most of their inorganic activities are going unnoticed by the media authorities, and the customers of the blackmarket services are less likely to be spotted. We refer to such unfair ways of bolstering social reputation in online media as collusion. This survey is the first attempt to provide readers a comprehensive outline of the latest studies dealing with the identification and analysis of blackmarket-driven collusion in online media. We present a broad overview of the problem, definitions of the related problems and concepts, the taxonomy of the proposed approaches, and a description of the publicly available datasets and online tools, and we discuss the outstanding issues. We believe that collusive entity detection is a newly emerging topic in anomaly detection and cyber-security research in general, and the current survey will provide readers with an easy-to-access and comprehensive list of methods, tools, and resources proposed so far for detecting and analyzing collusive entities on online media.",2022,255
A first look at developers’ live chat on Gitter,"Shi, Lin and Chen, Xiao and Yang, Ye and Jiang, Hanzhi and Jiang, Ziyou and Niu, Nan and Wang, Qing","Modern communication platforms such as Gitter and Slack play an increasingly critical role in supporting software teamwork, especially in open source development.Conversations on such platforms often contain intensive, valuable information that may be used for better understanding OSS developer communication and collaboration. However, little work has been done in this regard. To bridge the gap, this paper reports a first comprehensive empirical study on developers' live chat, investigating when they interact, what community structures look like, which topics are discussed, and how they interact. We manually analyze 749 dialogs in the first phase, followed by an automated analysis of over 173K dialogs in the second phase. We find that developers tend to converse more often on weekdays, especially on Wednesdays and Thursdays (UTC), that there are three common community structures observed, that developers tend to discuss topics such as API usages and errors, and that six dialog interaction patterns are identified in the live chat communities. Based on the findings, we provide recommendations for individual developers and OSS communities, highlight desired features for platform vendors, and shed light on future research directions. We believe that the findings and insights will enable a better understanding of developers' live chat, pave the way for other researchers, as well as a better utilization and mining of knowledge embedded in the massive chat history.",2021,256
Supporting Better Insights of Data Science Pipelines with Fine-grained Provenance,"Chapman, Adriane and Lauro, Luca and Missier, Paolo and Torlone, Riccardo","Successful data-driven science requires complex data engineering pipelines to clean, transform, and alter data in preparation for machine learning, and robust results can only be achieved when each step in the pipeline can be justified, and its effect on the data explained. In this framework, we aim at providing data scientists with facilities to gain an in-depth understanding of how each step in the pipeline affects the data, from the raw input to training sets ready to be used for learning. Starting from an extensible set of data preparation operators commonly used within a data science setting, in this work we present a provenance management infrastructure for generating, storing, and querying very granular accounts of data transformations, at the level of individual elements within datasets whenever possible. Then, from the formal definition of a core set of data science preprocessing operators, we derive a provenance semantics embodied by a collection of templates expressed in PROV, a standard model for data provenance. Using those templates as a reference, our provenance generation algorithm generalises to any operator with observable input/output pairs. We provide a prototype implementation of an application-level provenance capture library to produce, in a semi-automatic way, complete provenance documents that account for the entire pipeline. We report on the ability of that reference implementation to capture provenance in real ML benchmark pipelines and over TCP-DI synthetic data. We finally show how the collected provenance can be used to answer a suite of provenance benchmark queries that underpin some common pipeline inspection questions, as expressed on the Data Science Stack Exchange.",2024,257
Reward Reports for Reinforcement Learning,"Gilbert, Thomas Krendl and Lambert, Nathan and Dean, Sarah and Zick, Tom and Snoswell, Aaron and Mehta, Soham","Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta’s BlenderBot 3 chatbot. Several others for game-playing (DeepMind’s MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.",2023,258
MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories,,MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.,2024,259
On the Helpfulness of Answering Developer Questions on Discord with Similar Conversations and Posts from the Past,"Lill, Alexander and Meyer, Andr\'{e} N. and Fritz, Thomas","A big part of software developers' time is spent finding answers to their coding-task-related questions. To answer their questions, developers usually perform web searches, ask questions on Q&amp;A websites, or, more recently, in chat communities. Yet, many of these questions have frequently already been answered in previous chat conversations or other online communities. Automatically identifying and then suggesting these previous answers to the askers could, thus, save time and effort. In an empirical analysis, we first explored the frequency of repeating questions on the Discord chat platform and assessed our approach to identify them automatically. The approach was then evaluated with real-world developers in a field experiment, through which we received 142 ratings on the helpfulness of the suggestions we provided to help answer 277 questions that developers posted in four Discord communities. We further collected qualitative feedback through 53 surveys and 10 follow-up interviews. We found that the suggestions were considered helpful in 40% of the cases, that suggesting Stack Overflow posts is more often considered helpful than past Discord conversations, and that developers have difficulties describing their problems as search queries and, thus, prefer describing them as natural language questions in online communities.",2024,260
Zeuslite: a tool for botnet analysis in the classroom,"Hannah, Kyle and Gianvecchio, Steven","The security of information systems depends not only on technology, but also on the knowledge of practitioners. Botnets, which are networks of compromised computers, are among the foremost threats to computer security. As a major security problem, it is important for current and future practitioners to understand the threat. However, it is difficult to gain practical experience with botnets due to the arduous process of setting up and analyzing them. In this paper, we introduce ZeusLite, a lightweight and user-friendly tool for setting up and analyzing Zeus, one of the Internet's largest and most notorious botnets. ZeusLite intentionally limits Zeus's functionality, making it useful for study but not malicious purposes. The tool includes two main parts: (1) a modified version of the original Zeus binary with select features removed, and (2) a reverse-engineered version of its command and control (C&amp;C) server. ZeusLite can be set up in only a few minutes, allowing students to easily gain hands on experience responding to malware incident. Based on ZeusLite, we also outline a set of possible lab exercises that would teach students how to recognize and remove a Zeus infection.",2015,261
Teaching Testing with Modern Technology Stacks in Undergraduate Software Engineering Courses,"Chow, Scott P. and Komarlu, Tanay and Conrad, Phillip T.","Students' experience with software testing in undergraduate computing courses is often relatively shallow, as compared to the importance of the topic. This experience report describes introducing industrial-strength testing into CMPSC 156, an upper division course in software engineering at UC Santa Barbara. We describe our efforts to modify our software engineering course to introduce rigorous test-coverage requirements into full-stack web development projects, requirements similar to those the authors had experienced in a professional software development setting. We present student feedback on the course and coverage metrics for the projects. We reflect on what about these changes worked (or didn't), and provide suggestions for other instructors that would like to give their students a deeper experience with software testing in their software engineering courses.",2021,262
The Gendered Geography of Contributions to OpenStreetMap: Complexities in Self-Focus Bias,"Das, Maitraye and Hecht, Brent and Gergle, Darren","Millions of people worldwide contribute content to peer production repositories that serve human information needs and provide vital world knowledge to prominent artificial intelligence systems. Yet, extreme gender participation disparities exist in which men significantly outnumber women. A central concern has been that due to self-focus bias, these disparities can lead to corresponding gender content disparities, in which content of interest to men is better represented than content of interest to women. This paper investigates the relationship between participation and content disparities in OpenStreetMap. We replicate findings that women are dramatically under-represented as OSM contributors, and observe that men and women contribute different types of content and do so about different places. However, the character of these differences confound simple narratives about self-focus bias: we find that on a proportional basis, men produced a higher proportion of contributions in feminized spaces compared to women, while women produced a higher proportion of contributions in masculinized spaces compared to men.",2019,263
"Matching Skills, Past Collaboration, and Limited Competition: Modeling When Open-Source Projects Attract Contributors","Fang, Hongbo and Herbsleb, James and Vasilescu, Bogdan","Attracting and retaining new developers is often at the heart of open-source project sustainability and success.  
Previous research found many intrinsic (or endogenous) project characteristics associated with the attractiveness of projects to new developers, but the impact of factors external to the project itself have largely been overlooked.  
In this work, we focus on one such external factor, a project's labor pool, which is defined as the set of contributors active in the overall open-source ecosystem that the project could plausibly attempt to recruit from at a given time. How are the size and characteristics of the labor pool associated with a project's attractiveness to new contributors?  
Through an empirical study of over 516,893 Python projects, we found that the size of the project's labor pool, the technical skill match, and the social connection between the project's labor pool and members of the focal project all significantly influence the number of new developers that the focal project attracts, with the competition between projects with overlapping labor pools also playing a role.  
Overall, the labor pool factors add considerable explanatory power compared to models with only project-level characteristics.",2023,264
Nudge: Accelerating Overdue Pull Requests toward Completion,"Maddila, Chandra and Upadrasta, Sai Surya and Bansal, Chetan and Nagappan, Nachiappan and Gousios, Georgios and van Deursen, Arie","Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge’s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.",2023,265
CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,"Di, Peng and Li, Jianguo and Yu, Hang and Jiang, Wei and Cai, Wenting and Cao, Yang and Chen, Chaoyu and Chen, Dajun and Chen, Hongwei and Chen, Liang and Fan, Gang and Gong, Jie and Gong, Zi and Hu, Wen and Guo, Tingting and Lei, Zhichao and Li, Ting and Li, Zheng and Liang, Ming and Liao, Cong and Liu, Bingchang and Liu, Jiachen and Liu, Zhiwei and Lu, Shaojun and Shen, Min and Wang, Guangpei and Wang, Huan and Wang, Zhi and Xu, Zhaogui and Yang, Jiawei and Ye, Qing and Zhang, Gehao and Zhang, Yu and Zhao, Zelin and Zheng, Xunjin and Zhou, Hailian and Zhu, Lifu and Zhu, Xianying","Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",2024,266
Proximate Social Factors in First-Time Contribution to Online Communities,"Seering, Joseph and Hammer, Jessica and Kaufman, Geoff and Yang, Diyi","In the course of every member's integration into an online community, a decision must be made to participate for the first time. The challenges of effective recruitment, management, and retention of new users have been extensively explored in social computing research. However, little work has looked at in-the-moment factors that lead users to decide to participate instead of ""lurk"", conditions which can be shaped to draw new users in at crucial moments. In this work we analyze 183 million messages scraped from chatrooms on the livestreaming platform Twitch in order to understand differences between first-time participants' and regulars' behaviors and to identify conditions that encourage first-time participation. We find that presence of diverse types of users increases likelihood of new participation, with effects depending on the size of the community. We also find that information-seeking behaviors in first-time participation are negatively associated with retention in the short and medium term.",2020,267
Context-aware conversational developer assistants,"Bradley, Nick C. and Fritz, Thomas and Holmes, Reid","Building and maintaining modern software systems requires developers to perform a variety of tasks that span various tools and information sources. The crosscutting nature of these development tasks requires developers to maintain complex mental models and forces them (a) to manually split their high-level tasks into low-level commands that are supported by the various tools, and (b) to (re)establish their current context in each tool. In this paper we present Devy, a Conversational Developer Assistant (CDA) that enables developers to focus on their high-level development tasks. Devy reduces the number of manual, often complex, low-level commands that developers need to perform, freeing them to focus on their high-level tasks. Specifically, Devy infers high-level intent from developer's voice commands and combines this with an automatically-generated context model to determine appropriate workflows for invoking low-level tool actions; where needed, Devy can also prompt the developer for additional information. Through a mixed methods evaluation with 21 industrial developers, we found that Devy provided an intuitive interface that was able to support many development tasks while helping developers stay focused within their development environment. While industrial developers were largely supportive of the automation Devy enabled, they also provided insights into several other tasks and workflows CDAs could support to enable them to better focus on the important parts of their development tasks.",2018,268
Automated testing of software that uses machine learning APIs,"Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yifan and Hoffmann, Henry and Maire, Michael and Lu, Shan","An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.",2022,269
Toward human-like summaries generated from heterogeneous software artefacts,"Alghamdi, Mahfouth and Treude, Christoph and Wagner, Markus","Automatic text summarisation has drawn considerable interest in the field of software engineering. It can improve the efficiency of software developers, enhance the quality of products, and ensure timely delivery. In this paper, we present our initial work towards automatically generating human-like multi-document summaries from heterogeneous software artefacts. Our analysis of the text properties of 545 human-written summaries from 15 software engineering projects will ultimately guide heuristics searches in the automatic generation of human-like summaries.",2019,270
Your Smart Contracts Are Not Secure: Investigating Arbitrageurs and Oracle Manipulators in Ethereum,"Tjiam, Kevin and Wang, Rui and Chen, Huanhuan and Liang, Kaitai","Smart contracts on Ethereum enable billions of dollars to be transacted in a decentralized, transparent and trustless environment. However, adversaries lie await in the Dark Forest, waiting to exploit any and all smart contract vulnerabilities in order to extract profits from unsuspecting victims in this new financial system. As the blockchain space moves at a breakneck pace, exploits on smart contract vulnerabilities rapidly evolve, and existing research quickly becomes obsolete. It is imperative that smart contract developers stay up to date on the current most damaging vulnerabilities and countermeasures to ensure the security of users' funds, and to collectively ensure the future of Ethereum as a financial settlement layer. This research work focuses on two smart contract vulnerabilities: transaction-ordering dependency and oracle manipulation. Combined, these two vulnerabilities have been exploited to extract hundreds of millions of dollars from smart contracts in the past year (2020-2021). For each of them, this paper presents: (1) a literary survey from recent (as of 2021) formal and informal sources; (2) a reproducible experiment as code demonstrating the vulnerability and, where applicable, countermeasures to mitigate the vulnerability; and (3) analysis and discussion on proposed countermeasures. To conclude, strengths, weaknesses and trade-offs of these countermeasures are summarised, inspiring directions for future research.",2021,271
Mining software repositories with a collaborative heuristic repository,"Babii, Hlib and Prenner, Julian Aron and Stricker, Laurin and Karmakar, Anjan and Janes, Andrea and Robbes, Romain","Many software engineering studies or tasks rely on categorizing software engineering artifacts. In practice, this is done either by defining simple but often imprecise heuristics, or by manual labelling of the artifacts. Unfortunately, errors in these categorizations impact the tasks that rely on them. To improve the precision of these categorizations, we propose to gather heuristics in a collaborative heuristic repository, to which researchers can contribute a large amount of diverse heuristics for a variety of tasks on a variety of SE artifacts. These heuristics are then leveraged by state-of-the-art weak supervision techniques to train high-quality classifiers, thus improving the categorizations. We present an initial version of the heuristic repository, which we applied to the concrete task of commit classification.",2021,272
A comparative study of vulnerability reporting by software composition analysis tools,"Imtiaz, Nasif and Thorn, Seaver and Williams, Laurie","Background: Modern software uses many third-party libraries and frameworks as dependencies. Known vulnerabilities in these dependencies are a potential security risk. Software composition analysis (SCA) tools, therefore, are being increasingly adopted by practitioners to keep track of vulnerable dependencies. Aim: The goal of this study is to understand the difference in vulnerability reporting by various SCA tools. Understanding if and how existing SCA tools differ in their analysis may help security practitioners to choose the right tooling and identify future research needs. Method: We present an in-depth case study by comparing the analysis reports of 9 industry-leading SCA tools on a large web application, OpenMRS, composed of Maven (Java) and npm (JavaScript) projects. Results: We find that the tools vary in their vulnerability reporting. The count of reported vulnerable dependencies ranges from 17 to 332 for Maven and from 32 to 239 for npm projects across the studied tools. Similarly, the count of unique known vulnerabilities reported by the tools ranges from 36 to 313 for Maven and from 45 to 234 for npm projects. Our manual analysis of the tools' results suggest that accuracy of the vulnerability database is a key differentiator for SCA tools. Conclusion: We recommend that practitioners should not rely on any single tool at the present, as that can result in missing known vulnerabilities. We point out two research directions in the SCA space: i) establishing frameworks and metrics to identify false positives for dependency vulnerabilities; and ii) building automation technologies for continuous monitoring of vulnerability data from open source package ecosystems.",2021,273
Challenges in the collaborative development of a complex mathematical software and its ecosystem,"Zimmermann, Th\'{e}o","This is a contribution to the OpenSym 2018 Doctoral Symposium. This paper describes my PhD objectives. As an insider in the Coq development team, I've worked at making the release process of the Coq proof assistant smoother and more automated, at opening the development to external contributions, and at shaping the ecosystem around Coq. I'm intending to evaluate how well-known software engineering techniques and results about open source software communities apply in the specific case of the proof assistant I'm studying.",2018,274
Which Factors Predict the Chat Experience of a Natural Language Generation Dialogue Service?,"Chen, Eason","In this paper, we proposed a conceptual model to predict the chat experience in a natural language generation dialog system. We evaluated the model with 120 participants with Partial Least Squares Structural Equation Modeling (PLS-SEM) and obtained an R-square (R2) with 0.541. The model considers various factors, including the prompts used for generation; coherence, sentiment, and similarity in the conversation; and users’ perceived dialog agents’ favorability. We then further explore the effectiveness of the subset of our proposed model. The results showed that users’ favorability and coherence, sentiment, and similarity in the dialogue are positive predictors of users’ chat experience. Moreover, we found users may prefer dialog agents with characteristics of Extroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism. Through our research, an adaptive dialog system might use collected data to infer factors in our model, predict the chat experience for users through these factors, and optimize it by adjusting prompts.",2023,275
Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents,"Canizares, Pablo C. and \'{A}vila, Daniel and Perez-Soler, Sara and Guerra, Esther and De Lara, Juan","Conversational agents - or chatbots - are increasingly used as the user interface to many software services. While open-domain chatbots like ChatGPT excel in their ability to chat about any topic, task-oriented conversational agents are designed to perform goal-oriented tasks (e.g., booking or shopping) guided by a dialogue-based user interaction, which is explicitly designed. Like any kind of software system, task-oriented conversational agents need to be properly tested to ensure their quality. For this purpose, some tools permit defining and executing conversation test cases. However, there are currently no established means to assess the coverage of the design of a task-oriented agent by a test suite, or mechanisms to automate quality test case generation ensuring the agent coverage.To attack this problem, we propose test coverage criteria for task-oriented conversational agents, and define coverage-based strategies to synthesise test scenarios, some oriented to test case reduction. We provide an implementation of the criteria and the strategies that is independent of the agent development platform. Finally, we report on their evaluation on open-source Dialogflow and Rasa agents, and a comparison against a state-of-the-art testing tool. The experiment shows benefits in terms of test generation correctness, increased coverage and reduced testing time.",2024,276
Julia subtyping: a rational reconstruction,"Zappa Nardelli, Francesco and Belyakova, Julia and Pelenitsyn, Artem and Chung, Benjamin and Bezanson, Jeff and Vitek, Jan","Programming languages that support multiple dispatch rely on an expressive notion of subtyping to specify method applicability. In these languages, type annotations on method declarations are used to select, out of a potentially large set of methods, the one that is most appropriate for a particular tuple of arguments. Julia is a language for scientific computing built around multiple dispatch and an expressive subtyping relation. This paper provides the first formal definition of Julia's subtype relation and motivates its design. We validate our specification empirically with an implementation of our definition that we compare against the existing Julia implementation on a collection of real-world programs. Our subtype implementation differs on 122 subtype tests out of 6,014,476. The first 120 differences are due to a bug in Julia that was fixed once reported; the remaining 2 are under discussion.",2018,277
Automatically recommending code reviewers based on their expertise: an empirical comparison,"Hannebauer, Christoph and Patalas, Michael and St\""{u}nkel, Sebastian and Gruhn, Volker","Code reviews are an essential part of quality assurance in Free, Libre, and Open Source (FLOSS) projects. However, finding a suitable reviewer can be difficult, and delayed or forgotten reviews are the consequence. Automating reviewer selection with suitable algorithms can mitigate this problem. We compare empirically six algorithms based on modification expertise and two algorithms based on review expertise on four major FLOSS projects. Our results indicate that the algorithms based on review expertise yield better recommendations than those based on modification expertise. The algorithm Weighted Review Count (WRC) recommends at least one out of five reviewers correctly in 69 % to 75 % of all cases, which is one of the best results achieved in the comparison.",2016,278
An empirical analysis of build failures in the continuous integration workflows of Java-based open-source software,"Rausch, Thomas and Hummer, Waldemar and Leitner, Philipp and Schulte, Stefan","Continuous Integration (CI) has become a common practice in both industrial and open-source software development. While CI has evidently improved aspects of the software development process, errors during CI builds pose a threat to development efficiency. As an increasing amount of time goes into fixing such errors, failing builds can significantly impair the development process and become very costly. We perform an in-depth analysis of build failures in CI environments. Our approach links repository commits to data of corresponding CI builds. Using data from 14 open-source Java projects, we first identify 14 common error categories. Besides test failures, which are by far the most common error category (up to &gt;80% per project), we also identify noisy build data, e.g., induced by transient Git interaction errors, or general infrastructure flakiness. Second, we analyze which factors impact the build results, taking into account general process and specific CI metrics. Our results indicate that process metrics have a significant impact on the build outcome in 8 of the 14 projects on average, but the strongest influencing factor across all projects is overall stability in the recent build history. For 10 projects, more than 50% (up to 80%) of all failed builds follow a previous build failure. Moreover, the fail ratio of the last k=10 builds has a significant impact on build results for all projects in our dataset.",2017,279
SHARQL: Shape Analysis of Recursive SPARQL Queries,"Bonifati, Angela and Martens, Wim and Timm, Thomas","We showcase SHARQL, a system that allows to navigate SPARQL query logs, can inspect complex queries by visualizing their shape, and can serve as a back-end to flexibly produce statistics about the logs. Even though SPARQL query logs are increasingly available and have become public recently, their navigation and analysis is hampered by the lack of appropriate tools. SPARQL queries are sometimes hard to understand and their inherent properties, such as their shape, their hypertree properties, and their property paths are even more difficult to be identified and properly rendered. In SHARQL, we show how the analysis and exploration of several hundred million queries is possible. We offer edge rendering which works with complex hyperedges, regular edges, and property paths of SPARQL queries. The underlying database stores more than one hundred attributes per query and is therefore extremely flexible for exploring the query logs and as a back-end to compute and display analytical properties of the entire logs or parts thereof.",2020,280
Fine-Grained Just-In-Time Defect Prediction at the Block Level in Infrastructure-as-Code (IaC),"Begoug, Mahi and Chouchen, Moataz and Ouni, Ali and Abdullah Alomar, Eman and Mkaouer, Mohamed Wiem","Infrastructure-as-Code (IaC) is an emerging software engineering practice that leverages source code to facilitate automated configuration of software systems' infrastructure. IaC files are typically complex, containing hundreds of lines of code and dependencies, making them prone to defects, which can result in breaking online services at scale. To help developers early identify and fix IaC defects, research efforts have introduced IaC defect prediction models at the file level. However, the granularity of the proposed approaches remains coarse-grained, requiring developers to inspect hundreds of lines of code in a file, while only a small fragment of code is defective. To alleviate this issue, we introduce a machine-learning-based approach to predict IaC defects at a fine-grained level, focusing on IaC blocks, i.e., small code units that encapsulate specific behaviours within an IaC file. We trained various machine learning algorithms based on a mixture of code, process, and change-level metrics. We evaluated our approach on 19 open-source projects that use Terraform, a widely used IaC tool. The results indicated that there is no single algorithm that consistently outperforms the others in 19 projects. Overall, among the six algorithms, we observed that the LightGBM model achieved a higher average of 0.21 in terms of MCC and 0.71 in terms of AUC. Models analysis reveals that the developer's experience and the relative number of added lines tend to be the most important features. Additionally, we found that blocks belonging to the most frequent types are more prone to defects. Our defect prediction models have also shown sensitivity to concept drift, indicating that IaC practitioners should regularly retrain their models.",2024,281
Internetware '22: Proceedings of the 13th Asia-Pacific Symposium on Internetware,,,2022,282
The Content Quality of Crowdsourced Knowledge on Stack Overflow- A Systematic Mapping Study,"Shahrour, Gheida and Quincey, Ed De and Lal, Sangeeta","Community Question Answering (CQA) forums such as Stack Overflow (SO) are a form of crowdsourced knowledge for software engineers who seek solutions to development and programming challenges. While such a forum provides valuable support to engineers, it often contains low quality content that impacts users' experience and the longevity of new users. Past research shows that most of the low-quality content comes from violating general Netiquette Rules (NRs). In the past, several researchers have worked on analysing the content of SO and suggested approaches to increase its quality. However, to the best of our knowledge, there is no previous work that has reviewed the scale of scientific attention that is given to this cause and the recommendations that have been made. We have conducted a Systematic Mapping Study (SMS) using five relevant databases, reviewing 1,489 papers and selecting 18 that are relevant to help to address this gap. We have found that SO has attracted increasing research interest on reducing NRs violations to improve the quality of communication on SO. Interestingly, the majority of papers used manual qualitative and quantitative analysis approaches to investigate this area. We have found that further research is required to identify more violation features, generalisable sources of data and that the use of computational analysis approaches are still needed in this area.",2024,283
Developing a conversational agent with a globally distributed team: an experience report,"Ruane, Elayne and Smith, Ross and Bean, Dan and Tjalve, Michael and Ventresque, Anthony","In this experience report, we discuss the development of a solution that enables conflict-affected youth to discover and access relevant learning content. A team of individuals from a not-for-profit, a large multi-national technology company, and an academic institution, collaborated to develop that solution as a conversational agent named Hakeem. We provide a brief motivation and product description before outlining our design and development process including forming a distributed virtual team, engaging in user-centred design with conflict-affected youth in Lebanon, and using a minimum viable product approach while adapting Scrum for distributed development. We end this report with a reflection on the lessons learned thus far.",2020,284
Automatically prioritizing and assigning tasks from code repositories in puzzle driven development,"Bugayenko, Yegor and Bakare, Ayomide and Cheverda, Arina and Farina, Mirko and Kruglov, Artem and Plaksin, Yaroslav and Succi, Giancarlo and Pedrycz, Witold","Automatically prioritizing software development tasks extracted from codes could provide significant technical and organizational advantages. Tools exist for the automatic extraction of tasks, but they still lack the ability to capture their mutual dependencies; hence, the capability to prioritize them. Solving this important puzzle is the goal of the presented industrial challenge.",2022,285
Anti-patterns for multi-language systems,"Abidi, Mouna and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\""{e}l","Multi-language systems are common nowadays because most of the systems are developed using components written in different programming languages. These systems could arise from three different reasons: (1) to leverage the strengths and take benefits of each language, (2) to reduce the cost by reusing code written in other languages, (3) to include and accommodate legacy code. However, they also introduce additional challenges, including the increase in the complexity and the need for proper interfaces and interactions between the different languages. To address these challenges, the software-engineering research community, as well as the industry, should describe and provide common guidelines, idioms, and patterns to support the development, maintenance, and evolution of these systems. These patterns are an effective means of improving the quality of multi-language systems. They capture good practices to adopt and bad practices to avoid. In order to help to improve the quality of multi-language systems, we analysed open-source systems, developers' documentation, bug reports, and programming language specifications to extract bad practices of multi-language systems usage. We encoded and cataloged these practices in the form of design anti-patterns. We report here six anti-patterns. These results could help not only researchers but also professional developers considering the use of more than one programming language.",2019,286
Creating and migrating chatbots with conga,"P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan","Chatbots are agents that enable the interaction of users and software by means of written or spoken natural language conversation. Their use is growing, and many companies are starting to offer their services via chatbots, e.g., for booking, shopping or customer support. For this reason, many chatbot development tools have emerged, which makes choosing the most appropriate tool difficult. Moreover, there is hardly any support for migrating chatbots between tools.To alleviate these issues, we propose a model-driven engineering solution that includes: (i) a domain-specific language to model chatbots independently of the development tool; (ii) a recommender that suggests the most suitable development tool for the given chatbot requirements and model; (iii) code generators that synthesize the chatbot code for the selected tool; and (iv) parsers to extract chatbot models out of existing chatbot implementations. Our solution is supported by a web IDE called Conga that can be used for both chatbot creation and migration. A demo video is available at https://youtu.be/3sw1FDdZ7XY.",2021,287
Impacts of the Usage of Generative Artificial Intelligence on Software Development Process,"Santos, Patricia de Oliveira and Figueiredo, Allan Chamon and Nuno Moura, Pedro and Diirr, Bruna and Alvim, Adriana C. F. and Santos, Rodrigo Pereira Dos","Context: Over the years, tools have been created to improve the execution of development process activities. The emergence of generative Artificial Intelligence (AI) and, more recently, the launch and dissemination of Copilot, ChatGPT-3 and other generative tools, have broadened the discussion about the possibility of using conversational generative AI tools in diverse development tasks. Problem: There is still a lack of secondary studies to map the literature about how software development process activities can be affected by the usage of generative AI tools. Solution: This study aims to identify in which activities of the software development process Natural Language (NL) generative AI tools have been used and how they can impact requirements specification, design/architecture, development and testing activities. IS Theory: The study was developed under the aegis of the Task Technology Fit theory. Method: This work presents the results of a Systematic Mapping Review (SMR) carried out to collect research results that investigate the application of generative AI tools in the software development process. Results: Results indicate that the main activities affected are development and testing and that, although there are still some issues to be addressed, there are benefits in using AI generative tools compared to using more traditional methods like human-human pair programming and code testing made by software engineering professionals. Contribution: It was possible to collect studies to identify in which activities of the software development process generative AI tools can be applied and what are the impacts of using this technology.",2024,288
"An Open, Multi-Platform Software Architecture for Online Education in the Metaverse","Lombeyda, Santiago and Djorgovski, S. George and Tran, An and Liu, Joy","Use of online platforms for education is a vibrant and growing arena, incorporating a variety of software platforms and technologies, including various modalities of extended reality. We present our Enhanced Reality Teaching Concierge, an open networking hub architected to enable efficient and easy connectivity between a wide variety of services or applications to a wide variety of clients, designed to showcase 3D for academic purposes across web technologies, virtual reality, and even virtual worlds. The agnostic nature of the system, paired with efficient architecture, and simple and open protocols furnishes an ecosystem that can easily be tailored to maximize the innate characteristics of each 3D display environment while sharing common data and control systems with the ultimate goal of a seamless, expandable, nimble education metaverse.",2022,289
Geographic diversity in public code contributions: an exploratory large-scale study over 50 years,"Rossi, Davide and Zacchiroli, Stefano","We conduct an exploratory, large-scale, longitudinal study of 50 years of commits to publicly available version control system repositories, in order to characterize the geographic diversity of contributors to public code and its evolution over time. We analyze in total 2.2 billion commits collected by Software Heritage from 160 million projects and authored by 43 million authors during the 1971--2021 time period. We geolocate developers to 12 world regions derived from the United Nation geoscheme, using as signals email top-level domains, author names compared with names distributions around the world, and UTC offsets mined from commit metadata.We find evidence of the early dominance of North America in open source software, later joined by Europe. After that period, the geographic diversity in public code has been constantly increasing. We also identify relevant historical shifts related to the UNIX wars, the increase of coding literacy in Central and South Asia, and broader phenomena like colonialism and people movement across countries (immigration/emigration).",2022,290
Not What it Used to Be: Characterizing Content and User-base Changes in Newly Created Online Communities,"Atcheson, Alex and Koshy, Vinay and Karahalios, Karrie","Attracting new members is vital to the health of many online communities. Yet, prior qualitative work suggests that newcomers to online communities can be disruptive – either due to a lack of awareness around existing community norms or to differing expectations around how the community should operate. Consequently, communities may have to navigate a trade-off between growth and development of community identity. We evaluate the presence of this trade-off through a longitudinal analysis of two years of commenting data for each of 1,620 Reddit communities. We find that, on average, communities become less linguistically distinctive as they grow. These changes appear to be driven almost equally by newcomers and returning users. Surprisingly, neither heavily moderated communities nor communities undergoing major user-base diversification are any more or less likely to maintaining distinctiveness. Taken together, our results complicate the assumption that growth is inherently beneficial for online communities.",2024,291
Partisan: Enabling Real-World Protocol Evaluation,"Meiklejohn, Christopher S.","We present the design and implementation of Partisan, an Erlang library for enabling real-world experiments of dis- tributed protocols and applications. Partisan is a batteries- included""library facilitating internode communication in Er- lang, runtime selection of cluster topology, pluggable layers that provide additional functionality such as causal delivery and reliable message delivery, and a mechanism for perform- ing deterministic fault injection. Partisan has been used in the evaluation of one research prototype, two real-world ap- plications, and has seen industry adoption in the Erlang and Elixir communities.",2018,292
"Detect Review Manipulation by Leveraging Reviewer Historical Stylometrics in Amazon, Yelp, Facebook and Google Reviews","Sadman, Nafiz and Gupta, Kishor Datta and Haque, Ariful and Poudyal, Subash and Sen, Sajib","Consumers now check reviews and recommendations before consuming any services or products. But traders try to shape reviews and ratings of their merchandise to gain more consumers. Seldom they attempt to manage their competitor's review and recommendation. These manipulations are hard to detect by standard lookup from an everyday consumer, but by thoroughly examining, customers can identify these manipulations. In this paper, we try to mimic how a specialist will look to detect review manipulation and came up with algorithms that are compatible with significant and well known online services. We provide a historical stylometry based methodology to detect review manipulations and supported that with results from Amazon, Yelp, Google, and Facebook.",2020,293
FLOSS FAQ chatbot project reuse: how to allow nonexperts to develop a chatbot,"de Lacerda, Arthur R. T. and Aguiar, Carla S. R.","FAQ chatbots possess the capability to provide answers to frequently asked questions of a particular service, platform, or system. Currently, FAQ chatbot is the most popular domain of use of dialog assistants. However, developing a chatbot project requires a full-stack team formed by numerous specialists, such as dialog designer, data scientist, software engineer, DevOps, business strategist and experts from the domain, which can be both time and resources consuming. Language processing can be particularly challenging in languages other than English due to the scarcity of training datasets.Most of the requirements of FAQ chatbots are similar, domain-specific, and projects could profit from Open Source Software (OSS) reuse. In this paper, we examine how OSS FAQ chatbot projects can benefit from reuse at the project level (black-box reuse). We present an experience report of a FLOSS FAQ chatbot project developed in Portuguese to an e-government service in Brazil. It comprises of the chatbot distribution service, as well as for analytics tool integrated and deployed on-premises. We identified assets that could be reused as a black-box and the assets that should be customized for a particular application. We categorized these assets in architecture, corpus, dialog flows, machine learning models, and documentation. This paper discusses how automation, pre-configuration, and templates can aid newcomers to develop chatbots in Portuguese without the need for specialized skills required from tools in chatbot architecture. Our main contribution is to highlight the issues non-English FAQ chatbots projects will likely face and the assets that can be reused. It allows non-chatbot experts to develop a quality-assured OSS FAQ chatbot in a shorter project cycle.",2019,294
It Takes a Village: Integrating an Adaptive Chatbot into an Online Gaming Community,"Seering, Joseph and Luria, Michal and Ye, Connie and Kaufman, Geoff and Hammer, Jessica","While the majority of research in chatbot design has focused on creating chatbots that engage with users one-on-one, less work has focused on the design of conversational agents for online communities. In this paper we present results from a three week test of a social chatbot in an established online community. During this study, the chatbot ""grew up"" from ""birth"" through its teenage years, engaging with community members and ""learning"" vocabulary from their conversations. We discuss the design of this chatbot, how users' interactions with it evolved over the course of the study, and how it impacted the community as a whole. We discuss how we addressed challenges in developing a chatbot whose vocabulary could be shaped by users, and conclude with implications for the role of machine learning in social interactions in online communities and potential future directions for design of community-based chatbots.",2020,295
From the Inside Out: Organizational Impact on Open-Source Communities and Women's Representation,"Frluckaj, Hana and Qiu, Huilian Sophie and Vasilescu, Bogdan and Dabbish, Laura","The involvement of companies and public institutions in open-source software (OSS) has become widespread. While studies have explored the business models of for-profit organizations and their impact on software quality, little is known about their influence on OSS communities, especially in terms of diversity and inclusion. This knowledge gap is significant, considering that many organizations have the resources to enhance diversity and inclusion internally, but whether these efforts extend to OSS remains uncertain. To address this gap, we conducted interviews with maintainers of community-owned and organization-owned OSS projects, revealing tensions between organizations and their projects and identifying the impact of internal policies on OSS communities. Our findings reveal that, on the one hand, organization-owned projects often restrict external contributions due to stringent operating procedures and segmented communication, leading to limited external engagement. On the other hand, these organizations positively influence diversity and inclusion, notably in the representation and roles of women and the implementation of mentorship programs.",2024,296
Trailblazing the Artificial Intelligence for Cybersecurity Discipline: A Multi-Disciplinary Research Roadmap,"Samtani, Sagar and Kantarcioglu, Murat and Chen, Hsinchun","Cybersecurity has rapidly emerged as a grand societal challenge of the 21st century. Innovative solutions to proactively tackle emerging cybersecurity challenges are essential to ensuring a safe and secure society. Artificial Intelligence (AI) has rapidly emerged as a viable approach for sifting through terabytes of heterogeneous cybersecurity data to execute fundamental cybersecurity tasks, such as asset prioritization, control allocation, vulnerability management, and threat detection, with unprecedented efficiency and effectiveness. Despite its initial promise, AI and cybersecurity have been traditionally siloed disciplines that relied on disparate knowledge and methodologies. Consequently, the AI for Cybersecurity discipline is in its nascency. In this article, we aim to provide an important step to progress the AI for Cybersecurity discipline. We first provide an overview of prevailing cybersecurity data, summarize extant AI for Cybersecurity application areas, and identify key limitations in the prevailing landscape. Based on these key issues, we offer a multi-disciplinary AI for Cybersecurity roadmap that centers on major themes such as cybersecurity applications and data, advanced AI methodologies for cybersecurity, and AI-enabled decision making. To help scholars and practitioners make significant headway in tackling these grand AI for Cybersecurity issues, we summarize promising funding mechanisms from the National Science Foundation (NSF) that can support long-term, systematic research programs. We conclude this article with an introduction of the articles included in this special issue.",2020,297
The Distribution and Disengagement of Women Contributors in Open-Source: 2008--2021,"Zhao, Zihe H","The underrepresentation of women contributors in the open-source software (OSS) community has been a widely recognized problem. Past research has found that, in OSS collaboration, a gender-diverse team can enhance productivity and lower community smell [1]--[3]. However, these benefits will be hindered when a team lacks gender diversity. To better address this gender imbalance, we need to first understand the overall gender representation.",2023,298
Investigating code review practices in defective files: an empirical study of the Qt system,"Thongtanunam, Patanamon and McIntosh, Shane and Hassan, Ahmed E. and Iida, Hajimu","Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. To evaluate the impact that characteristics of MCR practices have on software quality, this paper comparatively studies MCR practices in defective and clean source code files. We investigate defective files along two perspectives: 1) files that will eventually have defects (i.e., future-defective files) and 2) files that have historically been defective (i.e., risky files). Through an empirical study of 11,736 reviews of changes to 24,486 files from the Qt open source project, we find that both future-defective files and risky files tend to be reviewed less rigorously than their clean counterparts. We also find that the concerns addressed during the code reviews of both defective and clean files tend to enhance evolvability, i.e., ease future maintenance (like documentation), rather than focus on functional issues (like incorrect program logic). Our findings suggest that although functionality concerns are rarely addressed during code review, the rigor of the reviewing process that is applied to a source code file throughout a development cycle shares a link with its defect proneness.",2015,299
The Big Wave: An Accessible Parallel Gameplay Information Gathering Puzzle Game made for the Global Game Jam,"Shooster, Forrest Z. and Slattery, Chase and Meier, Luna and Wengert, Rebecca and Campito, Michael and Woodward, Alex and Laroussini, Marc","Presented here is the design of a game with minimal required interactions of that player and minimal programming needed to implement. This game was designed for the 2017 Global Game Jam and supports accessibility for the colorblind and deaf. The Big Wave is an information gathering puzzle game which utilizes simultaneously playing audio tracks to hide information being conveyed by various channels on a HAM radio. Various hints throughout the story, which may be directly connected to the story or may just be coincidentally similar, are presented to clue the player into what needs to be done to proceed. This paper describes the design of this game and our process methodology in producing it over the course of under 48 hours. Also covered are our plans if more time was available to have worked on the project.",2017,300
Towards Collaborative Continuous Benchmarking for HPC,"Pearce, Olga and Scott, Alec and Becker, Gregory and Haque, Riyaz and Hanford, Nathan and Brink, Stephanie and Jacobsen, Doug and Poxon, Heidi and Domke, Jens and Gamblin, Todd","Benchmarking is integral to procurement of HPC systems, communicating HPC center workloads to HPC vendors, and verifying performance of the delivered HPC systems. Currently, HPC benchmarking is manual and challenging at every step, posing a high barrier to entry, and hampering reproducibility of the benchmarks across different HPC systems. In this paper, we propose collaborative continuous benchmarking to enable functional reproducibility, automation, and community collaboration in HPC benchmarking. Recent progress in HPC automation allows us to consider previously unimaginable large-scale improvements to the HPC ecosystem. We define the minimal requirements for collaborative continuous benchmarking and develop a common language to streamline the interactions between HPC centers, vendors, and researchers. We demonstrate the initial implementation of collaborative continuous benchmarking, and introduce an open source continuous benchmarking repository, Benchpark, for community collaboration. We believe collaborative continuous benchmarking will help overcome the human bottleneck in HPC benchmarking, enabling better evaluation of our systems and enabling a more productive collaboration within the HPC community.",2023,301
Experiences with Summer Camp Communication via Discord,"Moster, Makayla and Kokinda, Ella and Boyer, D. Matthew and Rodeghero, Paige","Teamwork and communication skills are essential for those entering the workforce, especially for software development positions. For remote development positions, the ability to work with a team and communicate remotely through a communication tool are important skills that are generally not taught in standard university courses. In this experience report, we discuss our experience using Discord for communication and collaboration during our virtual summer camp focused on teaching teamwork and game design to 27 autistic high school students. Overall, we found using Discord beneficial in many ways that we did not anticipate, including quicker instructor coordination, improved socialization, and more. Additionally, we provide recommendations for those who may want to use Discord in a similar virtual environment.",2024,302
Listen Veronica! Can You Give Me a Hand With This Bug?,"S\'{a}enz, Juan Pablo and De Russis, Luigi","Developing software implies looking for documentation, following tutorials, making implementation decisions, encountering errors, and overcoming them. Behind each aspect is the developer’s reasoning that, if not collected, is lost after the implementation. Conversely, if captured and linked to the code, the developers’ reasoning and motivations for each step they accomplish can become a valuable asset, meaningful for them and other developers. Looking for a mechanism to capture such knowledge seamlessly, we present Veronica. It is a conversational agent integrated directly into Visual Studio Code that, based on the developers’ self-explanatory reasoning, records memos and links them with the code they are writing. Furthermore, Veronica can interact with the web browser to automatically gather the sources consulted by the developer and attach them to the code. We validated our approach by conducting a usability study with eight participants that positively assessed the tool’s usefulness and suggested improvements in the graphical interface.",2023,303
Investigating the Benefits of Applying Artificial Intelligence Techniques to Enhance Learning Experiences in Capstone Courses,"A. Gonzalez, Luis","This research seeks to improve the learning experiences in Software Engineering Programs using Virtual Assistants based on Artificial Intelligence (AI) models. Students of Software Engineering Capstone Courses face real world situations and challenges that grant them valuable experiences for their professional preparation. However, since this knowledge is acquired through real-life exposure projects, it is difficult to transmit it among different generations of students. In consequence, all the gained knowledge, experiences, and computer codes developed are lost and cannot be reused outside the project context when they finish their assignment at the end of the semester. To address this challenge, this thesis considers the development of AI based virtual assistants applied in higher education, in a form of a lesson learned system, a recommender system integrated with a chatbot, to help students , solve problems similar to those they face in the different stages of their software project development by recommending previous lessons learned. The innovative contribution lies in the implementation of the described techniques from the state-of-art artificial intelligence field in an educational platform with the goal to leverage the experience gained during years of the teaching a Capstone Course in Software Engineering to new student generations who might benefit from this universal knowledge gained previously, in order to assist software engineering students to enhance their learning experience.",2021,304
Representation of Developer Expertise in Open Source Software,"Dey, Tapajit and Karnauch, Andrey and Mockus, Audris","Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.",2021,305
Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework,"Lin, ChungYi and Tung, Shen-Lung and Su, Hung-Ting and Hsu, Winston H.","Predicting vehicle flow is crucial for traffic management but is often limited by the scope of sensors. In contrast, extensive mobile network coverage enables us to utilize counts of mobile users' network activities (cellular traffic) on roadways as a proxy for vehicle flow. However, cellular traffic counts, which encompass various user types, may not directly align with vehicle flow. To address this issue, we present a new task: utilizing cellular traffic to predict vehicle flow in camera-free areas. This is supported by our Tel2Veh dataset, which comprises extensive cellular traffic and sparse vehicle flows. To tackle this task, we propose a two-stage framework. It first independently extracts features from multimodal data, and then integrates them using a graph neural network (GNN)-based fusion to generate predictions of vehicle flow in camera-free areas. We pioneer the fusion of telecom and vision-based data, paving the way for future expansions in web-based applications and systems.",2024,306
Investigating Reflection in Undergraduate Software Development Teams: An Analysis of Online Chat Transcripts,"Hundhausen, Christopher and Conrad, Phill and Adesope, Olusola and Tariq, Ahsun and Sbai, Samir and Lu, Andrew","Metacognition is widely acknowledged as a key soft skill in collaborative software development. The ability to plan, monitor, and reflect on cognitive and team processes is crucial to the efficient and effective functioning of a software team. To explore students' use of reflection--one aspect of metacognition--in undergraduate team software projects, we analyzed the online chat channels of teams participating in agile software development projects in two undergraduate courses that took place exclusively online (n = 23 teams, 117 students, and 4,915 chat messages). Teams' online chats were dominated by discussions of work completed and to be done; just two percent of all chat messages showed evidence of reflection. A follow-up analysis of chat vignettes centered around reflection messages (n = 63) indicates that three-fourths of the those messages were prompted by a course requirement; just 14% arose organically within the context of teams' ongoing project work. Based on our findings, we identify opportunities for computing educators to increase, through pedagogical and technological interventions, teams' use of reflection in team software projects.",2023,307
The Role of Likes: How Online Feedback Impacts Users' Mental Health,"Voggenreiter, Angelina and Brandt, Sophie and Putterer, Fabian and Frings, Andreas and Pfeffer, Juergen","Social media usage has been shown to have both positive and negative consequences for users’ mental health. Several studies indicated that peer feedback plays an important role in the relationship between social media use and mental health. In this research, we analyse the impact of receiving online feedback on users’ emotional experience, social connectedness and self-esteem. In an experimental study, we let users interact with others on a Facebook-like system over the course of a week while controlling for the amount of positive reactions they receive from their peers. We find that experiencing little to no reaction from others does not only elicit negative emotions and stress amongst users, but also induces low levels of self-esteem. In contrast, receiving much positive online feedback, evokes feelings of social connectedness and reduces overall loneliness. On a societal level, our study can help to better understand the mechanisms through which social media use impacts mental health in a positive or negative way. On a methodological level, we provide a new open-source tool for designing and conducting social media experiments.",2024,308
IAI MovieBot: A Conversational Movie Recommender System,"Habib, Javeria and Zhang, Shuo and Balog, Krisztian","Conversational recommender systems support users in accomplishing recommendation-related goals via multi-turn conversations. To better model dynamically changing user preferences and provide the community with a reusable development framework, we introduce IAI MovieBot, a conversational recommender system for movies. It features a task-specific dialogue flow, a multi-modal chat interface, and an effective way to deal with dynamically changing user preferences. The system is made available open source and is operated as a channel on Telegram.",2020,309
Automated Identification of Toxic Code Reviews Using ToxiCR,"Sarker, Jaydeb and Turzo, Asif Kamal and Dong, Ming and Bosu, Amiangshu","Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at .",2023,310
"GitWaterFlow: a successful branching model and tooling, for achieving continuous delivery with multiple version branches","Rayana, Rayene Ben and Killian, Sylvain and Trangez, Nicolas and Calmettes, Arnaud","Collaborative software development presents organizations with a near-constant flow of day-to-day challenges, and there is no available off-the-shelf solution that covers all needs. This paper provides insight into the hurdles that Scality's Engineering team faced in developing and extending a sophisticated storage solution, while coping with ever-growing development teams, challenging - and regularly shifting - business requirements, and non-trivial new feature development. The authors present a novel combination of a Git-based Version Control and Branching model with a set of innovative tools dubbed GitWaterFlow to cope with the issues encountered, including the need to both support old product versions and to provide time-critical delivery of bug fixes. In the spirit of Continuous Delivery, Scality Release Engineering aims to ensure high quality and stability, to present short and predictable release cycles, and to minimize development disruption. The team's experience with the GitWaterFlow model suggests that the approach has been effective in meeting these goals in the given setting, with room for unceasing fine-tuning and improvement of processes and tools.",2016,311
Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT,"Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald","In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.",2023,312
ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,,"We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.",2023,313
Deep reinforcement learning and imitation learning based on VizDoom,"Xu, Yingyu","Reinforcement learning is a field of machine learning that focuses on intelligent agents, primarily the concept of what actions an intelligent agent takes in the environment to maximize cumulative reward. In environments where rewards are scarce, a manual approach is necessary. However, manually designing the reward function to meet the desired behavior can be very complicated. A very useful solution is Imitation Learning (IL). This paper proposes two reinforcement learning algorithms for the basic scene of the VizDoom video game, and uses IL to improve the performance of one of the models.",2023,314
Pika: Empowering Non-Programmers to Author Executable Governance Policies in Online Communities,"Wang, Leijie and Vincent, Nicholas and Rukanskaitundefined, Julija and Zhang, Amy Xian","Internet users have formed a wide array of online communities with diverse community goals and nuanced norms. However, most online platforms only offer a limited set of governance models in their software infrastructure and leave little room for customization. Consequently, technical proficiency becomes a prerequisite for online communities to build governance policies in code, excluding non-programmers from participation in designing community governance. In this paper, we present Pika, a system that empowers non-programmers to author a wide range of executable governance policies. At its core, Pika incorporates a declarative language that decomposes governance policies into modular components, thereby facilitating expressive policy authoring through a user-friendly, form-based web interface. Our user studies with 10 non-programmers and 7 programmers show that Pika can empower non-programmers to author policies approximately 2.5 times faster than programmers who author in code. We also provide insights about Pika’s expressivity in supporting diverse policies online communities want.",2024,315
Studying the impact of continuous delivery adoption on bug-fixing time in apache's open-source projects,"de Almeida, Carlos D. A. and Feij\'{o}, Diego N. and Rocha, Lincoln S.","Buggy software impacts people's lives and businesses. Nowadays, a huge portion of a software project's cost is spent on debugging (finding and fixing bugs). Therefore, reducing the time needed to release new software versions free from bugs becomes crucial. Continuous delivery (CD) arises as an alternative to traditional software release engineering by providing the capability to faster and continuously release software to customers through automated pipelines. Previous studies claim that CD adoption leads to a reduction in the software release cycle time, including the time lag to fix reported bugs (bug-fixing time) and apply correction patches in the affected versions. However, there is a lack of empirical evidence supporting (or not) this claim. To fulfill this gap, we conducted an empirical study to evaluate the impact of CD adoption in the bug-fixing time. We study 25 open-source projects comparing the bug-fixing time before and after adopting CD. Our results show that bug-fixing time after CD adoption becomes shorter (with statistical significance) than the bug-fixing time before CD adoption.",2022,316
On the Cryptographic Fragility of the Telegram Ecosystem,"von Arx, Theo and Paterson, Kenneth G.","Telegram is a popular messenger with more than 550 million active users per month and with a large ecosystem of different clients. The wide adoption of Telegram by protestors relying on private and secure messaging provides motivation for developing a profound understanding of its cryptographic design and how this influences its security properties. Telegram has its own bespoke transport layer security protocol, MTProto&nbsp;2.0. This protocol was recently subjected to a detailed study by Albrecht et al. (IEEE S&amp;P 2022). They gave attacks on the protocol and its implementations, along with a security proof for a modified version of the protocol. We complement that study by analysing a range of third-party client implementations of MTProto&nbsp;2.0. We report practical replay attacks for the Pyrogram, Telethon and GramJS clients, and a more theoretical timing attack against the MadelineProto client. We show how vulnerable third-party clients can affect the security of the entire ecosystem, including official clients. Our analysis reveals that many third-party clients fail to securely implement MTProto&nbsp;2.0. We discuss the reasons for these failures, focussing on complications in the design of MTProto&nbsp;2.0 that lead developers to omit security-critical features or to implement the protocol in an insecure manner. We also discuss changes that could be made to MTProto&nbsp;2.0 to remedy this situation. Overall, our work highlights the cryptographic fragility of the Telegram ecosystem.",2023,317
EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering,,,2023,318
The Necessity of Interdisciplinary Software Development for Building Viable Research Platforms: Case Study in Automated Drug Delivery in Diabetes,"Hajek, Jeremy and Rashid, Mudassir and Sevil, Mert and Cinar, Ali and Alvarez Fernandez, Pablo Angel and Jain, Dhiraj","Developing viable and robust software is an inescapable artifact of graduate research. The challenges lie in the complexities of developing, deploying, and securing software to support the research objectives. Combined with the transitive nature of students, the management of the software development and launch process is an arduous task. A standardized framework for developing and launching complex software is required. Within a university, individual departments do not typically possess the expertise, resources, software and infrastructure to translate research results to a viable product or tool. Extending upon the research of Hilton et al., [5] we designed a software development pipeline in an integrated multi-disciplinary research context. The integrated and collaborative software pipeline formulated from the onset of the project streamlines the development phase and provides an iterative feedback and testing environment. This approach is applied to the development of automated insulin delivery systems, with the synergistic efforts of interdisciplinary teams yielding a mobile application and server software solutions, and a framework for the iterative advancement of the software capabilities into the future.",2020,319
An Information System for Law Integrating Ontological Bases with a Legal Reasoner Chatbot,"Monteiro, Lucas Henrique de Assis and Rodrigues, Cleyton M\'{a}rio de Oliveira and Sousa, A\^{e}da Monalliza Cunha de","Context: The Semantic Web aims to assign meanings to resources available on the internet so that humans and computers can understand them. It can be used in the most diverse contexts, facilitating the development of systems where expert knowledge is formalized through logical-mathematical resources, mitigating potential inconsistencies, and promoting more human-friendly interaction services. Problem: The existence of semantic anomalies (use of rhetorical language, polysemy and inaccuracies) in the Brazilian Legal Domain enables the use of Semantic Web standards and technologies to mitigate these problems. Solution: This work deals with the development of an Information System that uses resources from the Semantic Web for the formal representation and the realization of legal inferences about Crimes Against Property. SI Theory: The Behavioral Decision Theory was approached, mainly in the incorporation of real patterns of decision making. Method: Bibliographic and documentary research methods were used to list the main concepts related to the Criminal Types investigated. The research is prescriptive and has a quali-quantitative approach. Summary of Results: A prototype system is presented, integrating ontologies of Brazilian Law with a chatbot that enables interaction with users in natural language, as well as performing reasoning tasks based on the knowledge formalized in these ontologies. Contributions and Impact in the IS area: The research will contribute to the automation of decision-making processes involving crimes against property, serving as an aid for professionals or law students and for legal simulations by ordinary people. Furthermore, it will serve as a reference for the development of other information systems with similar objectives in other contexts.",2022,320
API Rate Limit Adoption -- A pattern collection,"Serbout, Souhaila and El Malki, Amine and Pautasso, Cesare and Zdun, Uwe","The API Rate Limit pattern controls the rate at which clients make API requests by counting the number of requests in a specified time interval and reacting against abusive clients, in order to protect the limited resources of the API from exhaustion and denial of service attacks. This practice helps service providers to prevent abuse and ensure fair resource allocation, maintain system stability, monitor and control service availability, protect against DDoS attacks In this research paper, we have identified patterns covering the API Rate Limit pattern adoption starting from its documentation to its implementation. Our objective is to elucidate the trade-offs associated with different identified patterns and offer guidance to developers in making informed decisions when choosing the most suitable Rate Limit method, scope, and granularity for their service. By providing a comprehensive overview of how to adopt the Rate Limit pattern, this paper aims to enhance the understanding of how APIs can be designed to facilitate high scalability, security, reliability, and service availability. Furthermore, we present each pattern along with known uses observed in real-world APIs and technologies.",2024,321
Identifying and Measuring Manipulative User Interfaces at Scale on the Web,"Mathur, Arunesh","In this dissertation, I present measurement methods to automatically identify manipulative user interfaces—colloquially known as “dark patterns”—at scale on the web. Using these methods, I quantify the prevalence of dark patterns in three studies and show how dark patterns are rampant on the web, thus a pressing concern for society. First, I examine whether social media content creators, or “influencers,” disclose their relationships with advertisers to their audience. Analyzing over 500K YouTube videos and 2.1M Pinterest pins, I find that only about 10% of all advertising content is disclosed to users. Second, I examine various types of dark patterns in shopping websites. Analyzing data from 11K shopping websites, I discover over 1,800 dark patterns on over 1,200 websites that mislead users into making more purchases or disclosing more information than they would otherwise. Third, I examine dark patterns in political emails from the 2020 U.S. election cycle. Through an analysis of over 100K emails, I find that over 40% of emails from the median sender contain dark patterns that nudge recipients to open emails or make donations they might otherwise not make. I further outlay the conceptual foundation of dark patterns and articulate a set of normative perspectives for analyzing the effects of dark patterns. I conclude with how the lessons learned from the studies can be used to build technical defenses and to lay out policy recommendations to mitigate the spread of these interfaces.",2021,322
Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals,"Zhu, Yutao and Nie, Jian-Yun and Zhou, Kun and Du, Pan and Jiang, Hao and Dou, Zhicheng","A proactive dialogue system has the ability to proactively lead the conversation. Different from the general chatbots which only react to the user, proactive dialogue systems can be used to achieve some goals, e.g., to recommend some items to the user. Background knowledge is essential to enable smooth and natural transitions in dialogue. In this paper, we propose a new multi-task learning framework for retrieval-based knowledge-grounded proactive dialogue. To determine the relevant knowledge to be used, we frame knowledge prediction as a complementary task and use explicit signals to supervise its learning. The final response is selected according to the predicted knowledge, the goal to achieve, and the context. Experimental results show that explicit modeling of knowledge prediction and goal selection can greatly improve the final response selection. Our code is available at https://github.com/DaoD/KPN/.",2021,323
NeuroMesh: IoT Security Enabled by a Blockchain Powered Botnet Vaccine,"Falco, Gregory and Li, Caleb and Fedorov, Pavel and Caldera, Carlos and Arora, Rahul and Jackson, Kelly","Internet-of-Things (IoT) devices are ubiquitous and growing rapidly in number. However, IoT manufacturers have focused on the functionality and features of the devices and made security an afterthought. Since IoT devices have small memory capacities and low-power processors, many security firms have not been able to develop anti-malware software for these devices. Current IoT security solutions are heavy and unreliable. We have developed a lightweight IoT security solution that uses hacker tools against the hackers -- in essence, a vaccine for IoT. Our software provides managed security and intelligence to IoT devices using a ""friendly"" botnet operated through a proven, existing communication infrastructure for distributed systems -- the Bitcoin blockchain.",2019,324
OnlyTips: Blockchain-Driven Tips Service,"Kurbatov, Dmitry and Rybnikova, Marianna and Madhwal, Yash and Yanovich, Yury and Zotov, Gleb","Gratuity (tips) plays a valuable role in the service sector. Tips exist in many countries more and more in a digital format. But digital tips lack transparency: givers have no proof of delivery, and recipients have no proof of assessing all the transactions and the amount’s immutability. A tips service provider is a single point of failure and should be trusted to keep the system working. Blockchain allows running a system in a trustless environment and can be a clue to the reliable decentralized digital gratuity. In the paper, we analyze a technology and business potential of a blockchain-based tips service and propose a full-stack prototype.",2023,325
ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,,"On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.",2022,326
Abnormal user identification in online social networks based on user behavior,"Wang, Nan and Sun, Qingyu and Jiao, Qingju","Recently, in online social networks zombie fans have been shown a more complex and humanizing form. The existing methods based on basic features such as the number of followee, follower scale, user name and information content are not very efficient, which may lead to many misrecognitions and missed detection. Behavior pattern is the most fundamental feature of online users and abnormal users certainly have particular actions which are different from the normal users. Therefore, in this paper, a zombie fans identification method has been proposed based on the behavior characteristics like retweet, comment and the corresponding regularity. Furthermore, with user behaviors, invalid user identification is also researched. The experimental results showed that the abnormal user recognition method proposed in this paper had high identification accuracy.",2021,327
FastReID: A Pytorch Toolbox for General Instance Re-identification,"He, Lingxiao and Liao, Xingyu and Liu, Wu and Liu, Xinchen and Cheng, Peng and Mei, Tao","General Instance Re-identification is a very important task in computer vision, which can be widely used in many practical applications, such as person/vehicle re-identification, face recognition, wildlife protection, commodity tracing, snapshots, and so on. To meet the increasing application demand for general instance re-identification, we present FastReID as a widely used software system. In FastReID, the highly modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art projects, including person re-id, partial re-id, cross-domain re-id, and vehicle re-id. Moreover, we plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most general and high-performance toolbox that supports single and multiple GPU servers, it can reproduce our project results very easily. The source codes and models have been released at https://github.com/JDAI-CV/fast-reid.",2023,328
Enabling distributed revision control systems in delay-tolerant networks,"Hagemeister, Philipp and Mauve, Martin","A distributed revision control system (dRCS) such as git or mercurial allows users to track changes1 to a common document.2 When multiple users commit, a primary challenge of a dRCS becomes to provide one view of the current repository state.",2017,329
Server-Side Browsers: Exploring the Web's Hidden Attack Surface,"Musch, Marius and Kirchner, Robin and Boll, Max and Johns, Martin","As websites grow ever more dynamic and load more of their content on the fly, automatically interacting with them via simple tools like curl is getting less of an option. Instead, headless browsers with JavaScript support, such as PhantomJS and Puppeteer, have gained traction on the Web over the last few years. For various use cases like messengers and social networks that display link previews, these browsers visit arbitrary, user-controlled URLs. To avoid compromise through known vulnerabilities, these browsers need to be diligently kept up-to-date. In this paper, we investigate the phenomenon of what we coin server-side browsers at scale and find that many websites are running severely outdated browsers on the server-side. Remarkably, the majority of them had not been updated for more than 6 months and over 60% of the discovered implementations were found to be vulnerable to publicly available proof-of-concept exploits.",2022,330
On the Relevance of Cross-project Learning with Nearest Neighbours for Commit Message Generation,"Etemadi, Khashayar and Monperrus, Martin","Commit messages play an important role in software maintenance and evolution. Nonetheless, developers often do not produce high-quality messages. A number of commit message generation methods have been proposed in recent years to address this problem. Some of these methods are based on neural machine translation (NMT) techniques. Studies show that the nearest neighbor algorithm (NNGen) outperforms existing NMT-based methods, although NNGen is simpler and faster than NMT. In this paper, we show that NNGen does not take advantage of cross-project learning in the majority of the cases. We also show that there is an even simpler and faster variation of the existing NNGen method which outperforms it in terms of the BLEU_4 score without using cross-project learning.",2020,331
ZoAM GameBot: a Journey to the Lost Computational World in the Amazonia,"Pessoa, Larissa and Martins, Lia and Hsu, Meng and Freitas, Rosiane de","The search for alternative teaching-learning processes that attract more interest and involvement of young people, has inspired the development of a game with a chatbot architecture based on interactive storytelling and multiple learning paths. Thus, we introduce in this article the GameBot ZoAm, developed for the Discord instant messaging and social platform. ZoAm offers a unique learning experience centered around storytelling, focusing on fundamental computing concepts and logical challenges that enhance computational thinking skills. Furthermore, the game also promotes an appreciation for Amazonian culture and folklore, with decision-making with human values. An action research study was conducted involving students from the last years of the end of elementary school. The research utilized a heuristic analysis based on the Gameplay Heuristics (PLAY) by Desurvire and Wiberg (ANO), and the evaluation model proposed by Korhonen and Koivisto (ANO) for mobile devices. The analysis employed a reduced and merged set of heuristics from these models, suited for the gamebot’s context, focusing on I) Usability, II) Gameplay and Immersion, and III) Mobility. Regarding the reliability coefficient used to evaluate the survey applied to students after playing the gamebot, Cronbach’s Alpha and Guttman Lambda-6 (G6(smc)) coefficients were applied. These metrics were chosen to ensure the internal consistency and reliability of survey items, reflecting on how effectively the questions measured the focuses proposed by the heuristic analysis. The findings indicate that the game has the potential to facilitate the assimilation of the integrated concepts and sustain student interest throughout gameplay.",2024,332
Optimizing Pressure Matrices: Interdigitation and Interpolation Methods for Continuous Position Input,"Strohmeier, Paul and H\r{a}kansson, Victor and Honnet, Cedric and Ashbrook, Daniel and Hornb\ae{}k, Kasper","This paper provides resources and design recommendations for optimizing position input for pressure sensor matrices, a sensor design often used in eTextiles. Currently applications using pressure matrices for precise continuous position control are rare. One reason designers opt against using these sensors for continuous position control is that when the finger transitions from one sensing electrode to the next, jerky motion, jumps or other non-linear artifacts appear. We demonstrate that interdigitation can improve transition behavior and discuss interpolation algorithms to best leverage such designs. We provide software for reproducing our sensors and experiment, as well as a dataset consisting of 1122 swipe gestures performed on 17 sensors.",2019,333
Mutation Testing for Task-Oriented Chatbots,"G\'{o}mez-Abajo, Pablo and P\'{e}rez-Soler, Sara and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan","Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.",2024,334
Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce,"Fu, Zhenxin and Ji, Feng and Hu, Wenpeng and Zhou, Wei and Zhao, Dongyan and Chen, Haiqing and Yan, Rui","Information-seeking conversation system aims at satisfying the information needs of users through conversations. Text matching between a user query and a pre-collected question is an important part of the information-seeking conversation in E-commerce. In the practical scenario, a sort of questions always correspond to a same answer. Naturally, these questions can form a bag. Learning the matching between user query and bag directly may improve the conversation performance, denoted as query-bag matching. Inspired by such opinion, we propose a query-bag matching model which mainly utilizes the mutual coverage between query and bag and measures the degree of the content in the query mentioned by the bag, and vice verse. In addition, the learned bag representation in word level helps find the main points of a bag in a fine grade and promotes the query-bag matching performance. Experiments on two datasets show the effectiveness of our model.",2019,335
SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering,,"Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.",2023,336
Climate Coach: A Dashboard for Open-Source Maintainers to Overview Community Dynamics,"Qiu, Huilian Sophie and Lieb, Anna and Chou, Jennifer and Carneal, Megan and Mok, Jasmine and Amspoker, Emily and Vasilescu, Bogdan and Dabbish, Laura","Open-source software projects have become an integral part of our daily life, supporting virtually every software we use today. Since open-source software forms the digital infrastructure, maintaining them is of utmost importance. We present Climate Coach, a dashboard that helps open-source project maintainers monitor the health of their community in terms of team climate and inclusion. Through a literature review and an exploratory survey (N=18), we identified important signals that can reflect a project’s health, and display them on a dashboard. We evaluated and refined our dashboard through two rounds of think-aloud studies (N=19). We then conducted a two-week longitudinal diary study (N=10) to test the usefulness of our dashboard. We found that displaying signals that are related to a project’s inclusion help improve maintainers’ management strategies.",2023,337
Colaroid: A Literate Programming Approach for Authoring Explorable Multi-Stage Tutorials,"Wang, April Yi and Head, Andrew and Zhang, Ashley Ge and Oney, Steve and Brooks, Christopher","Multi-stage programming tutorials are key learning resources for programmers, using progressive incremental steps to teach them how to build larger software systems. A good multi-stage tutorial describes the code clearly, explains the rationale and code changes for each step, and allows readers to experiment as they work through the tutorial. In practice, it is time-consuming for authors to create tutorials with these attributes. In this paper, we introduce Colaroid, an interactive authoring tool for creating high quality multi-stage tutorials. Colaroid tutorials are augmented computational notebooks, where snippets and outputs represent a snapshot of a project, with source code differences highlighted, complete source code context for each snippet, and the ability to load and tinker with any stage of the project in a linked IDE. In two laboratory studies, we found Colaroid makes it easy to create multi-stage tutorials, while offering advantages to readers compared to video and web-based tutorials.",2023,338
Evaluating Link-based Recommendations for Wikipedia,"Schwarzer, Malte and Schubotz, Moritz and Meuschke, Norman and Breitinger, Corinna and Markl, Volker and Gipp, Bela","Literature recommender systems support users in filtering the vast and increasing number of documents in digital libraries and on the Web. For academic literature, research has proven the ability of citation-based document similarity measures, such as Co-Citation (CoCit), or Co-Citation Proximity Analysis (CPA) to improve recommendation quality. In this paper, we report on the first large-scale investigation of the performance of the CPA approach in generating literature recommendations for Wikipedia, which is fundamentally different from the academic literature domain. We analyze links instead of citations to generate article recommendations. We evaluate CPA, CoCit, and the Apache Lucene MoreLikeThis (MLT) function, which represents a traditional text-based similarity measure. We use two datasets of 779,716 and 2.57 million Wikipedia articles, the Big Data processing framework Apache Flink, and a ten-node computing cluster. To enable our large-scale evaluation, we derive two quasi-gold standards from the links in Wikipedia's ""See also"" sections and a comprehensive Wikipedia clickstream dataset.Our results show that the citation-based measures CPA and CoCit have complementary strengths compared to the text-based MLT measure. While MLT performs well in identifying narrowly similar articles that share similar words and structure, the citation- based measures are better able to identify topically related information, such as information on the city of a certain university or other technical universities in the region. The CPA approach, which consistently outperformed CoCit, is better suited for identifying a broader spectrum of related articles, as well as popular articles that typically exhibit a higher quality. Additional benefits of the CPA approach are its lower runtime requirements and its language-independence that allows for a cross-language retrieval of articles. We present a manual analysis of exemplary articles to demonstrate and discuss our findings. The raw data and source code of our study, together with a manual on how to use them, are openly available at: https://github.com/wikimedia/citolytics",2016,339
An IoT Botnet Prediction Model Using Frequency based Dependency Graph: Proof-of-concept,"Yassin, Warusia and Abdullah, Raihana and Abdollah, Mohd Faizal and Mas'ud, Zaki and Bakhari, Farah Adeliena","Malware attacks are widespread in an era of growing technology by targeting most computing resources. Plenty of the technology nowadays is based on digital data exchange and it leads to the Internet of Things (IoT) development. A massive growth of IoT technology attracts attackers' interest in exploiting a number of IoT devices using a variety of attacks. Consequently, this has caused difficulty to the researcher in distinguishing a characteristic of such variant specifically for IoT botnet-based attack. Current approaches are weak in recognizing such behavior by analyzing registry information more accurately due to the fact that the attack pattern usually hard to construct. Hence, in this paper, selected features of suspicious registry information that's been affected by IoT botnet action i.e. Mirai is further analyzed using the graph-theoretical approach. Using a dependency graph, the similar and dissimilar pattern of distinct botnet composed to facilitate the process of malware variant characteristic identification. As a result of doing this, a precise attack pattern can be constructed and could be considered for future botnet prediction. A series of experiments conducted as a proof-of-concept in order to assess and validate the formed attack pattern. The findings have shown that the proposed prediction model could overcome the issues of undetectable IoT botnet behavior. From this forward, this model could be used to obtain accurate detection results for any variant of malware.",2020,340
ThingNet: a lightweight real-time mirai IoT variants hunter through CPU power fingerprinting,"Li, Zhuoran and Zhao, Dan","Internet of Things (IoT) devices have become attractive targets of cyber criminals, whereas attackers have been leveraging these vulnerable devices most notably via the infamous Mirai-based botnets, accounting for nearly 90% of IoT malware attacks in 2020. In this work, we propose a robust, universal and non-invasive Mirai-based malware detection engine employing a compact deep neural network architecture. Our design allows programmatic collection of CPU power footprints with integrated current sensors under various device states, such as idle, service and attack. A lightweight online inference model is deployed in the CPU for on-the-fly classification. Our model is robust against noisy environment with a lucid design of noise reduction function. This work appears to be the first step towards a viable CPU malware detection engine based on power fingerprinting. The extensive simulation study under ARM architecture that is widely used in IoT devices, demonstrates a high detection accuracy of 99.1% at a speed less than 1ms. By analyzing Mirai-based infection under distinguishable phases for power feature extraction, our model has further demonstrated an accuracy of 96.3% on model-unknown variants detection.",2022,341
A reactive language for analyzing cloud logs,"Baudart, Guillaume and Mandel, Louis and Tardieu, Olivier and Vaziri, Mandana","Log analysis is required in many domains, and especially in the emerging field of cloud computing. Cloud applications are often built by composing diverse services. When something goes wrong, finding the root cause of the problem can be difficult. Many services are only reachable through their Application Programming Interfaces (APIs) with no possibility for live introspection. In this context, logs become an essential tool for monitoring and debugging. Cloud services typically generate very large quantities of log messages, with formats that may not be well specified and may vary over time. In this paper, we present CloudLens, a language for the analysis of semi-structured textual data as found in logs, and specify its formal semantics. CloudLens is a reactive language and views logs as streams of objects. Our objective is to facilitate exploring the contents of logs interactively and to write reusable analyses succinctly, using familiar constructs. We implemented an interpreter for the Apache Zeppelin notebook to provide an interactive IDE. Our prototype implementation is open source and we report on a detailed case study using logs from the Apache OpenWhisk project.",2018,342
Safe and sound program analysis with Flix,"Madsen, Magnus and Lhot\'{a}k, Ond\v{r}ej","Program development tools such as bug finders, build automation tools, compilers, debuggers, integrated development environments, and refactoring tools increasingly rely on static analysis techniques to reason about program behavior. Implementing such static analysis tools is a complex and difficult task with concerns about safety and soundness. Safety guarantees that the fixed point computation -- inherent in most static analyses -- converges and ultimately terminates with a deterministic result. Soundness guarantees that the computed result over-approximates the concrete behavior of the program under analysis. But how do we know if we can trust the result of the static analysis itself? Who will guard the guards?  In this paper, we propose the use of automatic program verification techniques based on symbolic execution and SMT solvers to verify the correctness of the abstract domains used in static analysis tools. We implement a verification toolchain for Flix, a functional and logic programming language tailored for the implementation of static analyses. We apply this toolchain to several abstract domains. The experimental results show that we are able to prove 99.5% and 96.3% of the required safety and soundness properties, respectively.",2018,343
Watchman: monitoring dependency conflicts for Python library ecosystem,"Wang, Ying and Wen, Ming and Liu, Yepang and Wang, Yibo and Li, Zhenming and Wang, Chao and Yu, Hai and Cheung, Shing-Chi and Xu, Chang and Zhu, Zhiliang","The PyPI ecosystem has indexed millions of Python libraries to allow developers to automatically download and install dependencies of their projects based on the specified version constraints. Despite the convenience brought by automation, version constraints in Python projects can easily conflict, resulting in build failures. We refer to such conflicts as &lt;u&gt;D&lt;/u&gt;ependency &lt;u&gt;C&lt;/u&gt;onfict (DC) issues. Although DC issues are common in Python projects, developers lack tool support to gain a comprehensive knowledge for diagnosing the root causes of these issues. In this paper, we conducted an empirical study on 235 real-world DC issues. We studied the manifestation patterns and fixing strategies of these issues and found several key factors that can lead to DC issues and their regressions. Based on our findings, we designed and implemented Watchman, a technique to continuously monitor dependency conflicts for the PyPI ecosystem. In our evaluation, Watchman analyzed PyPI snapshots between 11 Jul 2019 and 16 Aug 2019, and found 117 potential DC issues. We reported these issues to the developers of the corresponding projects. So far, 63 issues have been confirmed, 38 of which have been quickly fixed by applying our suggested patches.",2020,344
SENSS Against Volumetric DDoS Attacks,"Ramanathan, Sivaramakrishnan and Mirkovic, Jelena and Yu, Minlan and Zhang, Ying","Volumetric distributed denial-of-service (DDoS) attacks can bring any network to a halt. Because of their distributed nature and high volume, the victim often cannot handle these attacks alone and needs help from upstream ISPs. Today's Internet has no automated mechanism for victims to ask ISPs for help in attack handling and ISPs themselves do not offer such services. We propose SENSS, a security service for collaborative mitigation of volumetric DDoS attacks. SENSS enables the victim of an attack to request attack monitoring and filtering on demand, and to pay for the services rendered. Requests can be sent both to the immediate and to remote ISPs, in an automated and secure manner, and can be authenticated by these ISPs, without having prior trust with the victim. Simple and generic SENSS APIs enable victims to build custom detection and mitigation approaches against a variety of DDoS attacks. SENSS is deployable with today's infrastructure, and it has strong economic incentives both for ISPs and for the attack victims. It is also very effective in sparse deployment, offering full protection to direct customers of early adopters, and considerable protection to remote victims when deployed strategically. Deployment on the largest 1% of ISPs protects not just direct customers of these ISPs, but everyone on the Internet, from 90% of volumetric DDoS attacks.",2018,345
The Landscape of Teaching Resources for AI Education,"Druga, Stefania and Otero, Nancy and Ko, Amy J.","Artificial Intelligence (AI) educational resources such as training tools, interactive demos, and dedicated curriculum are increasingly popular among educators and learners. While prior work has examined pedagogies for promoting AI literacy, it has yet to examine how well technology resources support these pedagogies. To address this gap, we conducted a systematic analysis of existing online resources for AI education, investigating what learning and teaching affordances these resources have to support AI education. We used the Technological Pedagogical Content Knowledge (TPACK) framework to analyze a final corpus of 50 AI resources. We found that most resources support active learning, have digital or physical dependencies, do not include all the five big ideas defined by AI4K12 guidelines, and do not offer built-in support for assessment or feedback. Teaching guides are hard to find or require technical knowledge. Based on our findings, we propose that future AI curricula move from singular activities and demos to more holistic designs that include support, guidance, and flexibility for how AI technology, concepts, and pedagogy play out in the classroom.",2022,346
SoK: Analysis of Software Supply Chain Security by Establishing Secure Design Properties,"Okafor, Chinenye and Schorlemmer, Taylor R. and Torres-Arias, Santiago and Davis, James C.","This paper systematizes knowledge about secure software supply chain patterns. It identifies four stages of a software supply chain attack and proposes three security properties crucial for a secured supply chain: transparency, validity, and separation. The paper describes current security approaches and maps them to the proposed security properties, including research ideas and case studies of supply chains in practice. It discusses the strengths and weaknesses of current approaches relative to known attacks and details the various security frameworks put out to ensure the security of the software supply chain. Finally, the paper highlights potential gaps in actor and operation-centered supply chain security techniques.",2022,347
A Machine Learning Approach for Vulnerability Curation,"Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David","Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.",2020,348
"CFar: A Tool to Increase Communication, Productivity, and Review Quality in Collaborative Code Reviews","Henley, Austin Z. and Mu\c{c}lu, K\i{}van\c{c} and Christakis, Maria and Fleming, Scott D. and Bird, Christian","Collaborative code review has become an integral part of the collaborative design process in the domain of software development. However, there are well-documented challenges and limitations to collaborative code review---for instance, high-quality code reviews may require significant time and effort for the programmers, whereas faster, lower-quality reviews may miss code defects. To address these challenges, we introduce CFar, a novel tool design for extending collaborative code review systems with an automated code reviewer whose feedback is based on program-analysis technologies. To validate this design, we implemented CFar as a production-quality tool and conducted a mixed-method empirical evaluation of the tool usage at Microsoft. Through the field deployment of our tool and a laboratory study of professional programmers using the tool, we produced several key findings showing that CFar enhances communication, productivity, and review quality in human--human collaborative code review.",2018,349
Identification and Mitigation of Toxic Communications Among Open Source Software Developers,"Sarker, Jaydeb","Toxic and unhealthy conversations during the developer’s communication may reduce the professional harmony and productivity of Free and Open Source Software (FOSS) projects. For example, toxic code review comments may raise pushback from an author to complete suggested changes. A toxic communication with another person may hamper future communication and collaboration. Research also suggests that toxicity disproportionately impacts newcomers, women, and other participants from marginalized groups. Therefore, toxicity is a barrier to promote diversity, equity, and inclusion. Since the occurrence of toxic communications is not uncommon among FOSS communities and such communications may have serious repercussions, the primary objective of my proposed dissertation is to automatically identify and mitigate toxicity during developers’ textual interactions. On this goal, I aim to: i) build an automated toxicity detector for Software Engineering (SE) domain, ii) identify the notion of toxicity across demographics, and iii) analyze the impacts of toxicity on the outcomes of Open Source Software (OSS) projects.",2023,350
"Don't Leak Your Keys: Understanding, Measuring, and Exploiting the AppSecret Leaks in Mini-Programs","Zhang, Yue and Yang, Yuqing and Lin, Zhiqiang","Mobile mini-programs in WeChat have gained significant popularity since their debut in 2017, reaching a scale similar to that of Android apps in the Play Store. Like Google, Tencent, the provider of WeChat, offers APIs to support the development of mini-programs and also maintains a mini-program market within the WeChat app. However, mini-program APIs often manage sensitive user data within the social network platform, both on the WeChat client app and in the cloud. As a result, cryptographic protocols have been implemented to secure data access. In this paper, we demonstrate that WeChat should have required the use of the ""appsecret"" master key, which is used to authenticate a mini-program, to be used only in the mini-program back-end. If this key is leaked in the front-end of the mini-programs, it can lead to catastrophic attacks on both mini-program developers and users. Using a mini-program crawler and a master key leakage inspector, we measured 3,450,586 crawled mini-programs and found that 40,880 of them had leaked their master keys, allowing attackers to carry out various attacks such as account hijacking, promotion abuse, and service theft. Similar issues were confirmed through testing and measuring of Baidu mini-programs too. We have reported these vulnerabilities and the list of vulnerable mini-programs to Tencent and Baidu, which awarded us with bug bounties, and also Tencent recently released a new API to defend against these attacks based on our findings.",2023,351
Upstanding by Design: Bystander Intervention in Cyberbullying,"DiFranzo, Dominic and Taylor, Samuel Hardman and Kazerooni, Franccesca and Wherry, Olivia D. and Bazarova, Natalya N.","Although bystander intervention can mitigate the negative effects of cyberbullying, few bystanders ever attempt to intervene. In this study, we explored the effects of interface design on bystander intervention using a simulated custom-made social media platform. Participants took part in a three-day, in-situ experiment, in which they were exposed to several cyberbullying incidents. Depending on the experimental condition, they received different information about the audience size and viewing notifications intended to increase a sense of personal responsibility in bystanders. Results indicated that bystanders were more likely to intervene indirectly than directly, and information about the audience size and viewership increased the likelihood of flagging cyberbullying posts through serial mediation of public surveillance, accountability, and personal responsibility. The study has implications for understanding bystander effect in cyberbullying, and how to develop design solutions to encourage bystander intervention in social media.",2018,352
Crossmod: A Cross-Community Learning-based System to Assist Reddit Moderators,"Chandrasekharan, Eshwar and Gandhi, Chaitrali and Mustelier, Matthew Wortley and Gilbert, Eric","In this paper, we introduce a novel sociotechnical moderation system for Reddit called Crossmod. Through formative interviews with 11 active moderators from 10 different subreddits, we learned about the limitations of currently available automated tools, and how a new system could extend their capabilities. Developed out of these interviews, Crossmod makes its decisions based on cross-community learning---an approach that leverages a large corpus of previous moderator decisions via an ensemble of classifiers. Finally, we deployed Crossmod in a controlled environment, simulating real-time conversations from two large subreddits with over 10M subscribers each. To evaluate Crossmod's moderation recommendations, 4 moderators reviewed comments scored by Crossmod that had been drawn randomly from existing threads. Crossmod achieved an overall accuracy of 86% when detecting comments that would be removed by moderators, with high recall (over 87.5%). Additionally, moderators reported that they would have removed 95.3% of the comments flagged by Crossmod; however, 98.3% of these comments were still online at the time of this writing (i.e., not removed by the current moderation system). To the best of our knowledge, Crossmod is the first open source, AI-backed sociotechnical moderation system to be designed using participatory methods.",2019,353
A Formal Explainer for Just-In-Time Defect Predictions,"Yu, Jinqiang and Fu, Michael and Ignatiev, Alexey and Tantithamthavorn, Chakkrit and Stuckey, Peter","Just-In-Time (JIT) defect prediction has been proposed to help teams to prioritize the limited resources on the most risky commits (or pull requests), yet it remains largely a black-box, whose predictions are not explainable nor actionable to practitioners. Thus, prior studies have applied various model-agnostic techniques to explain the predictions of JIT models. Yet, explanations generated from existing model-agnostic techniques are still not formally sound, robust, and actionable. In this paper, we propose FoX, a Formal eXplainer for JIT Defect Prediction, which builds on formal reasoning about the behaviour of JIT defect prediction models and hence is able to provide provably correct explanations, which are additionally guaranteed to be minimal. Our experimental results show that FoX&nbsp;is able to efficiently generate provably-correct, robust, and actionable explanations while existing model-agnostic techniques cannot. Our survey study with 54 software practitioners provides valuable insights into the usefulness and trustworthiness of our FoX&nbsp;approach. 86% of participants agreed that our approach is useful, while 74% of participants found it trustworthy. Thus, this paper serves as an important stepping stone towards trustable explanations for JIT models to help domain experts and practitioners better understand why a commit is predicted as defective and what to do to mitigate the risk.",2024,354
Tripwire: inferring internet site compromise,"DeBlasio, Joe and Savage, Stefan and Voelker, Geoffrey M. and Snoeren, Alex C.","Password reuse has been long understood as a problem: credentials stolen from one site may be leveraged to gain access to another site for which they share a password. Indeed, it is broadly understood that attackers exploit this fact and routinely leverage credentials extracted from a site they have breached to access high-value accounts at other sites (e.g., email accounts). However, as a consequence of such acts, this same phenomena of password reuse attacks can be harnessed to indirectly infer site compromises---even those that would otherwise be unknown. In this paper we describe such a measurement technique, in which unique honey accounts are registered with individual third-party websites, and thus access to an email account provides indirect evidence of credentials theft at the corresponding website. We describe a prototype system, called Tripwire, that implements this technique using an automated Web account registration system combined with email account access data from a major email provider. In a pilot study monitoring more than 2,300 sites over a year, we have detected 19 site compromises, including what appears to be a plaintext password compromise at an Alexa top-500 site with more than 45 million active users.",2017,355
"Technology We Can't Live Without!, revisited","Galanos, Ria and Ball, Michael and Dougherty, John and Hummel, Joe and Malan, David J.","The pace of technology for use in computing education is staggering. In recent years, the following technologies have completely transformed our teaching: Piazza, GradeScope, YouTube, Google Docs, Doodle and whenisgood.net, Skype and Google Hangout, and Khan Academy among others. Hardware has also played a part. We love our Zoom digital voice recorder (for recording CD-quality lecture audio), Blue Yeti USB mike (for audio/videoconferences), and iClickers (for engaging students in class). This panel is an outgrowth of a Technology that Educators of Computing Hail (TECH) Birds of a Feather session that we've held at SIGCSE for seven years, and the panel from SIGCSE 2015 [1] that served as a springboard for a regular column in ACM Inroads [2]. It will provide a chance for seasoned high school and university educators to show you the technologies that have ""bubbled to the top"" for them, and what key problems they solve. Like concert musicians, they will give live demonstrations and reveal the configuration options required to make their technology ""sing"". We hope this forum will allow the presenters to dive deeply into the common use cases of these technologies, highlight why they are invaluable, share any ""gotchas"" they've uncovered, and explain how others can adopt them at their institutions. The highlight of the panel is when the audience, inspired by the presentations, is invited to share their favorite ""can't live without"" technologies as well.",2018,356
CTCam: Enhancing Transportation Evaluation through Fusion of Cellular Traffic and Camera-Based Vehicle Flows,"Lin, ChungYi and Tung, Shen-Lung and Su, Hung-Ting and Hsu, Winston H.","Traffic prediction utility often faces infrastructural limitations, which restrict its coverage. To overcome this challenge, we present Geographical Cellular Traffic (GCT) flow that leverages cellular network data as a new source for transportation evaluation. The broad coverage of cellular networks allows GCT flow to capture various mobile user activities across regions, aiding city authorities in resource management through precise predictions. Acknowledging the complexity arising from the diversity of mobile users in GCT flow, we supplement it with camera-based vehicle flow data from limited deployments and verify their spatio-temporal attributes and correlations through extensive data analysis. Our two-stage fusion approach integrates these multi-source data, addressing their coverage and magnitude discrepancies, thereby enhancing the prediction of GCT flow for accurate transportation evaluation. Overall, we propose novel uses of telecom data in transportation and verify its effectiveness in multi-source fusion with vision-based data.",2023,357
A characterization study of testing contributors and their contributions in open source projects.,"Souza, Hugo Henrique Fumero de and Wiese, Igor and Steinmacher, Igor and R\'{e}, Reginaldo","Even though open source projects have some different characteristics from projects in the industry, the commitment of maintainers and contributors to achieve a high level of software quality is constant. Therefore, tests are among the main practices of the communities. Thus, motivating contributors to write new tests and maintain regression tests during testing activities is essential for the project’s health. The objective of our work is to characterize testers and their contributions to open source projects as part of a broad study about testers’ motivation. Thus, we conducted a study with 3,936 repositories and 7 different and important programming languages (C, C++, C#, Java, Javascript, Python, and Ruby), analyzing a total of 4,409,142 contributions to classify contributing members and their contributions. Our results show that test-only contributors exist, regardless of programming language or project. We conclude that, despite the unfavorable scenario, there are contributors who feel motivated and dedicate their time and effort to contribute to new tests or to the evolution of existing tests.",2022,358
The Double Edged Sword: Identifying Authentication Pages and their Fingerprinting Behavior,"Senol, Asuman and Ukani, Alisha and Cutler, Dylan and Bilogrevic, Igor","Browser fingerprinting is often associated with cross-site user tracking, a practice that many browsers (e.g., Safari, Brave, Edge, Firefox, and Chrome) want to block. However, less is publicly known about its uses to enhance online safety, where it can provide an additional security layer against service abuses (e.g., in combination with CAPTCHAs) or during user authentication. To the best of our knowledge, no fingerprinting defenses deployed thus far consider this important distinction when blocking fingerprinting attempts, so they might negatively affect website functionality and security.  To address this issue we make three main contributions. First, we introduce a novel machine learning-based method to automatically identify authentication pages (i.e. login and sign-up pages). Our supervised algorithm achieves 96-98% precision and recall on a manually-labelled dataset of almost 1,000 popular sites. Second, we compare our algorithm with methods from prior works on the same dataset, showing that it significantly outperforms all of them. Third, we quantify the prevalence of fingerprinting scripts across login and sign-up pages (10.2%) versus those executed on other pages (9.2%); while the rates of fingerprinting are similar, home pages and authentication pages differ in the third-party scripts they include and how often these scripts are labeled as tracking. We also highlight the substantial differences in fingerprinting on login and sign-up pages. Our work sheds light on the complicated reality that fingerprinting is used to both protect user security and invade user privacy; this dual nature must be considered by fingerprinting mitigations.",2024,359
Unveiling the Impact of User-Agent Reduction and Client Hints: A Measurement Study,"Senol, Asuman and Acar, Gunes","The user-agent string contains the details of a user's device, browser and platform. Prior work on browser fingerprinting showed that the user-agent string can facilitate covert fingerprinting and tracking of users. In order to address these privacy concerns, browsers including Chrome recently reduced the user-agent string to make it less identifying. Simultaneously, Chrome introduced several highly identifying (or high-entropy) user-agent client hints (UA-CH) to allow access to browser properties that are redacted from the user-agent string. In this empirical study, we attempt to characterize the effects of these major changes through a large-scale web measurement on the top 100K websites. Using an instrumented crawler, we quantify access to high-entropy browser features through UA-CH HTTP headers and the JavaScript API. We measure access delegation to third parties and investigate whether the new client hints are already used by tracking, advertising and browser fingerprinting scripts. Our results show that high-entropy UA-CHs are accessed by one or more scripts on 59.2% of the successfully visited sites and 93.8% of these calls were made by tracking and advertising-related scripts-primarily by those owned by Google. Overall, we find that scripts from -9K distinct registrable (eTLD+1) third-party domains take advantage of their unfettered access and retrieve the high-entropy UA-CHs. We find that on 91.6% of the sites where high-entropy client hints are accessed via the JavaScript API, the high-entropy hints are exfiltrated by a tracker script to a remote server. Turning to high-entropy UA-CHs sent in the HTTP headers-which require opt-in or delegation-we found very limited use. Only 1.3% of the websites use the Accept-CH header to receive high-entropy UA-CHs; and an even smaller fraction of websites (0.4%) delegate high-entropy hints to third-party domains. Overall, our findings indicate that user-agent reduction efforts were effective in minimizing the passive collection of identifying browser features, but third-party tracking and advertising scripts continue to enjoy their unfettered access.",2023,360
Scaling static analyses at Facebook,"Distefano, Dino and F\""{a}hndrich, Manuel and Logozzo, Francesco and O'Hearn, Peter W.",Key lessons for designing static analyses tools deployed to find bugs in hundreds of millions of lines of code.,2019,361
"PyPoll: A python library automating mining of networks, discussions and polarization on Twitter","Giakatos, Dimitrios Panteleimon and Sermpezis, Pavlos and Vakali, Athena","Today online social networks have a high impact in our society as more and more people use them for communicating with each other, express their opinions, participating in public discussions, etc. In particular, Twitter is one of the most popular social network platforms people mainly use for political discussions. This attracted the interest of many research studies that analyzed social phenomena on Twitter, by collecting data, analysing communication patterns, and exploring the structure of user networks. While previous works share many common methodologies for data collection and analysis, these are mainly re-implemented every time by researchers in a custom way. In this paper, we introduce PyPoll an open-source Python library that operationalizes common analysis tasks for Twitter discussions. With PyPoll users can perform Twitter graph mining, calculate the polarization index and generate interactive visualizations without needing third-party tools. We believe that PyPoll can help researchers automate their tasks by giving them methods that are easy to use. Also, we demonstrate the use of the library by presenting two use cases; the PyPoll visualization app, an online application for graph visualizing and sharing, and the Political Lighthouse, a Web portal for displaying the polarization in various political topics on Twitter.",2023,362
Low-code from frontend to backend: Connecting conversational user interfaces to backend services via a low-code IoT platform,"Weber, Irene","Current chatbot development platforms and frameworks facilitate setting up the language and dialog part of chatbots, while connecting it to backend services and business functions requires substantial manual coding effort and programming skills. This paper proposes an approach to overcome this situation. It proposes an architecture with a chatbot as frontend using an IoT (Internet of Things) platform as a middleware for connections to backend services. Specifically, it elaborates and demonstrates how to combine a chatbot developed on the open source development platform Rasa with the open source platform Node-RED, allowing low-code or no-code development of a transactional conversational user interface from frontend to backend.",2021,363
AST '24: Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024),,"AST continues to be a venue for researchers and practitioners where they can discuss high quality research contributions on methods for software test automation, and various case studies reporting practices in this field. Indeed, software test automation is a discipline that has produced noteworthy research in the last decade.The special theme of AST 2024 is ""Test automation for and with Generative AI"". This innovative and promising research direction deals with the application of test automation technologies to the testing of Generative AI applications, as well as the adoption of generative AI to facilitate test automation.",2024,364
"Biases and differences in code review using medical imaging and eye-tracking: genders, humans, and machines","Huang, Yu and Leach, Kevin and Sharafi, Zohreh and McKay, Nicholas and Santander, Tyler and Weimer, Westley","Code review is a critical step in modern software quality assurance, yet it is vulnerable to human biases. Previous studies have clarified the extent of the problem, particularly regarding biases against the authors of code,but no consensus understanding has emerged. Advances in medical imaging are increasingly applied to software engineering, supporting grounded neurobiological explorations of computing activities, including the review, reading, and writing of source code. In this paper, we present the results of a controlled experiment using both medical imaging and also eye tracking to investigate the neurological correlates of biases and differences between genders of humans and machines (e.g., automated program repair tools) in code review. We find that men and women conduct code reviews differently, in ways that are measurable and supported by behavioral, eye-tracking and medical imaging data. We also find biases in how humans review code as a function of its apparent author, when controlling for code quality. In addition to advancing our fundamental understanding of how cognitive biases relate to the code review process, the results may inform subsequent training and tool design to reduce bias.",2020,365
Taming the parallel effect zoo: extensible deterministic parallelism with LVish,"Kuper, Lindsey and Todd, Aaron and Tobin-Hochstadt, Sam and Newton, Ryan R.","A fundamental challenge of parallel programming is to ensure that the observable outcome of a program remains deterministic in spite of parallel execution. Language-level enforcement of determinism is possible, but existing deterministic-by-construction parallel programming models tend to lack features that would make them applicable to a broad range of problems. Moreover, they lack extensibility: it is difficult to add or change language features without breaking the determinism guarantee.The recently proposed LVars programming model, and the accompanying LVish Haskell library, took a step toward broadly-applicable guaranteed-deterministic parallel programming. The LVars model allows communication through shared monotonic data structures to which information can only be added, never removed, and for which the order in which information is added is not observable. LVish provides a Par monad for parallel computation that encapsulates determinism-preserving effects while allowing a more flexible form of communication between parallel tasks than previous guaranteed-deterministic models provided.While applying LVar-based programming to real problems using LVish, we have identified and implemented three capabilities that extend its reach: inflationary updates other than least-upper-bound writes; transitive task cancellation; and parallel mutation of non-overlapping memory locations. The unifying abstraction we use to add these capabilities to LVish---without suffering added complexity or cost in the core LVish implementation, or compromising determinism---is a form of monad transformer, extended to handle the Par monad. With our extensions, LVish provides the most broadly applicable guaranteed-deterministic parallel programming interface available to date. We demonstrate the viability of our approach both with traditional parallel benchmarks and with results from a real-world case study: a bioinformatics application that we parallelized using our extended version of LVish.",2014,366
StarCraft as a Testbed for Engineering Complex Distributed Systems Using Cognitive Agent Technology,"Koeman, Vincent J. and Griffioen, Harm J. and Plenge, Danny C. and Hindriks, Koen V.","It has been argued that the evaluation of cognitive agent systems requires richer benchmark problems. We think that real-time strategy (RTS) games can offer such a testbed, as AI for RTS requires the design of complicated strategies for coordinating hundreds of units that need to solve a range of challenges. Therefore, in this paper, we report on the design and development of the first multi-agent connector that provides full access to StarCraft (Brood War). We provide a new interface that is dedicated to a multi-agent approach by connecting each unit in the game to a cognitive agent. Two main challenges are addressed in this work. First, we decide on the right level of abstraction for unit control by means of agents, designing for instance the percepts that are available to units. Second, a sufficient level of performance needs to be ensured in order to allow a large variety of multi-agent implementations to be successful at tackling challenges of RTS AI. The resulting open-source connector readily supports the hundreds of agents that can come and go during the game. Based on the development of the connector and its initial use by over 200 students, we gained valuable insights.",2018,367
Low Code Conversation-based Hybrid UI Design Case Study and Reflection,"Liu, Yunxing and Lee, Minha and Yang, Bin and Martens, Jean-Bernard","This paper presents a comprehensive case study on the Q-Survey, a chatbot-based qualitative survey tool developed using the Double-Diamond design process. The study delves into the intricacies of balancing user experience (UX) with technical challenges inherent in chatbot development. A significant focus is placed on the role of Low Code (LC) and No Code (NC) tools in facilitating rapid prototype development and testing. While these tools offer agility and ease in the early stages, their limitations become evident as the complexity of the system grows, prompting a reflection on their continued utility in advanced development stages. The Repertory Grid Technique (RGT) is explored as a potential tool for online qualitative surveys, with discussions on its complexity and the potential enhancements using advanced Natural Language Processing (NLP) tools. Through the lens of the Q-Survey and experts’ evaluation of this case, we discuss the broader implications of LC tools in Human-Computer Interaction (HCI) design, emphasizing the need for a structured framework for HCI design with LC. The study concludes with reflections on the current design, potential future directions, and the importance of continuous exploration of LC, especially in the realm of LLMs and coding tools based on LLMs.",2024,368
The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development,"Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.","Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.",2023,369
A framework for writing trigger-action todo comments in executable format,"Nie, Pengyu and Rai, Rishabh and Li, Junyi Jessy and Khurshid, Sarfraz and Mooney, Raymond J. and Gligoric, Milos","Natural language elements, e.g., todo comments, are frequently used to communicate among developers and to describe tasks that need to be performed (actions) when specific conditions hold on artifacts related to the code repository (triggers), e.g., from the Apache Struts project: “remove expectedJDK15 and if() after switching to Java 1.6”. As projects evolve, development processes change, and development teams reorganize, these comments, because of their informal nature, frequently become irrelevant or forgotten. We present the first framework, dubbed TrigIt, to specify trigger-action todo comments in executable format. Thus, actions are executed automatically when triggers evaluate to true. TrigIt specifications are written in the host language (e.g., Java) and are evaluated as part of the build process. The triggers are specified as query statements over abstract syntax trees, abstract representation of build configuration scripts, issue tracking systems, and system clock time. The actions are either notifications to developers or code transformation steps. We implemented TrigIt for the Java programming language and migrated 44 existing trigger-action comments from several popular open-source projects. Evaluation of TrigIt, via a user study, showed that users find TrigIt easy to learn and use. TrigIt has the potential to enforce more discipline in writing and maintaining comments in large code repositories.",2019,370
SapFix: automated end-to-end repair at scale,"Marginean, A. and Bader, J. and Chandra, S. and Harman, M. and Jia, Y. and Mao, K. and Mols, A. and Scott, A.","We report our experience with SAPFIX: the first deployment of automated end-to-end fault fixing, from test case design through to deployed repairs in production code1. We have used SAPFIX at Facebook to repair 6 production systems, each consisting of tens of millions of lines of code, and which are collectively used by hundreds of millions of people worldwide.",2019,371
Unsupervised Learning Techniques for Malware Characterization: Understanding Certain DNS-based DDoS Attacks,"Burton, Ren\'{e}e","This article details data science research in the area of Cyber Threat Intelligence applied to a specific type of Distributed Denial of Service (DDoS) attack. We study a DDoS technique prevalent in the Domain Name System (DNS) for which little malware have been recovered. Using data from a globally distributed set of a passive collectors (pDNS), we create a statistical classifier to identify these attacks and then use unsupervised learning to investigate the attack events and the malware that generates them. The first known major study of this technique, this work demonstrates that current attacks have little resemblance to earlier published descriptions and identifies several features of the attacks. Through a combination of text and time-series features, we are able to characterize the dominant malware and demonstrate that the number of global-scale attack systems is relatively small.",2020,372
CVEfixes: automated collection of vulnerabilities and their fixes from open-source software,"Bhandari, Guru and Naseer, Amara and Moonen, Leon","Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.",2021,373
Commit Message Generation from Code Differences using Hidden Markov Models,"Awad, Ahmed and Nagaty, Khaled","Commit messages are developer-written messages that document code changes. Such change might be adding features, fixing bugs or simply code updates. Although these messages help in understanding the evolution of any software, it is quite often that developers disregard the process of writing these messages, when making a change. Many automated methods have been proposed to generate commit messages. Due to the inability of those techniques to represent higher order understanding of code changes, the quality of these messages in terms of logic and context representation is very low as opposed to developer written messages. To solve this problem, previous work used deep learning models -specifically, sequence-to-sequence models- were used to automate that task. This model delivered promising results on translating code differences to commit messages. However, after the model's performance was thoroughly investigated in previous work. It was found out that code differences corresponding to almost every high quality commit messages generated by the model were very similar to one or more training sample code differences on a token level. Motivated by that observation, a k-nearest neighbor algorithm that outputs the same exact message of the nearest code difference was proposed in previous work. Inspired by the traditional solution to sequence modeling; Hidden Markov Models, we show that HMMs outperforms sequence-to-sequence models without outputting the same exact message of the nearest code diff, our experiments show an enhancement of 4% against sequence to sequence models.",2019,374
Mining Bursty Groups from Interaction Data,"Gorovits, Alexander and Zhang, Lin and Gujral, Ekta and Papalexakis, Evangelos and Bogdanov, Petko","Empirical studies and theoretical models both highlight burstinessas a common temporal pattern in online behavior. A key driver for burstiness is the self-exciting nature of online interactions. For example, posts in online groups often incite posts in response. Such temporal dependencies are easily lost when interaction data is aggregated in snapshots which are subsequently analyzed independently. An alternative is to model individual interactions as a multi-dimensional self-exciting process, thus, enforcing both temporal and network dependencies. Point processes, however, are challenging to employ for large real-world datasets as fitting them incurs super-linear cost in the number of events. How can we efficiently detect online groups exhibiting bursty self-exciting temporal behavior in large real-world datasets?  We propose a bursty group detection framework, called MYRON, which explicitly models self-exciting behavior within groups while also accounting for network-wide baseline activity. MYRON imposes bursty temporal structure within a scalable tensor factorization framework to decouple within-group interactions as interpretable factors. Our framework can incorporate different ""shapes""of temporal burstiness via wavelet decomposition or kernels forself-exciting behavior. Our evaluation on both synthetic and real-world data demonstrates MYRON's utility in community detection.It is up to 40% more effective in detecting ground truth groups compared to state-of-the-art baselines. In addition, MYRON is able to uncover interpretable bursty patterns of behavior from user-photo interactions in Flickr.",2021,375
"Hashing it out: a survey of programmers' cannabis usage, perception, and motivation","Endres, Madeline and Boehnke, Kevin and Weimer, Westley","Cannabis is one of the most common mind-altering substances. It is used both medicinally and recreationally and is enmeshed in a complex and changing legal landscape. Anecdotal evidence suggests that some software developers may use cannabis to aid some programming tasks. At the same time, anti-drug policies and tests remain common in many software engineering environments, sometimes leading to hiring shortages for certain jobs. Despite these connections, little is actually known about the prevalence of, and motivation for, cannabis use while programming. In this paper, we report the results of the first large-scale survey of cannabis use by programmers. We report findings about 803 developers' (including 450 full-time programmers') cannabis usage prevalence, perceptions, and motivations. For example, we find that some programmers do regularly use cannabis while programming: 35% of our sample has tried programming while using cannabis, and 18% currently do so at least once a month. Furthermore, this cannabis usage is primarily motivated by a perceived enhancement to certain software development skills (such as brainstorming or getting into a programming zone) rather than medicinal reasons (such as pain relief). Finally, we find that cannabis use while programming occurs at similar rates for programming employees, managers, and students despite differences in cannabis perceptions and visibility. Our results have implications for programming job drug policies and motivate future research into cannabis use while programming.",2022,376
A Dataset and an Approach for Identity Resolution of 38 Million Author IDs extracted from 2B Git Commits,"Fry, Tanner and Dey, Tapajit and Karnauch, Andrey and Mockus, Audris","The data collected from open source projects provide means to model large software ecosystems, but often suffer from data quality issues, specifically, multiple author identification strings in code commits might actually be associated with one developer. While many methods have been proposed for addressing this problem, they are either heuristics requiring manual tweaking, or require too much calculation time to do pairwise comparisons for 38M author IDs in, for example, the World of Code collection. In this paper, we propose a method that finds all author IDs belonging to a single developer in this entire dataset, and share the list of all author IDs that were found to have aliases. To do this, we first create blocks of potentially connected author IDs and then use a machine learning model to predict which of these potentially related IDs belong to the same developer. We processed around 38 million author IDs and found around 14.8 million IDs to have an alias, which belong to 5.4 million different developers, with the median number of aliases being 2 per developer. This dataset can be used to create more accurate models of developer behaviour at the entire OSS ecosystem level and can be used to provide a service to rapidly resolve new author IDs.",2020,377
How do programmers use optional typing? an empirical study,"Souza, Carlos and Figueiredo, Eduardo","The recent popularization of dynamically typed languages, such as Ruby and JavaScript, has brought more attention to the discussion about the impact of typing strategies on development. Types allow the compiler to find type errors earlier and potentially improve the readability and maintainability of code. On the other hand, ""untyped"" code may be easier to change and require less work from programmers. This paper tries to identify the programmers' point of view about these tradeoffs. An analysis of the source code of 6638 projects written in Groovy, a programming language which features optional typing, shows in which scenarios programmers prefer to type or not to type their declarations. Our results show that types are popular in the definition of module interfaces, but are less used in scripts, test classes and frequently changed code. There is no correlation between the size and age of projects and how their constructs are typed. Finally, we also found evidence that the background of programmers influences how they use types.",2014,378
Towards the no-code era: a vision and plan for the future of software development,"ElBatanony, Ahmed and Succi, Giancarlo","This paper provides a highly opinionated and biased vision and a two-stage plan with guidelines to reach a new era of software development, where anyone can create software without bothering to write code. Moreover, this paper explores in depth the first of these stages, which consists of creating a no-code tool based on six principles: configuration driven development, APIs, open-source, cross-platform, cloud computing, and design systems. An examination of each principle is presented and a case is made for why such a combination of principles would lay the foundation for future development efforts. Possible enquiries are addressed and a path is laid out for future works.",2021,379
Mining file histories: should we consider branches?,"Kovalenko, Vladimir and Palomba, Fabio and Bacchelli, Alberto","Modern distributed version control systems, such as Git, offer support for branching — the possibility to develop parts of software outside the master trunk. Consideration of the repository structure in Mining Software Repository (MSR) studies requires a thorough approach to mining, but there is no well-documented, widespread methodology regarding the handling of merge commits and branches. Moreover, there is still a lack of knowledge of the extent to which considering branches during MSR studies impacts the results of the studies. In this study, we set out to evaluate the importance of proper handling of branches when calculating file modification histories. We analyze over 1,400 Git repositories of four open source ecosystems and compute modification histories for over two million files, using two different algorithms. One algorithm only follows the first parent of each commit when traversing the repository, the other returns the full modification history of a file across all branches. We show that the two algorithms consistently deliver different results, but the scale of the difference varies across projects and ecosystems. Further, we evaluate the importance of accurate mining of file histories by comparing the performance of common techniques that rely on file modification history — reviewer recommendation, change recommendation, and defect prediction — for two algorithms of file history retrieval. We find that considering full file histories leads to an increase in the techniques’ performance that is rather modest.",2018,380
Summary of the 1st ICSSP-ICGSE Joint Event,"Tell, Paolo and Raffo, David and Huang, Liguo and Steinmacher, Igor and Britto, Ricardo and T\""{u}z\""{u}n, Eray and Clarke, Paul","Having the common objective of bringing together researchers and industry practitioners to share their research findings, experiences, and new ideas as well as sharing topics of interest, the organizing committees of the 14th International Conference on Software and System Processes (ICSSP) and the 15th International Conference on Global Software Engineering (ICGSE) ceased the opportunity to explore the idea of bringing together the two communities once it was clear that the International Conference on Software Engineering and all its co-located events had to be redesigned as online events.",2021,381
Fostering Coopetition While Plugging Leaks: The Design and Implementation of the MS MARCO Leaderboards,"Lin, Jimmy and Campos, Daniel and Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine","We articulate the design and implementation of the MS MARCO document ranking and passage ranking leaderboards. In contrast to ""standard"" community-wide evaluations such as those at TREC, which can be characterized as simultaneous games, leaderboards represent sequential games, where every player move is immediately visible to the entire community. The fundamental challenge with this setup is that every leaderboard submission leaks information about the held-out evaluation set, which conflicts with the fundamental tenant in machine learning about separation of training and test data. These ""leaks"", accumulated over long periods of time, threaten the validity of the insights that can be derived from the leaderboards. In this paper, we share our experiences grappling with this issue over the past few years and how our considerations are operationalized into a coherent submission policy. Our work provides a useful guide to help the community understand the design choices made in the popular MS MARCO leaderboards and offers lessons for designers of future leaderboards.",2022,382
"A wiki for Mizar: motivation, considerations, and initial prototype","Urban, Josef and Alama, Jesse and Rudnicki, Piotr and Geuvers, Herman","Formal mathematics has so far not taken full advantage of ideas from collaborative tools such as wikis and distributed version control systems (DVCS). We argue that the field could profit from such tools, serving both newcomers and experts alike. We describe a preliminary system for such collaborative development based on the Git DVCS. We focus, initially, on the Mizar system and its library of formalized mathematics.",2010,383
Challenges in Chatbot Development: A Study of Stack Overflow Posts,"Abdellatif, Ahmad and Costa, Diego and Badran, Khaled and Abdalkareem, Rabe and Shihab, Emad","Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development.Therefore, in this paper, we examine the Q&amp;A website, Stack Overflow, to provide insights on the topics that chatbot developers are interested and the challenges they face. In particular, we leverage topic modeling to understand the topics that are being discussed by chatbot developers on Stack Overflow. Then, we examine the popularity and difficulty of those topics. Our results show that most of the chatbot developers are using Stack Overflow to ask about implementation guidelines. We determine 12 topics that developers discuss (e.g., Model Training) that fall into five main categories. Most of the posts belong to chatbot development, integration, and the natural language understanding (NLU) model categories. On the other hand, we find that developers consider the posts of building and integrating chatbots topics more helpful compared to other topics. Specifically, developers face challenges in the training of the chatbot's model. We believe that our study guides future research to propose techniques and tools to help the community at its early stages to overcome the most popular and difficult topics that practitioners face when developing chatbots.",2020,384
Tensor processing primitives: a programming abstraction for efficiency and portability in deep learning workloads,"Georganas, Evangelos and Kalamkar, Dhiraj and Avancha, Sasikanth and Adelman, Menachem and Anderson, Cristina and Breuer, Alexander and Bruestle, Jeremy and Chaudhary, Narendra and Kundu, Abhisek and Kutnick, Denise and Laub, Frank and Md, Vasimuddin and Misra, Sanchit and Mohanty, Ramanarayan and Pabst, Hans and Ziv, Barukh and Heinecke, Alexander","During the past decade, novel Deep Learning (DL) algorithms/workloads and hardware have been developed to tackle a wide range of problems. Despite the advances in workload/hardware ecosystems, the programming methodology of DL systems is stagnant. DL workloads leverage either highly-optimized, yet platform-specific and inflexible kernels from DL libraries, or in the case of novel operators, reference implementations are built via DL framework primitives with underwhelming performance. This work introduces the Tensor Processing Primitives (TPP), a programming abstraction striving for efficient, portable implementation of DL workloads with high-productivity. TPPs define a compact, yet versatile set of 2D-tensor operators (or a virtual Tensor ISA), which subsequently can be utilized as building-blocks to construct complex operators on high-dimensional tensors. The TPP specification is platform-agnostic, thus code expressed via TPPs is portable, whereas the TPP implementation is highly-optimized and platform-specific. We demonstrate the efficacy of our approach using standalone kernels and end-to-end DL workloads expressed entirely via TPPs that outperform state-of-the-art implementations on multiple platforms.",2021,385
Modeling interconnected social and technical risks in open source software ecosystems,"Schueller, William and Wachs, Johannes","Open source software ecosystems consist of thousands of interdependent libraries, which users can combine to great effect. Recent work has pointed out two kinds of risks in these systems: that technical problems like bugs and vulnerabilities can spread through dependency links, and that relatively few developers are responsible for maintaining even the most widely used libraries. However, a more holistic diagnosis of systemic risk in software ecosystem should consider how these social and technical sources of risk interact and amplify one another. Motivated by the observation that the same individuals maintain several libraries within dependency networks, we present a methodological framework to measure risk in software ecosystems as a function of both dependencies and developers. In our models, a library’s chance of failure increases as its developers leave and as its upstream dependencies fail. We apply our method to data from the Rust ecosystem, highlighting several systemically important libraries that are overlooked when only considering technical dependencies. We compare potential interventions, seeking better ways to deploy limited developer resources with a view to improving overall ecosystem health and software supply chain resilience.",2024,386
CompEd 2023: Proceedings of the ACM Conference on Global Computing Education Vol 1,,"It is our great pleasure to welcome participants to the 2nd ACM Global Conference on Computing Education (ACM CompEd 2023) being held in Hyderabad, India, 7th-9th December, 2023 with the Working Groups meetings being held on 5th and 6th December 2023.ACM CompEd is a recent addition to the list of ACM sponsored conferences devoted to research in all aspects of computing education, including education at the school and college levels. The Hyderabad edition is only the second in this promising series. The long hiatus due to Covid-19 pushed this conference by two years, but we are glad that it is finally here!This edition of ACM CompEd partly overlaps with COMPUTE 2023, ACM India's flagship conference on Computing Education. Having the two conferences adjacent to each other is a great way to build synergy between the Indian computing education community and the global community of computing education researchers.",2023,387
A Survey of Multi-modal Knowledge Graphs: Technologies and Trends,"Liang, Wanying and Meo, Pasquale De and Tang, Yong and Zhu, Jia","In recent years, Knowledge Graphs (KGs) have played a crucial role in the development of advanced knowledge-intensive applications, such as recommender systems and semantic search. However, the human sensory system is inherently multi-modal, as objects around us are often represented by a combination of multiple signals, such as visual and textual. Consequently, Multi-modal Knowledge Graphs (MMKGs), which combine structured knowledge representation with multiple modalities, represent a powerful extension of KGs. Although MMKGs can handle certain types of tasks (e.g., visual query answering) or queries that standard KGs cannot process, and they can effectively tackle some standard problems (e.g., entity alignment), we lack a widely accepted definition of MMKG. In this survey, we provide a rigorous definition of MMKGs along with a classification scheme based on how existing approaches address four fundamental challenges: representation, fusion, alignment, and translation, which are crucial to improving an MMKG. Our classification scheme is flexible and allows for easy incorporation of new approaches, as well as a comparison of two approaches in terms of how they address one of the fundamental challenges mentioned above. As the first comprehensive survey of MMKG, this article aims at inspiring and provide a reference for relevant researchers in the field of Artificial Intelligence.",2024,388
Automatic Discovery of Emerging Browser Fingerprinting Techniques,"Su, Junhua and Kapravelos, Alexandros","With the progression of modern browsers, online tracking has become the most concerning issue for preserving privacy on the web. As major browser vendors plan to or already ban third-party cookies, trackers have to shift towards browser fingerprinting by incorporating novel browser APIs into their tracking arsenal. Understanding how new browser APIs are abused in browser fingerprinting techniques is a significant step toward ensuring protection from online tracking. In this paper, we propose a novel hybrid system, named BFAD, that automatically identifies previously unknown browser fingerprinting APIs in the wild. The system combines dynamic and static analysis to accurately reveal browser API usage and automatically infer browser fingerprinting behavior. Based on the observation that a browser fingerprint is constructed by pulling information from multiple APIs, we leverage dynamic analysis and a locality-based algorithm to discover all involved APIs and static analysis on the dataflow of fingerprinting information to accurately associate them together. Our system discovers 231 fingerprinting APIs in Alexa top 10K domains, starting with only 35 commonly known fingerprinting APIs and 17 data transmission APIs. Out of 231 APIs, 161 of them are not identified by state-of-the-art detection systems. Since our approach is fully automated, we repeat our experiments 11 months later and discover 18 new fingerprinting APIs that were not discovered in our previous experiment. We present with case studies the fingerprinting ability of a total of 249 detected APIs.",2023,389
Towards Understanding of Deepfake Videos in the Wild,"Cho, Beomsang and Le, Binh M. and Kim, Jiwon and Woo, Simon and Tariq, Shahroz and Abuadbba, Alsharif and Moore, Kristen","Abstract: Deepfakes have become a growing concern in recent years, prompting researchers to develop benchmark datasets and detection algorithms to tackle the issue. However, existing datasets suffer from significant drawbacks that hamper their effectiveness. Notably, these datasets fail to encompass the latest deepfake videos produced by state-of-the-art methods that are being shared across various platforms. This limitation impedes the ability to keep pace with the rapid evolution of generative AI techniques employed in real-world deepfake production. Our contributions in this IRB-approved study are to bridge this knowledge gap from current real-world deepfakes by providing in-depth analysis. We first present the largest and most diverse and recent deepfake dataset, RWDF-23, collected from the wild to date, consisting of 2,000 deepfake videos collected from 4 platforms targeting 4 different languages span created from 21 countries: Reddit, YouTube, TikTok, and Bilibili. By expanding the dataset's scope beyond the previous research, we capture a broader range of real-world deepfake content, reflecting the ever-evolving landscape of online platforms. Also, we conduct a comprehensive analysis encompassing various aspects of deepfakes, including creators, manipulation strategies, purposes, and real-world content production methods. This allows us to gain valuable insights into the nuances and characteristics of deepfakes in different contexts. Lastly, in addition to the video content, we also collect viewer comments and interactions, enabling us to explore the engagements of internet users with deepfake content. By considering this rich contextual information, we aim to provide a holistic understanding of the evolving deepfake phenomenon and its impact on online platforms.",2023,390
Sobotify: A Framework for Turning Robots into Social Robots,"H\""{u}bert, Heiko and Yun, Hae Seon","Sobotify is a software framework, which aims at simplifying the process of using robots in the field of social robotics. This paper delineates the design and usage of the framework. With Sobotify, even non-technical people should be enabled to use robots for their specific purposes, such as teachers in a classroom, therapists in a physiological or psychological therapy or childcare workers in kindergarten. During the development process of Sobotify, feedback from teachers at a vocational school have been taken into account in order to adjust the tools to their needs. The framework is designed to work with different robots including both humanoid (NAO and Pepper) and non-humanoid robots such as toy robots (Cozmo) with advanced abilities as well as very simple toy robots (MyKeepon). The framework was tested by two research works which proved that Sobotify is applicable in different setups. Further development is already planned for the next months, e.g. integration of additional robots and extension of tools.",2024,391
Using the Model of Regulation to Understand Software Development Collaboration Practices and Tool Support,"Arciniegas-Mendez, Maryi and Zagalsky, Alexey and Storey, Margaret-Anne and Hadwin, Allyson Fiona","We developed the Model of Regulation to provide a vocabulary for comparing and analyzing collaboration practices and tools in software engineering. This paper discusses the model's ability to capture how individuals self-regulate their own tasks and activities, how they regulate one another, and how they achieve a shared understanding of project goals and tasks. Using the model, we created an ""action-oriented"" instrument that individuals, teams, and organizations can use to reflect on how they regulate their work and on the various tools they use as part of regulation. We applied this instrument to two industrial software projects, interviewing one or two stakeholders from each project. The model allowed us to identify where certain processes and communication channels worked well, while recognizing friction points, communication breakdowns, and regulation gaps. We believe this model also shows potential for application in other domains.",2017,392
DevOps education: an interview study of challenges and recommendations,"Fernandes, Marcelo and Ferino, Samuel and Fernandes, Anny and Kulesza, Uir\'{a} and Aranha, Eduardo and Treude, Christoph","Over the last years, the software industry has adopted several DevOps technologies related to practices such as continuous integration and continuous delivery. The high demand for DevOps practitioners requires non-trivial adjustments in traditional software engineering courses and educational methodologies. This work presents an interview study with 14 DevOps educators from different universities and countries, aiming to identify the main challenges and recommendations for DevOps teaching. Our study identified 83 challenges, 185 recommendations, and several association links and conflicts between them. Our findings can help educators plan, execute and evaluate DevOps courses. They also highlight several opportunities for researchers to propose new methods and tools for teaching DevOps.",2022,393
Analyzing the Evolution and Maintenance of ML Models on Hugging Face,"Casta\~{n}o, Joel and Silverio, Mart\'{\i}nez-Fern\'{a}ndez and Franch, Xavier and Bogner, Justus","Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF - aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.",2024,394
“We Feel Like We’re Winging It:” A Study on Navigating Open-Source Dependency Abandonment,"Miller, Courtney and K\""{a}stner, Christian and Vasilescu, Bogdan","While lots of research has explored how to prevent maintainers from abandoning the open-source projects that serve as our digital infras- tructure, there are very few insights on addressing abandonment when it occurs. We argue open-source sustainability research must expand its focus beyond trying to keep particular projects alive, to also cover the sustainable use of open source by supporting users when they face potential or actual abandonment. We interviewed 33 developers who have experienced open-source dependency aban- donment. Often, they used multiple strategies to cope with aban- donment, for example, first reaching out to the community to find potential alternatives, then switching to a community-accepted alternative if one exists. We found many developers felt they had little to no support or guidance when facing abandonment, leaving them to figure out what to do through a trial-and-error process on their own. Abandonment introduces cost for otherwise seem- ingly free dependencies, but users can decide whether and how to prepare for abandonment through a number of different strategies, such as dependency monitoring, building abstraction layers, and community involvement. In many cases, community members can invest in resources that help others facing the same abandoned dependency, but often do not because of the many other competing demands on their time – a form of the volunteer’s dilemma. We dis- cuss cost reduction strategies and ideas to overcome this volunteer’s dilemma. Our findings can be used directly by open-source users seeking resources on dealing with dependency abandonment, or by researchers to motivate future work supporting the sustainable use of open source.",2023,395
Recommending Who to Follow in the Software Engineering Twitter Space,"Sharma, Abhishek and Tian, Yuan and Sulistya, Agus and Wijedasa, Dinusha and Lo, David","With the advent of social media, developers are increasingly using it in their software development activities. Twitter is one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape.” Unfortunately, due to the general-purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets.To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domain-related tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over 1 month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-Measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-Measure by at least 7.63%.",2018,396
"GenderMag Improves Discoverability in the Field, Especially for Women: An Multi-Year Case Study of Suggest Edit, a Code Review Feature","Murphy-Hill, Emerson and Elizondo, Alberto and Murillo, Ambar and Harbach, Marian and Vasilescu, Bogdan and Carlson, Delphine and Dessloch, Florian","Prior research shows that the GenderMag method can help identify and address usability barriers that are more likely to affect women software users than men. However, the evidence for the effectiveness of GenderMag is limited to small lab studies. In this case study, by combining self-reported gender data from tens of thousands of users of an internal code review tool with software logs data gathered over a five-year period, we quantitatively show that GenderMag helped a team at Google (a) correctly identify discoverability as a usability barrier more likely to affect women than men, and (b) increase discoverability by 2.4x while also achieving gender parity. That is, compared to men using the original code review tool, women and men using the system redesigned with GenderMag were both 2.4x more likely to discover the ""Suggest Edit"" feature at any given time. Thus, this paper contributes the first large-scale evidence of the effectiveness of GenderMag in the field.",2024,397
Analyzing Student Attention and Acceptance of Conversational AI for Math Learning: Insights from a Randomized Controlled Trial,"Li, Chenglu and Zhu, Wangda and Xing, Wanli and Guo, Rui","The significance of nurturing a deep conceptual understanding in math learning cannot be overstated. Grounded in the pedagogical strategies of induction, concretization, and exemplification (ICE), we designed and developed a conversational AI using both rule- and generation-based techniques to facilitate math learning. Serving as a preliminary step, this study employed an experimental design involving 151 U.S.-based college students to reveal students’ attention patterns, technology acceptance model, and qualitative feedback when using the developed ConvAI. Our findings suggest that participants in the ConvAI group generally exhibit higher attention levels than those in the control group, aside from the initial stage where the control group was more attentive. Meanwhile, participants appreciated their experience with the ConvAI, particularly valuing the ICE support features. Finally, qualitative analysis of participants’ feedback was conducted to inform future refinement and to inspire educational researchers and practitioners.",2024,398
"Speculative Histories, Just Futures: From Counterfactual Artifacts to Counterfactual Actions","Forlano, Laura E. and Halpern, Megan K.","This article engages with history as a speculative space for the purpose of critically engaging with discourses around the politics of technology in HCI. Drawing on approaches within critical design and based on evidence from two different projects, we develop an approach, counterfactual actions, that moves beyond the creation of artifacts and towards more situated, embodied, and performative engagements. In one project, Reimaging Work, we used a participatory game to engage stakeholders from social and economic justice organizations in Chicago. The other project, Future Design Studio, invited audience members at a futurist festival to create artifacts from the future and then invited improvisational actors to build worlds around them. We argue that a focus on counterfactual actions supports a more relational approach to understanding the politics of socio-technical systems and infrastructures, allowing participants to gain a meaningful understanding of the ways in which technology could be designed otherwise in line with ethics, values and social justice concerns.",2023,399
Hero: On the Chaos When PATH Meets Modules,"Wang, Ying and Qiao, Liang and Xu, Chang and Liu, Yepang and Cheung, Shing-Chi and Meng, Na and Yu, Hai and Zhu, Zhiliang","Ever since its first release in 2009, the Go programming language (Golang) has been well received by software communities. A major reason for its success is the powerful support of library-based development, where a Golang project can be conveniently built on top of other projects by referencing them as libraries. As Golang evolves, it recommends the use of a new library-referencing mode to overcome the limitations of the original one. While these two library modes are incompatible, both are supported by the Golang ecosystem. The heterogeneous use of library-referencing modes across Golang projects has caused numerous dependency management (DM) issues, incurring reference inconsistencies and even build failures. Motivated by the problem, we conducted an empirical study to characterize the DM issues, understand their root causes, and examine their fixing solutions. Based on our findings, we developed HERO, an automated technique to detect DM issues and suggest proper fixing solutions. We applied HERO to 19,000 popular Golang projects. The results showed that HERO achieved a high detection rate of 98.5% on a DM issue benchmark and found 2,422 new DM issues in 2,356 popular Golang projects. We reported 280 issues, among which 181 (64.6%) issues have been confirmed, and 160 of them (88.4%) have been fixed or are under fixing. Almost all the fixes have adopted our fixing suggestions.",2021,400
AUGER: automatically generating review comments with pre-training models,"Li, Lingwei and Yang, Li and Jiang, Huaxi and Yan, Jun and Luo, Tiejian and Hua, Zihan and Liang, Geng and Zuo, Chun","Code review is one of the best practices as a powerful safeguard for software quality. In practice, senior or highly skilled reviewers inspect source code and provide constructive comments, consider- ing what authors may ignore, for example, some special cases. The collaborative validation between contributors results in code being highly qualified and less chance of bugs. However, since personal knowledge is limited and varies, the efficiency and effectiveness of code review practice are worthy of further improvement. In fact, it still takes a colossal and time-consuming effort to deliver useful review comments.  
This paper explores a synergy of multiple practical review comments to enhance code review and proposes AUGER (AUtomatically GEnerating Review comments): a review comments generator with pre-training models. We first collect empirical review data from 11 notable Java projects and construct a dataset of 10,882 code changes. By leveraging Text-to-Text Transfer Transformer (T5) models, the framework synthesizes valuable knowledge in the training stage and effectively outperforms baselines by 37.38% in ROUGE-L. 29% of our automatic review comments are considered useful according to prior studies. The inference generates just in 20 seconds and is also open to training further. Moreover, the performance also gets improved when thoroughly analyzed in case study.",2022,401
"A Proposal for a Publish/Subscribe, Disruption Tolerant Content Island for Fog Computing","Manzoni, Pietro and Hern\'{a}ndez-Orallo, Enrique and Calafate, Carlos T. and Cano, Juan-Carlos","The performance of applications for the Internet of Things (IoT) depends on the availability of effective transport services offered by the underlying network. In this sense, Fog Computing aims to provide alternative networking models that offer higher fluidity in distributing in-network functions, in addition to allowing fast and scalable processing and exchange of information. In this paper we present the architecture and a working prototype of a fog computing ""content island"" which interconnects groups of ""things"" packed-up together to interchange data and processing among themselves and with other content islands. These islands are based on the integration of a publish/subscribe system with DTN (Disruption Tolerant Networks) techniques to provide a higher flexibility with respect to data and computation sharing. Some preliminary results regarding the prototype performance are also given.",2017,402
End-to-End Rationale Reconstruction,"Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis","The logic behind design decisions, called design rationale, is very valuable. In the past, researchers have tried to automatically extract and exploit this information, but prior techniques are only applicable to specific contexts and there is insufficient progress on an end-to-end rationale information extraction pipeline. Here we outline a path towards such a pipeline that leverages several Machine Learning (ML) and Natural Language Processing (NLP) techniques. Our proposed context-independent approach, called Kantara, produces a knowledge graph representation of decisions and of their rationales, which considers their historical evolution and traceability. We also propose validation mechanisms to ensure the correctness of the extracted information and the coherence of the development process. We conducted a preliminary evaluation of our proposed approach on a small example sourced from the Linux Kernel, which shows promising results.",2023,403
Eavesdropping user credentials via GPU side channels on smartphones,"Yang, Boyuan and Chen, Ruirong and Huang, Kai and Yang, Jun and Gao, Wei","Graphics Processing Unit (GPU) on smartphones is an effective target for hardware attacks. In this paper, we present a new side channel attack on mobile GPUs of Android smartphones, allowing an unprivileged attacker to eavesdrop the user's credentials, such as login usernames and passwords, from their inputs through on-screen keyboard. Our attack targets on Qualcomm Adreno GPUs and investigate the amount of GPU overdraw when rendering the popups of user's key presses of inputs. Such GPU overdraw caused by each key press corresponds to unique variations of selected GPU performance counters, from which these key presses can be accurately inferred. Experiment results from practical use on multiple models of Android smartphones show that our attack can correctly infer more than 80% of user's credential inputs, but incur negligible amounts of computing overhead and network traffic on the victim device. To counter this attack, this paper suggests mitigations of access control on GPU performance counters, or applying obfuscations on the values of GPU performance counters.",2022,404
Tunability: importance of hyperparameters of machine learning algorithms,"Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd","Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define databased defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.",2019,405
"Quill: efficient, transferable, and rich analytics at scale","Chandramouli, Badrish and Fernandez, Raul Castro and Goldstein, Jonathan and Eldawy, Ahmed and Quamar, Abdul","This paper introduces Quill (stands for a quadrillion tuples per day), a library and distributed platform for relational and temporal analytics over large datasets in the cloud. Quill exposes a new abstraction for parallel datasets and computation, called ShardedStreamable. This abstraction provides the ability to express efficient distributed physical query plans that are transferable, i.e., movable from offline to real-time and vice versa. ShardedStreamable decouples incremental query logic specification, a small but rich set of data movement operations, and keying; this allows Quill to express a broad space of plans with complex querying functionality, while leveraging existing temporal libraries such as Trill. Quill's layered architecture provides a careful separation of responsibilities with independently useful components, while retaining high performance. We built Quill for the cloud, with a master-less design where a language-integrated client library directly communicates and coordinates with cloud workers using off-the-shelf distributed cloud components such as queues. Experiments on up to 400 cloud machines, and on datasets up to 1TB, find Quill to incur low overheads and outperform SparkSQL by up to orders-of-magnitude for temporal and 6\texttimes{} for relational queries, while supporting a rich space of transferable, programmable, and expressive distributed physical query plans.",2016,406
Mapping the Interdisciplinary Research on Non-consensual Pornography: Technical and Quantitative Perspectives,"Falduti, Mattia and Tessaris, Sergio","The phenomenon of the non-consensual distribution of intimate or sexually explicit digital images of adults, a.k.a. non-consensual pornography (NCP) or revenge pornography, is under the spotlight for the toll it is taking on society. Law enforcement statistics confirm a dramatic global rise in abuses. For this reason, the research community is investigating different strategies to fight and mitigate the abuses and their effects. Since the phenomenon involves different aspects of personal and social interaction among users of social media and content sharing platforms, in the literature it is addressed under different academic disciplines. However, while most of the literature reviews focus on non-consensual pornography either from a social science or psychological perspective, to the best of our knowledge a systematic review of the research on the technical aspects of the problem is still missing. In this work, we present a Systematic Mapping Study (SMS) of the literature, looking at this interdisciplinary phenomenon through a technical lens. Therefore, we focus on the cyber side of the crime of non-consensual pornography with the aim of describing the state-of-the-art and the future lines of research from a technical and quantitative perspective.",2023,407
ICCNS '22: Proceedings of the 2022 12th International Conference on Communication and Network Security,,,2022,408
C2Miner: Tricking IoT Malware into Revealing Live Command &amp; Control Servers,"Davanian, Ali and Faloutsos, Michail and Lindorfer, Martina","How can we identify live Command &amp; Control (C2) servers for a given IoT malware binary? An effective solution to this problem constitutes a significant capability towards detecting and containing botnets. This task is not trivial because C2 servers are short-lived, and they use sophisticated and proprietary communication protocols. We propose C2Miner, a novel approach to trick IoT malware binaries into revealing their currently live C2 servers. Our approach weaponizes old disposable IoT malware binaries and uses them to probe active servers. We provide novel solutions to overcome the following challenges: (a) disambiguating the C2-bound traffic generated by the malware and (b) determining if a target IP:port is indeed a C2 server as opposed to a benign server.In our evaluation, based on 3M distinct exploration attempts over 150K distinct IP addresses, we show that we can identify C2 servers within a given IP:port space with an F1 score of 86%. In addition, we show how our approach can be used in practice and at scale. Conducting a large-scale probing campaign has scalability issues given that the number of probes is proportional to the IP addresses, the number of ports, and the number of binaries from distinct families which we want to explore. To address this challenge, we propose a grammar-based method to fingerprint and cluster C2 communications which, among other applications, allows us to select malware binaries for weaponization efficiently. Additionally, we use spatio-temporal features of C2 servers to narrow down our search in the entire IP space. An optimistic observation from our study is that using only 2 (more than 6 months) old IoT malware binaries, we scan 18K IP:port pairs daily for 6 days and find 6 new live C2 servers.",2024,409
ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement,,,2022,410
Social Media Identity Deception Detection: A Survey,"Alharbi, Ahmed and Dong, Hai and Yi, Xun and Tari, Zahir and Khalil, Ibrahim","Social media have been growing rapidly and become essential elements of many people’s lives. Meanwhile, social media have also come to be a popular source for identity deception. Many social media identity deception cases have arisen over the past few years. Recent studies have been conducted to prevent and detect identity deception. This survey analyzes various identity deception attacks, which can be categorized into fake profile, identity theft, and identity cloning. This survey provides a detailed review of social media identity deception detection techniques. It also identifies primary research challenges and issues in the existing detection techniques. This article is expected to benefit both researchers and social media providers.",2021,411
An Empirical Study of Developer Discussions in the Gitter Platform,"Ehsan, Osama and Hassan, Safwat and Mezouar, Mariam El and Zou, Ying","Developer chatrooms (e.g., the Gitter platform) are gaining popularity as a communication channel among developers. In developer chatrooms, a developer (asker) posts questions and other developers (respondents) respond to the posted questions. The interaction between askers and respondents results in a discussion thread. Recent studies show that developers use chatrooms to inquire about issues, discuss development ideas, and help each other. However, prior work focuses mainly on analyzing individual messages of a chatroom without analyzing the discussion thread in a chatroom. Developer chatroom discussions are context-sensitive, entangled, and include multiple participants that make it hard to accurately identify threads. Therefore, prior work has limited capability to show the interactions among developers within a chatroom by analyzing only individual messages.In this article, we perform an in-depth analysis of the Gitter platform (i.e., developer chatrooms) by analyzing 6,605,248 messages of 709 chatrooms. To analyze the characteristics of the posted questions and the impact on the response behavior (e.g., whether the posted questions get responses), we propose an approach that identifies discussion threads in chatrooms with high precision (i.e., 0.81 F-score). Our results show that inactive members responded more often and unique questions take longer discussion time than simple questions. We also find that clear and concise questions are more likely to be responded to than poorly written questions.We further manually analyze a randomly selected sample of 384 threads to examine how respondents resolve the raised questions. We observe that more than 80% of the studied threads are resolved. Advanced-level/beginner-level questions along with the edited questions are the mostly resolved questions. Our results can help the project maintainers understand the nature of the discussion threads (e.g., the topic trends). Project maintainers can also benefit from our thread identification approach to spot the common repeated threads and use these threads as frequently asked questions (FAQs) to improve the documentation of their projects.",2021,412
Aide-m\'{e}moire: Improving a Project’s Collective Memory via Pull Request–Issue Links,"P\^{a}r\c{t}achi, Profir-Petru and White, David R. and Barr, Earl T.","Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-m\'{e}moire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-m\'{e}moire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project’s lifetime and continuously improve traceability. Aide-m\'{e}moire is tailored for two specific instances of the general traceability problem—namely, commit to issue and pull request to issue links, with a focus on the latter—and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-m\'{e}moire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers.",2023,413
On the Discoverability of npm Vulnerabilities in Node.js Projects,"Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad and Adams, Bram","The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42%) depend on undisclosed vulnerable packages, 206&nbsp;(4.63%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance.",2023,414
An Empirical Study of Artifacts and Security Risks in the Pre-trained Model Supply Chain,"Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan and Indarapu, Aryan and Hyatt, Matt and Schorlemmer, Taylor R. and Thiruvathukal, George K. and Davis, James C.","Deep neural networks achieve state-of-the-art performance on many tasks, but require increasingly complex architectures and costly training procedures. Engineers can reduce costs by reusing a pre-trained model (PTM) and fine-tuning it for their own tasks. To facilitate software reuse, engineers collaborate around model hubs, collections of PTMs and datasets organized by problem domain. Although model hubs are now comparable in popularity and size to other software ecosystems, the associated PTM supply chain has not yet been examined from a software engineering perspective. We present an empirical study of artifacts and security features in 8 model hubs. We indicate the potential threat models and show that the existing defenses are insufficient for ensuring the security of PTMs. We compare PTM and traditional supply chains, and propose directions for further measurements and tools to increase the reliability of the PTM supply chain.",2022,415
"CommentFinder: a simpler, faster, more accurate code review comments recommendation","Hong, Yang and Tantithamthavorn, Chakkrit and Thongtanunam, Patanamon and Aleti, Aldeida","Code review is an effective quality assurance practice, but can be labor-intensive since developers have to manually review the code and provide written feedback. Recently, a Deep Learning (DL)-based approach was introduced to automatically recommend code review comments based on changed methods. While the approach showed promising results, it requires expensive computational resource and time which limits its use in practice. To address this limitation, we propose CommentFinder – a retrieval-based approach to recommend code review comments. Through an empirical evaluation of 151,019 changed methods, we evaluate the effectiveness and efficiency of CommentFinder against the state-of-the-art approach. We find that when recommending the best-1 review comment candidate, our CommentFinder is 32% better than prior work in recommending the correct code review comment. In addition, CommentFinder is 49 times faster than the prior work. These findings highlight that our CommentFinder could help reviewers to reduce the manual efforts by recommending code review comments, while requiring less computational time.",2022,416
Where are you taking me?Understanding Abusive Traffic Distribution Systems,"Szurdi, Janos and Luo, Meng and Kondracki, Brian and Nikiforakis, Nick and Christin, Nicolas","Illicit website owners frequently rely on traffic distribution systems (TDSs) operated by less-than-scrupulous advertising networks to acquire user traffic. While researchers have described a number of case studies on various TDSs or the businesses they serve, we still lack an understanding of how users are differentiated in these ecosystems, how different illicit activities frequently leverage the same advertisement networks and, subsequently, the same malicious advertisers. We design ODIN (Observatory of Dynamic Illicit ad Networks), the first system to study cloaking, user differentiation and business integration at the same time in four different types of traffic sources: typosquatting, copyright-infringing movie streaming, ad-based URL shortening, and illicit online pharmacy websites. ODIN performed 874,494 scrapes over two months (June 19, 2019–August 24, 2019), posing as six different types of users (e.g., mobile, desktop, and crawler) and accumulating over 2TB of data. We observed 81% more malicious pages compared to using only the best performing crawl profile by itself. Three of the traffic sources we study redirect users to the same traffic broker domain names up to 44% of the time and all of them often expose users to the same malicious advertisers. Our experiments show that novel cloaking techniques could decrease by half the number of malicious pages observed. Worryingly, popular blacklists do not just suffer from the lack of coverage and delayed detection, but miss the vast majority of malicious pages targeting mobile users. We use these findings to design a classifier, which can make precise predictions about the likelihood of a user being redirected to a malicious advertiser.",2021,417
Generative AI in CS Education: Literature Review through a SWOT Lens,"Roberts, Jordan and Mohamed, Abdallah","The rapid growth of generative artificial intelligence (AI) models introduced challenges for educators, students and administrators across the academic sphere related to how to manage and regulate these tools. While some oppose their use, many researchers have begun to approach the topic of educational AI use from a different perspective. Despite being in its early stages; this field of research has produced notable insights into the capabilities and limitations of models like ChatGPT. This paper utilizes a SWOT analysis framework to analyze and consolidate existing literature, with a specific focus on Computer Science education. Through the analysis of this literature, we have created a set of use cases and guidelines to aid in the future development of strategies and tools within this field. Our findings indicate that while some concerns are valid, such as AI's ability to generate plagiarized work, we identified several promising avenues and opportunities for careful integration of this technology into education.",2024,418
Development Frameworks for Microservice-based Applications: Evaluation and Comparison,"Dinh-Tuan, Hai and Mora-Martinez, Maria and Beierle, Felix and Garzon, Sandro Rodriguez","The microservice architectural style has gained much attention from both academia and industry recently as a novel way to design, develop, and deploy cloud-native applications. This concept encourages the decomposition of a monolith into multiple independently deployable units. A typical microservices-based application is formed of two service types: functional services, which provide the core business logic, and infrastructure services, which provide essential functionalities for a microservices ecosystem. To improve developers' productivity, many software frameworks have been developed to provide those reusable infrastructure services, allowing programmers to focus on implementing microservices in arbitrary ways. In this work, we made use of four open source frameworks to develop a cloud-based application in order to compare and evaluate their usability and practicability. While all selected frameworks promote asynchronous microservice design in general, there are differences in the ways each implements services. This leads to interoperability issues, such as message topic naming convention. Additionally, a key finding is the long startup times of JVM-based services that might reduce application's resiliency and portability. Some other advantages come directly from the programming language, such as the ability of Go to generate native binary executables, which results in very small and compact Docker images (up to 78% smaller compared to other languages).",2020,419
MIRTO: an Open-Source Robotic Platform for Education,"Androutsopoulos, K. and Aristodemou, L. and Boender, J. and Bottone, M. and Currie, E. and El-Aroussi, I. and Fields, B. and Gheri, L. and Gorogiannis, N. and Heeney, M. and Micheletti, M. and Loomes, M. and Margolis, M. and Petridis, M. and Piermarteri, A. and Primiero, G. and Raimondi, F. and Weldin, N.","This paper introduces the Middlesex RoboTic platfOrm (MIRTO), an open-source platform that has been used for teaching First Year Computer Science students since the academic year 2013/2014, with the aim of providing a physical manifestation of Software Engineering concepts that are often delivered using only abstract or synthetic case studies. In this paper we provide a detailed description of the platform, whose hardware specifications and software libraries are all released open source; we describe a number of teaching usages of the platform, report students' projects, and evaluate some of its aspects in terms of effectiveness, usability, and maintenance.",2018,420
Towards Using Few-Shot Prompt Learning for Automating Model Completion,"Chaaben, Meriem Ben and Burgue\~{n}o, Lola and Sahraoui, Houari",We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.,2023,421
Sustainability forecasting for Apache incubator projects,"Yin, Likang and Chen, Zhuangzhi and Xuan, Qi and Filkov, Vladimir","Although OSS development is very popular, ultimately more than 80% of OSS projects fail. Identifying the factors associated with OSS success can help in devising interventions when a project takes a downturn. OSS success has been studied from a variety of angles, more recently in empirical studies of large numbers of diverse projects, using proxies for sustainability, e.g., internal metrics related to productivity and external ones, related to community popularity. The internal socio-technical structure of projects has also been shown important, especially their dynamics. This points to another angle on evaluating software success, from the perspective of self-sustaining and self-governing communities. To uncover the dynamics of how a project at a nascent development stage gradually evolves into a sustainable one, here we apply a socio-technical network modeling perspective to a dataset of Apache Software Foundation Incubator (ASFI), sustainability-labeled projects. To identify and validate the determinants of sustainability, we undertake a mix of quantitative and qualitative studies of ASFI projects’ socio-technical network trajectories. We develop interpretable models which can forecast a project becoming sustainable with 93+% accuracy, within 8 months of incubation start. Based on the interpretable models we describe a strategy for real-time monitoring and suggesting actions, which can be used by projects to correct their sustainability trajectories.",2021,422
ITA-ELECTION-2022: A Multi-Platform Dataset of Social Media Conversations Around the 2022 Italian General Election,"Pierri, Francesco and Liu, Geng and Ceri, Stefano","Online social media play a major role in shaping public discourse and opinion, especially during political events. We present the first public multi-platform dataset of Italian-language political conversations, focused on the 2022 Italian general election taking place on September 25th. Leveraging public APIs and a keyword-based search, we collected millions of posts published by users, pages and groups on Facebook, Instagram and Twitter, along with metadata of TikTok and YouTube videos shared on these platforms, over a period of four months. We augmented the dataset with a collection of political ads sponsored on Meta platforms, and a list of social media handles associated with political representatives. Our data resource will allow researchers and academics to further our understanding of the role of social media in the democratic process.",2023,423
RIoTMAN: a systematic analysis of IoT malware behavior,"Darki, Ahmad and Faloutsos, Michalis","How can we conduct dynamic analysis on IoT malware efficiently? A key challenge is that such malware target a plethora of different devices, which makes identifying the target device non-trivial. This problem does not appear nearly as much in PC and smartphones malware, where the devices are more uniform. The contribution of our work is two fold: (a) we develop RIoTMAN, a comprehensive emulation and dynamic analysis approach, and (b) we study the network behavior of 3024 IoT malware systematically. The power of our approach lies in two key novelties: (a) Iterative Adaptation, and (b) Automated Engagement. First, we employ an intelligent iterative process that incrementally ""builds"" the configuration of the target device. Second, our platform automates the interaction with the malware even during the C&amp;C server communication phase. In our experiments, we first show that we achieve an activation rate of 93% for our binaries, including 173 binaries, which Virustotal fails to identify as malicious. Second, we impersonate the C&amp;C server for 79% of the malware binaries successfully: we make the malware initiate DDoS attacks, or enter its proliferation phase. Finally, we observe several interesting malware techniques, including unusual communication behaviors. Our goal is to release our platform as an open-source tool to accelerate the efforts for understanding IoT malware in depth and at scale.",2020,424
"""Community Guidelines Make this the Best Party on the Internet"": An In-Depth Study of Online Platforms' Content Moderation Policies","Schaffner, Brennan and Bhagoji, Arjun Nitin and Cheng, Siyuan and Mei, Jacqueline and Shen, Jay L and Wang, Grace and Chetty, Marshini and Feamster, Nick and Lakier, Genevieve and Tan, Chenhao","Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.",2024,425
Flexible Enforcement of Multi-factor Authentication with SSH via Linux-PAM for Federated Identity Users,"Simmel, Derek and Filus, Shane","A computational science project with restricted-access data was awarded an allocation by XSEDE in 2016 to use the Bridges supercomputer at the Pittsburgh Supercomputing Center (PSC). As a condition of the license agreement for access to the data, multi-factor authentication (MFA) with XSEDE's Duo MFA service is required for users of this project to login to Bridges via SSH, in addition to filesystem access controls. Since not all Bridges users are required to authenticate to Bridges in this manner, a solution was implemented via Linux-PAM to require XSEDE Duo MFA for SSH login access by specific users, as identified by their local account name or membership in a local group. This paper describes the implementation on Bridges and its extensibility to other systems and environments with similar needs.",2017,426
Generative AI as a New Innovation Platform,"Cusumano, Michael A.",Considering the stability and longevity of a potential new foundational technology.,2023,427
A Survey on Online Judge Systems and Their Applications,"Wasik, Szymon and Antczak, Maciej and Badura, Jan and Laskowski, Artur and Sternal, Tomasz","Online judges are systems designed for the reliable evaluation of algorithm source code submitted by users, which is next compiled and tested in a homogeneous environment. Online judges are becoming popular in various applications. Thus, we would like to review the state of the art for these systems. We classify them according to their principal objectives into systems supporting organization of competitive programming contests, enhancing education and recruitment processes, facilitating the solving of data mining challenges, online compilers and development platforms integrated as components of other custom systems. Moreover, we introduce a formal definition of an online judge system and summarize the common evaluation methodology supported by such systems. Finally, we briefly discuss an Optil.io platform as an example of an online judge system, which has been proposed for the solving of complex optimization problems. We also analyze the competition results conducted using this platform. The competition proved that online judge systems, strengthened by crowdsourcing concepts, can be successfully applied to accurately and efficiently solve complex industrial- and science-driven challenges.",2018,428
"Bypassing antivirus detection: old-school malware, new tricks","Chatzoglou, Efstratios and Karopoulos, Georgios and Kambourakis, Georgios and Tsiatsikas, Zisis","Being on a mushrooming spree since at least 2013, malware can take a large toll on any system. In a perpetual cat-and-mouse chase with defenders, malware writers constantly conjure new methods to hide their code so as to evade detection by security products. In this context, focusing on the MS Windows platform, this work contributes a comprehensive empirical evaluation regarding the detection capacity of popular, off-the-shelf antivirus and endpoint detection and response engines when facing legacy malware obfuscated via more or less uncommon but publicly known methods. Our experiments exploit a blend of seven traditional AV evasion techniques in 16 executables built in C++, Go, and Rust. Furthermore, we conduct an incipient study regarding the ability of the ChatGPT chatbot in assisting threat actors to produce ready-to-use malware. The derived results in terms of detection rate are highly unexpected: approximately half of the 12 tested AV engines were able to detect less than half of the malware variants, four AVs exactly half of the variants, while only two of the rest detected all but one of the variants.",2023,429
Risks to the Public,"Neumann, Peter G.",,2015,430
Combining a Novel Scoring Approach with Arabic Stemming Techniques for Arabic Chatbots Conversation Engine,"Alshammari, Nasser O. and Alharbi, Fawaz D.","Arabic is recognized as one of the main languages around the world. Many attempts and efforts have been done to provide computing solutions to support the language. Developing Arabic chatbots is still an evolving research field and requires extra efforts due to the nature of the language. One of the common tasks of any natural language processing application is the stemming step. It is important for developing chatbots, since it helps with pre-processing the input data and it can be involved with different phases of the chatbot development process. The aim of this article is to combine a scoring approach with Arabic stemming techniques for developing an Arabic chatbot conversation engine. Two experiments are conducted to evaluate the proposed solution. The first experiment is to select which stemmer is more accurate when applying our solution, since our algorithm can support various stemmers. The second experiment was conducted to evaluate our proposed approach against various machine learning models. The results show that the ISRIS stemming algorithm is the best fit for our solution with accuracy 78.06%. The results also indicate that our novel solution achieved an F1 score of 65.5%, while the other machine learning models achieved slightly lower scores. Our study presents a novel technique by combining scoring mechanisms with stemming processes to produce the best answer for every query sent by chatbots users compared to other approaches. This can be helpful for developing Arabic chatbot and can support many domains such as education, business, and health. This technique is among the first techniques that developed purposefully to serve the development of Arabic chatbots conversation engine.",2022,431
LaCasa: lightweight affinity and object capabilities in Scala,"Haller, Philipp and Loiko, Alex","Aliasing is a known source of challenges in the context of imperative object-oriented languages, which have led to important advances in type systems for aliasing control. However, their large-scale adoption has turned out to be a surprisingly difficult challenge. While new language designs show promise, they do not address the need of aliasing control in existing languages.  This paper presents a new approach to isolation and uniqueness in an existing, widely-used language, Scala. The approach is unique in the way it addresses some of the most important obstacles to the adoption of type system extensions for aliasing control. First, adaptation of existing code requires only a minimal set of annotations. Only a single bit of information is required per class. Surprisingly, the paper shows that this information can be provided by the object-capability discipline, widely-used in program security. We formalize our approach as a type system and prove key soundness theorems. The type system is implemented for the full Scala language, providing, for the first time, a sound integration with Scala's local type inference. Finally, we empirically evaluate the conformity of existing Scala open-source code on a corpus of over 75,000 LOC.",2016,432
Supporting Collaboration in Introductory Programming Classes Taught in Hybrid Mode: A Participatory Design Study,"Goswami, Lahari and Zeinoddin, Pegah Sadat and Estier, Thibault and Cherubini, Mauro","Hybrid learning modalities, where learners can attend a course in-person or remotely, have gained particular significance in post-pandemic educational settings. In introductory programming courses, novices’ learning behaviour in the collaborative context of classrooms differs in hybrid mode from that of a traditional setting. Reflections from conducting an introductory programming course in hybrid mode led us to recognise the need for re-designing programming tools to support students’ collaborative learning practices. We conducted a participatory design study with nine students, directly engaging them in design to understand their interaction needs in hybrid pedagogical setups to enable effective collaboration during learning. Our findings first highlighted the difficulties that learners face in hybrid modes. The results then revealed learners’ preferences for design functionalities to enable collective notions, communication, autonomy, and regulation. Based on our findings, we discuss design principles and implications to inform the future design of collaborative programming environments for hybrid modes.",2023,433
"OAuth 2.0 Redirect URI Validation Falls Short, Literally","Innocenti, Tommaso and Golinelli, Matteo and Onarlioglu, Kaan and Mirheidari, Ali and Crispo, Bruno and Kirda, Engin","OAuth 2.0 requires a complex redirection trail between websites and Identity Providers (IdPs). In particular, the ""redirect URI"" parameter included in the popular Authorization Grant Code flow governs the callback endpoint that users are routed to, together with their security tokens. The protocol specification, therefore, includes guidelines on protecting the integrity of the redirect URI. In this work, we analyze the OAuth&nbsp;2.0 specification in light of modern systems-centric attacks and reveal that the prescribed redirect URI validation guidance exposes IdPs to path confusion and parameter pollution attacks. Based on this observation, we propose novel attack techniques and experiment with 16 popular IdPs, empirically verifying that the OAuth&nbsp;2.0 security guidance is under-specified. We finally present end-to-end attack scenarios that combine our attack techniques with common web application vulnerabilities, ultimately resulting in a complete compromise of the secure delegated access that OAuth&nbsp;2.0 promises.",2023,434
Orchestration Scripts: A System for Encoding an Organization’s Ways of Working to Support Situated Work,"Garg, Kapil and Gergle, Darren and Zhang, Haoqi","Ill-structured problems demand that people adopt sophisticated strategies for planning, seeking support, and using available resources along their work process. These practices involve a challenging monitoring and strategizing process that existing tools cannot support since they largely lack an understanding of an organization’s processes, social structures, venues, and tools. We introduce workplace programming for situationally-aware systems–an approach for encoding work situations using computational abstractions of an organization’s ways of working and surfacing support strategies at appropriate times and settings. With this approach, we implement Orchestration Scripts, a system that supports various situated work activities in a socio-technical organization. Through a case study and field study, we show how our approach encodes different aspects of working effectively and helps people identify situations to enact effective strategies using the available support opportunities. Our results show how a programmable technology can provide situated support in today’s workplaces.",2023,435
GIFdroid: automated replay of visual bug reports for Android apps,"Feng, Sidong and Chen, Chunyang","Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using software. However, it is difficult for non-technical users to write clear descriptions about the bug occurrence. Therefore, more and more users begin to record the screen for reporting bugs as it is easy to be created and contains detailed procedures triggering the bug. But it is still tedious and time-consuming for developers to reproduce the bug due to the length and unclear actions within the recording. To overcome these issues, we propose GIFdroid, a lightweight approach to automatically replay the execution trace from visual bug reports. GIFdroid adopts image processing techniques to extract the keyframes from the recording, map them to states in GUI Transitions Graph, and generate the execution trace of those states to trigger the bug. Our automated experiments and user study demonstrate its accuracy, efficiency, and usefulness of the approach.",2022,436
"ARES '22: Proceedings of the 17th International Conference on Availability, Reliability and Security",,,2022,437
A First Look at the Crypto-Mining Malware Ecosystem: A Decade of Unrestricted Wealth,"Pastrana, Sergio and Suarez-Tangil, Guillermo","Illicit crypto-mining leverages resources stolen from victims to mine cryptocurrencies on behalf of criminals. While recent works have analyzed one side of this threat, i.e.: web-browser cryptojacking, only commercial reports have partially covered binary-based crypto-mining malware.In this paper, we conduct the largest measurement of crypto-mining malware to date, analyzing approximately 4.5 million malware samples (1.2 million malicious miners), over a period of twelve years from 2007 to 2019. Our analysis pipeline applies both static and dynamic analysis to extract information from the samples, such as wallet identifiers and mining pools. Together with OSINT data, this information is used to group samples into campaigns. We then analyze publicly-available payments sent to the wallets from mining-pools as a reward for mining, and estimate profits for the different campaigns. All this together is is done in a fully automated fashion, which enables us to leverage measurement-based findings of illicit crypto-mining at scale.Our profit analysis reveals campaigns with multi-million earnings, associating over 4.4% of Monero with illicit mining. We analyze the infrastructure related with the different campaigns, showing that a high proportion of this ecosystem is supported by underground economies such as Pay-Per-Install services. We also uncover novel techniques that allow criminals to run successful campaigns.",2019,438
Interactive Abstract Interpretation with Demanded Summarization,"Stein, Benno and Chang, Bor-Yuh Evan and Sridharan, Manu","We consider the problem of making expressive, interactive static analyzers compositional. Such a technique could help bring the power of server-based static analyses to integrated development environments (IDEs), updating their results live as the code is modified. Compositionality is key for this scenario, as it enables reuse of already-computed analysis results for unmodified code. Previous techniques for interactive static analysis either lack compositionality, cannot express arbitrary abstract domains, or are not from-scratch consistent.We present demanded summarization, the first algorithm for incremental compositional analysis in arbitrary abstract domains that guarantees from-scratch consistency. Our approach analyzes individual procedures using a recent technique for demanded analysis, computing summaries on demand for procedure calls. A dynamically updated summary dependency graph enables precise result invalidation after program edits, and the algorithm is carefully designed to guarantee from-scratch-consistent results after edits, even in the presence of recursion and in arbitrary abstract domains. We formalize our technique and prove soundness, termination, and from-scratch consistency. An experimental evaluation of a prototype implementation on synthetic and real-world program edits provides evidence for the feasibility of this theoretical framework, showing potential for major performance benefits over non-demanded compositional analyses.",2024,439
Robust Learning from Observation with Model Misspecification,"Viano, Luca and Huang, Yu-Ting and Kamalaruban, Parameswaran and Innes, Craig and Ramamoorthy, Subramanian and Weller, Adrian","Imitation learning (IL) is a popular paradigm for training policies in robotic systems when specifying the reward function is difficult. However, despite the success of IL algorithms, they impose the somewhat unrealistic requirement that the expert demonstrations must come from the same domain in which a new imitator policy is to be learned. We consider a practical setting, where (i) state-only expert demonstrations from the real (deployment) environment are given to the learner, (ii) the imitation learner policy is trained in a simulation (training) environment whose transition dynamics is slightly different from the real environment, and (iii) the learner does not have any access to the real environment during the training phase beyond the batch of demonstrations given. Most of the current IL methods, such as generative adversarial imitation learning and its state-only variants, fail to imitate the optimal expert behavior under the above setting. By leveraging insights from the Robust reinforcement learning (RL) literature and building on recent adversarial imitation approaches, we propose a robust IL algorithm to learn policies that can effectively transfer to the real environment without fine-tuning. Furthermore, we empirically demonstrate on continuous-control benchmarks that our method outperforms the state-of-the-art state-only IL method in terms of the zero-shot transfer performance in the real environment and robust performance under different testing conditions.",2022,440
ICSE '23: Proceedings of the 45th International Conference on Software Engineering,,"ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.",2023,441
Building and maintaining a third-party library supply chain for productive and secure SGX enclave development,"Wang, Pei and Ding, Yu and Sun, Mingshen and Wang, Huibo and Li, Tongxin and Zhou, Rundong and Chen, Zhaofeng and Jing, Yiming","The big data industry is facing new challenges as concerns about privacy leakage soar. One of the remedies to privacy breach incidents is to encapsulate computations over sensitive data within hardware-assisted Trusted Execution Environments (TEE). Such TEE-powered software is called secure enclaves. Secure enclaves hold various advantages against competing for privacy-preserving computation solutions. However, enclaves are much more challenging to build compared with ordinary software. The reason is that the development of TEE software must follow a restrictive programming model to make effective use of strong memory encryption and segregation enforced by hardware. These constraints transitively apply to all third-party dependencies of the software. If these dependencies do not officially support TEE hardware, TEE developers have to spend additional engineering effort in porting them. High development and maintenance cost is one of the major obstacles against adopting TEE-based privacy protection solutions in production.In this paper, we present our experience and achievements with regard to constructing and continuously maintaining a third-party library supply chain for TEE developers. In particular, we port a large collection of Rust third-party libraries into Intel SGX, one of the most mature trusted computing platforms. Our supply chain accepts upstream patches in a timely manner with SGX-specific security auditing. We have been able to maintain the SGX ports of 159 open-source Rust libraries with reasonable operational costs. Our work can effectively reduce the engineering cost of developing SGX enclaves for privacy-preserving data processing and exchange.",2020,442
How Knowledge Workers Use and Want to Use LLMs in an Enterprise Context,"Brachman, Michelle and El-Ashry, Amina and Dugan, Casey and Geyer, Werner","Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers’ current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We discuss implications for adoption and design of generative AI technologies for knowledge work.",2024,443
An Experimental Analysis of Security Vulnerabilities in Industrial IoT Devices,"Jiang, Xingbin and Lora, Michele and Chattopadhyay, Sudipta","The revolutionary development of the Internet of Things has triggered a huge demand for Internet of Things devices. They are extensively applied to various fields of social activities, and concerning manufacturing, they are a key enabling concept for the Industry 4.0 ecosystem. Industrial Internet of Things (IIoT) devices share common vulnerabilities with standard IoT devices, which are increasingly exposed to the attackers. As such, connected industrial devices may become sources of cyber, as well as physical, threats for people and assets in industrial environments.In this work, we examine the attack surfaces of a networked embedded system, composed of devices representative of those typically used in the IIoT field. We carry on an analysis of the current state of the security of IIoT technologies. The analysis guides the identification of a set of attack vectors for the examined networked embedded system. We set up the corresponding concrete attack scenarios to gain control of the system actuators and perform some hazardous operations. In particular, we propose a couple of variations of Mirai attack specifically tailored for attacking industrial environments. Finally, we discuss some possible",2020,444
Socio-technical Constraints and Affordances of Virtual Collaboration - A Study of Four Online Hackathons,"Mendes, Wendy and Richard, Albert and Tillo, T\""{a}he-Kai and Pinto, Gustavo and Gama, Kiev and Nolte, Alexander","Hackathons and similar time-bounded events have become a popular form of collaboration in various domains. They are commonly organized as in-person events during which teams engage in intense collaboration over a short period of time to complete a project that is of interest to them. Most research to date has thus consequently focused on studying how teams collaborate in a co-located setting, pointing towards the advantages of radical co-location. The global pandemic of 2020, however, has led to many hackathons moving online, which challenges our current understanding of how they function. In this paper, we address this gap by presenting findings from a multiple-case study of 10 hackathon teams that participated in 4 hackathon events across two continents. By analyzing the collected data, we found that teams merged synchronous and asynchronous means of communication to maintain a common understanding of work progress as well as to maintain awareness of each other's tasks. Task division was self-assigned based on individual skills or interests, while leaders emerged from different strategies (e.g., participant experience, the responsibility of registering the team in an event). Some of the affordances of in-person hackathons, such as the radical co-location of team members, could be partially reproduced in teams that kept open synchronous communication channels while working (i.e., shared audio territories), in a sort of ""radical virtual co-location"". However, others, such as interactions with other teams, easy access to mentors, and networking with other participants, decreased. In addition, the technical constraints of the different communication tools and platforms brought technical problems and were overwhelming to participants. Our work contributes to understanding the virtual collaboration of small teams in the context of online hackathons and how technologies and event structures proposed by organizers imply this collaboration.",2022,445
SIGDOC '23: Proceedings of the 41st ACM International Conference on Design of Communication,,,2023,446
AI-Tutoring in Software Engineering Education,"Frankford, Eduard and Sauerwein, Clemens and Bassner, Patrick and Krusche, Stephan and Breu, Ruth","With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.",2024,447
"Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?","Guo, Zhaoqiang and Liu, Shiran and Liu, Xutong and Lai, Wei and Ma, Mingliang and Zhang, Xu and Ni, Chao and Yang, Yibiao and Li, Yanhui and Chen, Lin and Zhou, Guoqiang and Zhou, Yuming","Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach.",2023,448
Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality,"Li, Jiawei and Ahmed, Iftekhar","Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].",2023,449
Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications,"Gamage, Dilrukshi and Ghasiya, Piyush and Bonagiri, Vamshi and Whiting, Mark E. and Sasahara, Kazutoshi","Deepfakes are synthetic content generated using advanced deep learning and AI technologies. The advancement of technology has created opportunities for anyone to create and share deepfakes much easier. This may lead to societal concerns based on how communities engage with it. However, there is limited research available to understand how communities perceive deepfakes. We examined deepfake conversations on Reddit from 2018 to 2021—including major topics and their temporal changes as well as implications of these conversations. Using a mixed-method approach—topic modeling and qualitative coding, we found 6,638 posts and 86,425 comments discussing concerns of the believable nature of deepfakes and how platforms moderate them. We also found Reddit conversations to be pro-deepfake and building a community that supports creating and sharing deepfake artifacts and building a marketplace regardless of the consequences. Possible implications derived from qualitative codes indicate that deepfake conversations raise societal concerns. We propose that there are implications for Human Computer Interaction (HCI) to mitigate the harm created from deepfakes.",2022,450
P-Verifier: Understanding and Mitigating Security Risks in Cloud-based IoT Access Policies,"Jin, Ze and Xing, Luyi and Fang, Yiwei and Jia, Yan and Yuan, Bin and Liu, Qixu","Modern IoT device manufacturers are taking advantage of the managed Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) IoT clouds (e.g., AWS IoT, Azure IoT) for secure and convenient IoT development/deployment. The IoT access control is achieved by manufacturer-specified, cloud-enforced IoT access policies (cloud-standard JSON documents, called IoT Policies) stating which users can access which IoT devices/resources under what constraints. In this paper, we performed a systematic study on the security of cloud-based IoT access policies on modern PaaS/IaaS IoT clouds. Our research shows that the complexity in the IoT semantics and enforcement logic of the policies leaves tremendous space for device manufacturers to program a flawed IoT access policy, introducing convoluted logic flaws which are non-trivial to reason about. In addition to challenges/mistakes in the design space, it is astonishing to find that mainstream device manufacturers also generally make critical mistakes in deploying IoT Policies thanks to the flexibility offered by PaaS/IaaS clouds and the lack of standard practices for doing so. Our assessment of 36 device manufacturers and 310 open-source IoT projects highlights the pervasiveness and seriousness of the problems, which once exploited, can have serious impacts on IoT users' security, safety, and privacy. To help manufacturers identify and easily fix IoT Policy flaws, we introduce P-Verifier, a formal verification tool that can automatically verify cloud-based IoT Policies. With evaluated high effectiveness and low performance overhead, P-Verifier will contribute to elevating security assurance in modern IoT deployments and access control. We responsibly reported all findings to affected vendors and fixes were deployed or on the way.",2022,451
How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment,"Kazemitabaar, Majeed and Hou, Xinying and Henley, Austin and Ericson, Barbara Jane and Weintrop, David and Grossman, Tovi","As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners’ utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners’ use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.",2024,452
Using Incremental Many-to-One Queries to Build a Fast and Tight Heuristic for A* in Road Networks,"Strasser, Ben and Zeitz, Tim","We study exact, efficient, and practical algorithms for route planning applications in large road networks. On the one hand, such algorithms should be able to answer shortest path queries within milliseconds. On the other hand, routing applications often require integrating the current traffic situation, planning ahead with predictions for future traffic, respecting forbidden turns, and many other features depending on the specific application. Therefore, such algorithms must be flexible and able to support a variety of problem variants. In this work, we revisit the A* algorithm to build a simple, extensible, and unified algorithmic framework applicable to many route planning problems. A* has been previously used for routing in road networks. However, its performance was not competitive because no sufficiently fast and tight distance estimation function was available. We present a novel, efficient, and accurate A* heuristic using Contraction Hierarchies, another popular speedup technique. The core of our heuristic is a new Contraction Hierarchies query algorithm called Lazy RPHAST, which can efficiently compute shortest distances from many incrementally provided sources toward a common target. Additionally, we describe A* optimizations to accelerate the processing of low-degree vertices, which are typical in road networks, and present a new pruning criterion for symmetrical bidirectional A*. An extensive experimental study confirms the practicality of our approach for many applications.",2023,453
WCCCE '24: Proceedings of the 26th Western Canadian Conference on Computing Education,,,2024,454
Acoustically Aware Robots: Detecting and Evaluating Sounds Robots Make and Hear,"Goedicke, David and Tennent, Hamish and Moore, Dylan and Ju, Wendy","The sound a robot or automated system makes and the sounds it listens for in our shared acoustic environment can greatly expand its contextual understanding and to shape its behaviors to the interactions it is trying to perform.People convey significant information with sound in interpersonal communication in social contexts. Para-linguistic information about where we are, how loud we're speaking, or if we sound happy, sad or upset are relevant to understand for a robot that looks to adapt its interactions to be socially appropriate.Similarly, the qualities of the sound an object makes can change how people perceive that object and can alter whether or not it attracts attention, interrupts other interactions, reinforces or contradicts an emotional expression, and as such should be aligned with the designer's intention for the object. In this tutorial, we will introduce the participants to software and design methods to help robots recognize and generate sound for human-robot interaction (HRI). Using open-source tools and methods designers can apply to their own robots, we seek to increase the application of sound to robot design and stimulate HRI research in robot sound.",2021,455
Pair programming conversations with agents vs. developers: challenges and opportunities for SE community,"Robe, Peter and Kuttal, Sandeep K. and AuBuchon, Jake and Hart, Jacob","Recent research has shown feasibility of an interactive pair-programming conversational agent, but implementing such an agent poses three challenges: a lack of benchmark datasets, absence of software engineering specific labels, and the need to understand developer conversations. To address these challenges, we conducted a Wizard of Oz study with 14 participants pair programming with a simulated agent and collected 4,443 developer-agent utterances. Based on this dataset, we created 26 software engineering labels using an open coding process to develop a hierarchical classification scheme. To understand labeled developer-agent conversations, we compared the accuracy of three state-of-the-art transformer-based language models, BERT, GPT-2, and XLNet, which performed interchangeably. In order to begin creating a developer-agent dataset, researchers and practitioners need to conduct resource intensive Wizard of Oz studies. Presently, there exists vast amounts of developer-developer conversations on video hosting websites. To investigate the feasibility of using developer-developer conversations, we labeled a publicly available developer-developer dataset (3,436 utterances) with our hierarchical classification scheme and found that a BERT model trained on developer-developer data performed ~10% worse than the BERT trained on developer-agent data, but when using transfer-learning, accuracy improved. Finally, our qualitative analysis revealed that developer-developer conversations are more implicit, neutral, and opinionated than developer-agent conversations. Our results have implications for software engineering researchers and practitioners developing conversational agents.",2022,456
Characterizing search activities on stack overflow,"Liu, Jiakun and Baltes, Sebastian and Treude, Christoph and Lo, David and Zhang, Yun and Xia, Xin","To solve programming issues, developers commonly search on Stack Overflow to seek potential solutions. However, there is a gap between the knowledge developers are interested in and the knowledge they are able to retrieve using search engines. To help developers efficiently retrieve relevant knowledge on Stack Overflow, prior studies proposed several techniques to reformulate queries and generate summarized answers. However, few studies performed a large-scale analysis using real-world search logs. In this paper, we characterize how developers search on Stack Overflow using such logs. By doing so, we identify the challenges developers face when searching on Stack Overflow and seek opportunities for the platform and researchers to help developers efficiently retrieve knowledge. To characterize search activities on Stack Overflow, we use search log data based on requests to Stack Overflow's web servers. We find that the most common search activity is reformulating the immediately preceding queries. Related work looked into query reformulations when using generic search engines and found 13 types of query reformulation strategies. Compared to their results, we observe that 71.78% of the reformulations can be fitted into those reformulation strategies. In terms of how queries are structured, 17.41% of the search sessions only search for fragments of source code artifacts (e.g., class and method names) without specifying the names of programming languages, libraries, or frameworks. Based on our findings, we provide actionable suggestions for Stack Overflow moderators and outline directions for future research. For example, we encourage Stack Overflow to set up a database that includes the relations between all computer programming terminologies shared on Stack Overflow, e.g., method name, data structure name, design pattern, and IDE name. By doing so, Stack Overflow could improve the performance of search engines by considering related programming terminologies at different levels of granularity.",2021,457
Towards detecting device fingerprinting on iOS with API function hooking,"Heid, Kris and Andrae, Vincent and Heider, Jens","Device fingerprinting is a technique that got popular at the end of the 90s by websites, to identify and track users. One of the biggest drivers behind such practices are advertising companies to identify users interests to personalize ads. From a user’s perspective, this, of course, raises privacy concerns. While device fingerprinting and its detection has been extensively studied in the context of web browsing, little research has been conducted on device fingerprinting in mobile apps and especially iOS apps. In this paper, we capture the current state of device fingerprinting in iOS apps, and explore possible approaches for fingerprinting detection on mobile devices using static and dynamic app analysis techniques. Finally, we present a first heuristic approach for automatic behavior-based fingerprinting detection on iOS only using spatial and temporal context of relevant API-calls.",2023,458
Mechanising and verifying the WebAssembly specification,"Watt, Conrad","WebAssembly is a new low-level language currently being implemented in all major web browsers. It is designed to become the universal compilation target for the web, obsoleting existing solutions in this area, such as asm.js and Native Client. The WebAssembly working group has incorporated formal techniques into the development of the language, but their efforts so far have focussed on pen and paper formal specification.We present a mechanised Isabelle specification for the WebAssembly language, together with a verified executable interpreter and type checker. Moreover, we present a fully mechanised proof of the soundness of the WebAssembly type system, and detail how our work on this proof has exposed several issues with the official WebAssembly specification, influencing its development. Finally, we give a brief account of our efforts in performing differential fuzzing of our interpreter against industry implementations.",2018,459
Verifying Data Constraint Equivalence in FinTech Systems,"Wang, Chengpeng and Fan, Gang and Yao, Peisen and Pan, Fuxiong and Zhang, Charles","Data constraints are widely used in FinTech systems for monitoring data consistency and diagnosing anomalous data manipulations. However, many equivalent data constraints are created redundantly during the development cycle, slowing down the FinTech systems and causing unnecessary alerts. We present EqDAC, an efficient decision procedure to determine the data constraint equivalence. We first propose the symbolic representation for semantic encoding and then introduce two light-weighted analyses to refute and prove the equivalence, respectively, which are proved to achieve in polynomial time. We evaluate EqDAC upon 30,801 data constraints in a FinTech system. It is shown that EqDAC detects 11,538 equivalent data constraints in three hours. It also supports efficient equivalence searching with an average time cost of 1.22 seconds, enabling the system to check new data constraints upon submission.",2023,460
Scaling open source communities: an empirical study of the Linux kernel,"Tan, Xin and Zhou, Minghui and Fitzgerald, Brian","Large-scale open source communities, such as the Linux kernel, have gone through decades of development, substantially growing in scale and complexity. In the traditional workflow, maintainers serve as ""gatekeepers"" for the subsystems that they maintain. As the number of patches and authors significantly increases, maintainers come under considerable pressure, which may hinder the operation and even the sustainability of the community. A few subsystems have begun to use new workflows to address these issues. However, it is unclear to what extent these new workflows are successful, or how to apply them. Therefore, we conduct an empirical study on the multiple-committer model (MCM) that has provoked extensive discussion in the Linux kernel community. We explore the effect of the model on the i915 subsystem with respect to four dimensions: pressure, latency, complexity, and quality assurance. We find that after this model was adopted, the burden of the i915 maintainers was significantly reduced. Also, the model scales well to allow more committers. After analyzing the online documents and interviewing the maintainers of i915, we propose that overloaded subsystems which have trustworthy candidate committers are suitable for adopting the model. We further suggest that the success of the model is closely related to a series of measures for risk mitigation---sufficient precommit testing, strict review process, and the use of tools to simplify work and reduce errors. We employ a network analysis approach to locate candidate committers for the target subsystems and validate this approach and contextual success factors through email interviews with their maintainers. To the best of our knowledge, this is the first study focusing on how to scale open source communities. We expect that our study will help the rapidly growing Linux kernel and other similar communities to adapt to changes and remain sustainable.",2020,461
CC2Vec: distributed representations of code changes,"Hoang, Thong and Kang, Hong Jin and Lo, David and Lawall, Julia","Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.",2020,462
Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming,"Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi","AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex.",2023,463
A Literature Survey on Open Source Large Language Models,"Kukreja, Sanjay and Kumar, Tarun and Purohit, Amit and Dasgupta, Abhijit and Guha, Debashis","Since the 1950s, post the Turing test, humans have been striving hard to make machines learn the art of mastering linguistic intelligence. Language being a complex and intricate tool of expression used by humans, poses a large number of challenges for AI enabled algorithms to grasp its understanding in entirety. Over the past few years, a chain of efforts have been made to make machines understand linguistic intricacies. Small scale models such as BERT and pre-trained language models (PLMs) have demonstrated strong capabilities in understanding and solving various language based tasks. Over the period of years, it is also observed that by increasing the parameters scale to larger size, large language models show a significant improvement in performance and showcase abilities to understand context. For the PLMs of a humongous size i.e in the tune of tens or hundreds of billions of parameters, and to understand the large parametric scales, the scientific community introduced the term LLMs - large language models. The whole world witnessed the launch and quick adoption of ChatGPT, an AI chatbot built on LLMs. As the usage of AI algorithms changes the way the scientific community, society and industry works, it is imperative to review the advances of LLMs. Since 2022, almost daily nearly a dozen LLMs are released. These LLMs are categorized as open and closed source. This paper aims to focus on major aspects of open source LLMs - pre-training covering data collection and pre-processing, model architecture and training. We will select open source models released in June, July and August 2023 with training parameters greater than 70 billion parameters and provide a comprehensive survey on the mentioned aspects. As new models are released on daily / weekly basis in the LLM space, in order to keep the survey concise and targeted to important models, we chose to select time-box of 3 months and a large parameter range of 70 billion in our literature survey. We will also cover historical evolution of LLMs and list open items for future directions.",2024,464
A hybrid anonymization pipeline to improve the privacy-utility balance in sensitive datasets for ML purposes,"Verdonck, Jenno and De Boeck, Kevin and Willocx, Michiel and Lapon, Jorn and Naessens, Vincent","The modern world is data-driven. Businesses increasingly take strategic decisions based on customer data, and companies are founded with a sole focus of performing machine-learning driven data analytics for third parties. External data sources containing sensitive records are often required to build qualitative machine learning models and, hence, perform accurate and meaningful predictions. However, exchanging sensitive datasets is no sinecure. Personal data must be managed according to privacy regulation. Similarly, loss of strategic data can negatively impact the competitiveness of a company. In both cases, dataset anonymization can overcome the aforementioned obstacles. This work proposes a hybrid anonymization pipeline combining masking and (intelligent) sampling to improve the privacy-utility balance of anonymized datasets. The approach is validated via in-depth experiments on a representative machine learning scenario. A quantitative privacy assessment of the proposed hybrid anonymization pipeline is performed and relies on two well-known privacy metrics, namely re-identification risk and certainty. Furthermore, this work shows that the utility level of the anonymized dataset remains acceptable, and that the overall privacy-utility balance increases when complementing masking with intelligent sampling. The study further restrains the common misconception that dataset anonymization is detrimental to the quality of machine learning models. The empirical study shows that anonymous datasets – generated by the hybrid anonymization pipeline – can compete with the original (identifiable) ones when they are used as input for training a machine learning model.",2023,465
"Socio-technical Affordances for Stigmergic Coordination Implemented in MIDST, a Tool for Data-Science Teams","Crowston, Kevin and Saltz, Jeff S. and Rezgui, Amira and Hegde, Yatish and You, Sangseok","We present a conceptual framework for socio-technical affordances for stigmergic coordination, that is, coordination supported by a shared work product. Based on research on free/libre open source software development, we theorize that stigmergic coordination depends on three sets of socio-technical affordances: the visibility and combinability of the work, along with defined genres of work contributions. As a demonstration of the utility of the developed framework, we use it as the basis for the design and implementation of a system, MIDST, that supports these affordances and that we thus expect to support stigmergic coordination. We describe an initial assessment of the impact of the tool on the work of project teams of three to six data-science students that suggests that the tool was useful but also in need of further development. We conclude with plans for future research and an assessment of theory-driven system design.",2019,466
"Social machines in practice: solutions, stakeholders and scopes","Hooper, Clare J. and Bailey, Brian and Glaser, Hugh and Hendler, James","This paper frames social machines as problem solving entities, demonstrating how their ecosystems address multiple stakeholders' problems. It enumerates aspects relevant to the theory and real-world practice of social machines, based on qualitative observations from our experiences building them. We frame evolving issues including: changing functionality, users, data and context; geographical and temporal scope (considering data granularity and visibility); and social scope. The latter is wide-ranging, including motivation, trust, experience, security, governance, control, provenance, privacy and law. We provide suggestions about building flexibility into social machines to allow for change, and defining social machines in terms of problems and stakeholders.",2016,467
"INFless: a native serverless system for low-latency, high-throughput inference","Yang, Yanan and Zhao, Laiping and Li, Yiming and Zhang, Huanyu and Li, Jie and Zhao, Mingyang and Chen, Xingzhen and Li, Keqiu","Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services. While simply ”patching” general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-the-art systems by 2\texttimes{}-5\texttimes{} on system throughput, meeting the latency goals of ML services.",2022,468
Towards a cognizant virtual software modeling assistant using model clones,"Stephan, Matthew","We present our new ideas on taking the first steps towards cultivating synergy between model-driven engineering (MDE), machine learning, and software clones. Specifically, we describe our vision in realizing a cognizant virtual software modeling assistant that uses the latter two to improve software design and MDE. Software engineering has benefited greatly from knowledge-based cognizant source code completion and assistance, but MDE has few and limited analogous capabilities. We outline our research directions by describing our vision for a prototype assistant that provides suggestions to modelers performing model creation or extension in the form of 1) complete models for insertion or guidance, and 2) granular single-step operations. These suggestions are derived by detecting clones of the in-progress model and existing domain, organizational, and exemplar models. We overview our envisioned workflow between modeler and assistant, and, using Simulink as an example, illustrate different manifestations including multiple overlays with percentages and employing variant elements.",2019,469
FRaMED: full-fledge role modeling editor (tool demo),"K\""{u}hn, Thomas and Bierzynski, Kay and Richly, Sebastian and A\ss{}mann, Uwe","Since the year 1977, role modeling has been continuously investigated as promising paradigm to model complex, dynamic systems. However, this research had almost no influence on the design of todays increasingly complex and context-sensitive software systems. The reason for that is twofold. First, most modeling languages focused either on the behavioral, relational or context-dependent nature of roles rather than combining them. Second, there is a lack of tool support for the design, validation, and generation of role-based software systems. In particular, there exists no graphical role modeling editor supporting the three natures as well as the various proposed constraints. To overcome this deficiency, we introduce the Full-fledged Role Modeling Editor (FRaMED), a graphical modeling editor embracing all natures of roles and modeling constraints featuring generators for a formal representation and source code of a role-based programming language. To show its applicability for the development of role-based software systems, an example from the banking domain is employed.",2016,470
Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution,"Dilhara, Malinda and Ketkar, Ameya and Dig, Danny","Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries.We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings.Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.",2021,471
How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project,"Zhang, Yuxia and Qin, Mian and Stol, Klaas-Jan and Zhou, Minghui and Liu, Hui","It is now commonplace for organizations to pay developers to work on specific open source software (OSS) projects to pursue their business goals. Such paid developers work alongside voluntary contributors, but given the different motivations of these two groups of developers, conflict may arise, which may pose a threat to a project's sustainability. This paper presents an empirical study of paid developers and volunteers in Rust, a popular open source programming language project. Rust is a particularly interesting case given considerable concerns about corporate participation. We compare volunteers and paid developers through contribution characteristics and long-term participation, and solicit volunteers' perceptions on paid developers. We find that core paid developers tend to contribute more frequently; commits contributed by onetime paid developers have bigger sizes; peripheral paid developers implement more features; and being paid plays a positive role in becoming a long-term contributor. We also find that volunteers do have some prejudices against paid developers. This study suggests that the dichotomous view of paid vs. volunteer developers is too simplistic and that further subgroups can be identified. Companies should become more sensitive to how they engage with OSS communities, in certain ways as suggested by this study.",2024,472
Finding broken promises in asynchronous JavaScript programs,"Alimadadi, Saba and Zhong, Di and Madsen, Magnus and Tip, Frank","Recently, promises were added to ECMAScript 6, the JavaScript standard, in order to provide better support for the asynchrony that arises in user interfaces, network communication, and non-blocking I/O. Using promises, programmers can avoid common pitfalls of event-driven programming such as event races and the deeply nested counterintuitive control ow referred to as “callback hell”. Unfortunately, promises have complex semantics and the intricate control– and data- ow present in promise-based code hinders program comprehension and can easily lead to bugs. The promise graph was proposed as a graphical aid for understanding and debugging promise-based code. However, it did not cover all promise-related features in ECMAScript 6, and did not present or evaluate any technique for constructing the promise graphs. In this paper, we extend the notion of promise graphs to include all promise-related features in ECMAScript 6, including default reactions, exceptions, and the synchronization operations race and all. Furthermore, we report on the construction and evaluation of PromiseKeeper, which performs a dynamic analysis to create promise graphs and infer common promise anti-patterns. We evaluate PromiseKeeper by applying it to 12 open source promise-based Node.js applications. Our results suggest that the promise graphs constructed by PromiseKeeper can provide developers with valuable information about occurrences of common anti-patterns in their promise-based code, and that promise graphs can be constructed with acceptable run-time overhead.",2018,473
Modern Code Reviews—Survey of Literature and Practice,"Badampudi, Deepika and Unterkalmsteiner, Michael and Britto, Ricardo","Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners’ opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues.Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners’ perception of the relevance of MCR research, and analyzed the primary studies’ research impact.Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor– and support systems–related research.Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research.",2023,474
"Gaming Ecosystems for Education and Research: Where Artificial Intelligence meets with Software Engineering, at scale","Kalles, Dimitrios and Giagtzoglou, Kyriakos and Mitropoulos, Konstantinos","We present aspects of ecosystem engineering for a strategy board game. Human and machine players of the ecosystem can pick opponents or form teams and play against other teams or players. We present the key features of the ecosystem, the highlights of the development process and we propose concrete potential uses of the ecosystem in research and education.",2020,475
Towards flexible runtime monitoring support for ROS-based applications,"Stadler, Marco and Vierhauser, Michael and Cleland-Huang, Jane","Robotic systems are becoming common in different domains and for various purposes, such as unmanned aerial vehicles performing search and rescue operations, or robots operating in manufacturing plants. Such systems are characterized by close interactions, or even collaborations, between hardware and machinery on the one hand, and humans on the other. Furthermore, as Cyber-Physical Systems (CPS) in general and robotic applications in particular typically operate in an emergent environment, unanticipated events may occur during their operation, making the need for runtime monitoring support a crucial yet often time-consuming task. Runtime monitoring typically requires establishing support for collecting data, aggregating and transporting the data to a monitoring framework for persistence and further processing, and finally, performing checks of functional and non-functional properties. In this paper, we present our initial efforts towards a flexible monitoring framework for ROS-based systems. We report on challenges for establishing runtime monitoring support and present our preliminary architecture that aims to significantly reduce the setup and maintenance effort when creating monitors and establishing constraint checks.",2023,476
Cantonese to Written Chinese Translation via HuggingFace Translation Pipeline,"Kwok, Raptor Yick-Kan and Au Yeung, Siu-Kei and Li, Zongxi and Hung, Kevin","Cantonese, a low-resource language [5] that has been used in Southeastern China for hundreds of years, with over 85 million native speakers worldwide, is poorly supported in the mainstream language model for existing translation platforms such as Baidu, Google and Bing. This paper presents a large parallel corpus of 130 thousand Cantonese and Written Chinese pairs. The data are used to train a translation model using the translation pipeline of the Hugging Face Transformers architecture, a dominant architecture for natural language processing nowadays [18]. The BLEU score and manual assessment evaluate the performance. The translation results achieve a BLEU score of 41.35and chrF++ score of 44.88on the entire validation set. The model also works reasonably well with long sentences of over 20 Chinese characters. It achieves a BLEU score of 48.61and chrF++ score of 39.87on long sentences. Those results are comparable with the existing Baidu Fanyi and Bing Translate. We also establish a Cantonese sentence evaluation metric to classify the quality of the source Cantonese sentence by professional translators. We then compare the BLEU and chrF++ scores with the corresponding evaluation score and found that the better the quality of the source sentence, the higher the BLEU and chrF++ scores. Last, we proved that our corpus enabled the Cantonese translation capability of the Chinese BART pre-trained model.",2024,477
On the Reliability and Explainability of Language Models for Program Generation,"Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Li, Li","Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises a question: are these techniques sufficiently trustworthy for automated program generation? Consequently, further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing overoptimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.",2024,478
ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings,,"ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.",2024,479
Detection of algorithmically generated domain names used by botnets: a dual arms race,"Spooren, Jan and Preuveneers, Davy and Desmet, Lieven and Janssen, Peter and Joosen, Wouter","Malware typically uses Domain Generation Algorithms (DGAs) as a mechanism to contact their Command and Control server. In recent years, different approaches to automatically detect generated domain names have been proposed, based on machine learning. The first problem that we address is the difficulty to systematically compare these DGA detection algorithms due to the lack of an independent benchmark. The second problem that we investigate is the difficulty for an adversary to circumvent these classifiers when the machine learning models backing these DGA-detectors are known. In this paper we compare two different approaches on the same set of DGAs: classical machine learning using manually engineered features and a 'deep learning' recurrent neural network. We show that the deep learning approach performs consistently better on all of the tested DGAs, with an average classification accuracy of 98.7% versus 93.8% for the manually engineered features. We also show that one of the dangers of manual feature engineering is that DGAs can adapt their strategy, based on knowledge of the features used to detect them. To demonstrate this, we use the knowledge of the used feature set to design a new DGA which makes the random forest classifier powerless with a classification accuracy of 59.9%. The deep learning classifier is also (albeit less) affected, reducing its accuracy to 85.5%.",2019,480
ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension,,ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.,2024,481
Deceiving Portable Executable Malware Classifiers into Targeted Misclassification with Practical Adversarial Examples,"Kucuk, Yunus and Yan, Guanhua","Due to voluminous malware attacks in the cyberspace, machine learning has become popular for automating malware detection and classification. In this work we play devil's advocate by investigating a new type of threats aimed at deceiving multi-class Portable Executable (PE) malware classifiers into targeted misclassification with practical adversarial samples. Using a malware dataset with tens of thousands of samples, we construct three types of PE malware classifiers, the first one based on frequencies of opcodes in the disassembled malware code (opcode classifier), the second one the list of API functions imported by each PE sample (API classifier), and the third one the list of system calls observed in dynamic execution (system call classifier). We develop a genetic algorithm augmented with different support functions to deceive these classifiers into misclassifying a PE sample into any target family. Using an Rbot malware sample whose source code is publicly available, we are able to create practical adversarial samples that can deceive the opcode classifier into targeted misclassification with a successful rate of 75%, the API classifier with a successful rate of 83.3%, and the system call classifier with a successful rate of 91.7%.",2020,482
N-gram and Word2Vec Feature Engineering Approaches for Spam Recognition on Some Influential Twitter Topics in Saudi Arabia,"Balfagih, Ahmed and Keselj, Vlado and Taylor, Stacey","Social media platforms, such as Twitter, have become powerful sources of information on people's perception of major events. Many people use Twitter to express their views on various issues and events and use it to develop their opinion on the diverse economic, political, technical, and social occurrences related to their daily lives. Spam and non-relevant tweets are a major challenge for Twitter trend detection. Saudi Arabia is a top ranked country in Twitter usage worldwide, and in recent years has experienced difficulties due to the use and rise of hashtags based on misleading tweets and spam. The goal of this paper is to apply machine learning techniques to identify spam on the Saudi tweets collected to the end of 2020. To date, spam detection on Twitter data has been mostly done in English, leaving other major languages, such as Arabic, insufficiently covered. Additionally, publicly accessible Arabic Twitter datasets are hard to find. For our research, we use eight Twitter datasets on some significant topics in politics, health, national affairs, economy, and sport, to train and evaluate different machine learning algorithms, with a focus on two feature generation techniques based on N-grams and Word2Vec embeddings. One contribution of this paper is providing these new labelled datasets with embeddings. The experimental results show improvement from using embeddings over N-grams in more balanced datasets vs. more unbalanced ones. We also find a superior performance of the Random Forest algorithm over other algorithms in most experiments.",2022,483
Explaining the Wait: How Justifying Chatbot Response Delays Impact User Trust,"Zhang, Zhengquan and Tsiakas, Konstantinos and Schneegass, Christina","In human communication, responding to a question very slowly or quickly influences our trust in the answer. As chatbots evolve to increasingly mimic human speech, response speed can be artificially varied to create certain impressions on users. However, studies remain inconclusive, potentially due to the absence of contextual cues that allow for interpretation of the delay. Thus, this study explores textual explanations that justify the instant and dynamic – dependent on answer length – response delays. We derive five design variations based on prior work and evaluate their impact on the chatbot’s perceived social presence and transparency (N = 10). In a between-subject online study (N = 194), we then evaluate the influence of the highest-rated justification on users’ perceptions of chatbot transparency, social presence, and trust for the two delay conditions. Results demonstrate that while such justifications enhance perceived transparency and trust in the immediate response scenario, they show no effect in the dynamic delay context.",2024,484
ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training,,,2024,485
A Kubernetes Framework for Learning Cloud Native Development,"Reppert, Austin and Montecinos-Velazquez, Brian and Kahl, Harrison and Reid, Rackeem and Rivas, Danielle and Spampinato, Dominic and Zhong, Hudson and Ngo, Linh B.","This work describes a Kubernetes framework that can be automatically deployed on CloudLab, a federal cloud resource, to support learning activities in cloud computing education. The framework enables instructors and students to study cloud services' full product development life-cycle, including aspects such as automated deployment, availability, and security. This framework is easy to deploy, is freely available to academic institutions, and can be extended to support more advanced learning scenarios. The effectiveness of this framework is demonstrated through two complex and full-stack student projects.",2023,486
ACSW '23: Proceedings of the 2023 Australasian Computer Science Week,,,2023,487
PCI '23: Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics,,,2023,488
How do companies collaborate in open source ecosystems? an empirical study of OpenStack,"Zhang, Yuxia and Zhou, Minghui and Stol, Klaas-Jan and Wu, Jianyu and Jin, Zhi","Open Source Software (OSS) has come to play a critical role in the software industry. Some large ecosystems enjoy the participation of large numbers of companies, each of which has its own focus and goals. Indeed, companies that otherwise compete, may become collaborators within the OSS ecosystem they participate in. Prior research has largely focused on commercial involvement in OSS projects, but there is a scarcity of research focusing on company collaborations within OSS ecosystems. Some of these ecosystems have become critical building blocks for organizations worldwide; hence, a clear understanding of how companies collaborate within large ecosystems is essential. This paper presents the results of an empirical study of the OpenStack ecosystem, in which hundreds of companies collaborate on thousands of project repositories to deliver cloud distributions. Based on a detailed analysis, we identify clusters of collaborations, and identify four strategies that companies adopt to engage with the OpenStack ecosystem. We alsofind that companies may engage in intentional or passive collaborations, or may work in an isolated fashion. Further, wefi nd that a company's position in the collaboration network is positively associated with its productivity in OpenStack. Our study sheds light on how large OSS ecosystems work, and in particular on the patterns of collaboration within one such large ecosystem.",2020,489
Unhelpful Assumptions in Software Security Research,"Ryan, Ita and Roedig, Utz and Stol, Klaas-Jan","In the study of software security many factors must be considered. Once venturing beyond the simplest of laboratory experiments, the researcher is obliged to contend with exponentially complex conditions. Software security has been shown to be affected by priming, tool usability, library documentation, organisational security culture, the content and format of internet resources, IT team and developer interaction, Internet search engine ordering, developer personality, security warning placement, mentoring, developer experience and more. In a systematic review of software security papers published since 2016, we have identified a number of unhelpful assumptions that are commonly made by software security researchers. In this paper we list these assumptions, describe why they sometimes do not reflect reality, and suggest implications for researchers.",2023,490
EICS '23 Companion: Companion Proceedings of the 2023 ACM SIGCHI Symposium on Engineering Interactive Computing Systems,,,2023,491
Open Source Software Sustainability: Combining Institutional Analysis and Socio-Technical Networks,"Yin, Likang and Chakraborti, Mahasweta and Yan, Yibo and Schweik, Charles and Frey, Seth and Filkov, Vladimir","Sustainable Open Source Software (OSS) forms much of the fabric of our digital society, especially successful and sustainable ones. But many OSS projects do not become sustainable, resulting in abandonment and even risks for the world's digital infrastructure. Prior work has looked at the reasons for this mainly from two very different perspectives. In software engineering, the focus has been on understanding success and sustainability from the socio-technical perspective: the OSS programmers' day-to-day activities and the artifacts they create. In institutional analysis, on the other hand, emphasis has been on institutional designs (e.g., policies, rules, and norms) that structure project governance. Even though each is necessary for a comprehensive understanding of OSS projects, the connection and interaction between the two approaches have been barely explored.In this paper, we make the first effort toward understanding OSS project sustainability using a dual-view analysis, by combining institutional analysis with socio-technical systems analysis. In particular, we (i) use linguistic approaches to extract institutional rules and norms from OSS contributors' communications to represent the evolution of their governance systems, and (ii) construct socio-technical networks based on longitudinal collaboration records to represent each project's organizational structure. We combined the two methods and applied them to a dataset of developer digital traces from 253 nascent OSS projects within the Apache Software Foundation (ASF) incubator. We find that the socio-technical and institutional features relate to each other, and provide complimentary views into the progress of the ASF's OSS projects. Refining these combined analyses can help provide a more precise understanding of the synchronization between the evolution of institutional governance and organizational structure.",2022,492
Ex Pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network,"Liu, Zhe and Chen, Chunyang and Wang, Junjie and Su, Yuhui and Huang, Yuekai and Hu, Jun and Wang, Qing","Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.",2023,493
Dynamic Malware Analysis in the Modern Era—A State of the Art Survey,"Or-Meir, Ori and Nissim, Nir and Elovici, Yuval and Rokach, Lior","Although malicious software (malware) has been around since the early days of computers, the sophistication and innovation of malware has increased over the years. In particular, the latest crop of ransomware has drawn attention to the dangers of malicious software, which can cause harm to private users as well as corporations, public services (hospitals and transportation systems), governments, and security institutions. To protect these institutions and the public from malware attacks, malicious activity must be detected as early as possible, preferably before it conducts its harmful acts. However, it is not always easy to know what to look for—especially when dealing with new and unknown malware that has never been seen. Analyzing a suspicious file by static or dynamic analysis methods can provide relevant and valuable information regarding a file's impact on the hosting system and help determine whether the file is malicious or not, based on the method's predefined rules. While various techniques (e.g., code obfuscation, dynamic code loading, encryption, and packing) can be used by malware writers to evade static analysis (including signature-based anti-virus tools), dynamic analysis is robust to these techniques and can provide greater understanding regarding the analyzed file and consequently can lead to better detection capabilities. Although dynamic analysis is more robust than static analysis, existing dynamic analysis tools and techniques are imperfect, and there is no single tool that can cover all aspects of malware behavior. The most recent comprehensive survey performed in this area was published in 2012. Since that time, the computing environment has changed dramatically with new types of malware (ransomware, cryptominers), new analysis methods (volatile memory forensics, side-channel analysis), new computing environments (cloud computing, IoT devices), new machine-learning algorithms, and more. The goal of this survey is to provide a comprehensive and up-to-date overview of existing methods used to dynamically analyze malware, which includes a description of each method, its strengths and weaknesses, and its resilience against malware evasion techniques. In addition, we include an overview of prominent studies presenting the usage of machine-learning methods to enhance dynamic malware analysis capabilities aimed at detection, classification, and categorization.",2019,494
"
Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation","Wang, Chengpeng and Wang, Wenyang and Yao, Peisen and Shi, Qingkai and Zhou, Jinguo and Xiao, Xiao and Zhang, Charles","Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours.",2023,495
FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,,,2016,496
Assessing Unsupervised Machine Learning Solutions for Anomaly Detection in Cloud Gaming Sessions,"Ky, Jo\""{e}l Roman and Mathieu, Bertrand and Lahmadi, Abdelkader and Boutaba, Raouf","Cloud gaming applications have gained great adoption on the Internet particularly benefiting from the wide availability of broadband access networks. However, they still fail to meet users' quality requirements when accessed using cellular networks due to common wireless channel degradations. Machine Learning (ML) techniques can be leveraged to detect such anomalies during users' cloud gaming sessions. In this respect, unsupervised ML approaches are particularly interesting since they do not require labeled datasets. In this work, we investigate these approaches to understand their performance and their robustness. Our dataset consists of game sessions played on the public Google Stadia Cloud Gaming servers. The game sessions are played using a 4G network emulation replicating the capacity variations sampled on a commercial 4G network. We compare different models ranging from traditional approaches to deep learning and we evaluate their default performance while varying the level of contamination in their training datasets. Our experiments show that Auto-Encoders models achieve the best performance without contamination while the OC-SVM and the Isolation Forest are the most robust to data contamination.",2023,497
Muzeel: assessing the impact of JavaScript dead code elimination on mobile web performance,"Kupoluyi, Jesutofunmi and Chaqfeh, Moumena and Varvello, Matteo and Coke, Russell and Hashmi, Waleed and Subramanian, Lakshmi and Zaki, Yasir","To quickly create interactive web pages, developers heavily rely on (large) general-purpose JavaScript libraries. This practice bloats web pages with complex unused functions dead code which are unnecessarily downloaded and processed by the browser. The identification and the elimination of these functions is an open problem, which this paper tackles with Muzeel, a black-box approach requiring neither knowledge of the code nor execution traces. While the state-of-the-art solutions stop analyzing JavaScript when the page loads, the core design principle of Muzeel is to address the challenge of dynamically analyzing JavaScript after the page is loaded, by emulating all possible user interactions with the page, such that the used functions (executed when interactivity events fire) are accurately identified, whereas unused functions are filtered out and eliminated. We run Muzeel against 15,000 popular web pages and show that half of the 300,000 JavaScript files used in these pages have at least 70% of unused functions, accounting for 55% of the files' sizes. To assess the impact of dead code elimination on Mobile Web performance, we serve 200 Muzeel-ed pages to several Android phones and browsers, under variable network conditions. Our evaluation shows that Muzeel can speed up page loads by 25-30% thanks to a combination of lower CPU and bandwidth usage. Most importantly, we show that such savings are achieved while maintaining the pages' visual appearance and interactive functionality.",2022,498
Detecting Spying and Fraud Browser Extensions: Short Paper,"Varshney, Gaurav and Misra, Manoj and Atrey, Pradeep K.","Due to the flaws in policy followed by web browsers for granting permissions to browser extensions and due to a lack of effective static and dynamic detection systems for identifying malicious extensions uploaded on the web stores, malicious browser extensions have become the easiest way to carry out phishing, spying, fraud and other kinds of advanced attacks. This paper identifies and analyzes a subset of these attacks which can be performed with the use of malicious browser extensions (using Google Chrome) and discusses the research gaps of the existing prevention and detection schemes to adequately defend against these attacks. An initial set of malicious signatures responsible for cyber fraud and spying is identified during the study. We use this set of signatures to develop a lightweight malicious extension detection system which can alert users of suspected spying or fraud extensions installed on the Chrome browser on a PC. Results show that the proposed detection system performs better than known malicious extension detectors such as Chrome Cleanup tool and Chrome safeguard tool.",2017,499
Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing,"Liu, Zhe and Chen, Chunyang and Wang, Junjie and Che, Xing and Huang, Yuekai and Hu, Jun and Wang, Qing","Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.",2023,500
Worldwide gender differences in public code contributions: and how they have been affected by the COVID-19 pandemic,"Rossi, Davide and Zacchiroli, Stefano","Gender imbalance is a well-known phenomenon observed throughout sciences which is particularly severe in software development and Free/Open Source Software communities. Little is know yet about the geography of this phenomenon in particular when considering large scales for both its time and space dimensions.We contribute to fill this gap with a longitudinal study of the population of contributors to publicly available software source code. We analyze the development history of 160 million software projects for a total of 2.2 billion commits contributed by 43 million distinct authors over a period of 50 years. We classify author names by gender using name frequencies and author geographical locations using heuristics based on email addresses and time zones. We study the evolution over time of contributions to public code by gender and by world region.For the world overall, we confirm previous findings about the low but steadily increasing ratio of contributions by female authors. When breaking down by world regions we find that the long-term growth of female participation is a world-wide phenomenon. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women's ability to contribute to public code has been more hindered than that of men.Software developers around the world work together to produce publicly available software (or public code). They do so using public identities and disclosing information about their work that include their names and when a software change was made. We use this information to characterize the gender gap in public code, that is, the difference in participation to public software development between men and women. Specifically, we study the development history of 160 million pieces of public software, developed over a period of 50 years by 43 million authors. We characterize the gender gap on this corpus over time and by world region. To determine author genders we rely on public data about name frequencies by gender around the world. To determine author locations we use email addresses, name frequencies around the world, and the timezone associated to each software change. We confirm that the gender gap in public code is huge. Female authors are only 8.1% of the total and have authored only 13.5% software versions. The gender gap is however shrinking, with women participation having increased steadily over the past 12 years. This improvement is a global phenomenon, observable in most world regions. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women have been more hindered than men in their ability to contribute to public code.",2022,501
Index/Author’s Biography,,"This book provides a comprehensive study of the many ways to interact with computers and computerized devices. An “interaction technique” starts when the user performs an action that causes an electronic device to respond, and includes the direct feedback from the device to the user. Examples include physical buttons and switches, on-screen menus and scrollbars operated by a mouse, touchscreen widgets, gestures such as flick-to-scroll, text entry on computers and touchscreens, input for virtual reality systems, interactions with conversational agents such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, and adaptations of all of these for people with disabilities. Pick, Click, Flick! is written for anyone interested in interaction techniques, including computer scientists and designers working on human-computer interaction, as well as implementers and consumers who want to understand and get the most out of their digital devices.REVIEWS“Pick, Click, Flick! is an impressive reference manual of the many years of interaction design development. It is a reference book, invaluable when questions arise, whether while you are busy designing something, or learning, or teaching, where assigning sections of the reference will be a valuable resource and learning tool for students. Brad Myers has provided a great service to the interaction community.” ‐ Don Norman, Distinguished Prof. Emeritus, Design Lab, University of California, San Diego“Every UX professional should immerse themselves in this book. Not only does it unravel the fascinating and complex history of GUI widgets that will captivate any user interface nerd, but it also stands as the definitive guide to an incredibly diverse array of interaction techniques. This is not just an engaging read; it’s an essential toolkit.” ‐ Jakob Nielsen, Principal, Nielsen Design Group",2024,502
"Trade-offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly","Kuttal, Sandeep Kaur and Ong, Bali and Kwasny, Kate and Robe, Peter","Pair programming has a documented history of benefits, such as increased code quality, productivity, self-efficacy, knowledge transfer, and reduced gender gap. Research uncovered problems with pair programming related to scheduling, collocating, role imbalance, and power dynamics. We investigated the trade-offs of substituting a human with an agent to simultaneously provide benefits and alleviate obstacles in pair programming. We conducted gender-balanced studies with human-human pairs in a remote lab with 18 programmers and Wizard-of-Oz studies with 14 programmers, then analyzed results quantitatively and qualitatively. Our comparative analysis of the two studies showed no significant differences in productivity, code quality, and self-efficacy. Further, agents facilitated knowledge transfer; however, unlike humans, agents were unable to provide logical explanations or discussions. Human partners trusted and showed humility towards agents. Our results demonstrate that agents can act as effective pair programming partners and open the way towards new research on conversational agents for programming.",2021,503
Time-travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum,"Wu, Siwei and Wu, Lei and Zhou, Yajin and Li, Runhuai and Wang, Zhi and Luo, Xiapu and Wang, Cong and Ren, Kui","Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage.In this article, we propose a scalable attack detection framework named EthScope, which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around  ( text{2,300}times )  when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.",2022,504
Predicting developers' negative feelings about code review,"Egelman, Carolyn D. and Murphy-Hill, Emerson and Kammer, Elizabeth and Hodges, Margaret Morrow and Green, Collin and Jaspan, Ciera and Lin, James","During code review, developers critically examine each others' code to improve its quality, share knowledge, and ensure conformance to coding standards. In the process, developers may have negative interpersonal interactions with their peers, which can lead to frustration and stress; these negative interactions may ultimately result in developers abandoning projects. In this mixed-methods study at one company, we surveyed 1,317 developers to characterize the negative experiences and cross-referenced the results with objective data from code review logs to predict these experiences. Our results suggest that such negative experiences, which we call ""pushback"", are relatively rare in practice, but have negative repercussions when they occur. Our metrics can predict feelings of pushback with high recall but low precision, making them potentially appropriate for highlighting interactions that may benefit from a self-intervention.",2020,505
Patterns of Effort Contribution and Demand and User Classification based on Participation Patterns in NPM Ecosystem,"Dey, Tapajit and Ma, Yuxing and Mockus, Audris","Background: Open source requires participation of volunteer and commercial developers (users) in order to deliver functional high-quality components. Developers both contribute effort in the form of patches and demand effort from the component maintainers to resolve issues reported against it. Open source components depend on each other directly and transitively, and evidence suggests that more effort is required for reporting and resolving the issues reported further upstream in this supply chain. Aim: Identify and characterize patterns of effort contribution and demand throughout the open source supply chain and investigate if and how these patterns vary with developer activity; identify different groups of developers; and predict developers' company affiliation based on their participation patterns. Method: 1,376,946 issues and pull-requests created for 4433 NPM packages with over 10,000 monthly downloads and full (public) commit activity data of the 272,142 issue creators is obtained and analyzed and dependencies on NPM packages are identified. Fuzzy c-means clustering algorithm is used to find the groups among the users based on their effort contribution and demand patterns, and Random Forest is used as the predictive modeling technique to identify their company affiliations. Result: Users contribute and demand effort primarily from packages that they depend on directly with only a tiny fraction of contributions and demand going to transitive dependencies. A significant portion of demand goes into packages outside the users' respective supply chains (constructed based on publicly visible version control data). Three and two different groups of users are observed based on the effort demand and effort contribution patterns respectively. The Random Forest model used for identifying the company affiliation of the users gives a AUC-ROC value of 0.68, and variables representing aggregate participation patterns proved to be the important predictors. Conclusion: Our results give new insights into effort demand and supply at different parts of the supply chain of the NPM ecosystem and its users and suggests the need to increase visibility further upstream.",2019,506
Augmenting Human Teams with Robots in Knowledge Work Settings: Insights from the Literature,"Ren, Yuqing and Clement, Jeffrey","Recent developments in large language models open doors for Artificial Intelligence and robots to augment knowledge workers and teams in a variety of domains, such as customer service, data science, legal work, and software development. In this article, we review 317 articles from multiple disciplines and summarize the insights in a theoretical framework linking key robot attributes to human perceptions and behaviors. The robot attributes include embodiment, nonverbal and verbal communication, perceived gender and race, emotions, perceived personality, and competence. The outcomes include human perceptions, acceptance, engagement, compliance, trust, and willingness to help. We identify four differences between one human and one robot settings and team settings and use them as the springboard to generalize insights from the literature review to the design and impact of a robot in assisting humans in knowledge work teams. We report two high-level observations around the interplay among robot attributes and context dependent designs and discuss their implications.",2024,507
Inside Quasimodo: Exploring Construction and Usage of Commonsense Knowledge,"Romero, Julien and Razniewski, Simon","Quasimodo is an open-source commonsense knowledge base that significantly advanced the state of salient commonsense knowledge base construction. It introduced a pipeline that gathers, normalizes, validates and scores statements coming from query log and question answering forums. In this demonstration, we present a companion web portal which allows (i) to explore the data, (ii) to run and analyze the extraction pipeline live, and (iii) inspect the usage of Quasimodo's knowledge in several downstream use cases. The web portal is available at https://quasimodo.r2.enst.fr.",2020,508
Extracting Skill Endorsements from Personal Communication Data,"Shankaralingappa, Darshan M. and De Fransicsi Morales, Gianmarco and Gionis, Aristides","People are increasingly communicating and collaborating via digital platforms, such as email and messaging applications. Data exchanged on these digital communication platforms can be a treasure trove of information on people who participate in the discussions: who they are collaborating with, what they are working on, what their expertise is, and so on. Yet, personal communication data is very rarely analyzed due to the sensitivity of the information it contains. In this paper, we mine personal communication data with the goal of generating skill endorsements of the type ""person A endorses person B on skill X."" To address privacy concerns, we consider that each person has access only to their own data (i.e., conversations with their peers). By using our method, they can generate endorsements for their peers, which they can inspect and opt to publish. To identify meaningful skills we use a knowledge base created from the StackExchange Q&amp;A forum. We study two different approaches, one based on building a skill graph, and one based on information retrieval techniques. We find that the latter approach outperforms the graph-based algorithms when tested on a dataset of user profiles from StackOverflow. We also conduct a user study on email data from nine volunteers, and we find that the information retrieval-based approach achieves a MAP@10 score of 0.617.",2016,509
"How Do Mothers and Fathers Talk About Parenting to Different Audiences? Stereotypes and Audience Effects: An Analysis of r/Daddit, r/Mommit, and r/Parenting Using Topic Modelling","Sepahpour-Fard, Melody and Quayle, Michael","While major strides have been made towards gender equality in public life, serious inequality remains in the domestic sphere, especially around parenting. The present study analyses discussions about parenting on Reddit (i.e., a content aggregation website) to explore audience effects and gender stereotypes. It suggests a novel method to study topical variation in individuals’ language when interacting with different audiences. Comments posted in 2020 were collected from three parenting subreddits (i.e., topical communities), described as being for fathers (r/Daddit), mothers (r/Mommit), and all parents (r/Parenting). Users posting on r/Parenting and r/Daddit or on r/Parenting and r/Mommit were assumed to identify as fathers or mothers, respectively, allowing gender comparison. Users’ comments on r/Parenting (to a mixed-gender audience) were compared with their comments to single-gender audiences on r/Daddit or r/Mommit using Latent Dirichlet Allocation (LDA) topic modelling. Results show that the most discussed topic among parents is about education and family advice, a topic mainly discussed in the mixed-gender subreddit and more by fathers than mothers. The topic model also indicates that, when it comes to the basic needs of children (sleep, food, and medical care), mothers seem to be more concerned regardless of the audience. In contrast, topics such as birth and pregnancy announcements and physical appearance are more discussed by fathers in the father-centric subreddit. Overall, findings seem to show that mothers are generally more concerned about the practical sides of parenting while fathers’ expressed concerns are more contextual: with other fathers, there seems to be a desire to show their fatherhood and be recognized for it while they discuss education with mothers. These results demonstrate that concerns expressed by parents on Reddit are context-sensitive but also consistent with gender stereotypes, potentially reflecting a persistent gendered and unequal division of labour in parenting.",2022,510
ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering,,,2024,511
DoME: An Architecture for Domain Model Evolution at Runtime Using NLP,"Gomes, Anderson and Maia, Paulo Henrique M.","In traditional information systems, domain models are represented as database tables with attributes and relationships. Changes in the domain models exist due to system evolution and the emergence of new requirements. In these applications, domain models evolve using CRUD operations requested by users. However, it is necessary to support changes in domain models during the applications’ runtime when new (unforeseen) situations may occur. This work presents an architecture called DoME, which relies on natural language processing (NLP) to allow users to trigger changes in the domain models and self-adaptation techniques to update the models at runtime. It is instantiated in a concrete architecture using a chatbot in Telegram and Transformers Libraries for NLP. The architecture has been preliminary evaluated regarding its assertiveness and user satisfaction, resulting in an 82.55% hit rate and confirming that NL provides good usability and facilitates data manipulation.",2023,512
"A reference architecture for datacenter scheduling: design, validation, and experiments","Andreadis, Georgios and Versluis, Laurens and Mastenbroek, Fabian and Iosup, Alexandru","Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.",2018,513
Adoption and Actual Privacy of Decentralized CoinJoin Implementations in Bitcoin,"St\""{u}tz, Rainer and Stockinger, Johann and Moreno-Sanchez, Pedro and Haslhofer, Bernhard and Maffei, Matteo","We present a first measurement study on the adoption and actual privacy of two popular decentralized CoinJoin implementations, Wasabi and Samourai, in the broader Bitcoin ecosystem. By applying highly accurate (¿ 99%) algorithms we can effectively detect 30,251 Wasabi and 223,597 Samourai transactions within the block range 530,500 to 725,348 (2018-07-05 to 2022-02-28). We also found a steady adoption of these services with a total value of mixed coins of ca. 4.74 B USD and average monthly mixing amounts of ca. 172.93 M USD) for Wasabi and ca. 41.72 M USD for Samourai. Furthermore, we could trace ca. 322 M USD directly received by cryptoasset exchanges and ca. 1.16 B USD indirectly received via two hops. Our analysis further shows that the traceability of addresses during the pre-mixing and post-mixing narrows down the anonymity set provided by these coin mixing services. It also shows that the selection of addresses for the CoinJoin transaction can harm anonymity. Overall, this is the first paper to provide a comprehensive picture of the adoption and privacy of distributed CoinJoin transactions. Understanding this picture is particularly interesting in the light of ongoing regulatory efforts that will, on the one hand, affect compliance measures implemented in cryptocurrency exchanges and, on the other hand, the privacy of end-users.",2023,514
Suspicious Behavior Detection near Vehicles in University Environment: An Approach using Object Detection and Body Angles,"Santos, Caio Nery Matos and Claro, Daniela Barreiro and Medrado Gondim, Jo\~{a}o and Mane, Babacar","Context: With the advancement of smart cities, the University environment demands surveillance camera systems to increase their monitoring capabilities to prevent malicious behaviors without prohibiting people’s circulation. Problem: Universities have large parking lots with many vehicles and face daily security problems, such as robberies and kidnappings, due to the lack of cameras capable of detecting suspicious behavior and alerting security personnel. Solution: Our approach enhances security in the university environment by developing a system capable of recognizing vehicles and individuals, assessing their proximity, and detecting gestures and actions labeled as suspicious behavior while interoperating with camera systems to alert the appropriate security authorities. Information systems theory: This work was conceived based on the General System Theory to interact with pre-existing heterogeneous systems. It relates to the Technological Frames of Reference theory, which involves the perception and interpretation of real-time object detection technology to monitor, alert, and ensure security. Method: Our research method is an experimental, descriptive investigation of collecting quantitative data, and our evaluation is conducted through the proof of concept. Results: Our artifact demonstrated its feasibility by exhibiting good performance, enabling the detection of pre-defined suspicious behaviors near vehicles with a precision of 94,25% and accuracy of 86,99%. Contributions and Impact in the area of information systems: Our contributions are two-fold: From the organization’s perspective, our security system interoperability, generating interoperable alerts; the artifact to detect suspicious behaviors, protecting people in the University environment. Our approach impacts the three pilars from IS area: People, Process and Technology. Additionally, we provide a dataset of security camera videos in university parking lots.",2024,515
Termination and Expressiveness of Execution Strategies for Networks of Bidirectional Model Transformations,"Klare, Heiko and Gleitze, Joshua","When developers describe a software system with multiple models, such as architecture diagrams, deployment descriptions, and source code, these models must represent the system in a uniform way, i.e., they must be and stay consistent. One means to automatically preserve consistency after changes to models are model transformations, of which bidirectional transformations that preserve consistency between two models have been well researched. To preserve consistency between multiple models, such transformations can be combined to networks. When transformations are developed independently and reused modularly, the resulting network can be of arbitrary topology. For such networks, no universal strategy exists to orchestrate the execution of transformations such that the resulting models are consistent.In this article, we prove that termination of such a strategy can only be guaranteed if it is incomplete, i.e., if it is allowed to fail to restore consistency for some changes although an execution order of transformations exists that yields consistent models. We propose such a strategy, for which we prove termination and show that and why it makes it easier for users of model transformation networks to understand the reasons whenever the strategy fails. In addition, we provide a simulator for the comparison of different execution strategies. These findings help transformation developers and users in understanding when and why they can expect the execution of a transformation network to terminate and when they can even expect it to succeed. Furthermore, the proposed strategy guarantees them termination and supports them in finding the reason whenever it is not successful.",2023,516
BuildSonic: Detecting and Repairing Performance-Related Configuration Smells for Continuous Integration Builds,"Zhang, Chen and Chen, Bihuan and Hu, Junhao and Peng, Xin and Zhao, Wenyun","Despite the benefits, continuous integration (CI) can incur high&nbsp;costs. One of the well-recognized costs is long build time, which greatly&nbsp;affects the speed of software development and increases the&nbsp;cost&nbsp;in&nbsp;computational resources. While there exist configuration options&nbsp;in&nbsp;the&nbsp;CI infrastructure to accelerate builds, the&nbsp;CI infrastructure is often&nbsp;not&nbsp;optimally configured, leading to CI configuration smells. Attempts&nbsp;have been made to detect or repair CI configuration smells.&nbsp;However,&nbsp;none of them is specifically&nbsp;designed to improve build performance&nbsp;in&nbsp;CI. In this paper, we first create a catalog of 20 performance-related&nbsp;CI configuration smells (PCSs) in three tools (i.e., Travis CI, Maven&nbsp;and Gradle) of the CI infrastructure for Java projects. Then, we propose an automated approach, named BuildSonic, to detect and repair&nbsp;15 types of PCSs by analyzing configuration files.&nbsp;We&nbsp;have&nbsp;conducted large-scale experiments to evaluate BuildSonic. We detected&nbsp;20,318 PCSs in 99.0% of the 4,140 Java projects, with a precision of 0.998&nbsp;and a recall of 0.991. We submitted 1,138 pull requests&nbsp;for&nbsp;sampled&nbsp;PCSs of each PCS type, 246 and 11 of which&nbsp;have&nbsp;been respectively&nbsp;merged and accepted by developers. We successfully triggered CI builds&nbsp;before and&nbsp;after merging 288 pull requests, and observed an average build performance improvement of 12.4% after repairing a PCS.",2023,517
From Freebase to Wikidata: The Great Migration,"Pellissier Tanon, Thomas and Vrande\v{c}i\'{c}, Denny and Schaffert, Sebastian and Steiner, Thomas and Pintscher, Lydia","Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two major collaborative knowledge bases are Wikimedia's Wikidata and Google's Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Primary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases.",2016,518
No keys to the kingdom required: a comprehensive investigation of missing authentication vulnerabilities in the wild,"Karl, Manuel and Musch, Marius and Ma, Guoli and Johns, Martin and Lekies, Sebastian","Nowadays, applications expose administrative endpoints to the Web that can be used for a plethora of security sensitive actions. Typical use cases range from running small snippets of user-provided code for rapid prototyping, administering databases, and running CI/CD pipelines, to managing job scheduling on whole clusters of computing devices. While accessing these applications over the Web make the lives of their users easier, they can be leveraged by attackers to compromise the underlying infrastructure if not properly configured.In this paper, we comprehensively investigate inadequate authentication mechanisms in such web endpoints. For this, we looked at 25 popular applications and exposed 18 of them to the Internet because they were either vulnerable in their default configuration or were easy to misconfigure. We identified ongoing attacks against 7 of them, some were even compromised within a few hours from the deployment. In an Internet-wide scan of the IPv4 address space, we examine the prevalence of such vulnerable applications at scale. Thereby, we found 4,221 vulnerable instances, enough to create a small botnet with little technical knowledge. We observed these vulnerable instances and found that even after four weeks, more than half of them were still online and vulnerable.Currently, most of the identified vulnerabilities are seen as features of the software and are often not yet considered by common security scanners or vulnerability databases. However, via our experiments, we found missing authentication vulnerabilities to be common and already actively exploited at scale. They thus represent a prevalent but often disregarded danger.",2022,519
6G Network Needs to Support Embedded Trust,"Kantola, Raimo","A slogan coined at the recent first Levi 6G Summit by Peter Wetter of Nokia Bell Labs was ""the 6G is about the 6th sense"". This can be understood in at least two ways. One is that the network just knows what to do in all kinds of situations because of the use of AI and the second is that 6G radio will be widely used to sense the environment where the users are. In this view, 6G is seen as a continuation of the merge of the physical and the virtual words. An outcome of the Summit is a 6G White Paper documenting the ideas of some 60 invited people from the 300 participants about the future generation coming after 5G. This paper provides further discussion and justification of the trust and security aspects of the Networking Chapter in the White Paper. The paper focuses on principles and only refers to some verification in order not to clutter the discussion with technical detail. Opinions expressed here are of the author of this paper who was also the main editor of the Networking Chapter. The members of the White paper group on Networking or the editors of the White paper should not be held liable for the views expressed in the paper.",2019,520
ASONAM '22: Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,,"We were delighted to welcome each participant at ASONAM 2022 and thank you for having contributed virtually or in person in Istanbul. ASONAM 2022 was the fourteenth annual conference in the successful ASONAM conferences series and also the first hybrid version of the conference. Previous ASONAM conferences were held in Athens (2009), Odense (2010), Kaohsiung (2011), Istanbul (2012), Niagara Falls (2013), Beijing (2014), Paris (2015), San Francisco (2016), Sydney (2017), Barcelona (2018), Vancouver (2019), Virtual (2020), Virtual (2021). The pre-pandemic locations of the conferences have enabled the participants to enjoy local sights and to engage in person-to-person interactions, making new contacts and form new scientific collaborations. These possibilities were only available in a limited form during the virtual conferences. As the covid pandemic seems to be moving towards an endemic form it was decided to have the conference in the hybrid form, as a move towards normal endemic in-person conferences.For more than a century, social networks have been studied in a variety of disciplines including sociology, anthropology, psychology, and economics. The Internet, the social Web, and other large-scale, sociotechnological infrastructures have triggered a growing interest and resulted in significant methodological advancements in social network analysis and mining. Method development in graph theory, statistics, data mining, machine learning, and AI have inspired new research problems and, in turn, opens up further possibilities for application. These spiraling trends have led to a rising prominence of social network analysis and mining methods and tools in academia, politics, security, and business.",2022,521
ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice,,,2023,522
Enhancing the Configuration Tuning Pipeline of Large-Scale Distributed Applications Using Large Language Models (Idea Paper),"Somashekar, Gagan and Kumar, Rajat","The performance of distributed applications implemented using microservice architecture depends heavily on the configuration of various parameters, which are hard to tune due to large configuration search space and inter-dependence of parameters. While the information in product manuals and technical documents guides the tuning process, manual collection of meta-data for all application parameters is laborious and not scalable. Prior works have largely overlooked the automated use of product manuals, technical documents and source code for extracting such meta-data. In the current work, we propose using large language models for automated meta-data extraction and enhancing the configuration tuning pipeline. We further ideate on building an in-house knowledge system using experimental data to learn important parameters in configuration tuning using historical data on parameter dependence, workload statistics, performance metrics and resource utilization. We expect productionizing the proposed system will reduce the total time and experimental iterations required for configuration tuning in new applications, saving an organization both developer time and money.",2023,523
After Death: Big Data and the Promise of Resurrection by Proxy,"Ahmad, Muhammad Aurangzeb",With the advent of Big Data and the possibility of capturing massive personal data it is possible to simulate some aspects of a person's personality. The imitation game is based on the observation that it is possible to convince a person of a fake identity if sufficient information is available about the identity being faked. Imitation is however not limited to a person who is alive but also a person who is not alive; the question of simulating a deceased person for the purpose of having the simulation interact with a person is addressed. Various challenges and background considerations for such an endeavor are discussed. The goal of the paper is to open up discussion on this subject and examine its feasibility.,2016,524
"LApps: Technological, Legal and Market Potentials of Blockchain Lightning Network Applications","Miraz, Mahdi H. and Donald, David C.","Following in the footsteps of pioneer Bitcoin, many altcoins as well as coloured coins have been being developed and merchandised adopting blockchain as the core enabling technology. However, since interoperability and scalability, due to high and capped (in particular cases) transaction latency are deep-rooted in the architecture of blockchain technology, they are by default inherited in any blockchain based applications. Lightning Network (LN) is one of the supporting technologies developed to eliminate this impediment of blockchain technology by facilitating instantaneous transfers of cryptos. Since the potentials of LN is still relatively unknown, this paper investigates the current states of development along with possible non-monetary usage of LN, especially in settlement coloured coins such as securities, as well as creation of new business models based on Lightning Applications (LApps) and microchannel payments as well as micro-trades. The legal challenges that may act as impediment to the adoption of LN is also discussed.",2019,525
Mining Scholarly Communication and Interaction on the Social Web,"Hadgu, Asmelash Teka","The explosion of Web 2.0 platforms including social networking sites such as Twitter, blogs and wikis affects all web users: scholars included. As a result, there is a need for a comprehensive approach to gain a broader understanding and timely signals of scientific communication as well as how researchers interact on the social web. Most current work in this area deals with either a low number of researchers and heavily relies on manual annotation or large-scale analysis without deep understanding of the underlying researcher population. In this proposal, we present a holistic approach to solve these problems. This research proposes novel methods to collect, filter, analyze and make sense of scholars and scholarly communication by integrating heterogeneous data sources from fast social media streams as well as the academic web. Applying reproducible research, contributing applications and data sets, the thesis proposal strives to add value by mining the social web for social good.",2015,526
NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs,"Antoniak, Maria and Naik, Aakanksha and Alvarado, Carla S. and Wang, Lucy Lu and Chen, Irene Y.","Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of guiding principles. We propose nine principles for ethical use of NLP for maternal healthcare, grouped into three themes: (i) recognizing contextual significance (ii) holistic measurements, and (iii) who/what is valued. For each principle, we describe its underlying rationale and provide practical advice. This set of principles can provide a methodological pattern for other researchers and serve as a resource to practitioners working on maternal health and other healthcare fields to emphasize the importance of technical nuance, historical context, and inclusive design when developing NLP technologies for clinical use.",2024,527
Towards Unsupervised Introspection of Containerized Application,"Cui, Pinchen and Umphress, David","Container (or containerization) as one of the new concepts of virtualization, has attracted increasing attention and occupied a considerable amount of market size owing to the inherent lightweight characteristic. However, the lightweight advantage is achieved at the price of the security. Attacks against weak isolation of the container have been reported, and the use of a shared kernel is another targeted vulnerable point. This work aims to provide secure monitoring of containerized applications, which can help i) the infrastructure owner to ensure the running application is harmless, ii) the application owner to detect anomalous behaviors. We propose to use unsupervised introspection tools to perform the non-intrusive monitoring, which leverages the system call traces to classify the anomalies. Since the traditional dataset used for anomaly detection either only focus on network traces or limited to few attributes of system calls, we crafted and collected various normal and abnormal behaviors of a containerized application, and an optimized and open-source system call based dataset has been built. Unsupervised machine learning classifiers are trained over the proposed dataset, a comprehensive case study has been performed and analyzed. The results show the feasibility of unsupervised introspection of containerized applications.",2021,528
A deep multitask learning approach for requirements discovery and annotation from open forum,"Li, Mingyang and Shi, Lin and Yang, Ye and Wang, Qing","The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91% and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.",2021,529
The Elephant in the Background: A Quantitative Approachto Empower Users Against Web Browser Fingerprinting,"Fietkau, Julian and Thimmaraju, Kashyap and Kybranz, Felix and Neef, Sebastian and Seifert, Jean-Pierre","Tracking users is a ubiquitous practice on the web today. User activity is recorded and analyzed on a large scale to create personalized products, forecast future behavior, and prevent online fraud. While HTTP cookies have been the weapon of choice so far, new and more pervasive techniques such as browser fingerprinting are gaining traction. This paper describes how users can be empowered against fingerprinting by showing them when, how, and who is tracking them. To this end, we conduct a systematic analysis of various fingerprinting tools to create FPMON: a browser extension to measure and rate fingerprinting activity on any website in real-time. With FPMON, we evaluate the 10k most popular websites to i) study the pervasiveness of fingerprinting; ii) review the latest countermeasures; and iii) identify the networks that foster the use of fingerprinting. Our evaluations reveal that i) fingerprinters subvert privacy regulations; ii) they are present on privacy-sensitive websites (insurance, finances, NGOs); and iii) current countermeasures cannot sufficiently protect users. Hence, we publish FPMON as a free browser extension to empower users against this growing threat.",2021,530
ICSE-NIER'24: Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results,,,2024,531
Evaluating the Security Posture of Real-World FIDO2 Deployments,"Kuchhal, Dhruv and Saad, Muhammad and Oest, Adam and Li, Frank","FIDO2 is a suite of protocols that combines the usability of local authentication (e.g., biometrics) with the security of public-key cryptography to deliver passwordless authentication. It eliminates shared authentication secrets (i.e., passwords, which could be leaked or phished) and provides strong security guarantees assuming the benign behavior of the client-side protocol components.However, when this assumption does not hold true, such as in the presence of malware, client authentications pose a risk that FIDO2 deployments must account for. FIDO2 provides recommendations for deployments to mitigate such situations. Yet, to date, there has been limited empirical investigation into whether deployments adopt these mitigations and what risks compromised clients present to real-world FIDO2 deployments, such as unauthorized account access or registration.In this work, we aim to fill in the gap by: 1) systematizing the threats to FIDO2 deployments when assumptions about the client-side protocol components do not hold, 2) empirically evaluating the security posture of real-world FIDO2 deployments across the Tranco Top 1K websites, considering both the server-side and client-side perspectives, and 3) synthesizing the mitigations that the ecosystem can adopt to further strengthen the practical security provided by FIDO2. Through our investigation, we identify that compromised clients pose a practical threat to FIDO2 deployments due to weak configurations, and known mitigations exhibit critical shortcomings and/or minimal adoption. Based on our findings, we propose directions for the ecosystem to develop additional defenses into their FIDO2 deployments. Ultimately, our work aims to drive improvements to FIDO2's practical security.",2023,532
Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection,"Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario","Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",2023,533
NLBSE '22: Proceedings of the 1st International Workshop on Natural Language-based Software Engineering,,"Welcome to the 1st edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.",2022,534
ICSE-SEIS '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society,,"We are delighted to introduce the Software Engineering in Society (SEIS) track program as part of the 45th IEEE/ACM International Conference on Software Engineering, to be held in Melbourne, Australia, on May 14-20, 2023. The aim of the track is to bring together researchers studying various roles that software engineering plays in society.",2023,535
Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors,"Zhou, Jijie and Hu, Yuhan","Recently, large language models have facilitated the emergence of highly intelligent conversational AI capable of engaging in human-like dialogues. However, a notable distinction lies in the fact that these AI models predominantly generate responses rapidly, often producing extensive content without emulating the thoughtful process characteristic of human cognition and typing. This paper presents a design aimed at simulating human-like typing behaviors, including patterns such as hesitation and self-editing, as well as a preliminary user experiment to understand whether and to what extent the agent with human-like typing behaviors could potentially affect conversational engagement and its trustworthiness. We’ve constructed an interactive platform featuring user-adjustable parameters, allowing users to personalize the AI’s communication style and thus cultivate a more enriching and immersive conversational experience. Our user experiment, involving interactions with three types of agents—a baseline agent, one simulating hesitation, and another integrating both hesitation and self-editing behaviors—reveals a preference for the agent that incorporates both behaviors, suggesting an improvement in perceived naturalness and trustworthiness. Through the insights from our design process and both quantitative and qualitative feedback from user experiments, this paper contributes to the multimodal interaction design and user experience for conversational AI, advocating for a more human-like, engaging, and trustworthy communication paradigm.",2024,536
An Empirical Evaluation of Property Recommender Systems for Wikidata and Collaborative Knowledge Bases,"Zangerle, Eva and Gassler, Wolfgang and Pichl, Martin and Steinhauser, Stefan and Specht, G\""{u}nther","The Wikidata platform is a crowdsourced, structured knowledgebase aiming to provide integrated, free and language-agnostic facts which are---amongst others---used by Wikipedias. Users who actively enter, review and revise data on Wikidata are assisted by a property suggesting system which provides users with properties that might also be applicable to a given item. We argue that evaluating and subsequently improving this recommendation mechanism and hence, assisting users, can directly contribute to an even more integrated, consistent and extensive knowledge base serving a huge variety of applications. However, the quality and usefulness of such recommendations has not been evaluated yet. In this work, we provide the first evaluation of different approaches aiming to provide users with property recommendations in the process of curating information on Wikidata. We compare the approach currently facilitated on Wikidata with two state-of-the-art recommendation approaches stemming from the field of RDF recommender systems and collaborative information systems. Further, we also evaluate hybrid recommender systems combining these approaches. Our evaluations show that the current recommendation algorithm works well in regards to recall and precision, reaching a recall@7 of 79.71% and a precision@7 of 27.97%. We also find that generally, incorporating contextual as well as classifying information into the computation of property recommendations can further improve its performance significantly.",2016,537
Bridging Performance of X (formerly known as Twitter) Users: A Predictor of Subjective Well-Being During the Pandemic,"Chen, Ninghan and Chen, Xihui and Zhong, Zhiqiang and Pang, Jun","The outbreak of the COVID-19 pandemic triggered the perils of misinformation over social media. By amplifying the spreading speed and popularity of trustworthy information, influential social media users have been helping overcome the negative impacts of such flooding misinformation. In this article, we use the COVID-19 pandemic as a representative global health crisisand examine the impact of the COVID-19 pandemic on these influential users’ subjective well-being (SWB), one of the most important indicators of mental health. We leverage X (formerly known as Twitter) as a representative social media platform and conduct the analysis with our collection of 37,281,824 tweets spanning almost two years. To identify influential X users, we propose a new measurement called user bridging performance (UBM) to evaluate the speed and wideness gain of information transmission due to their sharing. With our tweet collection, we manage to reveal the more significant mental sufferings of influential users during the COVID-19 pandemic. According to this observation, through comprehensive hierarchical multiple regression analysis, we are the first to discover the strong relationship between individual social users’ subjective well-being and their bridging performance. We proceed to extend bridging performance from individuals to user subgroups. The new measurement allows us to conduct a subgroup analysis according to users’ multilingualism and confirm the bridging role of multilingual users in the COVID-19 information propagation. We also find that multilingual users not only suffer from a much lower SWB in the pandemic, but also experienced a more significant SWB drop.",2024,538
ICSE-NIER '23: Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results,,"ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.",2023,539
WWW '19: Companion Proceedings of The 2019 World Wide Web Conference,,"It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.",2019,540
ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice,,,2024,541
Taming Android fragmentation: characterizing and detecting compatibility issues for Android apps,"Wei, Lili and Liu, Yepang and Cheung, Shing-Chi","Android ecosystem is heavily fragmented. The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps. As a result, various compatibility issues arise, causing poor user experience. However, little is known on the characteristics of such fragmentation-induced compatibility issues and no mature tools exist to help developers quickly diagnose and fix these issues. To bridge the gap, we conducted an empirical study on 191 real-world compatibility issues collected from popular open-source Android apps. Our study characterized the symptoms and root causes of compatibility issues, and disclosed that the patches of these issues exhibit common patterns. With these findings, we propose a technique named FicFinder to automatically detect compatibility issues in Android apps. FicFinder performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues are triggered. FicFinder reports actionable debugging information to developers when it detects potential issues. We evaluated FicFinder with 27 large-scale open-source Android apps. The results show that FicFinder can precisely detect compatibility issues in these apps and uncover previously-unknown issues.",2016,542
Automatically Matching Bug Reports With Related App Reviews,"Haering, Marlo and Stanik, Christoph and Maalej, Walid","App stores allow users to give valuable feedback on apps, and developers to find this feedback and use it for the software evolution. However, finding user feedback that matches existing bug reports in issue trackers is challenging as users and developers often use a different language. In this work, we introduce DeepMatcher, an automatic approach using state-of-the-art deep learning methods to match problem reports in app reviews to bug reports in issue trackers. We evaluated DeepMatcher with four open-source apps quantitatively and qualitatively. On average, DeepMatcher achieved a hit ratio of 0.71 and a Mean Average Precision of 0.55. For 91 problem reports, DeepMatcher did not find any matching bug report. When manually analyzing these 91 problem reports and the issue trackers of the studied apps, we found that in 47 cases, users actually described a problem before developers discovered and documented it in the issue tracker. We discuss our findings and different use cases for DeepMatcher.",2021,543
Detecting Anomalous Computation with RNNs on GPU-Accelerated HPC Machines,"Zou, Pengfei and Li, Ang and Barker, Kevin and Ge, Rong","This paper presents a workload classification framework that accurately discriminates illicit computation from authorized workloads on GPU-accelerated HPC systems at runtime. As such systems become increasingly powerful and widely-adopted, attackers have begun to run illicit and for-profit programs that typically require extremely high computing capability to be successful, depriving mission-critical and authorized workloads of execution cycles and increasing risks of data leaking and empowered attacks. Traditional measures on CPU hosts are oblivious to such attacks. Our classification framework leverages the distinctive signatures between illicit and authorized GPU workloads, and explores machine learning methods and workload profiling to classify them. We face multiple challenges in designing the framework: achieving high detection accuracy, maintaining low profiling and inference overhead, and overcoming the limitation of lacking data types and volumes typically required by deep learning models. To address these challenges, we use lightweight, non-intrusive, high-level workload profiling, collect multiple sequences of easily obtainable multimodal input data, and build recurrent neural networks (RNNs) to learn from history for online anomalous workload detection. Evaluation results on three generations of GPU machines demonstrate that the workload classification framework can tell apart the illicit workloads with a high accuracy of over 95%. The collected dataset, detection framework, and neural network models are released on github1.",2020,544
Domain-Slot Relationship Modeling Using a Pre-Trained Language Encoder for Multi-Domain Dialogue State Tracking,"An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim, Misuk","Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. As using pre-trained language models is the de facto standard for natural language processing tasks, many recent studies use them to encode the dialogue context for predicting the dialogue states. Model architectures that have certain inductive biases for modeling the relationship among different domain-slot pairs are also emerging. Our work is based on these research approaches on multi-domain dialogue state tracking. We propose a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special &lt;inline-formula&gt;&lt;tex-math notation=""LaTeX""&gt;$[CLS]$&lt;/tex-math&gt;&lt;/inline-formula&gt; token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results on the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our model outperforms other models with the same setting. Our ablation studies incorporate three main parts. The first component shows the effectiveness of our approach exploiting the relationship modeling. The second component compares the effect of using different pre-trained language encoders. The final component involves comparing different initialization methods that could be used for the special tokens. Qualitative analysis of the attention map of the pre-trained language encoder shows that our special tokens encode relevant information through the encoding process by attending to each other.",2022,545
Form-From: A Design Space of Social Media Systems,"Zhang, Amy X. and Bernstein, Michael S. and Karger, David R. and Ackerman, Mark S.","Social media systems are as varied as they are pervasive. They have been almost universally adopted for a broad range of purposes including work, entertainment, activism, and decision making. As a result, they have also diversified, with many distinct designs differing in content type, organization, delivery mechanism, access control, and many other dimensions. In this work, we aim to characterize and then distill a concise design space of social media systems that can help us understand similarities and differences, recognize potential consequences of design choice, and identify spaces for innovation. Our model, which we call Form-From, characterizes social media based on (1) the form of the content, either threaded or flat, and (2) from where or from whom one might receive content, ranging from spaces to networks to the commons. We derive Form-From inductively from a larger set of 62 dimensions organized into 10 categories. To demonstrate the utility of our model, we trace the history of social media systems as they traverse the Form-From space over time, and we identify common design patterns within cells of the model.",2024,546
Group-based corpus scheduling for parallel fuzzing,"Gu, Taotao and Li, Xiang and Lu, Shuaibing and Tian, Jianwen and Nie, Yuanping and Kuang, Xiaohui and Lin, Zhechao and Liu, Chenyifan and Liang, Jie and Jiang, Yu","Parallel fuzzing relies on hardware resources to guarantee test throughput and efficiency. In industrial practice, it is well known that parallel fuzzing faces the challenge of task division, but most works neglect the important process of corpus allocation. In this paper, we proposed a group-based corpus scheduling strategy to address these two issues, which has been accepted by the LLVM community. And we implement a parallel fuzzer based on this strategy called glibFuzzer. glibFuzzer first groups the global corpus into different subsets and then assigns different energy scores and different scores to them. The energy scores were mainly determined by the seed size and the length of coverage information, and the difference score can describe the degree of difference in the code covered by different subsets of seeds. In each round of key local corpus construction, the master node selects high-quality seeds by combining the two scores to improve test efficiency and avoid task conflict. To prove the effectiveness of the strategy, we conducted an extensive evaluation on the real-world programs and FuzzBench. After 4\texttimes{}24 CPU-hours, glibFuzzer covered 22.02% more branches and executed 19.42 times more test cases than libFuzzer in 18 real-world programs. glibFuzzer showed an average branch coverage increase of 73.02%, 55.02%, 55.86% over AFL, PAFL, UniFuzz, respectively. More importantly, glibFuzzer found over 100 unique vulnerabilities.",2022,547
SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality,,,2023,548
ITiCSE 2024: Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1,,"Welcome to the 29th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2024), hosted by Universit\`{a} degli Studi di Milano in Milan, Italy.ITiCSE 2024 will take place from Friday July 5 to Wednesday July 10. The conference program includes a keynote address, paper sessions, a panel, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet July 5-7 and will submit draft reports before the conference begins on July 8.The submissions to ITiCSE 2024 were reviewed by 446 researchers and practitioners from computing education and related fields, including 44 program committee members and 402 reviewers. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.",2024,549
WDC '24: Proceedings of the 3rd ACM Workshop on the Security Implications of Deepfakes and Cheapfakes,,,2024,550
Deliberation and Resolution on Wikipedia: A Case Study of Requests for Comments,"Im, Jane and Zhang, Amy X. and Schilling, Christopher J. and Karger, David","Resolving disputes in a timely manner is crucial for any online production group. We present an analysis of Requests for Comments (RfCs), one of the main vehicles on Wikipedia for formally resolving a policy or content dispute. We collected an exhaustive dataset of 7,316 RfCs on English Wikipedia over the course of 7 years and conducted a qualitative and quantitative analysis into what issues affect the RfC process. Our analysis was informed by 10 interviews with frequent RfC closers. We found that a major issue affecting the RfC process is the prevalence of RfCs that could have benefited from formal closure but that linger indefinitely without one, with factors including participants' interest and expertise impacting the likelihood of resolution. From these findings, we developed a model that predicts whether an RfC will go stale with 75.3% accuracy, a level that is approached as early as one week after dispute initiation.",2018,551
Input splitting for cloud-based static application security testing platforms,"Christakis, Maria and Cottenier, Thomas and Filieri, Antonio and Luo, Linghui and Mansur, Muhammad Numair and Pike, Lee and Rosner, Nicol\'{a}s and Sch\""{a}f, Martin and Sengupta, Aritra and Visser, Willem","As software development teams adopt DevSecOps practices, application security is increasingly the responsibility of development teams, who are required to set up their own Static Application Security Testing (SAST) infrastructure. Since development teams often do not have the necessary infrastructure and expertise to set up a custom SAST solution, there is an increased need for cloud-based SAST platforms that operate as a service and run a variety of static analyzers. Adding a new static analyzer to a cloud-based SAST platform can be challenging because static analyzers greatly vary in complexity, from linters that scale efficiently to interprocedural dataflow engines that use cubic or even more complex algorithms. Careful manual evaluation is needed to decide whether a new analyzer would slow down the overall response time of the platform or may timeout too often. We explore the question of whether this can be simplified by splitting the input to the analyzer into partitions and analyzing the partitions independently. Depending on the complexity of the static analyzer, the partition size can be adjusted to curtail the overall response time. We report on an experiment where we run different analysis tools with and without splitting the inputs. The experimental results show that simple splitting strategies can effectively reduce the running time and memory usage per partition without significantly affecting the findings produced by the tool.",2022,552
MAI - A Proactive Speech Agent for Metacognitive Mediation in Collaborative Learning,"Edwards, Justin and Nguyen, Andy and Sobocinski, Marta and L\""{a}ms\""{a}, Joni and de Araujo, Adelson and Dang, Belle and Whitehead, Ridwan and Roberts, Anni-Sofia and Kaarlela, Matti and Jarvela, Sanna","We introduce MAI - a proactive speech agent aimed at enhancing metacognitive awareness among learners in collaborative learning settings. Background is presented around Socially Shared Regulation of Learning and the role of metacognition in learning. Next, the design of the rules that MAI uses to prompt learners and mediate metacognition are introduced. We describe the conditions in which MAI has been piloted thus far, including as a Wizard of Oz prototype and as a fully functional prototype using natural language processing. We discuss the ethical considerations that went into the prototyping and testing of MAI. Finally, we describe our next steps for understanding the interactions learners had with MAI already, planned design changes, and the future of testing the agent.",2024,553
Jessy: A Conversational Assistant for Tutoring Digital Board Games,"Allameh, Mahdieh and Zaman, Loutfouz","We present Jessy, an interactive intelligent digital board game assistant for The Royal Game of Ur. Jessy is capable of answering questions regarding the game rules, giving suggestions for best moves considering the player's state, and informing the player about the important events in the game. An explanatory non-comparative study was conducted to evaluate the usability and usefulness of Jessy in engaging and learning how to play the game. The study showed Jessy was helpful in general and the findings suggest insights on how to craft Jessy for its target application – difficult board games.",2021,554
PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics,,,2022,555
Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform,"Edge, Darren and Larson, Jonathan and White, Christopher","The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.",2018,556
Automatic API Usage Scenario Documentation from Technical Q&amp;A Sites,"Uddin, Gias and Khomh, Foutse and Roy, Chanchal K.","The online technical Q&amp;A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.",2021,557
CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes,"Liffiton, Mark and Sheese, Brad E and Savelka, Jaromir and Denny, Paul","Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students’ usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.",2024,558
The Effect of Third Party Implementations on Reproducibility,"Hidasi, Bal\'{a}zs and Czapp, \'{A}d\'{a}m Tibor","Reproducibility of recommender systems research has come under scrutiny during recent years. Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility. We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility. Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets. In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility.",2023,559
Origins of the D programming language,"Bright, Walter and Alexandrescu, Andrei and Parker, Michael","As its name suggests, the initial motivation for the D programming language was to improve on C and C++ while keeping their spirit. The D language was to preserve those languages' efficiency, low-level access, and Algol-style syntax. The areas D set out to improve focused initially on rapid development, convenience, and simplifying the syntax without hampering expressiveness.  The genesis of D has its peculiarities, as is the case with many other languages. Walter Bright, D's creator, is a mechanical engineer by education who started out working for Boeing designing gearboxes for the 757. He was programming games on the side, and in trying to make his game Empire run faster, became interested in compilers. Despite having no experience, Bright set out in 1982 to implement a compiler that produced better code than those on the market at the time.  This interest materialized into a C compiler, followed by compilers for C++, Java, and JavaScript. Best known of these would be the Zortech C++ compiler, the first (and to date only) C++-to-native compiler developed by a single person. The D programming language began in 1999 as an effort to pull the best features of these languages into a new one. Fittingly, D would use the by that time mature C/C++ back end (optimizer and code generator) that had been under continued development and maintenance since 1982.  Between 1999 and 2006, Bright worked alone on the D language definition and its implementation, although a steadily increasing volume of patches from users was incorporated. The new language would be based on the past successes of the languages he'd used and implemented, but would be clearly looking to the future. D started with choices that are obvious today but were less clear winners back in the 1990s: full support for Unicode, IEEE floating point, 2s complement arithmetic, and flat memory addressing (memory is treated as a linear address space with no segmentation). It would do away with certain compromises from past languages imposed by shortages of memory (for example, forward declarations would not be required). It would primarily appeal to C and C++ users, as expertise with those languages would be readily transferrable. The interface with C was designed to be zero cost.  The language design was begun in late 1999. An alpha version appeared in 2001 and the initial language was completed, somewhat arbitrarily, at version 1.0 in January 2007. During that time, the language evolved considerably, both in capability and in the accretion of a substantial worldwide community that became increasingly involved with contributing. The front end was open-sourced in April 2002, and the back end was donated by Symantec to the open source community in 2017. Meanwhile, two additional open-source back ends became mature in the 2010s: `gdc` (using the same back end as the GNU C++ compiler) and `ldc` (using the LLVM back end).  The increasing use of the D language in the 2010s created an impetus for formalization and development management. To that end, the D Language Foundation was created in September 2015 as a nonprofit corporation overseeing work on D's definition and implementation, publications, conferences, and collaborations with universities.",2020,560
Symbolic Boolean derivatives for efficiently solving extended regular expression constraints,"Stanford, Caleb and Veanes, Margus and Bj\o{}rner, Nikolaj","The manipulation of raw string data is ubiquitous in security-critical software, and verification of such software relies on efficiently solving string and regular expression constraints via SMT. However, the typical case of Boolean combinations of regular expression constraints exposes blowup in existing techniques. To address solvability of such constraints, we propose a new theory of derivatives of symbolic extended regular expressions (extended meaning that complement and intersection are incorporated), and show how to apply this theory to obtain more efficient decision procedures. Our implementation of these ideas, built on top of Z3, matches or outperforms state-of-the-art solvers on standard and handwritten benchmarks, showing particular benefits on examples with Boolean combinations.  Our work is the first formalization of derivatives of regular expressions which both handles intersection and complement and works symbolically over an arbitrary character theory. It unifies existing approaches involving derivatives of extended regular expressions, alternating automata and Boolean automata by lifting them to a common symbolic platform. It relies on a parsimonious augmentation of regular expressions: a construct for symbolic conditionals is shown to be sufficient to obtain relevant closure properties for derivatives over extended regular expressions.",2021,561
Application Layer Denial-of-Service Attacks and Defense Mechanisms: A Survey,"Tripathi, Nikhil and Hubballi, Neminath","Application layer Denial-of-Service (DoS) attacks are generated by exploiting vulnerabilities of the protocol implementation or its design. Unlike volumetric DoS attacks, these are stealthy in nature and target a specific application running on the victim. There are several attacks discovered against popular application layer protocols in recent years. In this article, we provide a structured and comprehensive survey of the existing application layer DoS attacks and defense mechanisms. We classify existing attacks and defense mechanisms into different categories, describe their working, and compare them based on relevant parameters. We conclude the article with directions for future research.",2021,562
Was self-admitted technical debt removal a real removal? an in-depth perspective,"Zampetti, Fiorella and Serebrenik, Alexander and Di Penta, Massimiliano","Technical Debt (TD) has been defined as ""code being not quite right yet"", and its presence is often self-admitted by developers through comments. The purpose of such comments is to keep track of TD and appropriately address it when possible. Building on a previous quantitative investigation by Maldonado et al. on the removal of self-admitted technical debt (SATD), in this paper we perform an in-depth quantitative and qualitative study of how SATD is addressed in five Java open source projects. On the one hand, we look at whether SATD is ""accidentally"" removed, and the extent to which the SATD removal is being documented. We found that that (i) between 20% and 50% of SATD comments are accidentally removed while entire classes or methods are dropped, (ii) 8% of the SATD removal is acknowledged in commit messages, and (iii) while most of the changes addressing SATD require complex source code changes, very often SATD is addressed by specific changes to method calls or conditionals. Our results can be used to better plan TD management or learn patterns for addressing certain kinds of TD and provide recommendations to developers.",2018,563
Summary of Tutorials at The Web Conference 2021,"West, Robert and Bhagat, Smriti and Groth, Paul and Zitnik, Marinka and Couto, Francisco M. and Lisena, Pasquale and Mero\~{n}o-Pe\~{n}uela, Albert and Zhao, Xiangyu and Fan, Wenqi and Yin, Dawei and Tang, Jiliang and Shou, Linjun and Gong, Ming and Pei, Jian and Geng, Xiubo and Zhou, Xingjie and Jiang, Daxin and Ricaud, Benjamin and Aspert, Nicolas and Miz, Volodymyr and Dy, Jennifer and Ioannidis, Stratis and Y\i{}ld\i{}z, undefinedlkay and Rezapour, Rezvaneh and Aref, Samin and Dinh, Ly and Diesner, Jana and Drutsa, Alexey and Ustalov, Dmitry and Popov, Nikita and Baidakova, Daria and Mishra, Shubhanshu and Gopalan, Arjun and Juan, Da-Cheng and Ilharco Magalhaes, Cesar and Ferng, Chun-Sung and Heydon, Allan and Lu, Chun-Ta and Pham, Philip and Yu, George and Fan, Yicheng and Wang, Yueqi and Laurent, Florian and Schraner, Yanick and Scheller, Christian and Mohanty, Sharada and Chen, Jiawei and Wang, Xiang and Feng, Fuli and He, Xiangnan and Teinemaa, Irene and Albert, Javier and Goldenberg, Dmitri and Vasile, Flavian and Rohde, David and Jeunen, Olivier and Benhalloum, Amine and Sakhi, Otmane and Rong, Yu and Huang, Wenbing and Xu, Tingyang and Bian, Yatao and Cheng, Hong and Sun, Fuchun and Huang, Junzhou and Fakhraei, Shobeir and Faloutsos, Christos and \c{C}elebi, Onur and M\""{u}ller, Martin and Schneider, Manuel and Altunina, Olesia and Wingerath, Wolfram and Wollmer, Benjamin and Gessert, Felix and Succo, Stephan and Ritter, Norbert and Courdier, Evann and Avram, Tudor Mihai and Cvetinovic, Dragan and Tsinadze, Levan and Jose, Johny and Howell, Rose and Koenig, Mario and Defferrard, Micha\""{e}l and Kenthapadi, Krishnaram and Packer, Ben and Sameki, Mehrnoosh and Sephus, Nashlie",This report summarizes the 23 tutorials hosted at The Web Conference 2021: nine lecture-style tutorials and 14 hands-on tutorials.,2021,564
Kattis vs ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence,"Dunder, Nora and Lundborg, Saga and Wong, Jacqueline and Viberg, Olga","AI-powered education technologies can support students and teachers in computer science education. However, with the recent developments in generative AI, and especially the increasingly emerging popularity of ChatGPT, the effectiveness of using large language models for solving programming tasks has been underexplored. The present study examines ChatGPT’s ability to generate code solutions at different difficulty levels for introductory programming courses. We conducted an experiment where ChatGPT was tested on 127 randomly selected programming problems provided by Kattis, an automatic software grading tool for computer science programs, often used in higher education. The results showed that ChatGPT independently could solve 19 out of 127 programming tasks generated and assessed by Kattis. Further, ChatGPT was found to be able to generate accurate code solutions for simple problems but encountered difficulties with more complex programming tasks. The results contribute to the ongoing debate on the utility of AI-powered tools in programming education.",2024,565
Security Aspects of Behavioral Biometrics for Strong User Authentication,"Jancok, Vladimir and Ries, Michal","Efficient user identification and authentication are fundamental for securing access to systems processing sensitive data. This paper provides an analysis of current research in the field of user identification and identity verification with a focus on the behavioral biometrics supported by Machine Learning. It identifies the methods for user modeling with the potential of application in real-world scenarios such as strong authentication and fraud detection domain. This paper further elaborates on the current state-of-the-art approaches, feature extraction, and classification methods. We describe our experimental setup and provide an evaluation of our method in the selected deployment. We focus on user interactions in a controlled web environment. We performed classification experiments with the machine learning models on various datasets showing promising results in the robustness and proving relevance as a modern non-intrusive security measure.",2022,566
Multiple Linear Regression Prediction Model for DDOS Attack Detection in Cloud ELB,"Sambangi, Swathi and Gondi, Lakshmeeswari","An ongoing research challenge in cloud computing is to address the security and data availability challenges. Although, DDoS attacks in cloud are not new but still they have been continuously throwing new challenges to the network community which makes detection of these attacks an ongoing research challenge with respect to cloud security. One of the reasons for these challenges is the high non-linearity of the real-world data. Thus, we bring into light the importance of understanding the non-linearity of data. Understanding nature of traffic instances in network datasets helps to build efficient machine learning models. For building a machine learning model, we choose to apply regression analysis. Two datasets namely CICIDS 2017 and CICIDS 2019 are considered for the present study as these datasets show high non-linearity. In this paper, we propose to apply regression analysis after performing feature engineering addressing the problem of DDoS attack detection. We propose to visualize the regression model by plotting residual plot, fit chart. The learning models can also be evaluated by comparing their respective MAPE and accuracy values. To the best of our knowledge, the research addressed in this paper is the first contribution in cloud computing which depicts the importance data visualization in analyzing the machine learning models. We believe that this paper paves a way for future researchers in cloud computing to concentrate on data visualization.",2021,567
Rethinking data-driven networking with foundation models: challenges and opportunities,"Le, Franck and Srivatsa, Mudhakar and Ganti, Raghu and Sekar, Vyas","Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking.",2022,568
(Nothing But) Many Eyes Make All Bugs Shallow,"Wyss, Elizabeth and De Carli, Lorenzo and Davidson, Drew","Open source package repositories have become a crucial component of the modern software supply chain since they enable developers to easily and rapidly import code written by others. However, low quality, poorly vetted code residing in such repositories exposes developers and end-users to dangerous bugs and vulnerabilities at a large scale. Such issues have recently led to the creation of government-backed verification standards pertaining to packages, as well as a significant body of developer folklore regarding what constitutes a reliable package. However, there exists little academic research assessing the relationships between recommended development practices and known package issues in this domain. Motivated by this gap in understanding, we conduct a large-scale study that formally evaluates whether adherence to these guidelines meaningfully impacts reported issues and bug maintenance activity across the most widely utilized npm packages (encompassing 7,162 packages with over 100K weekly downloads each), which unveiled wide disparities across package-level metrics. We find that it is only recommendations pertaining to a broad notion of scrutiny that provide strong and reliable insights into the reporting and resolving of package issues. These findings pose significant implications for developers, who seek to identify well-maintained packages for use, as well as security researchers, who seek to identify suspicious packages for critical observation.",2023,569
BugListener: identifying and synthesizing bug reports from collaborative live chats,"Shi, Lin and Mu, Fangwen and Zhang, Yumin and Yang, Ye and Chen, Junjie and Chen, Xiao and Jiang, Hanzhi and Jiang, Ziyou and Wang, Qing","In community-based software development, developers frequently rely on live-chatting to discuss emergent bugs/errors they encounter in daily development tasks. However, it remains a challenging task to accurately record such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the task of identifying and synthesizing bug reports from community live chats, and propose a novel approach, named BugListener, to address the challenges. Specifically, BugListener automates three sub-tasks: 1) Disentangle the dialogs from massive chat logs by using a Feed-Forward neural network; 2) Identify the bug-report dialogs from separated dialogs by leveraging the Graph neural network to learn the contextual information; 3) Synthesize the bug reports by utilizing Transfer Learning techniques to classify the sentences into: observed behaviors (OB), expected behaviors (EB), and steps to reproduce the bug (SR). BugListener is evaluated on six open source projects. The results show that: for bug report identification, BugListener achieves the average F1 of 77.74%, improving the best baseline by 12.96%; and for bug report synthesis task, BugListener could classify the OB, EB, and SR sentences with the F1 of 84.62%, 71.46%, and 73.13%, improving the best baselines by 9.32%, 12.21%, 10.91%, respectively. A human evaluation study also confirms the effectiveness of BugListener in generating relevant and accurate bug reports. These demonstrate the significant potential of applying BugListener in community-based software development, for promoting bug discovery and quality improvement.",2022,570
Overview of the Eighth Dialog System Technology Challenge: DSTC8,"Kim, Seokhwan and Galley, Michel and Gunasekara, Chulaka and Lee, Sungjin and Atkinson, Adam and Peng, Baolin and Schulz, Hannes and Gao, Jianfeng and Li, Jinchao and Adada, Mahmoud and Huang, Minlie and Lastras, Luis and Kummerfeld, Jonathan K. and Lasecki, Walter S. and Hori, Chiori and Cherian, Anoop and Marks, Tim K. and Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav","This paper introduces the Eighth Dialog System Technology Challenge. In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.",2021,571
Where do Databases and Digital Forensics meet? A Comprehensive Survey and Taxonomy,"Seufitelli, Danilo B. and Brand\~{a}o, Michele A. and Fernandes, Ayane C. A. and Siqueira, Kayque M. and Moro, Mirella M.","We present a systematic literature review and propose a taxonomy for research at the intersection of Digital Forensics and Databases. The merge between these two areas has become more prolific due to the growing volume of data and mobile apps on the Web, and the consequent rise in cyber attacks. Our review has identified 91 relevant papers. The taxonomy categorizes such papers into: Cyber-Attacks (subclasses SQLi, Attack Detection, Data Recovery) and Criminal Intelligence (subclasses Forensic Investigation, Research Products, Crime Resolution). Overall, we contribute to better understanding the intersection between digital forensics and databases, and open opportunities for future research and development with potential for significant social, economic, and technical-scientific contributions.",2023,572
DifFuzz: differential fuzzing for side-channel analysis,"Nilizadeh, Shirin and Noller, Yannic and P\u{a}s\u{a}reanu, Corina S.","Side-channel attacks allow an adversary to uncover secret program data by observing the behavior of a program with respect to a resource, such as execution time, consumed memory or response size. Side-channel vulnerabilities are difficult to reason about as they involve analyzing the correlations between resource usage over multiple program paths. We present DifFuzz, a fuzzing-based approach for detecting side-channel vulnerabilities related to time and space. DifFuzz automatically detects these vulnerabilities by analyzing two versions of the program and using resource-guided heuristics to find inputs that maximize the difference in resource consumption between secret-dependent paths. The methodology of DifFuzz is general and can be applied to programs written in any language. For this paper, we present an implementation that targets analysis of Java programs, and uses and extends the Kelinci and AFL fuzzers. We evaluate DifFuzz on a large number of Java programs and demonstrate that it can reveal unknown side-channel vulnerabilities in popular applications. We also show that DifFuzz compares favorably against Blazer and Themis, two state-of-the-art analysis tools for finding side-channels in Java programs.",2019,573
Chatbot Development Using LangChain: A Case Study to Foster Critical Thinking and Creativity,"Farinetti, Laura and Canale, Lorenzo","Critical thinking and creativity are fundamental skills for engineers and computer scientists. The emergence of Large Language Models (LLMs) able to create chatbots that use natural language is an opportunity for educators to foster these skills. The well-known risk of generative AI for potential misinformation offers fertile ground to practice critical thinking.This paper describes a hands-on experience within a database course, where students had to develop a chatbot using the LangChain framework, and to evaluate it from different points of view. The students were free to choose the domain of their chatbot. The learning goal was twofold: on the one hand, to make them practice with state-of-the-art technologies, and on the other hand to stimulate critical analysis on their output. The paper discusses the students' evaluation of the chatbots under several metrics, including document retrieval, syntax and grammar accuracy, semantic relevance and information reliability. Students' assessments were also compared to the teachers' ones, to gain an insight on the critical attitude of the students and to offer a ground for discussion.The experience was stimulating and appreciated by the students. The final results highlight that the majority of students successfully produced chatbot responses that were grammatically and syntactically correct, and that consistently extracted pertinent sections from documents, yielding semantically relevant outputs. Despite these achievements, a significant portion of students expressed reservations about the reliability of the chatbot's responses to prompts, gaining awareness of LLMs' capability to generate responses that make sense to humans but may be potentially misleading.",2024,574
ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering,,,2022,575
ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security,,"It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.",2024,576
Social Media and Credibility: Civil Society Organizations in Mongolia,"Baasanjav, Undrah","This study aims at conveying an understanding and perception of the potentials and pitfalls of social media by Mongolians who work in not-for-profit organizations. By speaking to the experts in civil society institutions the researcher analyzed why the participants use social media, and how they assess the credibility of information. This exploratory study documents journalists’, educators’, and civil society experts’ accounts in relation to political campaign and mobilization tactics on social media. The participants' accounts to a great extent speak to communicative and deliberative potentials and affordances of social media use for civic discourses posited by the scholars in the tradition of the expanding “deliberative sphere.” They also speak to the platform-specific affordances that either constrain or enable different potentials and possibilities.",2020,577
Data flow refinement type inference,"Pavlinovic, Zvonimir and Su, Yusen and Wies, Thomas","Refinement types enable lightweight verification of functional programs. Algorithms for statically inferring refinement types typically work by reduction to solving systems of constrained Horn clauses extracted from typing derivations. An example is Liquid type inference, which solves the extracted constraints using predicate abstraction. However, the reduction to constraint solving in itself already signifies an abstraction of the program semantics that affects the precision of the overall static analysis. To better understand this issue, we study the type inference problem in its entirety through the lens of abstract interpretation. We propose a new refinement type system that is parametric with the choice of the abstract domain of type refinements as well as the degree to which it tracks context-sensitive control flow information. We then derive an accompanying parametric inference algorithm as an abstract interpretation of a novel data flow semantics of functional programs. We further show that the type system is sound and complete with respect to the constructed abstract semantics. Our theoretical development reveals the key abstraction steps inherent in refinement type inference algorithms. The trade-off between precision and efficiency of these abstraction steps is controlled by the parameters of the type system. Existing refinement type systems and their respective inference algorithms, such as Liquid types, are captured by concrete parameter instantiations. We have implemented our framework in a prototype tool and evaluated it for a range of new parameter instantiations (e.g., using octagons and polyhedra for expressing type refinements). The tool compares favorably against other existing tools. Our evaluation indicates that our approach can be used to systematically construct new refinement type inference algorithms that are both robust and precise.",2021,578
WEBSCI '24: Proceedings of the 16th ACM Web Science Conference,,,2024,579
User Perspectives on Ethical Challenges in Human-AI Co-Creativity: A Design Fiction Study,"Rezwana, Jeba and Maher, Mary Lou","In a human-AI co-creation, AI not only categorizes, evaluates and interprets data but also generates new content and interacts with humans. As co-creative AI is a form of intelligent technology that directly involves humans, it is critical to anticipate and address ethical issues during all design stages. The open-ended nature of human-AI interactions in co-creation poses many challenges for designing ethical co-creative AI systems. Researchers have been exploring ethical issues associated with autonomous AI in recent years, but ethics in human-AI co-creativity is a relatively new research area. In order to design human-centered ethical AI, it is important to understand the perspectives, expectations, and ethical concerns of potential users. In this paper, we present a study with 18 participants to explore several ethical dilemmas and challenges in human-AI co-creation from the perspective of potential users using a design fiction (DF) methodology. DF is a speculative research method that depicts a new concept or technology through stories as an intangible prototype. We present the findings from the study as potential users’ perspectives, stances, and expectations around ethical challenges in human-AI co-creativity as a basis for designing human-centered ethical AI partners for human-AI co-creation.",2023,580
Automatically Resolving Dependency-Conflict Building Failures via Behavior-Consistent Loosening of Library Version Constraints,"Wang, Huiyan and Liu, Shuguan and Zhang, Lingyu and Xu, Chang","Python projects grow quickly by code reuse and building automation based on third-party libraries. However, the version constraints associated with these libraries are prone to mal-configuration, and this forms a major obstacle to correct project building (known as dependency-conflict (DC) building failure). Our empirical findings suggest that such mal-configured version constraints were mainly prepared manually, and could essentially be refined for better quality to improve the chance of successful project building. We propose a LooCo approach to refining Python projects’ library version constraints by automatically loosening them to maximize their solutions, while keeping the libraries to observe their original behaviors. Our experimental results with real-life Python projects report that LooCo could efficiently refine library version constraints (0.4s per version loosening) by effective loosening (5.5 new versions expanded on average) automatically, and transform 54.8% originally unsolvable cases into solvable ones (i.e., successful building) and significantly increase solutions (21 more on average) for originally solvable cases.",2023,581
"JavaScript&amp;Me, A Tool to Support Research into Code Transformation and Browser Security","Lima, Susana and Morla, Ricardo and Routar, Jo\~{a}o","Doing research into code variations and their applications to browser security is challenging. One of the most important aspects of this research is to choose a relevant dataset on which machine learning algorithms can be applied to yield useful results. Although JavaScript code is widely available on various sources, such as package managers, code hosting platforms, and websites, collecting a large corpus of JavaScript and curating it is not a simple task. We present a novel open-source tool that helps with this task by allowing the automatic and systematic collection, processing, and transformation of JavaScript code. These three steps are performed by independent modules, and each one can be extended to incorporate new features, such as additional code sources, or transformation tools, adding to the flexibility of our tool and expanding its usability. Additionally, we use our tool to create a corpus of around 270k JavaScript files, including regular, minified, and obfuscated code, on which we perform a brief analysis. The conclusions from this analysis show the importance of properly curating a dataset before using it in research tasks, such as machine learning classifiers, reinforcing the relevance of our tool.",2022,582
A Survey of Learning-based Automated Program Repair,"Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu","Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .",2023,583
ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings,,"ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.",2023,584
Towards real-time and large-scale web accessbility,"P. Carvalho, Lu\'{\i}s and Guerreiro, Tiago and Lawson, Shaun and Montague, Kyle","We rely on large-scale web accessibility evaluations to obtain snapshots of Internet Health and understand trends and behaviours impacting overall web accessibility. Such evaluations are financially and time exhaustive, making the possibility of more real-time measurements of Internet Health infeasible. In this paper, we investigate the impacts of optimising the page selection processes of large-scale web accessibility evaluations. We set out to conduct an automated accessibility evaluation of 1500 websites using the ‘Home+’ sampling method (for each website, we evaluated the home page and all pages linked belonging to the same domain) as our baseline; then compared the agreement rates of web accessibility evaluations on further sub-sampled datasets. Accessibility data was successfully captured on 987 websites. Our findings demonstrate that a strong accessibility evaluation agreement between the baseline and the sub-sample datasets could be reached with a sub-sample of just 20% of the pages, significantly reducing the effort and resources required to conduct large-scale web accessibility evaluations.",2023,585
The A.I. Pandorica: Linking Ethically-challenged Technical Outputs to Prospective Policy Approaches,"J. Domanski, Robert","Artificial intelligence increasingly drives the modern world. Its rapid and continuous integration into the economic and political fabric of societies across the globe has placed the ethical challenges associated with A.I. onto national political agendas. This paper will deconstruct both the technical and regulatory challenges wrought by A.I. through an ethical lens. By providing a brief overview of how modern A.I. works, and defining and mapping its associated ethical issues to specific technical outputs, this paper will explore several promising paths forward.",2019,586
Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs,"Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin","Understanding and identifying the causes behind developers' emotions (e.g., Frustration caused by 'delays in merging pull requests') can be crucial towards finding solutions to problems and fostering collaboration in open-source communities. Effectively identifying such information in the high volume of communications across the different project channels, such as chats, emails, and issue comments, requires automated recognition of emotions and their causes. To enable this automation, large-scale software engineering-specific datasets that can be used to train accurate machine learning models are required. However, such datasets are expensive to create with the variety and informal nature of software projects' communication channels.In this paper, we explore zero-shot LLMs that are pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our evaluation indicates that these recently available models can identify emotion categories when given detailed emotions, although they perform worse than the top-rated models. For emotion cause identification, our results indicate that zero-shot LLMs are effective at recognizing the correct emotion cause with a BLEU-2 score of 0.598. To highlight the potential use of these techniques, we conduct a case study of the causes of Frustration in the last year of development of a popular open-source project, revealing several interesting insights.",2024,587
Towards Supporting Programming Education at Scale via Live Streaming,"Chen, Yan and Lasecki, Walter S. and Dong, Tao","Live streaming, which allows streamers to broadcast their work to live viewers, is an emerging practice for teaching and learning computer programming. Participation in live streaming is growing rapidly, despite several apparent challenges, such as a general lack of training in pedagogy among streamers and scarce signals about a stream's characteristics (e.g., difficulty, style, and usefulness) to help viewers decide what to watch. To understand why people choose to participate in live streaming for teaching or learning programming, and how they cope with both apparent and non-obvious challenges, we interviewed 14 streamers and viewers about their experience with live streaming programming. Among other results, we found that the casual and impromptu nature of live streaming makes it easier to prepare than pre-recorded videos, and viewers have the opportunity to shape the content and learning experience via real-time communication with both the streamer and each other. Nonetheless, we identified several challenges that limit the potential of live streaming as a learning medium. For example, streamers voiced privacy and harassment concerns, and existing streaming platforms do not adequately support viewer-streamer interactions, adaptive learning, and discovery and selection of streaming content. Based on these findings, we suggest specialized tools to facilitate knowledge sharing among people teaching and learning computer programming online, and we offer design recommendations that promote a healthy, safe, and engaging learning environment.",2021,588
From Participatory Sensing to Public-Private Partnership: The Development of AirBox Project in Taiwan,"Chung, Ming-Kuang and Ching, Fu-Shiang and Chen, Ling-Jyh","A complete sensor network should include sensors, data processing, and data services. However, to establish the legitimacy of sensor data for urban governance, sensor networks should go beyond simple deployment of sensors in the built environment and strive for deeper integration of data services within civil society. This paper presents the Taiwan AirBox Project as an exemplary case of practical deployment of a sensor network to discuss the topics of open data, value-added services, and joint calibration services; as well as how these services generate productive public-private partnerships.The AirBox project adopted a strategy of combining open-source hardware, flexible database API, multiple value-added data services, and open-joint calibration to gradually enhance the data quality. The results suggested that: 1. open hardware and open source software are keys to expanding the deployment of the sensor network; 2. open data and diverse value-added services enhance the public's environmental awareness and advocacy; 3. the open joint-calibration system helps connect government policy formulation with public environmental awareness.In addition, the AirBox project demonstrates the feasibility of a democratized deployment strategy. ""Openness"" serves as the foundation for mutual trust, communication, cooperation, and co-creation among stakeholders involved in the deployment process.",2023,589
Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,"Zhang, Lyuye and Liu, Chengwei and Xu, Zhengzi and Chen, Sen and Fan, Lingling and Zhao, Lida and Wu, Jiahui and Liu, Yang","With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compilation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that CORAL not only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.",2023,590
Advanced Domain-Driven Design for Consistency in Distributed Data-Intensive Systems,"Braun, Susanne and Bieniusa, Annette and Elberzhager, Frank","More and more data-intensive systems have emerged lately. Big Data, Artificial Intelligence, or cloud-native applications all require high scalability and availability. Data is no longer persisted in one central relational database with serialized and transactional access, but rather distributed and replicated among different nodes running only under eventual consistency. This poses a number of design challenges for software architects, as they cannot rely on a single system to mask the concurrency anomalies of concurrent access to distributed and replicated data. Based on three case studies, we developed a theory regarding how practitioners handle synchronization and consistency design challenges in distributed data-intensive applications. We also identified the ""white spots"" of missing design guidance needed by practitioners to handle the aforementioned challenges appropriately. We are currently evaluating our theory in the context of an action research study. In this study, we are also evaluating the novel design guidelines we are proposing in this regard, which, according to our theory, meet the needs of practitioners. Our design guidelines integrate with Domain-Driven Design, which is widely used in practice. Following the idea of multilevel serializability, we investigate the compatibility of business operations beyond commutativity. We provide concrete practical design guidance to achieve compatibility of non-commutative business operations. We also describe the basic infrastructure guarantees our design guidelines require from replication frameworks.",2021,591
On the use of fine-grained vulnerable code statements for software vulnerability assessment models,"Le, Triet Huynh Minh and Babar, M. Ali","Many studies have developed Machine Learning (ML) approaches to detect Software Vulnerabilities (SVs) in functions and fine-grained code statements that cause such SVs. However, there is little work on leveraging such detection outputs for data-driven SV assessment to give information about exploitability, impact, and severity of SVs. The information is important to understand SVs and prioritize their fixing. Using large-scale data from 1,782 functions of 429 SVs in 200 real-world projects, we investigate ML models for automating function-level SV assessment tasks, i.e., predicting seven Common Vulnerability Scoring System (CVSS) metrics. We particularly study the value and use of vulnerable statements as inputs for developing the assessment models because SVs in functions are originated in these statements. We show that vulnerable statements are 5.8 times smaller in size, yet exhibit 7.5--114.5% stronger assessment performance (Matthews Correlation Coefficient (MCC)) than non-vulnerable statements. Incorporating context of vulnerable statements further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score). Overall, we provide the initial yet promising ML-based baselines for function-level SV assessment, paving the way for further research in this direction.",2022,592
PEARC '23: Practice and Experience in Advanced Research Computing,,,2023,593
ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis,,"It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.",2023,594
CHASE '24: Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering,,"CHASE 2024 continues the tradition of a high-quality venue for research related to the cooperative and human aspects of software engineering. Researchers and practitioners have long recognized the need to investigate the cooperative and human aspects. However, their articles have been scattered across many conferences and communities. The CHASE conference provides academics and practitioners with a unified forum for discussing high-quality research studies, models, methods, and tools for human and cooperative aspects of software engineering.",2024,595
How Humans Perceive Human-like Behavior in Video Game Navigation,"Zuniga, Evelyn and Milani, Stephanie and Leroy, Guy and Rzepecki, Jaroslaw and Georgescu, Raluca and Momennejad, Ida and Bignell, Dave and Sun, Mingfei and Shaw, Alison and Costello, Gavin and Jacob, Mikhail and Devlin, Sam and Hofmann, Katja","The goal of this paper is to understand how people assess human-likeness in human- and AI-generated behavior. To this end, we present a qualitative study of hundreds of crowd-sourced assessments of human-likeness of behavior in a 3D video game navigation task. In particular, we focus on an AI agent that has passed a Turing Test, in the sense that human judges were not able to reliably distinguish between videos of a human and AI agent navigating on a quantitative level. Our insights shine a light on the characteristics that people consider as human-like. Understanding these characteristics is a key first step for improving AI agents in the future.",2022,596
A Longitudinal Study of Vulnerable Client-side Resources and Web Developers' Updating Behaviors,"Lim, Kyungchan and Kwon, Yonghwi and Kim, Doowon","Modern Websites rely on various client-side web resources, such as JavaScript libraries, to provide end-users with rich and interactive web experiences. Unfortunately, anecdotal evidence shows that improperly managed client-side resources could open up attack surfaces that adversaries can exploit. However, there is still a lack of a comprehensive understanding of the updating practices among web developers and the potential impact of inaccuracies in Common Vulnerabilities and Exposures (CVE) information on the security of the web ecosystem. In this paper, we conduct a longitudinal (four-year) measurement study of the security practices and implications on client-side resources (e.g., JavaScript libraries and Adobe Flash) across the Web. Specifically, we first collect a large-scale dataset of 157.2M webpages of Alexa Top 1M websites for four years in the wild. Analyzing the dataset, we find an average of 41.2% of websites (in each year of the four years) carry at least one vulnerable client-side resource (e.g., JavaScript or Adobe Flash). We also reveal that vulnerable JavaScript library versions are frequently observed in the wild, suggesting a concerning level of lagging update practice in the wild. On average, we observe 531.2 days with 25,337 websites of the window of vulnerability due to the unpatched client-side resources from the release of security patches. Furthermore, we manually investigate the fidelity of CVE (Common Vulnerabilities and Exposures) reports on client-side resources, leveraging PoC (Proof of Concept) code. We find that 13 CVE reports (out of 27) have incorrect vulnerable version information, which may impact security-related tasks such as security updates.",2023,597
Systemic Gender Inequities in Who Reviews Code,"Murphy-Hill, Emerson and Dicker, Jillian and Horvath, Amber and Hodges, Maggie Morrow and Egelman, Carolyn D. and Weingart, Laurie R. and Jaspan, Ciera and Green, Collin and Chen, Nina","Code review is an essential task for modern software engineers, where the author of a code change assigns other engineers the task of providing feedback on the author's code. In this paper, we investigate the task of code review through the lens of equity, the proposition that engineers should share reviewing responsibilities fairly. Through this lens, we quantitatively examine gender inequities in code review load at Google. We found that, on average, women perform about 25% fewer reviews than men, an inequity with multiple systemic antecedents, including authors' tendency to choose men as reviewers, a recommender system's amplification of human biases, and gender differences in how reviewer credentials are assigned and earned. Although substantial work remains to close the review load gap, we show how one small change has begun to do so.",2023,598
Detecting interpersonal conflict in issues and code review: cross pollinating open- and closed-source approaches,"Qiu, Huilian Sophie and Vasilescu, Bogdan and K\""{a}stner, Christian and Egelman, Carolyn and Jaspan, Ciera and Murphy-Hill, Emerson","Interpersonal conflict in code review, such as toxic language or an unnecessary pushback, is associated with negative outcomes such as stress and turnover. Automatic detection is one approach to prevent and mitigate interpersonal conflict. Two recent automatic detection approaches were developed in different settings: a toxicity detector using text analytics for open source issue discussions and a pushback detector using logs-based metrics for corporate code reviews. This paper tests how the toxicity detector and the pushback detector can be generalized beyond their respective contexts and discussion types, and how the combination of the two can help improve interpersonal conflict detection. The results reveal connections between the two concepts.Software engineers often communicate with one another on platforms that support tasks like discussing bugs and inspecting each others' code. Such discussions sometimes contain interpersonal conflict, which can lead to stress and abandonment. In this paper, we investigate how to automatically detect interpersonal conflict, both by analyzing the text of the what the engineers are saying and by analyzing the properties of that text.",2022,599
Practical Proactive DDoS-Attack Mitigation via Endpoint-Driven In-Network Traffic Control,"Liu, Zhuotao and Jin, Hao and Hu, Yih-Chun and Bailey, Michael","Volumetric attacks, which overwhelm the bandwidth of a destination, are among the most common distributed denial-of-service DDoS attacks today. Despite considerable effort made by both research and industry, our recent interviews with over 100 potential DDoS victims in over 10 industry segments indicate that today’s DDoS prevention is far from perfect. On one hand, few academical proposals have ever been deployed in the Internet; on the other hand, solutions offered by existing DDoS prevention vendors are not silver bullet to defend against the entire attack spectrum. Guided by such large-scale study of today’s DDoS defense, in this paper, we present MiddlePolice, the first readily deployable and proactive DDoS prevention mechanism. We carefully architect MiddlePolice such that it requires no changes from both the Internet core and the network stack of clients, yielding instant deployability in the current Internet architecture. Further, relying on our novel capability feedback mechanism, MiddlePolice is able to enforce destination-driven traffic control so that it guarantees to deliver victim-desired traffic regardless of the attacker strategies. We implement a prototype of MiddlePolice and demonstrate its feasibility via extensive evaluations in the Internet, hardware testbed, and large-scale simulations.",2018,600
Code smells for multi-language systems,"Abidi, Mouna and Grichi, Manel and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\""{e}l","Software quality becomes a necessity and no longer an advantage. In fact, with the advancement of technologies, companies must provide software with good quality. Many studies introduce the use of design patterns as improving software quality and discuss the presence of occurrences of design defects as decreasing software quality. Code smells include low-level problems in source code, poor coding decisions that are symptoms of the presence of anti-patterns in the code. Most of the studies present in the literature discuss the occurrences of design defects for mono-language systems. However, nowadays most of the systems are developed using a combination of several programming languages, in order to use particular features of each of them. As the number of languages increases, so does the number of design defects. They generally do not prevent the program from functioning correctly, but they indicate a higher risk of future bugs and makes the code less readable and harder to maintain. We analysed open-source systems, developers' documentation, bug reports, and programming language specifications and extracted bad practices related to multi-language systems. We encoded these practices in the form of code smells. We report in this paper 12 code smells.",2019,601
Catching Transparent Phish: Analyzing and Detecting MITM Phishing Toolkits,"Kondracki, Brian and Azad, Babak Amin and Starov, Oleksii and Nikiforakis, Nick","For over a decade, phishing toolkits have been helping attackers automate and streamline their phishing campaigns. Man-in-the- Middle (MITM) phishing toolkits are the latest evolution in this space, where toolkits act as malicious reverse proxy servers of online services, mirroring live content to users while extracting cre- dentials and session cookies in transit. These tools further reduce the work required by attackers, automate the harvesting of 2FA- authenticated sessions, and substantially increase the believability of phishing web pages.In this paper, we present the first analysis of MITM phishing toolkits used in the wild. By analyzing and experimenting with these toolkits, we identify intrinsic network-level properties that can be used to identify them. Based on these properties, we develop a machine learning classifier that identifies the presence of such toolkits in online communications with 99.9% accuracy.We conduct a large-scale longitudinal study of MITM phishing toolkits by creating a data-collection framework that monitors and crawls suspicious URLs from public sources. Using this infrastruc- ture, we capture data on 1,220 MITM phishing websites over the course of a year. We discover that MITM phishing toolkits occupy a blind spot in phishing blocklists, with only 43.7% of domains and 18.9% of IP addresses associated with MITM phishing toolkits present on blocklists, leaving unsuspecting users vulnerable to these attacks. Our results show that our detection scheme is resilient to the cloaking mechanisms incorporated by these tools, and is able to detect previously hidden phishing content. Finally, we propose methods that online services can utilize to fingerprint requests origi- nating from these toolkits and stop phishing attempts as they occur.",2021,602
OpenRank Leaderboard: Motivating Open Source Collaborations Through Social Network Evaluation in Alibaba,"Zhao, Shengyu and Xia, Xiaoya and Fitzgerald, Brian and Li, Xiaozhou and Lenarduzzi, Valentina and Taibi, Davide and Wang, Rong and Wang, Wei and Tian, Chunqi","Open source has revolutionized how software development is carried out, with a growing number of individuals and organizations contributing to open source projects. As the importance of open source continues to grow, companies also expect to grow thriving and sustainable open source communities with continued contributions and better collaborations. In this study, we applied the contribution leaderboard to seven open source projects initiated by Alibaba. We conducted a case study to investigate the perceptions and facts regarding how to motivate collaboration through gamification. Specifically, we employed a social network algorithm, OpenRank, to evaluate and steer developers' contributions. We validated the effectiveness of OpenRank by comparing it with other evaluation metrics and surveying developers. Through semi-structured interviews and project metric analysis, we found that the OpenRank Leaderboard can promote transparent communication environments, a better community atmosphere, and improved collaboration behavior.",2024,603
Affective Dynamics and Control in Group Processes,"Hoey, Jesse and Schr\""{o}der, Tobias and Morgan, Jonathan H. and Rogers, Kimberly B. and Nagappan, Meiyappan","The computational modeling of groups requires models that connect micro-level with macro-level processes and outcomes. Recent research in computational social science has started from simple models of human behaviour, and attempted to link to social structures. However, these models make simplifying assumptions about human understanding of culture that are of ten not realistic and may be limiting in their generality. In this paper, we present work on Bayesian affect control theory as a more comprehensive, yet highly parsimonious model that integrates artificial intelligence, social psychology, and emotions into a single predictive model of human activities in groups. We illustrate these developments with examples from an ongoing research project aimed at computational analysis of virtual software development teams.",2018,604
CAIN '24: Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI,,"The goal of the CAIN Conference Series is to bring together researchers and practitioners in software engineering, data science, and artificial intelligence (AI) as part of a growing community that is targeting the challenges of Software Engineering for AI-enabled systems.",2024,605
On the Use of Refactoring in Security Vulnerability Fixes: An Exploratory Study on Maven Libraries,"Ikegami, Ayano and Kula, Raula Gaikovina and Chinthanet, Bodin and Maeprasart, Vittunyuta and Ouni, Ali and Ishio, Takashi and Matsumoto, Kenichi","Third-party library dependencies are commonplace in today’s software development. With the growing threat of security vulnerabilities, applying security fixes in a timely manner is important to protect software systems. As such, the community developed a list of software and hardware weakness known as Common Weakness Enumeration (CWE) to assess vulnerabilities. Prior work has revealed that maintenance activities such as refactoring code potentially correlate with security-related aspects in the source code. In this work, we explore the relationship between refactoring and security by analyzing refactoring actions performed jointly with vulnerability fixes in practice. We conducted a case study to analyze 143 maven libraries in which 351 known vulnerabilities had been detected and fixed. Surprisingly, our exploratory results show that developers incorporate refactoring operations in their fixes, with 31.9% (112 out of 351) of the vulnerabilities paired with refactoring actions. We envision this short paper to open up potential new directions to motivate automated tool support, allowing developers to deliver fixes faster, while maintaining their code.",2022,606
HAI '22: Proceedings of the 10th International Conference on Human-Agent Interaction,,,2022,607
CoNEXT '22: Proceedings of the 18th International Conference on emerging Networking EXperiments and Technologies,,"CoNEXT is a premier and highly selective venue in computer networking. This year's exciting technical program helps us better understand and improve the performance, reliability and security of networks in all layers.",2022,608
SCORED '23: Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses,,"It is our great pleasure to welcome you to ACM SCORED '23, the second edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Copenhagen, Denmark with extensive support for in-person and virtual attendance.This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.",2023,609
What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,"Gong, Lina and Zhang, Jingxuan and Wei, Mingqiang and Zhang, Haoxiang and Huang, Zhiqiu","There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.",2023,610
On the Shoulders of Giants: A New Dataset for Pull-based Development Research,"Zhang, Xunhui and Rastogi, Ayushi and Yu, Yue","Pull-based development is a widely adopted paradigm for collaboration in distributed software development, attracting eyeballs from both academic and industry. To better study pull-based development model, this paper presents a new dataset containing 96 features collected from 11,230 projects and 3,347,937 pull requests. We describe the creation process and explain the features in details. To the best of our knowledge, our dataset is the most comprehensive and largest one toward a complete picture for pull-based development research.",2020,611
"Teaching AI to K-12 Learners: Lessons, Issues, and Guidance","Grover, Shuchi","There is growing recognition of the need to teach artificial intelli- gence (AI) and machine learning (ML) at the school level. This push acknowledges the meteoric growth in the range and diversity of ap- plications of ML in all industries and everyday consumer products, with Large Language Models (LLMs) being only the latest and most compelling example yet. Efforts to bring AI, especially ML educa- tion to school learners are being propelled by substantial industry interest, research efforts, as well as technological developments that make sophisticated ML tools readily available to learners of all ages. These early efforts span a variety of learning goals captured by the AI4K12 ""big ideas"" framework and employ a plurality of pedagogies.This paper provides a sense for the current state of the field, shares lessons learned from early K-12 AI education as well as CS education efforts that can be leveraged, highlights issues that must be addressed in designing for teaching AI in K-12, and provides guidance for future K-12 AI education efforts and tackle what to many feels like ""the next new thing"".",2024,612
An Approach for Improving DBpedia as a Research Data Hub,"Ngomo, Jean Gabriel Nguema and Lopes, Giseli Rabello and Campos, Maria Luiza Machado and Cavalcanti, Maria Claudia Reis","Extracted from Wikipedia content, DBpedia is considered one of the most important knowledge bases of the Semantic Web, which has editions in several languages, among which those in English (DBpedia EN) and Portuguese (DBpedia PT). All DBpedia editions are subject to quality issues, more especially DBpedia PT suffers from inconsistencies and lack of data in several domains. This paper describes a semi-automatic and incremental process for publishing data on DBpedia, coming from reliable external sources, while seeking to improve aspects of its quality. In an open science context, the proposal aims at consolidating DBpedia as a reference hub for research data, so that research from any area supported by the Semantic Web data can use its data reliably. Although the approach is independent from a specific DBpedia edition, the supporting prototype tool, named ETL4DBpedia, was built for DBpedia PT, based on ETL workflows (Extract, Transform, Load). This paper also describes the assessment of the approach, applying the tool in a real-usage scenario involving data from the field of botany. This application resulted in an increase by 127% in the completeness of species of medicinal plants in DBpedia PT, besides showing satisfactory performance for ETL4Bpedia components.",2020,613
Online Hackathons as an Engaging Tool to Promote Group Work in Emergency Remote Learning,"Gama, Kiev and Zimmerle, Carlos and Rossi, Pedro","In 2020, due to the COVID-19 pandemic, educational activities had to be done remotely as a way to avoid the spread of the disease. Instead of shifting to an online learning model, it consisted of a transition to what was called Emergency Remote Teaching. This is a strategy to keep activities going on until it is safe again to return to the physical facilities of universities. This new setting became a challenge to both teachers and students. The lack of interaction and classroom socialization became obstacles for students to continue engaged.Before the pandemic, hackathons -- short-lived events (1 to 3 days) of intensive collaboration to develop software prototypes -- were being explored as an alternative venue to engage students in acquiring and practicing technical skills. In this paper, we present an experience report on the usage of an online hackathon as a resource to engage students in the development of their semester project in a distributed applications course during this emergency remote teaching period. We describe the intervention and analyze the students' perspective of the approach. One of the findings was the importance of the Discord communication tool -- used by students for playing games -- which helped them socialize and be engaged in synchronous group work, ""virtually collocated"".",2021,614
ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,,,2016,615
I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,"Venturini, Daniel and Cogo, Filipe Roseiro and Polato, Ivanilton and Gerosa, Marco A. and Wiese, Igor Scaliante","Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages’ builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider’s version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them.",2023,616
"Honey, I Cached our Security Tokens Re-usage of Security Tokens in the Wild","Trampert, Leon and Stock, Ben and Roth, Sebastian","In order to mitigate the effect of Web attacks, modern browsers support a plethora of different security mechanisms. Mechanisms such as anti-Cross-Site Request Forgery (CSRF) tokens or nonces in a Content Security Policy rely on a random number that must only be used once. Notably, those Web security mechanisms are shipped through HTML tags or HTTP response headers from the server to the client side. To decrease the server load and the traffic burdened on the server infrastructure, many Web applications are served via a Content Delivery Network (CDN), which caches certain responses from the server to deliver them to multiple clients. This, however, affects not only the content but also the settings of the security mechanisms deployed via HTML meta tags or HTTP headers. If those are also cached, their content is fixed, and the security tokens are no longer random for each request. Even if the responses are not cached, operators may re-use tokens, as generating random numbers that are unique for each request introduces additional complexity for preserving the state on the server side. This work sheds light on the re-usage of security tokens in the wild, investigates what caused the static tokens, and elaborates on the security impact of the non-random security tokens.",2023,617
An internet census taken by an illegal botnet: a qualitative assessment of published measurements,"Krenc, Thomas and Hohlfeld, Oliver and Feldmann, Anja","On March 17, 2013, an Internet census data set and an accompanying report were released by an anonymous author or group of authors. It created an immediate media buzz, mainly because of the unorthodox and unethical data collection methodology (i.e., exploiting default passwords to form the Carna botnet), but also because of the alleged unprecedented large scale of this census (even though legitimate census studies of similar and even larger sizes have been performed in the past). Given the unknown source of this released data set, little is known about it. For example, can it be ruled out that the data is faked? Or if it is indeed real, what is the quality of the released data?The purpose of this paper is to shed light on these and related questions and put the contributions of this anonymous Internet census study into perspective. Indeed, our findings suggest that the released data set is real and not faked, but that the measurements suffer from a number of methodological flaws and also lack adequate meta-data information. As a result, we have not been able to verify several claims that the anonymous author(s) made in the published report.In the process, we use this study as an educational example for illustrating how to deal with a large data set of unknown quality, hint at pitfalls in Internet-scale measurement studies, and discuss ethical considerations concerning third-party use of this released data set for publications.",2014,618
Method overloading the circuit,"Meiklejohn, Christopher and Stark, Lydia and Celozzi, Cesare and Ranney, Matt and Miller, Heather","Circuit breakers are frequently deployed in microservice applications to improve their reliability. They achieve this by short circuiting RPC invocations issued to overloaded or failing services, thereby relieving pressure on those services and allowing them to recover. In this paper, we systematically examine the state of the art in industrial circuit breakers designs. We first present a taxonomy of existing, open-source circuit breaker designs and implementations based on a systematic mapping study. We then examine the relationship between these circuit breaker designs and application reliability. We make a clear case that incorrect application of circuit breakers to an application can hurt reliability in the process of trying to improve it. To address the deficiencies in the state of the art, we propose two new circuit breaker designs and provide guidance on how to properly structure microservice applications for the best circuit breaker use. Finally, we identify several open challenges in circuit breaker usage and design for future researchers.",2022,619
Types from data: making structured data first-class citizens in F#,"Petricek, Tomas and Guerra, Gustavo and Syme, Don","Most modern applications interact with external services and access data in structured formats such as XML, JSON and CSV. Static type systems do not understand such formats, often making data access more cumbersome. Should we give up and leave the messy world of external data to dynamic typing and runtime checks? Of course, not! We present F# Data, a library that integrates external structured data into F#. As most real-world data does not come with an explicit schema, we develop a shape inference algorithm that infers a shape from representative sample documents. We then integrate the inferred shape into the F# type system using type providers. We formalize the process and prove a relative type soundness theorem. Our library significantly reduces the amount of data access code and it provides additional safety guarantees when contrasted with the widely used weakly typed techniques.",2016,620
Correlations between deep neural network model coverage criteria and model quality,"Yan, Shenao and Tao, Guanhong and Liu, Xuwei and Zhai, Juan and Ma, Shiqing and Xu, Lei and Zhang, Xiangyu","Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.",2020,621
"ICCVIT '23: Proceedings of the 2023 International Conference on Computer, Vision and Intelligent Technology",,,2023,622
"""They Can Only Ever Guide"": How an Open Source Software Community Uses Roadmaps to Coordinate Effort","Klug, Daniel and Bogart, Christopher and Herbsleb, James D.","Unlike in commercial software development, open source software (OSS) projects do not generally have managers with direct control over how developers spend their time, yet for projects with large, diverse sets of contributors, the need exists to focus and steer development in a particular direction in a coordinated way. This is especially important for ""infrastructure"" projects, such as critical libraries and programming languages that many other people depend on. Some projects have taken the approach of borrowing planning tools that originated in commercial development, despite the fact that these techniques were designed for very different contexts, e.g. strong top-down control and profit motives. Little research has been done to understand how these practices are adapted to a new context. In this paper, we examine the Rust project's use of roadmaps: how has an important OSS infrastructure project adapted an inherently top-down tool to the freewheeling world of OSS? We find that because Rust's roadmaps are built in part by summarizing what motivated developers most prefer to work on, they are in some ways more a description of the motivated labor available than they are a directive that the community move in a particular direction. They allow the community to avoid wasting time on unpopular proposals by revealing that there will be little help in building them, and encouraging work on popular features by making visible the amount of consensus in those features. Roadmaps generate a collective focus without limiting the full scope of what developers work on: roadmap issues consume proportionally more effort than other issues, but constitute a minority of the work done (i.e issues and pull requests made) by both central and peripheral participants. They also create transparency among and beyond the community into what central contributors' plans are, and allow more rational decision-making by providing a way for evidence about community needs to be linked to decision-making.",2021,623
Text2EL+: Expert Guided Event Log Enrichment Using Unstructured Text,"Kapugama Geeganage, Dakshi Tharanga and Wynn, Moe Thandar and ter Hofstede, Arthur H. M.","Through the application of process mining, business processes can be improved on the basis of process execution data captured in event logs. Naturally, the quality of this data determines the quality of the improvement recommendations. Improving data quality is non-trivial, and there is great potential to exploit unstructured text, e.g., from notes, reviews, and comments, for this purpose and to enrich event logs. To this end, this article introduces Text2EL+&nbsp;, a three-phase approach to enrich event logs using unstructured text. In its first phase, events and (case and event) attributes are derived from unstructured text linked to organisational processes. In its second phase, these events and attributes undergo a semantic and contextual validation before their incorporation in the event log. In its third and final phase, recognising the importance of human domain expertise, expert guidance is used to further improve data quality by removing redundant and irrelevant events. Expert input is used to train a Named Entity Recognition (NER) model with customised tags to detect event log elements. The approach applies natural language processing techniques, sentence embeddings, training pipelines and models, as well as contextual and expression validation. Various unstructured clinical notes associated with a healthcare case study were analysed, and completeness, concordance, and correctness of the derived event log elements were evaluated through experiments. The results show that the proposed method is feasible and applicable.",2024,624
Xanthus: Push-button Orchestration of Host Provenance Data Collection,"Han, Xueyuan and Mickens, James and Gehani, Ashish and Seltzer, Margo and Pasquier, Thomas","Host-based anomaly detectors generate alarms by inspecting audit logs for suspicious behavior. Unfortunately, evaluating these anomaly detectors is hard. There are few high-quality, publicly-available audit logs, and there are no pre-existing frameworks that enable push-button creation of realistic system traces. To make trace generation easier, we created Xanthus, an automated tool that orchestrates virtual machines to generate realistic audit logs. Using Xanthus' simple management interface, administrators select a base VM image, configure a particular tracing framework to use within that VM, and define post-launch scripts that collect and save trace data. Once data collection is finished, Xanthus~creates a self-describing archive, which contains the VM, its configuration parameters, and the collected trace data. We demonstrate that Xanthus~hides many of the tedious (yet subtle) orchestration tasks that humans often get wrong; Xanthus~avoids mistakes that lead to non-replicable experiments.",2020,625
Exploring the Profile of University Assessments Flagged as Containing AI-Generated Material,"Gooch, Daniel and Waugh, Kevin and Richards, Mike and Slaymaker, Mark and Woodthorpe, John",,2024,626
SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering,,,2022,627
What News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations,"Chouaki, Salim and Chakraborty, Abhijnan and Goga, Oana and Zannettou, Savvas","Understanding how exposure to news on social media impacts public discourse and exacerbates political polarization is a significant endeavor in both computer and social sciences. Unfortunately, progress in this area is hampered by limited access to data due to the closed nature of social media platforms. Consequently, prior studies have been constrained to considering only fragments of users' news exposure and reactions. To overcome this obstacle, we present an innovative measurement approach centered on donating personal data for scientific purposes, facilitated through a privacy-preserving tool that captures users' interactions with news on Facebook. This approach offers a nuanced perspective on users' news exposure and consumption, encompassing different types of news exposure: selective, incidental, algorithmic, and targeted, driven by the diverse underlying mechanisms governing news appearance on users' feeds. Our analysis of data from 472 participants based in the U.S. reveals several interesting findings. For instance, users are more prone to encountering misinformation because of their active selection of low-quality news sources rather than being exposed solely due to friends or platform algorithms. Furthermore, our study uncovers that users are open to engaging with news sources with opposite political ideology as long as these interactions are not visible to their immediate social circles. Overall, our study showcases the viability of data donation as a means to provide clarity to longstanding questions in this field, offering new perspectives on the intricate dynamics of social media news consumption and its effects.",2024,628
PropR: property-based automatic program repair,"Gissurarson, Matth\'{\i}as P\'{a}ll and Applis, Leonhard and Panichella, Annibale and van Deursen, Arie and Sands, David","Automatic program repair (APR) regularly faces the challenge of overfitting patches --- patches that pass the test suite, but do not actually address the problems when evaluated manually. Currently, overfit detection requires manual inspection or an oracle making quality control of APR an expensive task. With this work, we want to introduce properties in addition to unit tests for APR to address the problem of overfitting. To that end, we design and implement PropR, a program repair tool for Haskell that leverages both property-based testing (via QuickCheck) and the rich type system and synthesis offered by the Haskell compiler. We compare the repair-ratio, time-to-first-patch and overfitting-ratio when using unit tests, property-based tests, and their combination. Our results show that properties lead to quicker results and have a lower overfit ratio than unit tests. The created overfit patches provide valuable insight into the underlying problems of the program to repair (e.g., in terms of fault localization or test quality). We consider this step towards fitter, or at least insightful, patches a critical contribution to bring APR into developer workflows.",2022,629
A Computation of the Ninth Dedekind Number using FPGA Supercomputing,"Hirtum, Lennart Van and Causmaecker, Patrick De and Goemaere, Jens and Kenter, Tobias and Riebler, Heinrich and Lass, Michael and Plessl, Christian","This manuscript makes the claim of having computed the  (9^{th})  Dedekind number, D(9). This was done by accelerating the core operation of the process with an efficient FPGA design that outperforms an optimized 64-core CPU reference by 95 (times) . The FPGA execution was parallelized on the Noctua 2 supercomputer at Paderborn University. The resulting value for D(9) is (286386577668298411128469151667598498812366) 
. This value can be verified in two steps. We have made the data file containing the 490M results available, each of which can be verified separately on CPU, and the whole file sums to our proposed value. The paper explains the mathematical approach in the first part, before putting the focus on a deep dive into the FPGA accelerator implementation followed by a performance analysis. The FPGA implementation was done in RTL using a dual-clock architecture and shows how we achieved an impressive FMax of 450MHz on the targeted Stratix 10 GX 2800 FPGAs. The total compute time used was 47’000 FPGA Hours.",2024,630
ASONAM '23: Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,,The ASONAM conference series brings together researchers from around the world to share the latest advances in the attractive field of Social Networks Analysis and Mining.,2023,631
What makes a code change easier to review: an empirical investigation on code change reviewability,"Ram, Achyudh and Sawant, Anand Ashok and Castelluccio, Marco and Bacchelli, Alberto","Peer code review is a practice widely adopted in software projects to improve the quality of code. In current code review practices, code changes are manually inspected by developers other than the author before these changes are integrated into a project or put into production. We conducted a study to obtain an empirical understanding of what makes a code change easier to review. To this end, we surveyed published academic literature and sources from gray literature (blogs and white papers), we interviewed ten professional developers, and we designed and deployed a reviewability evaluation tool that professional developers used to rate the reviewability of 98 changes. We find that reviewability is defined through several factors, such as the change description, size, and coherent commit history. We provide recommendations for practitioners and researchers. Public preprint [https://doi.org/10.5281/zenodo.1323659]; data and materials [https://doi.org/10.5281/zenodo.1323659].",2018,632
TsuNAME: exploiting misconfiguration and vulnerability to DDoS DNS,"Moura, Giovane C. M. and Castro, Sebastian and Heidemann, John and Hardaker, Wes","TheInternet's Domain Name System (DNS) is a part of every web request and e-mail exchange, so DNS failures can be catastrophic, taking out major websites and services. This paper identifies TsuNAME, a vulnerability where some recursive resolvers can greatly amplify queries, potentially resulting in a denial-of-service to DNS services. TsuNAME is caused by cyclical dependencies in DNS records. A recursive resolver repeatedly follows these cycles, coupled with insufficient caching and application-level retries greatly amplify an initial query, stressing authoritative servers. Although issues with cyclic dependencies are not new, the scale of amplification has not previously been understood. We document real-world events in .nz (a country-level domain), where two misconfigured domains resulted in a 50% increase on overall traffic. We reproduce and document root causes of this event through experiments, and demostrate a 500\texttimes{} amplification factor. In response to our disclosure, several DNS software vendors have documented their mitigations, including Google public DNS and Cisco OpenDNS. For operators of authoritative DNS services we have developed and released CycleHunter, an open-source tool that detects cyclic dependencies and prevents attacks. We use CycleHunter to evaluate roughly 184 million domain names in 7 large, top-level domains (TLDs), finding 44 cyclic dependent NS records used by 1.4k domain names. The TsuNAME vulnerability is weaponizable, since an adversary can easily create cycles to attack the infrastructure of a parent domains. Documenting this threat and its solutions is an important step to ensuring it is fully addressed.",2021,633
RecPipe: Co-designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance,"Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David","Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 \texttimes{} and 6 \texttimes{}.",2021,634
Graph Augmentation Learning,"Yu, Shuo and Huang, Huafei and Dao, Minh N. and Xia, Feng","Graph Augmentation Learning (GAL) provides outstanding solutions for graph learning in handling incomplete data, noise data, etc. Numerous GAL methods have been proposed for graph-based applications such as social network analysis and traffic flow forecasting. However, the underlying reasons for the effectiveness of these GAL methods are still unclear. As a consequence, how to choose optimal graph augmentation strategy for a certain application scenario is still in black box. There is a lack of systematic, comprehensive, and experimentally validated guideline of GAL for scholars. Therefore, in this survey, we in-depth review GAL techniques from macro (graph), meso (subgraph), and micro (node/edge) levels. We further detailedly illustrate how GAL enhance the data quality and the model performance. The aggregation mechanism of augmentation strategies and graph learning models are also discussed by different application scenarios, i.e., data-specific, model-specific, and hybrid scenarios. To better show the outperformance of GAL, we experimentally validate the effectiveness and adaptability of different GAL strategies in different downstream tasks. Finally, we share our insights on several open issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability, and generalization.",2022,635
Cyber Attack Intent Recognition and Active Deception using Factored Interactive POMDPs,"Shinde, Aditya and Doshi, Prashant and Setayeshfar, Omid","This paper presents an intelligent and adaptive agent that employs deception to recognize a cyber adversary's intent on a honeypot host. Unlike previous approaches to cyber deception, which mainly focus on delaying or confusing the attackers, we focus on engaging with them to learn their intent. We model cyber deception as a sequential decision-making problem in a two-agent context. We introduce factored finitely-nested interactive POMDPs (I-POMDPX) and use this framework to model the problem with multiple attacker types. Our approach models cyber attacks on a single honeypot host across multiple phases from the attacker's initial entry to reaching its adversarial objective. The defending I-POMDPX-based agent uses decoys to engage with the attacker at multiple phases to form increasingly accurate predictions of the attacker's behavior and intent. The use of I-POMDPs also enables us to model the adversary's mental state and investigate how deception affects their beliefs. Our experiments in both simulation and with the agent deployed on a host system show that the I-POMDPX-based agent performs significantly better at intent recognition than commonly used deception strategies on honeypots. This emerging application of autonomous agents offers a new approach that contrasts with the traditional action-reaction dynamic that has defined interactions between cyber attackers and defenders for years.",2021,636
Designing Better Location Fields in User Profiles,"Wang, Ting-Yu and Harper, F. Maxwell and Hecht, Brent","Twitter, Facebook, Pinterest and many other online communities ask their users to populate a location field in their user profiles. The information that is entered into this field has many uses in both industry and academia, with location field data providing valuable geographic context for operators of online communities and playing key roles in numerous research projects. However, despite the importance of location field entries, we know little about how to design location fields effectively. In this paper, we report the results of the first controlled study of the design of location fields in user profiles. After presenting a survey of location field design decisions in use across many online communities, we show that certain design decisions can lead to more granular location information or a higher percentage of users that fill out the field, but that there is a trade-off between granularity and the percent of non-empty fields. We also add context to previous work that found that location fields tend to have a high rate of non-geographic information (e.g. Location: ""Justin Bieber's Heart""), showing that this result may be site-specific rather than endemic to all location fields. Finally, we provide evidence that verifying users' location field entries against a database of known-valid locations can eliminate toponym (place name) ambiguity and any non-geographic location field entries while at the same time having little effect on field population rate and granularity.",2014,637
SONAR: Automatic Detection of Cyber Security Events over the Twitter Stream,"Le Sceller, Quentin and Karbab, ElMouatez Billah and Debbabi, Mourad and Iqbal, Farkhund","Everyday, security experts face a growing number of security events that affecting people well-being, their information systems and sometimes the critical infrastructure. The sooner they can detect and understand these threats, the more they can mitigate and forensically investigate them. Therefore, they need to have a situation awareness of the existing security events and their possible effects. However, given the large number of events, it can be difficult for security analysts and researchers to handle this flow of information in an adequate manner and answer the following questions in near-real time: what are the current security events? How long do they last? In this paper, we will try to answer these issues by leveraging social networks that contain a massive amount of valuable information on many topics. However, because of the very high volume, extracting meaningful information can be challenging. For this reason, we propose SONAR: an automatic, self-learned framework that can detect, geolocate and categorize cyber security events in near-real time over the Twitter stream. SONAR is based on a taxonomy of cyber security events and a set of seed keywords describing type of events that we want to follow in order to start detecting events. Using these seed keywords, it automatically discovers new relevant keywords such as malware names to enhance the range of detection while staying in the same domain. Using a custom taxonomy describing all type of cyber threats, we demonstrate the capabilities of SONAR on a dataset of approximately 47.8 million tweets related to cyber security in the last 9 months. SONAR could efficiently and effectively detect, categorize and monitor cyber security related events before getting on the security news, and it could automatically discover new security terminologies with their event. Additionally, SONAR is highly scalable and customizable by design; therefore we could adapt SONAR framework for virtually any type of events that experts are interested in.",2017,638
Gender and Participation in Open Source Software Development,"Frluckaj, Hana and Dabbish, Laura and Widder, David Gray and Qiu, Huilian Sophie and Herbsleb, James D.","Open source software represents an important form of digital infrastructure as well as a pathway to technical careers for many developers, but women are drastically underrepresented in this setting. Although there is a good body of literature on open source participation, there is very little understanding of the participation trajectories and contribution experiences of women developers, and how they compare to those of men developers, in open source software projects. In order to understand their joining and participation trajectories, we conducted interviews with 23 developers (11 men and 12 women) who became core in an open source project. We identify differences in women and men's motivations for initial contributions and joining processes (e.g. women participating in projects that they have been invited to) and sustained involvement in a project. We also describe unique negative experiences faced by women contributors in this setting in each stage of participation. Our results have implications for diversifying participation in open source software and understanding open source as a pathway to technical careers.",2022,639
Community Connect: A Mock Social Media Platform to Study Online Behavior,"Mahajan, Khyati and Roy Choudhury, Sourav and Levens, Sara and Gallicano, Tiffany and Shaikh, Samira","We present Community Connect, a custom social media platform for conducting controlled experiments of human behavior. The key distinguishing factor of Community Connect is the ability to control the visibility of user posts based on the groups they belong to, allowing careful and controlled investigation into how information propagates through a social network. We release this platform as a resource to the broader community, to facilitate research on data collected through controlled experiments on social networks.",2021,640
"Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review","Araujo, Hugo and Mousavi, Mohammad Reza and Varshosaz, Mahsa","We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which 195 were included, reviewed, and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy, the majority use domain-agnostic generic measures such as number of failures, size of state-space, or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall, there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.",2023,641
ICEGOV '22: Proceedings of the 15th International Conference on Theory and Practice of Electronic Governance,,,2022,642
Analysis of Microservice Evolution using Cohesion Metrics,"Moreira, Mateus Gabi and De Fran\c{c}a, Breno Bernard Nicolau","The adoption of Microservices Architecture (MSA) has increased in recent years due to several claimed benefits, such as reducing deployment complexity, supporting technology diversity, and better scalability. However, MSA is not free from maintainability issues, especially the lack of cohesion, in which microservices possibly concentrate or miss responsibilities. Also, the lack of empirically-validated cohesion metrics for MSA makes the quantitative assessment even more challenging. In this paper, we empirically explore the practical applicability of service-level cohesion metrics in an open-source MSA application context. The qualitative results show the possibility of assessing MSA cohesion using these service-level metrics, the feasibility of tracking software evolution, and an indication of possible technical debts along the way.",2022,643
Co-design in the wild: a case study on meme creation tools,"Maceli, Monica","The internet meme has become a vital form of self-expression in social communities throughout the Internet. The tools facilitating meme-creation, specifically image macros, have been little-studied but endow non-technical users with the ability to create the multi-layered graphics typical to such memes. The use of these creativity tools provides a unique setting in which to explore the concept of co-design, wherein tools are shaped in response to emergent user needs. Users and designers of meme-creation tools have evolved ways of collaborating and communicating over time, in a fully naturalistic setting. This study explores these processes through survey and interviews of tool designer/developers and an analysis of users' design ideas generated over time. The study finds that, while co-design may be commonly practiced today, these activities raise numerous challenges to participatory design, including: creating trust between designers and users, managing unwieldy system growth, and supporting features specific to aspiring end-user crafters.",2016,644
A Qualitative Study of Dependency Management and Its Security Implications,"Pashchenko, Ivan and Vu, Duc-Ly and Massacci, Fabio","Several large scale studies on the Maven, NPM, and Android ecosystems point out that many developers do not often update their vulnerable software libraries thus exposing the user of their code to security risks. The purpose of this study is to qualitatively investigate the choices and the interplay of functional and security concerns on the developers' overall decision-making strategies for selecting, managing, and updating software dependencies.We run 25 semi-structured interviews with developers of both large and small-medium enterprises located in nine countries. All interviews were transcribed, coded, and analyzed according to applied thematic analysis. They highlight the trade-offs that developers are facing and that security researchers must understand to provide effective support to mitigate vulnerabilities (for example bundling security fixes with functional changes might hinder adoption due to lack of resources to fix functional breaking changes).We further distill our observations to actionable implications on what algorithms and automated tools should achieve to effectively support (semi-)automatic dependency management.",2020,645
"Sustaining Scientific Open-Source Software Ecosystems: Challenges, Practices, and Opportunities","Sun, Jiayi","Scientific open-source software (scientific OSS) has facilitated scientific research due to its transparent and collaborative nature. The sustainability of such software is becoming crucial given its pivotal role in scientific endeavors. While past research has proposed strategies for the sustainability of the scientific software or general OSS communities in isolation, it remains unclear when the two scenarios are merged if these approaches are directly applicable to developing scientific OSS. In this research, we propose to investigate the unique challenges in sustaining the scientific OSS ecosystems. We first conduct a case study to empirically understand the interdisciplinary team's collaboration in scientific OSS ecosystems and identify the collaboration challenges. Further, to generalize our findings, we plan to conduct a large-scale quantitative study in broader scientific OSS ecosystems to identify the cross-project collaboration inefficiencies. Finally, we would like to design and develop interventions to mitigate the problems identified.",2024,646
CSCW'22 Companion: Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing,,,2022,647
Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks,"Li, Zongjie and Wang, Chaozheng and Wang, Shuai and Gao, Cuiyun","The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their ""synonyms"" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API.We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",2023,648
Towards fair and pro-social employment of digital pieceworkers for sourcing machine learning training data,"Rothschild, Annabel and Booker, Justin and Davoll, Christa and Hill, Jessica and Ivey, Venise and DiSalvo, Carl and Rydal Shapiro, Ben and DiSalvo, Betsy",,2022,649
Using a context-aware approach to recommend code reviewers: findings from an industrial case study,"Strand, Anton and Gunnarson, Markus and Britto, Ricardo and Usman, Muhmmad","Code review is a commonly used practice in software development. It refers to the process of reviewing new code changes before they are merged with the code base. However, to perform the review, developers are mostly assigned manually to code changes. This may lead to problems such as: a time-consuming selection process, limited pool of known candidates and risk of over-allocation of a few reviewers. To address the above problems, we developed Carrot, a machine learning-based tool to recommend code reviewers. We conducted an improvement case study at Ericsson. We evaluated Carrot using a mixed approach. we evaluated the prediction accuracy using historical data and the metrical Mean Reciprocal Rank (MRR). Furthermore, we deployed the tool in one Ericsson project and evaluated how adequate the recommendations were from the point of view of the tool users and the recommended reviewers. We also asked the opinion of senior developers about the usefulness of the tool. The results show that Carrot can help identify relevant non-obvious reviewers and be of great assistance to new developers. However, there were mixed opinions on Carrot's ability to assist with workload balancing and the decrease code review lead time.",2020,650
WhoDo: automating reviewer suggestions at scale,"Asthana, Sumit and Kumar, Rahul and Bhagwan, Ranjita and Bird, Christian and Bansal, Chetan and Maddila, Chandra and Mehta, Sonu and Ashok, B.","Today's software development is distributed and involves continuous changes for new features and yet, their development cycle has to be fast and agile. An important component of enabling this agility is selecting the right reviewers for every code-change - the smallest unit of the development cycle. Modern tool-based code review is proven to be an effective way to achieve appropriate code review of software changes. However, the selection of reviewers in these code review systems is at best manual. As software and teams scale, this poses the challenge of selecting the right reviewers, which in turn determines software quality over time. While previous work has suggested automatic approaches to code reviewer recommendations, it has been limited to retrospective analysis. We not only deploy a reviewer suggestions algorithm - WhoDo - and evaluate its effect but also incorporate load balancing as part of it to address one of its major shortcomings: of recommending experienced developers very frequently. We evaluate the effect of this hybrid recommendation + load balancing system on five repositories within Microsoft. Our results are based around various aspects of a commit and how code review affects that. We attempt to quantitatively answer questions which are supposed to play a vital role in effective code review through our data and substantiate it through qualitative feedback of partner repositories.",2019,651
The Surprising Performance of Simple Baselines for Misinformation Detection,"Pelrine, Kellin and Danovitch, Jacob and Rabbany, Reihaneh","As social media becomes increasingly prominent in our day to day lives, it is increasingly important to detect informative content and prevent the spread of disinformation and unverified rumours. While many sophisticated and successful models have been proposed in the literature, they are often compared with older NLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the performance of a broad set of modern transformer-based language models and show that with basic fine-tuning, these models are competitive with and can even significantly outperform recently proposed state-of-the-art methods. We present our framework as a baseline for creating and evaluating new methods for misinformation detection. We further study a comprehensive set of benchmark datasets, and discuss potential data leakage and the need for careful design of the experiments and understanding of datasets to account for confounding variables. As an extreme case example, we show that classifying only based on the first three digits of tweet ids, which contain information on the date, gives state-of-the-art performance on a commonly used benchmark dataset for fake news detection –Twitter16. We provide a simple tool to detect this problem and suggest steps to mitigate it in future datasets.",2021,652
SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1,,"Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is ""Blazing New Trails in CS Education."" This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.",2024,653
RecSys '22: Proceedings of the 16th ACM Conference on Recommender Systems,,,2022,654
Large Scale Automatic Web Accessibility Validation,"Iannuzzi, Nicola and Manca, Marco and Patern\`{o}, Fabio and Santoro, Carmen","Digital accessibility is considered an important aspect to allow all people, including those with permanent or temporary disabilities, to access the continuously increasing number of digital services. This raises the need for tools able to provide support for monitoring the level of accessibility of a large number of websites in order to understand their actual level of accessibility, and identify the areas that need more interventions for their improvement. We present how we have extended a tool for accessibility validation for this purpose, and the results that we obtained in the validation of about 2.7 million Web pages of Italian public administration Web sites.",2023,655
Workgraph: personal focus vs. interruption for engineers at Meta,"Chen, Yifen and Rigby, Peter C. and Chen, Yulin and Jiang, Kun and Dehghani, Nader and Huang, Qianying and Cottle, Peter and Andrews, Clayton and Lee, Noah and Nagappan, Nachiappan","All engineers dislike interruptions because it takes away from the deep focus time needed to write complex code. Our goal is to reduce unnecessary interruptions at . We first describe our Workgraph platform that logs how engineers use our internal work tools at . Using these anonymized logs, we create sessions. sessions are defined in opposition to interruption and are the amount of time until the engineer is interrupted by, for example, a work chat message. We describe descriptive statistics related to how long engineers are able to focus. We find that at Meta, Engineers have a total of 14.25 hours of personal-focus time per week. These numbers are comparable with those reported by other software firms. We then create a Random Forest model to understand which factors influence the median daily personal-focus time. We find that the more time an engineer spends in the IDE the longer their focus. We also find that the more central an engineer is in the social work network, the shorter their personal-focus time. Other factors such as role and domain/pillar have little impact on personal-focus at Meta. To help engineers achieve longer blocks of personal-focus and help them stay in flow, Meta developed the AutoFocus tool that blocks work chat notifications when an engineer is working on code for 12 minutes or longer. AutoFocus allows the sender to still force a work chat message using “@notify” ensuring that urgent messages still get through, but allowing the sender to reflect on the importance of the message. In a large experiment, we find that AutoFocus increases the amount of personal-focus time by 20.27%, and it has now been rolled out widely at Meta.",2022,656
L2DART: A Trust Management System Integrating Blockchain and Off-Chain Computation,"De Salve, Andrea and Franceschi, Luca and Lisi, Andrea and Mori, Paolo and Ricci, Laura","The blockchain technology has been gaining an increasing popularity for the last years, and smart contracts are being used for a growing number of applications in several scenarios. The execution of smart contracts on public blockchains can be invoked by any user with a transaction, although in many scenarios there would be the need for restricting the right of executing smart contracts only to a restricted set of users. To help deal with this issue, this article proposes a system based on a popular access control framework called RT, Role-based Trust Management, to regulate smart contracts execution rights. The proposed system, called Layer 2 DecentrAlized Role-based Trust management (L2DART), implements the RT framework on a public blockchain, and it is designed as a layer-2 technology that involves both on-chain and off-chain functionalities to reduce the blockchain costs while keeping blockchain auditability, i.e., immutability and transparency. The on-chain costs of L2DART have been evaluated on Ethereum and compared with a previous solution implementing on-chain all the functionalities. The results show that the on-chain costs of L2DART are relatively low, making the system deployable in real-world scenarios.",2023,657
Communicating Study Design Trade-offs in Software Engineering,"Robillard, Martin P. and Arya, Deeksha M. and Ernst, Neil A. and Guo, Jin L. C. and Lamothe, Maxime and Nassif, Mathieu and Novielli, Nicole and Serebrenik, Alexander and Steinmacher, Igor and Stol, Klaas-Jan","Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs. We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers’ work.",2024,658
MLNLP '22: Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing,,,2022,659
SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering,,,2023,660
Using Personality Detection Tools for Software Engineering Research: How Far Can We Go?,"Calefato, Fabio and Lanubile, Filippo","Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-the-shelf solutions trained on non-technical corpora, which might not be readily applicable to technical domains like software engineering. In this article, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers’ e-mails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in software engineering by replacing the personality detection tool used to infer developers’ personalities from pull-request discussions and e-mails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain.",2022,661
Finding MNEMON: Reviving Memories of Node Embeddings,"Shen, Yun and Han, Yufei and Zhang, Zhikun and Chen, Min and Yu, Ting and Backes, Michael and Zhang, Yang and Stringhini, Gianluca","Previous security research efforts orbiting around graphs have been exclusively focusing on either (de-)anonymizing the graphs or understanding the security and privacy issues of graph neural networks. Little attention has been paid to understand the privacy risks of integrating the output from graph embedding models (e.g., node embeddings) with complex downstream machine learning pipelines. In this paper, we fill this gap and propose a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes. We show that an adversary can recover edges with decent accuracy by only gaining access to the node embedding matrix of the original graph without interactions with the node embedding models. We demonstrate the effectiveness and applicability of our graph recovery attack through extensive experiments.",2022,662
What Makes Civic Tech Initiatives To Last Over Time? Dissecting Two Global Cases,"Hamm, Andrea and Shibuya, Yuya and Ullrich, Stefan and Cerratto Pargman, Teresa Cerratto","Civic tech initiatives dedicated to environmental issues have become a worldwide phenomenon and made invaluable contributions to data, community building, and publics. However, many of them stop after a relatively short time. Therefore, we studied two long-lasting civic tech initiatives of global scale, to understand what makes them sustain over time. To this end, we conducted two mixed-method case studies, combining social network analysis and qualitative content analysis of Twitter data with insights from expert interviews. Drawing on our findings, we identified a set of key factors that help the studied civic tech initiatives to grow and last. Contributing to Digital Civics in HCI, we argue that the civic tech initiatives’ scaling and sustaining are configured through the entanglement of (1) civic data both captured and owned by the citizens for the citizens, (2) the use of open and accessible technology, and (3) the initiatives’ public narrative, giving them a voice on the environmental issue.",2021,663
Gotta Catch ’em All: A Multistage Framework for Honeypot Fingerprinting,"Srinivasa, Shreyas and Pedersen, Jens Myrup and Vasilomanolakis, Emmanouil","Honeypots are decoy systems that lure attackers by presenting them with a seemingly vulnerable system. They provide an early detection mechanism as well as a method for learning how adversaries work and think. However, over the past years, several researchers have shown methods for fingerprinting honeypots. This significantly decreases the value of a honeypot; if an attacker is able to recognize the existence of such a system, they can evade it. In this article, we revisit the honeypot identification field, by providing a holistic framework that includes state-of-the-art and novel fingerprinting components. We decrease the probability of false positives by proposing a rigid multi-step approach for labeling a system as a honeypot. We perform extensive scans covering 2.9 billion addresses of the IPv4 space and identify a total of 21,855 honeypot instances. Moreover, we present several interesting side findings such as the identification of around 355,000 non-honeypot systems that represent potentially misconfigured or unpatched vulnerable servers (e.g., SSH servers with default password configurations and vulnerable versions). We ethically disclose our findings to network administrators about the default configuration and the honeypot developers about the gaps in implementation that lead to possible honeypot fingerprinting. Last, we discuss countermeasures against honeypot fingerprinting techniques.",2023,664
NaNofuzz: A Usable Tool for Automatic Test Generation,"Davis, Matthew C. and Choi, Sangheon and Estep, Sam and Myers, Brad A. and Sunshine, Joshua","In the United States alone, software testing labor is estimated to cost $48 billion USD per year. Despite widespread test execution automation and automation in other areas of software engineering, test suites continue to be created manually by software engineers. We have built a test generation tool, called NaNofuzz, that helps users find bugs in their code by suggesting tests where the output is likely indicative of a bug, e.g., that return NaN (not-a-number) values. NaNofuzz is an interactive tool embedded in a development environment to fit into the programmer's workflow. NaNofuzz tests a function with as little as one button press, analyses the program to determine inputs it should evaluate, executes the program on those inputs, and categorizes outputs to prioritize likely bugs. We conducted a randomized controlled trial with 28 professional software engineers using NaNofuzz as the intervention treatment and the popular manual testing tool, Jest, as the control treatment. Participants using NaNofuzz on average identified bugs more accurately (p &lt; .05, by 30%), were more confident in their tests (p &lt; .03, by 20%), and finished their tasks more quickly (p &lt; .007, by 30%).",2023,665
Evolution of distributed neural controllers for voxel-based soft robots,"Medvet, Eric and Bartoli, Alberto and De Lorenzo, Andrea and Fidel, Giulio","Voxel-based soft robots (VSRs) are aggregations of elastic, cubic blocks that have sparkled the interest of Robotics and Artificial Life researchers. VSRs can move by varying the volume of individual blocks, according to control signals dictated by a controller, possibly based on inputs coming from sensors embedded in the blocks. Neural networks (NNs) have been used as centralized processing units for those sensing controllers, with weights optimized using evolutionary computation. This structuring breaks the intrinsic modularity of VSRs: decomposing a VSR into modules to be assembled in a different way is very hard.In this work we propose an alternative approach that enables full modularity and is based on a distributed neural controller. Each block contains a small NN that outputs signals to adjacent blocks and controls the local volume, based on signals from adjacent blocks and on local sensor readings. We show experimentally for the locomotion task that our controller is as effective as the centralized one. Our experiments also suggest that the proposed framework indeed allows exploiting modularity: VSRs composed of pre-trained parts (body and controller) can be evolved more efficiently than starting from scratch.",2020,666
WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining,,"It is our great pleasure to welcome you to the 17th ACM International Conference on Web Search and Data Mining - WSDM 2024. WSDM is one of the premier conferences in the fields of web search and data mining, with a dynamic and growing community from academia and industry. After two years of virtual conferences and in-person conferences in Singapore, the 2024 edition is an in-person conference with virtual elements. We hope you enjoy the conference at the ""Centro Internacional de Congresos de Yucatan (CIC)"" in Merida from March 4 to March 8, 2024.We are excited to kick off the program with a dynamic mix of Tutorials and Industry Day. Our seven tutorials will cover a broad range of search and data mining topics. Industry Day will provide valuable insights from leaders at major technology companies. The core technical program continues WSDM's tradition of a single-track format, featuring 109 thought-provoking papers from both academic and industry experts. We're honored to have inspiring keynote speakers each day: Nicolas Christin (CMU), Elizabeth Reid (Google), and Saiph Savage (Civic A.I. Lab). Additionally, 17 interactive demonstrations will showcase the latest prototypes and systems. The final day offers a stimulating Doctoral Consortium and six engaging workshops on topics including integrity in social networks, large language model for society, psychology-informed information access system, interactive and scalable information retrieval system and machine learning on graphs. WSDM 2024 proudly presents WSDM day on information retrieval and Web in the region. WSDM Cup Day highlights finalists' presentations addressing challenges in Conversational Multi-Doc QA. This diverse and stimulating program promises to be an enriching experience for all!.",2024,667
Interface Quality Patterns: Communicating and Improving the Quality of Microservices APIs,"Stocker, Mirko and Zimmermann, Olaf and Zdun, Uwe and L\""{u}bke, Daniel and Pautasso, Cesare","The design and evolution of Application Programming Interfaces (APIs) in microservices architectures is challenging. General design issues in integration and programming have been covered in great detail in many pattern languages since the beginnings of the patterns movement, and service-oriented infrastructure design patterns have also been published in the last decade. However, the interface representations (i.e., the content of message payloads) have received less attention. We presented five structural representation patterns in our previous work; in this paper we continue our coverage of the API design space and propose five interface quality patterns that deal with the observable aspects of quality-attribute-driven interface design for efficiency, security, and manageability: An API Key allows API providers to identify clients. Providers may offer rich data contracts in their responses, which not all consumers might need. A Wish List allows the client to request only the attributes in a response data set that it is interested in. If a client makes many API calls, the provider can employ a Rate Limit and bill clients according to a specified Rate Plan. A provider has to provide a high-quality service while at the same time having to use its available resources economically. The resulting compromise is expressed in a provider's Service Level Agreement.",2018,668
Ethical issues in research using datasets of illicit origin,"Thomas, Daniel R. and Pastrana, Sergio and Hutchings, Alice and Clayton, Richard and Beresford, Alastair R.","We evaluate the use of data obtained by illicit means against a broad set of ethical and legal issues. Our analysis covers both the direct collection, and secondary uses of, data obtained via illicit means such as exploiting a vulnerability, or unauthorized disclosure. We extract ethical principles from existing advice and guidance and analyse how they have been applied within more than 20 recent peer reviewed papers that deal with illicitly obtained datasets. We find that existing advice and guidance does not address all of the problems that researchers have faced and explain how the papers tackle ethical issues inconsistently, and sometimes not at all. Our analysis reveals not only a lack of application of safeguards but also that legitimate ethical justifications for research are being overlooked. In many cases positive benefits, as well as potential harms, remain entirely unidentified. Few papers record explicit Research Ethics Board (REB) approval for the activity that is described and the justifications given for exemption suggest deficiencies in the REB process.",2017,669
Teaching Software Development for Real-World Problems using a Microservice-Based Collaborative Problem-Solving Approach,"Lau, Yi Meng and Koh, Christian Michael and Jiang, Lingxiao","Experienced and skillful software developers are needed in organizations to develop software products effective for their business with shortened time-to-market. Such developers will not only need to code but also be able to work in teams and collaboratively solve real-world problems that organizations are facing. It is challenging for educators to nurture students to become such developers with strong technical, social, and cognitive skills.Towards addressing the challenge, this study presents a Collaborative Software Development Project Framework for a course that focuses on learning microservices architectures and developing a software application for a real-world business. Students get to work in teams to solve a real-world problem of their own choice. They are given opportunities to recognize that the software development process goes beyond writing code and that social and cognitive skills in engaging with each other are also essential. By adopting microservices architectures in the course, students learn to break down the functionalities of their applications into smaller pieces of code with standardized interfaces that can be developed, tested, and deployed independently. This not only helps students to learn various technical skills needed for developing and implementing the functionalities needed by the application in the form of microservices but also facilitates task allocation and coordination among their team members and provides a platform for them to solve problems collaboratively. Upon completion of their projects, students are also asked to reflect on their development process and encouraged to think beyond the basics for better software design and development approaches.The course curriculum incorporates the framework, especially for the student team projects. The earlier teaching weeks introduce a combination of concepts and lab exercises to students as the building blocks. The survey studies show that the framework is effective in enhancing the students' learning of technical, social, and cognitive skills, while further improvements, such as closer collaboration with other courses, can be done to improve a holistic learning curriculum.",2024,670
Test Generation Strategies for Building Failure Models and Explaining Spurious Failures,"Jodat, Baharin A. and Chandar, Abhishek and Nejati, Shiva and Sabetzadeh, Mehrdad","Test inputs fail not only when the system under test is faulty but also when the inputs are invalid or unrealistic. Failures resulting from invalid or unrealistic test inputs are spurious. Avoiding spurious failures improves the effectiveness of testing in exercising the main functions of a system, particularly for compute-intensive (CI) systems where a single test execution takes significant time. In this article, we propose to build failure models for inferring interpretable rules on test inputs that cause spurious failures. We examine two alternative strategies for building failure models: (1)&nbsp;machine learning (ML)-guided test generation and (2)&nbsp;surrogate-assisted test generation. ML-guided test generation infers boundary regions that separate passing and failing test inputs and samples test inputs from those regions. Surrogate-assisted test generation relies on surrogate models to predict labels for test inputs instead of exercising all the inputs. We propose a novel surrogate-assisted algorithm that uses multiple surrogate models simultaneously, and dynamically selects the prediction from the most accurate model. We empirically evaluate the accuracy of failure models inferred based on surrogate-assisted and ML-guided test generation algorithms. Using case studies from the domains of cyber-physical systems and networks, we show that our proposed surrogate-assisted approach generates failure models with an average accuracy of 83%, significantly outperforming ML-guided test generation and two baselines. Further, our approach learns failure-inducing rules that identify genuine spurious failures as validated against domain knowledge.",2024,671
A turing test for genetic improvement,"Afzal, Afsoon and Lacomis, Jeremy and Goues, Claire Le and Timperley, Christopher S.","Genetic improvement is a research field that aims to develop search-based techniques for improving existing code. GI has been used to automatically repair bugs, reduce energy consumption, and to improve run-time performance. In this paper, we reflect on the often-overlooked relationship between GI and developers within the context of continually evolving software systems. We introduce a distinction between transparent and opaque patches based on intended lifespan and developer interaction. Finally, we outline a Turing test for assessing the ability of a GI system to produce opaque patches that are acceptable to humans. This motivates research into the role GI systems will play in transparent development contexts.",2018,672
A DSL and model transformations to specify learning corpora for modeling assistants,"Boubekeur, Younes and Singh, Prabhsimran and Mussbacher, Gunter","Software engineering undergraduate students spend a significant time learning various topics related to software design, including notably model-driven engineering (MDE), where different types of structural and behavioral models are used to design, implement, and validate an application. MDE instructors spend a lot of time covering modeling concepts, which is more difficult with ever-increasing class sizes. Online resources, such as learning corpora for domain modeling, can aid in this learning process by serving as a more dynamic textbook alternative or as part of a larger interactive application with domain modeling exercises and tutorials. A Learning Corpus (LC) is an extensible list of entries representing possible mistakes that could occur when defining a model, e.g., Missing Abstraction-Occurrence pattern in the case of a domain model. Each LC entry includes progressive levels of feedback, including written responses, quizzes, and references to external resources. To make it easy for instructors to customize the entries as well as add their own, we propose a novel, simple, and intuitive approach based on an internal domain-specific language that supports features such as context-specific information and concise arbitrary metamodel navigation with shorthands. Transformations to source code as well as Markdown and LATEX enable use of the LC entries in different contexts. These transformations as well as the integration of the generated code in a sample Modeling Assistant application verify and validate the LC metamodel and specification.",2022,673
"Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition","Song, Yi-Fan and Zhang, Zhang and Shan, Caifeng and Wang, Liang","One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the State-Of-The-Art (SOTA) models of this task tends to be exceedingly sophisticated and over-parameterized, where the low efficiency in model training and inference has obstructed the development in the field, especially for large-scale action datasets. In this work, we propose an efficient but strong baseline based on Graph Convolutional Network (GCN), where three main improvements are aggregated, i.e., early fused Multiple Input Branches (MIB), Residual GCN (ResGCN) with bottleneck structure and Part-wise Attention (PartAtt) block. Firstly, an MIB is designed to enrich informative skeleton features and remain compact representations at an early fusion stage. Then, inspired by the success of the ResNet architecture in Convolutional Neural Network (CNN), a ResGCN module is introduced in GCN to alleviate computational costs and reduce learning difficulties in model training while maintain the model accuracy. Finally, a PartAtt block is proposed to discover the most essential body parts over a whole action sequence and obtain more explainable representations for different skeleton action sequences. Extensive experiments on two large-scale datasets, i.e., NTU RGB+D 60 and 120, validate that the proposed baseline slightly outperforms other SOTA models and meanwhile requires much fewer parameters during training and inference procedures, e.g., at most 34 times less than DGNN, which is one of the best SOTA methods.",2020,674
ACSAC '23: Proceedings of the 39th Annual Computer Security Applications Conference,,,2023,675
Verification Witnesses,"Beyer, Dirk and Dangl, Matthias and Dietsch, Daniel and Heizmann, Matthias and Lemberger, Thomas and Tautschnig, Michael","Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker, UAutomizer, CPA-witness2test, and FShell-witness2test.",2022,676
"BuildSys '22: Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",,"Over the past thirteen years, BuildSys has been an interdisciplinary conference that brings together various stakeholders, including researchers, practitioners, and policymakers from different disciplines, including civil engineering, mechanical engineering, environmental science, electrical and computer engineering, computer science, system management and control, and many others. This year is no exception, with papers and attendees from all these disciplines and regions worldwide. The conference's focus extends beyond building systems to the built environment more generally.",2022,677
DYNAMICS '20: Proceedings of the 2020 Workshop on DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security,,,2020,678
Separation logic,"O'Hearn, Peter","Separation logic is a key development in formal reasoning about programs, opening up new lines of attack on longstanding problems.",2019,679
Procedural zelda: a PCG environment for player experience research,"Heijne, Norbert and Bakkes, Sander","To contribute to the domain of player experience research, this paper presents a new PCG environment with a relatively wide expressive range that builds upon the iconic The Legend of Zelda: A Link to the Past action-RPG game; it contributes by providing the openly-available Procedural Zelda environment for gaming research. The paper presents the design goals and design context of the research environment, and provides a detailed overview of the procedural capabilities of Procedural Zelda, together with its capabilities for data logging, to benefit, e.g., player modelling investigations.",2017,680
Is it Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues,"Tan, Xin and Chen, Yiran and Wu, Haohua and Zhou, Minghui and Zhang, Li","Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.",2023,681
The Need for Verbal Robot Explanations and How People Would Like a Robot to Explain Itself,"Han, Zhao and Phillips, Elizabeth and Yanco, Holly A.","Although non-verbal cues such as arm movement and eye gaze can convey robot intention, they alone may not provide enough information for a human to fully understand a robot’s behavior. To better understand how to convey robot intention, we conducted an experiment (N = 366) investigating the need for robots to explain, and the content and properties of a desired explanation such as timing, engagement importance, similarity to human explanations, and summarization. Participants watched a video where the robot was commanded to hand an almost-reachable cup and one of six reactions intended to show the unreachability : doing nothing (No Cue), turning its head to the cup (Look), or turning its head to the cup with the addition of repeated arm movement pointed towards the cup (Look &amp; Point), and each of these with or without a Headshake. The results indicated that participants agreed robot behavior should be explained across all conditions, in situ, in a similar manner as what human explain, and provide concise summaries and respond to only a few follow-up questions by participants. Additionally, we replicated the study again with N = 366 participants after a 15-month span and all major conclusions still held.",2021,682
IUI '24: Proceedings of the 29th International Conference on Intelligent User Interfaces,,,2024,683
On the violation of honesty in mobile apps: automated detection and categories,"Obie, Humphrey O. and Ilekura, Idowu and Du, Hung and Shahin, Mojtaba and Grundy, John and Li, Li and Whittle, Jon and Turhan, Burak","Human values such as integrity, privacy, curiosity, security, and honesty are guiding principles for what people consider important in life. Such human values may be violated by mobile software applications (apps), and the negative effects of such human value violations can be seen in various ways in society. In this work, we focus on the human value of honesty. We present a model to support the automatic identification of violations of the value of honesty from app reviews from an end-user perspective. Beyond the automatic detection of honesty violations by apps, we also aim to better understand different categories of honesty violations expressed by users in their app reviews. The result of our manual analysis of our honesty violations dataset shows that honesty violations can be characterised into ten categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. Based on these results, we argue for a conscious effort in developing more honest software artefacts including mobile apps, and the promotion of honesty as a key value in software development practices. Furthermore, we discuss the role of app distribution platforms as enforcers of ethical systems supporting human values, and highlight some proposed next steps for human values in software engineering (SE) research.",2022,684
"Some Useful Things to Know When Combining IR and NLP: the Easy, the Hard and the Ugly","Alonso, Omar and Church, Kenneth","Deep nets such as GPT are at the core of the current advances in many systems and applications. Things are moving very fast, and it appears that techniques are out of date within weeks. How can we take advantage of new discoveries and incorporate them into our existing work? Are these radical new developments, repetitions of older concepts, or both?In this tutorial, we aim to bring interested researchers and practitioners up to speed on the recent and ongoing techniques around ML and Deep learning in the context of IR and NLP. Additionally, our goal is to clarify terminology, emphasize fundamentals, and outline new research opportunities.",2023,685
MMSys '23: Proceedings of the 14th ACM Multimedia Systems Conference,,,2023,686
PaLM: scaling language modeling with pathways,"Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah","Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",2024,687
Warmonger: Inflicting Denial-of-Service via Serverless Functions in the Cloud,"Xiong, Junjie and Wei, Mingkui and Lu, Zhuo and Liu, Yao","We debut the Warmonger attack, a novel attack vector that can cause denial-of-service between a serverless computing platform and an external content server. The Warmonger attack exploits the fact that a serverless computing platform shares the same set of egress IPs among all serverless functions, which belong to different users, to access an external content server. As a result, a malicious user on this platform can purposefully misbehave and cause these egress IPs to be blocked by the content server, resulting in a platform-wide denial of service. To validate the Warmonger attack, we ran months-long experiments, collected and analyzed the egress IP usage pattern of four major serverless service providers (SSPs). We also conducted an in-depth evaluation of an attacker's possible moves to inflict an external server and cause IP-blockage. We demonstrate that some SSPs use surprisingly small numbers of egress IPs (as little as only four) and share them among their users, and that the serverless platform provides sufficient leverage for a malicious user to conduct well-known misbehaviors and cause IP-blockage. Our study unveiled a potential security threat on the emerging serverless computing platform, and shed light on potential mitigation approaches.",2021,688
ICSE-SEET '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training,,,2023,689
"Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming","Guo, Philip J.","One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming. But there are usually far fewer experts than learners. To enable a single expert to help more learners at once, we built Codeopticon, an interface that enables a programming tutor to monitor and chat with dozens of learners in real time. Each learner codes in a workspace that consists of an editor, compiler, and visual debugger. The tutor sees a real-time view of each learner's actions on a dashboard, with each learner's workspace summarized in a tile. At a glance, the tutor can see how learners are editing and debugging their code, and what errors they are encountering. The dashboard automatically reshuffles tiles so that the most active learners are always in the tutor's main field of view. When the tutor sees that a particular learner needs help, they can open an embedded chat window to start a one-on-one conversation. A user study showed that 8 first-time Codeopticon users successfully tutored anonymous learners from 54 countries in a naturalistic online setting. On average, in a 30-minute session, each tutor monitored 226 learners, started 12 conversations, exchanged 47 chats, and helped 2.4 learners.",2015,690
Workflow Integration Alleviates Identity and Access Management in Serverless Computing,"Sankaran, Arnav and Datta, Pubali and Bates, Adam","As serverless computing continues to revolutionize the design and deployment of web services, it has become an increasingly attractive target to attackers. These adversaries are developing novel tactics for circumventing the ephemeral nature of serverless functions, exploiting container reuse optimizations and achieving lateral movement by “living off the land” provided by legitimate serverless workflows. Unfortunately, the traditional security controls currently offered by cloud providers are inadequate to counter these new threats. In this work, we propose will.iam,1 a workflow-aware access control model and reference monitor that satisfies the functional requirements of the serverless computing paradigm. will.iam encodes the protection state of a serverless application as a permissions graph that describes the permissible transitions of its workflows, associating web requests with a permissions set at the point of ingress according to a graph-based labeling state. By proactively enforcing the permissions requirements of downstream workflow components, will.iam is able to avoid the costs of partially processing unauthorized requests and reduce the attack surface of the application. We implement the will.iam framework in Go and evaluate its performance as compared to recent related work against the well-established Nordstrom “Hello, Retail!” application. We demonstrate that will.iam imposes minimal burden to requests, averaging 0.51% overhead across representative workflows, but dramatically improves performance when handling unauthorized requests (e.g., DDoS attacks) as compared to past solutions. will.iam thus demonstrates an effective and practical alternative for authorization in the serverless paradigm.",2020,691
Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals,"Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard","Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron’s usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report reflections both from our interviewees and from independent reviewers who critiqued performances of several of the scripts to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations—including plagiarism and bias—and participatory models for the design and deployment of such tools.",2023,692
Dr.Emotion: Disentangled Representation Learning for Emotion Analysis on Social Media to Improve Community Resilience in the COVID-19 Era and Beyond,"Ju, Mingxuan and Song, Wei and Sun, Shiyu and Ye, Yanfang and Fan, Yujie and Hou, Shifu and Loparo, Kenneth and Zhao, Liang","During the pandemic caused by coronavirus disease (COVID-19), social media has played an important role by enabling people to discuss their experiences and feelings of this global crisis. To help combat the prolonged pandemic that has exposed vulnerabilities impacting community resilience, in this paper, based on our established large-scale COVID-19 related social media data, we propose and develop an integrated framework (named Dr.Emotion) to learn disentangled representations of social media posts (i.e., tweets) for emotion analysis and thus to gain deep insights into public perceptions towards COVID-19. In Dr.Emotion, for given social media posts, we first post-train a transformer-based model to obtain the initial post embeddings. Since users may implicitly express their emotions in social media posts which could be highly entangled with other descriptive information in the post content, to address this challenge for emotion analysis, we propose an adversarial disentangler by integrating emotion-independent (i.e., sentiment-neutral) priors of the posts generated by another post-trained transformer-based model to separate and disentangle the implicitly encoded emotions from the content in latent space for emotion classification at the first attempt. Extensive experimental studies are conducted to fully evaluate Dr.Emotion and promising results demonstrate its performance in emotion analysis by comparison with the state-of-the-art baseline methods. By exploiting our developed Dr.Emotion, we further perform emotion analysis over a large number of social media posts and provide in-depth investigation from both temporal and geographical perspectives, based on which additional work can be conducted to extract and transform the constructive ideas, experiences and support into actionable information to improve community resilience in responses to a variety of crises created by COVID-19 and well beyond.",2021,693
Proactive Defense for Internet-of-things: Moving Target Defense With Cyberdeception,"Ge, Mengmeng and Cho, Jin-Hee and Kim, Dongseong and Dixit, Gaurav and Chen, Ing-Ray","Resource constrained Internet-of-Things (IoT) devices are highly likely to be compromised by attackers, because strong security protections may not be suitable to be deployed. This requires an alternative approach to protect vulnerable components in IoT networks. In this article, we propose an integrated defense technique to achieve intrusion prevention by leveraging cyberdeception (i.e., a decoy system) and moving target defense (i.e., network topology shuffling). We evaluate the effectiveness and efficiency of our proposed technique analytically based on a graphical security model in a software-defined networking (SDN)-based IoT network. We develop four strategies (i.e., fixed/random and adaptive/hybrid) to address “when” to perform network topology shuffling and three strategies (i.e., genetic algorithm/decoy attack path-based optimization/random) to address “how” to perform network topology shuffling on a decoy-populated IoT network, and we analyze which strategy can best achieve a system goal, such as prolonging the system lifetime, maximizing deception effectiveness, maximizing service availability, or minimizing defense cost. We demonstrated that a software-defined IoT network running our intrusion prevention technique at the optimal parameter setting prolongs system lifetime, increases attack complexity of compromising critical nodes, and maintains superior service availability compared with a counterpart IoT network without running our intrusion prevention technique. Further, when given a single goal or a multi-objective goal (e.g., maximizing the system lifetime and service availability while minimizing the defense cost) as input, the best combination of “when” and “how” strategies is identified for executing our proposed technique under which the specified goal can be best achieved.",2021,694
REBA: a refinement-based architecture for knowledge representation and reasoning in robotics,"Sridharan, Mohan and Gelfond, Michael and Zhang, Shiqi and Wyatt, Jeremy","This article describes REBA, a knowledge representation and reasoning architecture for robots that is based on tightly-coupled transition diagrams of the domain at two different levels of granularity. An action language is extended to support non-boolean fluents and non-deterministic causal laws, and used to describe the domain's transition diagrams, with the fine-resolution transition diagram being defined as a refinement of the coarse-resolution transition diagram. The coarse-resolution system description, and a history that includes prioritized defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this abstract transition. The zoomed fine-resolution system description, and a probabilistic representation of the uncertainty in sensing and actuation, are used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract transition as a sequence of concrete actions. The fine-resolution outcomes of executing these concrete actions are used to infer coarse-resolution outcomes that are added to the coarse-resolution history and used for subsequent coarse-resolution reasoning. The architecture thus combines the complementary strengths of declarative programming and probabilistic graphical models to represent and reason with non-monotonic logic-based and probabilistic descriptions of uncertainty and incomplete domain knowledge. In addition, we describe a general methodology for the design of software components of a robot based on these knowledge representation and reasoning tools, and provide a path for proving the correctness of these components. The architecture is evaluated in simulation and on a mobile robot finding and moving target objects to desired locations in indoor domains, to show that the architecture supports reliable and efficient reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.",2019,695
Scalable VPN-forwarded Honeypots: Dataset and Threat Intelligence Insights,"Aung, Yan Lin and Tiang, Hui Hui and Wijaya, Herman and Ochoa, Mart\'{\i}n and Zhou, Jianying","After distributed denial-of-service attacks by the Mirai malware in 2016, large-scale attacks exploiting IoT devices raise significant security concerns for the stakeholders involved. The efficacy of setting up honeypots to survey the threat landscape and for early detection of threats to IoT devices is evident. However, the availability of dataset collected by these IoT honeypots to advance research on IoT security has been scarce and limited. With this paper, we contribute network traffic dataset collected by a high-interaction IoT honeypots deployed in the wild for 1.5 years during 2017-2018. The honeypots are manifested on 40 public IP addresses in the wild while forwarding the traffic to 11 real IoT devices. Using Zeek tool, the dataset is generated in JSON format from 258,871 PCAP files resulting more than 81.5 million logs. To foster further research, the attacks, exploitation and intrusion attempts present in the dataset as well as threat intelligence insights are provided with an aid of an open-source threat-hunting and security monitoring platform.",2021,696
Why computing students should contribute to open source software projects,"Spinellis, Diomidis","Acquiring developer-prized practical skills, knowledge, and experiences.",2021,697
Deep Transfer Learning &amp; Beyond: Transformer Language Models in Information Systems Research,"Gruetzemacher, Ross and Paradice, David","AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving transformer language models (TLMs) offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizing text mining in top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher quality text analytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.",2022,698
Software-related Slack Chats with Disentangled Conversations,"Chatterjee, Preetha and Damevski, Kostadin and Kraft, Nicholas A. and Pollock, Lori","More than ever, developers are participating in public chat communities to ask and answer software development questions. With over ten million daily active users, Slack is one of the most popular chat platforms, hosting many active channels focused on software development technologies, e.g., python, react. Prior studies have shown that public Slack chat transcripts contain valuable information, which could provide support for improving automatic software maintenance tools or help researchers understand developer struggles or concerns.In this paper, we present a dataset of software-related Q&amp;A chat conversations, curated for two years from three open Slack communities (python, clojure, elm). Our dataset consists of 38,955 conversations, 437,893 utterances, contributed by 12,171 users. We also share the code for a customized machine-learning based algorithm that automatically extracts (or disentangles) conversations from the downloaded chat transcripts.",2020,699
CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems,,,2024,700
On the use of DGAs in malware: an everlasting competition of detection and evasion,"Spooren, Jan and Preuveneers, Davy and Desmet, Lieven and Janssen, Peter and Joosen, Wouter","Malware typically makes use of Domain Generation Algorithms (DGAs) as a mechanism to contact their Command and Control server. In recent years, different approaches to automatically detect generated domain names have been proposed, based on machine learning. The first problem that we address is the difficulty to systematically compare these DGA detection algorithms due to the lack of an independent benchmark. The second problem that we investigate is the difficulty for an adversary to circumvent these classifiers when the machine learning models backing these DGA-detectors are known. In this paper we compare two different approaches on the same set of DGAs: classical machine learning using manually engineered features and a 'deep learning' recurrent neural network. We show that the deep learning approach performs consistently better on all of the tested DGAs, with an average classification accuracy of 98.7% versus 93.8% for the manually engineered features. We demonstrate that the deep learning solution yields better results even when only 10,000 malicious samples are available. We also show that one of the dangers of manual feature engineering is that DGAs can adapt their strategy, based on knowledge of the features used to detect them. To demonstrate this, we use the knowledge of the used feature set to design a new DGA which makes the Random Forest classifier powerless with a classification accuracy of 57.3%. The deep learning classifier is also (albeit less) affected, reducing its accuracy to 78.9%.",2019,701
WebSci '23: Proceedings of the 15th ACM Web Science Conference 2023,,,2023,702
Destructive Criticism in Software Code Review Impacts Inclusion,"Gunawardena, Sanuri Dananja and Devine, Peter and Beaumont, Isabelle and Garden, Lola Piper and Murphy-Hill, Emerson and Blincoe, Kelly","The software industry lacks gender diversity. Recent research has suggested that a toxic working culture is to blame. Studies have found that communications in software repositories directed towards women are more negative in general. In this study, we use a destructive criticism lens to examine gender differences in software code review feedback. Software code review is a practice where code is peer reviewed and negative feedback is often delivered. We explore differences in perceptions, frequency, and impact of destructive criticism across genders. We surveyed 93 software practitioners eliciting perceived reactions to hypothetical scenarios (or vignettes) where participants are asked to imagine receiving either constructive or destructive criticism. In addition, the survey collected general opinions on feedback obtained during software code review as well as the frequency that participants give and receive destructive criticism.We found that opinions on destructive criticism vary. Women perceive destructive criticism as less appropriate and are less motivated to continue working with the developer after receiving destructive criticism. Destructive criticism is fairly common with more than half of respondents having received nonspecific negative feedback and nearly a quarter having received inconsiderate negative feedback in the past year. Our results suggest that destructive criticism in code review could be a contributing factor to the lack of gender diversity observed in the software industry.",2022,703
The Labor of Maintaining and Scaling Free and Open-Source Software Projects,"Geiger, R. Stuart and Howard, Dorothy and Irani, Lilly","Free and/or open-source software (or F/OSS) projects now play a major and dominant role in society, constituting critical digital infrastructure relied upon by companies, academics, non-profits, activists, and more. As F/OSS has become larger and more established, we investigate the labor of maintaining and sustaining those projects at various scales. We report findings from an interview-based study with contributors and maintainers working in a wide range of F/OSS projects. Maintainers of F/OSS projects do not just maintain software code in a more traditional software engineering understanding of the term: fixing bugs, patching security vulnerabilities, and updating dependencies. F/OSS maintainers also perform complex and often-invisible interpersonal and organizational work to keep their projects operating as active communities of users and contributors. We particularly focus on how this labor of maintaining and sustaining changes as projects and their software grow and scale across many dimensions. In understanding F/OSS to be as much about maintaining a communal project as it is maintaining software code, we discuss broadly applicable considerations for peer production communities and other socio-technical systems more broadly.",2021,704
CRS: a hybrid course recommendation system for software engineering education,"Vo, Nhi N. Y. and Vu, Nam H. and Vu, Tu A. and Vu, Quang T. and Mach, Bang D.","With the increasing numbers of elective courses at universities and the Massive Open Online Courses (MOOCs), Software Engineering (SE) students are facing challenges in selecting their study paths in tech. On the other hand, the skills in SE-related fields have been changing significantly for the past decade, which requires more frequent updates to the curriculum and teaching materials. There is a strong demand for a better course guide and recommendation system to aid higher education in SE to keep up with the industry requirements. In this work, we incorporate data mining techniques, a natural language processing model, and a recommendation system in a web application that helps SE students and university faculty with those challenges. Our proposed hybrid Course Recommendation System (CRS) consists of two web applications (user and admin web apps) to provide multiple features, including user-specific suggestions for university courses, careers, jobs, industry-demanded skills together with online materials to learn those skills, and various analysis dashboards for both SE students and lecturers. We conduct a survey on SE students and faculty members to evaluate the initial impact of our CRS on the end users, which proved the effectiveness of our approach in addressing the mentioned issues. Demo: CRS User Web App and CRS Admin Web App.",2022,705
EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System,"Lu, Chengjie and Xu, Qinghua and Yue, Tao and Ali, Shaukat and Schwitalla, Thomas and Nyg\r{a}rd, Jan F.","The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.",2023,706
A method to identify and correct problematic software activity data: exploiting capacity constraints and data redundancies,"Zheng, Qimu and Mockus, Audris and Zhou, Minghui","Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10% and rapidly rising. We found the corrected values to be 50% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research.",2015,707
ICPP Workshops '23: Proceedings of the 52nd International Conference on Parallel Processing Workshops,,,2023,708
SBSI '24: Proceedings of the 20th Brazilian Symposium on Information Systems,,,2024,709
12 Angry Developers - A Qualitative Study on Developers' Struggles with CSP,"Roth, Sebastian and Gr\""{o}ber, Lea and Backes, Michael and Krombholz, Katharina and Stock, Ben","The Web has improved our ways of communicating, collaborating, teaching, and entertaining us and our fellow human beings. However, this cornerstone of our modern society is also one of the main targets of attacks, most prominently Cross-Site Scripting (XSS). A correctly crafted Content Security Policy (CSP) is capable of effectively mitigating the effect of those Cross-Site Scripting attacks. However, research has shown that the vast majority of all policies in the wild are trivially bypassable.To uncover the root causes behind the omnipresent misconfiguration of CSP, we conducted a qualitative study involving 12 real-world Web developers. By combining a semi-structured interview, a drawing task, and a programming task, we were able to identify the participant's misconceptions regarding the attacker model covered by CSP as well as roadblocks for secure deployment or strategies used to create a CSP.",2021,710
Code review practices for refactoring changes: an empirical study on OpenStack,"AlOmar, Eman Abdullah and Chouchen, Moataz and Mkaouer, Mohamed Wiem and Ouni, Ali","Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the code base. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.",2022,711
Learning patterns in configuration,"Bhagwan, Ranjita and Mehta, Sonu and Radhakrishna, Arjun and Garg, Sahil","Large services depend on correct configuration to run efficiently and seamlessly. Checking such configuration for correctness is important because services use a large and continuously increasing number of configuration files and parameters. Yet, very few such tools exist because the permissible values for a configuration parameter are seldom specified or documented, existing at best as tribal knowledge among a few domain experts.In this paper, we address the problem of configuration pattern mining: learning configuration rules from examples. Using program synthesis and a novel string profiling algorithm, we show that we can use file contents and histories of commits to learn patterns in configuration. We have built a tool called ConfMiner that implements configuration pattern mining and have evaluated it on four large repositories containing configuration for a large-scale enterprise service. Our evaluation shows that ConfMiner learns a large variety of configuration rules with high precision and is very useful in flagging anomalous configuration.",2022,712
Analysis of the Technical Debt of Software Projects Based on Merge Code Comments,"Melo de Ara\'{u}jo, Marcos Henrique and Costa, Catarina and Font\~{a}o, Awdren","Developers use code comments for various reasons, such as explaining the produced code, documenting specifications, communicating with other developers, and highlighting future tasks. Software projects with minimal documentation often have a significant number of comments. In this regard, code comment analysis techniques can be used as tools to examine more complex aspects of software projects, such as technical debt generated by merge conflicts. Technical debt resulting from the resolution of merge conflicts occurs when the resulting code contains comments indicating tasks to be performed in the future. No studies directly linking merge conflicts and technical debt were found in the literature. The objective of this work is to identify and analyze code comments generated during the resolution of merge conflicts from this perspective. This process can lead to improvements in software quality and assist in managing technical debt. To achieve this, an exploratory analysis was conducted on 100 software projects, specifically focusing on task annotations originating from the merge conflict resolution. The results revealed that 60.61% of the analyzed projects have at least one code comment indicating the creation or maintenance of technical debt.",2023,713
Crisis Couture: A Study on Motivations and Practices of Mask Makers During A Crisis,"Buford, Mikayla and Nattar Ranganathan, Vaishnavi and Roseway, Asta and Seyed, Teddy","The landscape of everyday fashion has been transformed due to the COVID-19 pandemic. Part of this can be attributed to different types of communities (e.g. fashion, makers, sewing), who have designed and fabricated masks to counter global shortages and negative culture backlashes. In this paper, we present a mix-methods study of individuals and groups within these communities on their motivations and practices in designing and creating face masks during a global crisis. We conducted a survey with 66 mask makers in the Summer of 2020, and we interviewed 23 of them about their attitudes and reflections on their mask making processes, their unique innovations, and the meaning of contributing in an impactful manner in local and global communities. We unpack themes around technology, self-expression and statement making, making and remixing, sustainable practices, as well as the role of design inspirations on methods and practices for mask makers during a crisis.",2021,714
FORGE '24: Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering,,"FORGE aims to bring researchers, practitioners, and educators from the AI and Software Engineering community to solve the new challenges we meet in the era of foundation models.",2024,715
"The ""Shut the f**k up"" Phenomenon: Characterizing Incivility in Open Source Code Review Discussions","Ferreira, Isabella and Cheng, Jinghui and Adams, Bram","Code review is an important quality assurance activity for software development. Code review discussions among developers and maintainers can be heated and sometimes involve personal attacks and unnecessary disrespectful comments, demonstrating, therefore, incivility. Although incivility in public discussions has received increasing attention from researchers in different domains, the knowledge about the characteristics, causes, and consequences of uncivil communication is still very limited in the context of software development, and more specifically, code review. To address this gap in the literature, we leverage the mature social construct of incivility as a lens to understand confrontational conflicts in open source code review discussions. For that, we conducted a qualitative analysis on 1,545 emails from the Linux Kernel Mailing List (LKML) that were associated with rejected changes. We found that more than half (66.66%) of the non-technical emails included uncivil features. Particularly, frustration, name calling, and impatience are the most frequent features in uncivil emails. We also found that there are civil alternatives to address arguments, while uncivil comments can potentially be made by any people when discussing any topic. Finally, we identified various causes and consequences of such uncivil communication. Our work serves as the first study about the phenomenon of in(civility) in open source software development, paving the road for a new field of research about collaboration and communication in the context of software engineering activities.",2021,716
Can Students without Prior Knowledge Use ChatGPT to Answer Test Questions? An Empirical Study,"Shoufan, Abdulhadi","With the immense interest in ChatGPT worldwide, education has seen a mix of both excitement and skepticism. To properly evaluate its impact on education, it is crucial to understand how far it can help students without prior knowledge answer assessment questions. This study aims to address this question as well as the impact of the question type. We conducted multiple experiments with computer engineering students (experiment group: n=41 to 56), who were asked to use ChatGPT to answer previous test questions before learning about the related topics. Their scores were then compared with the scores of previous-term students who answered the same questions in a quiz or exam setting (control group: n=24 to 61). The results showed a wide range of effect sizes, from -2.55 to 1.23, depending on the question type and content. The experiment group performed best answering code analysis and conceptual questions but struggled with code completion and questions that involved images. However, the performance in code generation tasks was inconsistent. Overall, the ChatGPT group’s answers lagged slightly behind the control group’s answers with an effect size of -0.16. We conclude that ChatGPT, at least in the field of this study, is not yet ready to rely on by students who do not have sufficient background to evaluate generated answers. We suggest that educators try using ChatGPT and educate students on effective questioning techniques and how to assess the generated responses. This study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.",2023,717
"I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI","Chromik, Michael and Eiband, Malin and Buchner, Felicitas and Kr\""{u}ger, Adrian and Butz, Andreas","Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework. We observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding decreases when it is examined.",2021,718
The list is the process: reliable pre-integration tracking of commits on mailing lists,"Ramsauer, Ralf and Lohmann, Daniel and Mauerer, Wolfgang","A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history.We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth.Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements.",2019,719
"SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse",,,2022,720
SIGITE '23: Proceedings of the 24th Annual Conference on Information Technology Education,,,2023,721
MuC '23: Proceedings of Mensch und Computer 2023,,,2023,722
MET '22: Proceedings of the 7th International Workshop on Metamorphic Testing,,"MET series has provided a good forum for discussing novel ideas, new perspectives, new applications, and the state of research, related to or inspired by Metamorphic Testing. This workshop aims to bring together researchers and practitioners in both academia and industry to discuss their research results, experiences and insights into Metamorphic Testing. In this year, we are facing the similar situation of the pandemic as the previous MET in 2020 and 2021, and hence we will run this workshop virtually again.",2022,723
"ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security",,,2023,724
Entity Recommendation for Everyday Digital Tasks,"Jacucci, Giulio and Daee, Pedram and Vuong, Tung and Andolina, Salvatore and Klouche, Khalil and Sj\""{o}berg, Mats and Ruotsalo, Tuukka and Kaski, Samuel","Recommender systems can support everyday digital tasks by retrieving and recommending useful information contextually. This is becoming increasingly relevant in services and operating systems. Previous research often focuses on specific recommendation tasks with data captured from interactions with an individual application. The quality of recommendations is also often evaluated addressing only computational measures of accuracy, without investigating the usefulness of recommendations in realistic tasks. The aim of this work is to synthesize the research in this area through a novel approach by (1) demonstrating comprehensive digital activity monitoring, (2) introducing entity-based computing and interaction, and (3) investigating the previously overlooked usefulness of entity recommendations and their actual impact on user behavior in real tasks. The methodology exploits context from screen frames recorded every 2 seconds to recommend information entities related to the current task. We embodied this methodology in an interactive system and investigated the relevance and influence of the recommended entities in a study with participants resuming their real-world tasks after a 14-day monitoring phase. Results show that the recommendations allowed participants to find more relevant entities than in a control without the system. In addition, the recommended entities were also used in the actual tasks. In the discussion, we reflect on a research agenda for entity recommendation in context, revisiting comprehensive monitoring to include the physical world, considering entities as actionable recommendations, capturing drifting intent and routines, and considering explainability and transparency of recommendations, ethics, and ownership of data.",2021,725
SCALA 2016: Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala,,,2016,726
FakeNewsSetGen: a Process to Build Datasets that Support Comparison Among Fake News Detection Methods,"da Silva, Fl\'{a}vio Roberto Matias and Freire, Paulo M\'{a}rcio Souza and de Souza, Marcelo Pereira and de A. B. Plenamente, Gustavo and Goldschmidt, Ronaldo Ribeiro","Due to easy access and low cost, social media online news consumption has increased significantly for the last decade. Despite their benefits, some social media allow anyone to post news with intense spreading power, which amplifies an old problem: the dissemination of Fake News. In the face of this scenario, several machine learning-based methods to automatically detect Fake News (MLFN) have been proposed. All of them require datasets to train and evaluate their detection models. Although recent MLFN were designed to consider data regarding the news propagation on social media, most of the few available datasets do not contain this kind of data. Hence, comparing the performances amid those recent MLFN and the others is restricted to a very limited number of datasets. Moreover, all existing datasets with propagation data do not contain news in Portuguese, which impairs the evaluation of the MLFN in this language. Thus, this work proposes FakeNewsSetGen, a process that builds Fake News datasets that contain news propagation data and support comparison amid the state-of-the-art MLFN. FakeNewsSetGen's software engineering process was guided to include all kind of data required by the existing MLFN. In order to illustrate FakeNewsSetGen's viability and adequacy, a case study was carried out. It encompassed the implementation of a FakeNewsSetGen prototype and the application of this prototype to create a dataset called FakeNewsSet, with news in Portuguese. Five MLFN with different kind of data requirements (two of them demanding news propagation data) were applied to FakeNewsSet and compared, demonstrating the potential use of both the proposed process and the created dataset.",2020,727
What the fork? finding hidden code clones in npm,"Wyss, Elizabeth and De Carli, Lorenzo and Davidson, Drew","This work presents findings and mitigations on an understudied issue, which we term shrinkwrapped clones, that is endemic to the npm software package ecosystem. A shrink-wrapped clone is a package which duplicates, or near-duplicates, the code of another package without any indication or reference to the original package. This phenomenon represents a challenge to the hygiene of package ecosystems, as a clone package may siphon interest from the package being cloned, or create hidden duplicates of vulnerable, insecure code which can fly under the radar of audit processes.Motivated by these considerations, we propose unwrapper, a mechanism to programmatically detect shrinkwrapped clones and match them to their source package. unwrapper uses a package difference metric based on directory tree similarity, augmented with a prefilter which quickly weeds out packages unlikely to be clones of a target. Overall, our prototype can compare a given package within the entire npm ecosystem (1,716,061 packages with 20,190,452 different versions) in 72.85 seconds, and it is thus practical for live deployment. Using our tool, we performed an analysis of a subset of npm packages, which resulted in finding up to 6,292 previously unknown shrinkwrapped clones, of which up to 207 carried vulnerabilities from the original package that had already been fixed in the original package. None of such vulnerabilities were discoverable via the standard npm audit process.",2022,728
Source Code Recommender Systems: The Practitioners' Perspective,"Ciniselli, Matteo and Pascarella, Luca and Aghajani, Emad and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele","The automatic generation of source code is one of the long-lasting dreams in software engineering research. Several techniques have been proposed to speed up the writing of new code. For example, code completion techniques can recommend to developers the next few tokens they are likely to type, while retrieval-based approaches can suggest code snippets relevant for the task at hand. Also, deep learning has been used to automatically generate code statements starting from a natural language description. While research in this field is very active, there is no study investigating what the users of code recommender systems (i.e., software practitioners) actually need from these tools. We present a study involving 80 software developers to investigate the characteristics of code recommender systems they consider important. The output of our study is a taxonomy of 70 ""requirements"" that should be considered when designing code recommender systems. For example, developers would like the recommended code to use the same coding style of the code under development. Also, code recommenders being ""aware"" of the developers' knowledge (e.g., what are the framework/libraries they already used in the past) and able to customize the recommendations based on this knowledge would be appreciated by practitioners. The taxonomy output of our study points to a wide set of future research directions for code recommenders.",2023,729
Investigating Agile Practices in Software Startups,"Souza, Renata and Rocha, Larissa and Silva, Franklin and Machado, Ivan","Software development practices have smoothly shifted from traditional software development to new approaches that fit better to the real and unpredictable world. Agile practices might help practitioners respond quickly to customer change requests and deliver a working software on-schedule. Software startups are companies that develop innovative and software-intensive products and services in a dynamic and fast-growing market. This study aims to investigate the use of agile practices in software startups. We conducted 14 in-depth semi-structured interviews with the CEO and CTO from early-stage software startups. The results indicate that DevOps, Fundamentals, Design and Extreme Programming are the most used agile practice areas. Our results open up an opportunity to improve software engineering practices in early-stage software startups.",2019,730
Help! I need somebody. A Mapping Study about Expert Identification in Software Development,"Braga, Carlos and Santos Jr, Paulo and Barcellos, Monalessa","Context: Software development is a knowledge-intensive activity, and its success in an organization relies deeply on knowledge sharing. Knowledge management challenges are often increased in agile environments, which involve a lot of tacit knowledge, commonly acquired through experiences and hard to be made explicit. Therefore, knowledge sharing among practitioners is crucial. However, identifying suitable experts to share specific knowledge is not trivial. It involves not only discovering the individuals with the desired knowledge but also considering other factors that may improve the expert responsiveness, such as social connections and availability. Objective: Considering the important role experts play in knowledge sharing, we decided to investigate approaches that help identify experts that can share knowledge in software development. Our goal is to provide a panorama of the existing approaches and shine a light on research opportunities. Method: We carried out a systematic literature mapping and analyzed 17 publications. Results: The results show that most approaches have relied on code repositories as a source of evidence for identifying experts and, consequently, focus on supporting developers and aiding in the codification activity. Additionally, expert identification has been mostly automated, and factors beyond possessing the desired knowledge have often been disregarded. Conclusion: Although there are several expert identification approaches, there has been a lack of concern with factors that influence reaching the most suitable expert for a specific situation (e.g., considering the characteristics of the person seeking knowledge). Moreover, there is a need for deeper reflection on how to better explore different artifacts as sources of expert evidence and how to combine them to improve expert identification.",2023,731
MLstruct: principal type inference in a Boolean algebra of structural types,"Parreaux, Lionel and Chau, Chun Yin","Intersection and union types are becoming more popular by the day, entering the mainstream in programming languages like TypeScript and Scala 3. Yet, no language so far has managed to combine these powerful types with principal polymorphic type inference. We present a solution to this problem in MLstruct, a language with subtyped records, equirecursive types, first-class unions and intersections, class-based instance matching, and ML-style principal type inference. While MLstruct is mostly structurally typed, it contains a healthy sprinkle of nominality for classes, which gives it desirable semantics, enabling the expression of a powerful form of extensible variants that does not need row variables. Technically, we define the constructs of our language using conjunction, disjunction, and negation connectives, making sure they form a Boolean algebra, and we show that the addition of a few nonstandard but sound subtyping rules gives us enough structure to derive a sound and complete type inference algorithm. With this work, we hope to foster the development of better type inference for present and future programming languages with expressive subtyping systems.",2022,732
A parallelism profiler with what-if analyses for OpenMP programs,"Boushehrinejadmoradi, Nader and Yoga, Adarsh and Nagarakatte, Santosh","This paper proposes OMP-WhIP, a profiler that measures inherent parallelism in the program for a given input and provides what-if analyses to estimate improvements in parallelism. We propose a novel OpenMP series-parallel graph representation (OSPG) that precisely captures series-parallel relations induced by various directives between different fragments of dynamic execution. OMP-WhIP constructs the OSPG and measures the computation performed by each dynamic fragment using hardware performance counters. This series-parallel representation along with measurement of computation is a performance model of the program for a given input, which enables computation of inherent parallelism. This novel performance model also enables what-if analyses where a programmer can estimate improvements in parallelism when bottlenecks are addressed. We have used OMP-WhIP to identify parallelism bottlenecks in more than forty applications and then designed strategies to improve the speedup in seven applications.",2018,733
MOCO '24: Proceedings of the 9th International Conference on Movement and Computing,,,2024,734
"""Was my contribution fairly reviewed?"": a framework to study the perception of fairness in modern code reviews","German, Daniel M. and Robles, Gregorio and Poo-Caama\~{n}o, Germ\'{a}n and Yang, Xin and Iida, Hajimu and Inoue, Katsuro","Modern code reviews improve the quality of software products. Although modern code reviews rely heavily on human interactions, little is known regarding whether they are performed fairly. Fairness plays a role in any process where decisions that affect others are made. When a system is perceived to be unfair, it affects negatively the productivity and motivation of its participants. In this paper, using fairness theory we create a framework that describes how fairness affects modern code reviews. To demonstrate its applicability, and the importance of fairness in code reviews, we conducted an empirical study that asked developers of a large industrial open source ecosystem (OpenStack) what their perceptions are regarding fairness in their code reviewing process. Our study shows that, in general, the code review process in OpenStack is perceived as fair; however, a significant portion of respondents perceive it as unfair. We also show that the variability in the way they prioritize code reviews signals a lack of consistency and the existence of bias (potentially increasing the perception of unfairness). The contributions of this paper are: (1) we propose a framework---based on fairness theory---for studying and managing social behaviour in modern code reviews, (2) we provide support for the framework through the results of a case study on a large industrial-backed open source project, (3) we present evidence that fairness is an issue in the code review process of a large open source ecosystem, and, (4) we present a set of guidelines for practitioners to address unfairness in modern code reviews.",2018,735
COMPUTE '23: Proceedings of the 16th Annual ACM India Compute Conference,,,2023,736
ICAAI '22: Proceedings of the 6th International Conference on Advances in Artificial Intelligence,,,2022,737
A behavioral notion of robustness for software systems,"Zhang, Changjian and Garlan, David and Kang, Eunsuk","Software systems are designed and implemented with assumptions about the environment. However, once the system is deployed, the actual environment may deviate from its expected behavior, possibly undermining desired properties of the system. To enable systematic design of systems that are robust against potential environmental deviations, we propose a rigorous notion of robustness for software systems. In particular, the robustness of a system is defined as the largest set of deviating environmental behaviors under which the system is capable of guaranteeing a desired property. We describe a new set of design analysis problems based on our notion of robustness, and a technique for automatically computing robustness of a system given its behavior description. We demonstrate potential applications of our robustness notion on two case studies involving network protocols and safety-critical interfaces.",2020,738
Going Beyond The Challenge! Investigating The Aspects That Attract People to Participate in Hackathons,"de Almeida Melo, Leandro and da Silva Junior, F\'{a}bio Freire and Leite, Tiago Henrique da S. and Filho, Fernando Figueira and de Souza, Cleidson R. B.","Hackathons are events where their participants face the challenge of working intensively and collaboratively with other people. In these events, the participants have the opportunity to develop functional prototypes and solutions to real problems in a short time period (one to three days). Those kind of events have become increasingly popular and spread to several fields of knowledge. Also, they have been adopted by many organizations over the world. However, there is little scientific data that explains why people have participated in hackathons. In this paper, we report findings from an exploratory study in two hackathons organized by a big IT company. We aim to understand why their participants have voluntarily participated in such events. We identified four sets of motivational factors: technical, social, individual and business motivations. Technical motivations are associated with skill acquisition, while social motivations are related to the interaction between people during the event. Also, individual motivations are associated with a sense of autonomy and enjoyment of work. Finally, business motivations include the opportunity of publicizing work and the possibility of establishing partnerships. Our findings contributes to a broader understanding of the motivations for participating in hackathons, as well as providing recommendations for the organization of such events.",2018,739
Quantifying permissiveness of access control policies,"Eiers, William and Sankaran, Ganesh and Li, Albert and O'Mahony, Emily and Prince, Benjamin and Bultan, Tevfik","Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information, without verification and validation techniques that can assist developers in writing policies, complex policy specifications are likely to have errors that can lead to unintended and unauthorized access to data, possibly with disastrous consequences. In this paper, we present a quantitative and differential policy analysis framework that not only identifies if one policy is more permissive than another policy, but also quantifies the relative permissiveness of access control policies. We quantify permissiveness of policies using a model counting constraint solver. We present a heuristic that transforms constraints extracted from access control policies and significantly improves the model counting performance. We demonstrate the effectiveness of our approach by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language and Microsoft's Azure policy language.",2022,740
Towards a Conversational Agent to Support the Software Testing Education,"Paschoal, Leo Natan and Turci, Lucas Fernandes and Conte, Tayana Uch\^{o}a and Souza, Simone R. S.","The training of professionals in the field of software testing is increasing its relevance in the past few years and, therefore, efforts in appropriate methodologies for the learning-teaching process in this context have been proposed and appreciated. The emergence of pedagogical models, such as flipped classroom and team-based learning, which demand from the students a previous study of the theory before the lecture, creates a concern: how to support the before class learning? Because of the hybrid nature of these pedagogical models, which means they mix elements from traditional and distance education, it is possible that the support mechanisms used in distance learning platforms, such as conversational agents, can be applied for this matter. At the same time in which the academic work tries carefully to provide a proper software testing formation, there are also many contributions being established regarding the training and non-formal learning. Improvement and personal training courses about criteria, tools, and software testing good practices are being created by teaching institutes and offered in Massive Open Online Courses platforms (MOOCs). However, in this type of course, in the absence of a teacher, the student might be in a situation where there is nobody available to answer their questions about the topic. In this paper, we propose the use of conversational agents in solving the problems and challenges which encompass the learning through MOOCs and hybrid models. A conversational agent, called TOB-SST is proposed to support software testing education. A viability study was conducted to understand the quality of the given answers by TOB-SST and the possibility of it serving as a learning support tool. The results indicate that it is promising to employ a conversational agent to guide student study.",2019,741
"Designing Adaptive Developer-Chatbot Interactions: Context Integration, Experimental Studies, and Levels of Automation","Melo, Glaucia","The growing demand for software developers and the increasing development complexity have emphasized the need for support in software engineering projects. This is especially relevant in light of advancements in artificial intelligence, such as conversational systems. A significant contributor to the complexity of software development is the multitude of tools and methods used, creating various contexts in which software developers must operate. Moreover, there has been limited investigation into the interaction between context-based chatbots and software developers through experimental user studies. Assisting software developers in their work becomes essential. In particular, understanding the context surrounding software development and integrating this context into chatbots can lead to novel insight into what software developers expect concerning these human-chatbot interactions and their levels of automation. In my research, I study the design of context-based adaptive interactions between software developers and chatbots to foster solutions and knowledge to support software developers at work.",2023,742
"AIES '23: Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,2023,743
"FAccT '24: Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency",,,2024,744
SAC '24: Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing,,"On behalf of the Organizing Committee, I extend a warm welcome to you at the 39th Annual ACM Symposium on Applied Computing (SAC 2024), taking place in \'{A}vila, Spain, and hosted by the University of Salamanca. For more than three decades, this international forum has been dedicated to computer scientists, engineers, and practitioners, providing a platform for presenting their research findings and results in various areas of applied computing. The organizing committee sincerely appreciates your participation in this exciting international event, and we hope that the conference proves interesting and beneficial for all attendees.",2024,745
Generating Python Type Annotations from Type Inference: How Far Are We?,"Guo, Yimeng and Chen, Zhifei and Chen, Lin and Xu, Wenjie and Li, Yanhui and Zhou, Yuming and Xu, Baowen","In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go.In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.",2024,746
dg.o '23: Proceedings of the 24th Annual International Conference on Digital Government Research,,,2023,747
Dark Web Marketplaces: Data for Collaborative Threat Intelligence,"Connolly, Kate and Klempay, Anna and McCann, Mary and Brenner, Paul","The dark web has become an increasingly important landscape for the sale of illicit cyber goods. Given the prevalence of malware and tools that are used to steal data from individuals on these markets, it is crucial that every company, governing body, and cyber professional be aware of what information is sold on these marketplaces. Knowing this information will allow these entities to protect themselves against cyber attacks and from information breaches. In this article, we announce the public release of a data set on dark web marketplaces’ cybersecurity-related listings. We spent multiple years seeking out websites that sold illicit digital goods and collected data on the available products. Due to the marketplaces’ varied and complex layers of security, we leveraged the flexible Selenium WebDriver with Python to navigate the web pages and collect data. We present analysis of categories of malicious cyber goods sold on marketplaces, prices, persistent vendors, ratings, and other basic information on marketplace storefronts. Additionally, we share the tools and techniques we’ve compiled, enabling others to scrape dark web marketplaces at a significantly lower risk. We invite professionals who opt to gather data from the dark web to contribute to the publicly shared threat intelligence resource.",2023,748
CopypastaVulGuard – A browser extension to prevent copy and paste spreading of vulnerable source code in forum posts,"Schmidt, Holger and van Aerssen, Max and Leich, Christian and Benni, Abdulkader and Al Ali, Salar and Tanz, Jakob","Forums such as Stack Overflow are used by many software developers to find a solution for a given coding problem. Found solutions, i.e. forum posts containing relevant source code, are utilized in a copy and paste manner. This behavior carries the risk that vulnerabilities contained in the source code of the forum posts are spread. Software developers should be able to identify vulnerable source code at an early stage, thereby preventing copying the corresponding source code. In this paper, we introduce the tool CopypastaVulGuard that identifies vulnerable source code in forum posts and allows software developers to omit the source code by marking the forum posts as dangerous. Our tool consists of a browser extension and a management application capable to address as examples SQL injections, remote code executions and deprecated functions based on a dump of the archive.org Stack Overflow data set. We present an evaluation of our tool’s possible impact and relevance considering pros/cons and selected research questions.",2022,749
"MobiHoc '23: Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing",,ACM MobiHoc is a premier international annual conference with a highly selective single-track technical program dedicated to addressing the challenges emerging from networked systems that must operate in the face of dynamics.,2023,750
HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration,"Jin, Xianhao and Servant, Francisco","Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice—Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer.",2023,751
CSCS '23: Proceedings of the 7th ACM Computer Science in Cars Symposium,,,2023,752
Proactive detection of collaboration conflicts,"Brun, Yuriy and Holmes, Reid and Ernst, Michael D. and Notkin, David","Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results.First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems.Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations.Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts.",2011,753
CompEd 2023: Proceedings of the ACM Conference on Global Computing Education Vol 2,,"It is our great pleasure to welcome participants to the 2nd ACM Global Conference on Computing Education (ACM CompEd 2023) being held in Hyderabad, India, 7th-9th December, 2023 with the Working Groups meetings being held on 5th and 6th December 2023.ACM CompEd is a recent addition to the list of ACM sponsored conferences devoted to research in all aspects of computing education, including education at the school and college levels. The Hyderabad edition is only the second in this promising series. The long hiatus due to Covid-19 pushed this conference by two years, but we are glad that it is finally here!This edition of ACM CompEd partly overlaps with COMPUTE 2023, ACM India's flagship conference on Computing Education. Having the two conferences adjacent to each other is a great way to build synergy between the Indian computing education community and the global community of computing education researchers.",2023,754
Prototyping ways of prototyping AI,"van Allen, Philip",,2018,755
EuroPLoP '23: Proceedings of the 28th European Conference on Pattern Languages of Programs,,,2023,756
Unsupervised Anomaly Detectors to Detect Intrusions in the Current Threat Landscape,"Zoppi, Tommaso and Ceccarelli, Andrea and Capecchi, Tommaso and Bondavalli, Andrea","Anomaly detection aims at identifying unexpected fluctuations in the expected behavior of a given system. It is acknowledged as a reliable answer to the identification of zero-day attacks to such extent, several ML algorithms that suit for binary classification have been proposed throughout years. However, the experimental comparison of a wide pool of unsupervised algorithms for anomaly-based intrusion detection against a comprehensive set of attacks datasets was not investigated yet. To fill such gap, we exercise 17 unsupervised anomaly detection algorithms on 11 attack datasets. Results allow elaborating on a wide range of arguments, from the behavior of the individual algorithm to the suitability of the datasets to anomaly detection. We conclude that algorithms as Isolation Forests, One-Class Support Vector Machines, and Self-Organizing Maps are more effective than their counterparts for intrusion detection, while clustering algorithms represent a good alternative due to their low computational complexity. Further, we detail how attacks with unstable, distributed, or non-repeatable behavior such as Fuzzing, Worms, and Botnets are more difficult to detect. Ultimately, we digress on capabilities of algorithms in detecting anomalies generated by a wide pool of unknown attacks, showing that achieved metric scores do not vary with respect to identifying single attacks.",2021,757
MOTION – A Framework for Mixed-Protocol Multi-Party Computation,"Braun, Lennart and Demmler, Daniel and Schneider, Thomas and Tkachenko, Oleksandr","We present MOTION, an efficient and generic open-source framework for mixed-protocol secure multi-party computation&nbsp;(MPC). MOTION is built in a user-friendly, modular, and extensible way, intended to be used as a tool in MPC research and to increase adoption of MPC protocols in practice. Our framework incorporates several important engineering decisions such as full communication serialization, which enables MPC over arbitrary messaging interfaces and removes the need of owning network sockets. MOTION also incorporates several performance optimizations that improve the communication complexity and latency, e.g.,  ( 2times ) &nbsp;better online round complexity of precomputed correlated&nbsp;Oblivious Transfer&nbsp;(OT).We instantiate our framework with protocols for N&nbsp;parties and security against up to  ( N-1 )  passive corruptions: the MPC protocols of Goldreich-Micali-Wigderson&nbsp;(GMW) in its arithmetic and Boolean version and OT-based BMR&nbsp;(Ben-Efraim et&nbsp;al., CCS’16), as well as novel and highly efficient conversions between them, including a non-interactive conversion from BMR to arithmetic GMW.MOTION is highly efficient, which we demonstrate in our experiments. Compared to secure evaluation of AES-128 with  ( N=3 )  parties in a high-latency network with OT-based BMR, we achieve a 16 ( times )  better throughput of 16&nbsp;AES evaluations per second using BMR. With this, we show that BMR is much more competitive than previously assumed. For  ( N=3 )  parties and full-threshold protocols in a LAN, MOTION is  ( 10times ) – ( 18times )  faster than the previous best passively secure implementation from the MP-SPDZ framework, and  ( 190times ) – ( 586times )  faster than the actively secure SCALE-MAMBA framework. Finally, we show that our framework is highly efficient for privacy-preserving neural network inference.",2022,758
Chatting with AI: Deciphering Developer Conversations with ChatGPT,"Mohamed, Suad and Parvin, Abdullah and Parra, Esteban","Large Language Models (LLMs) have been widely adopted and are becoming ubiquitous and integral to software development. However, we have little knowledge as to how these tools are being used by software developers beyond anecdotal evidence and word-of-mouth reports. In this work, we present a study toward understanding how developers engage with and utilize LLMs by reporting the results of an empirical study identifying patterns in the conversation that developers have with LLMs. We identified a total of 19 topics describing the purpose of the developers in their conversations with LLMs. Our findings reveal that developers use LLMs to facilitate various aspects of their software development processes (e.g., information-seeking about programming languages and frameworks and soliciting high-level design recommendations) to a similar extent to which they use them for non-development purposes such as writing assistance, general purpose queries, and conducting Turing tests to assess the intrinsic capabilities of the models. This work not only sheds light on the diverse applications of LLMs in software development but also underscores their emerging role as critical tools in enhancing developer productivity and creativity as we move closer to widespread AI-assisted software development.",2024,759
CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing,,"Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous ""revise and resubmit"" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on ""Conversational AI &amp; Lessons Learned."" Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, ""The Science Gap."" We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, ""Mobility in Collaboration.""",2017,760
IoT '22: Proceedings of the 12th International Conference on the Internet of Things,,,2022,761
Co-Creation in Fully Remote Software Teams,"Jackson, Victoria and Prikladnicki, Rafael and van der Hoek, Andre","In this paper, we use the lens of co-creation---a concept originally coined and applied in the fields of management and design that denotes how groups of people collaboratively create something of meaning through an orchestration of people, activities, and tools---to study how fully remote software teams co-create digital artifacts that can be considered as a form of documentation. We report on the results of a qualitative, interview-based study with 25 software professionals working in remote teams. Our primary findings are the definition of four models of co-creation, examples of sequencing these models into work chains to produce artifacts, factors that influence how developers match tasks to models and chains, and insights into tool support for co-creation. Together, our findings illustrate how co-creation is an intentional activity that has a significant role in how remote software teams' choose to structure their collaborative activities.",2024,762
Containing Malicious Package Updates in npm with a Lightweight Permission System,"Ferreira, Gabriel and Jia, Limin and Sunshine, Joshua and K\""{a}stner, Christian","The large amount of third-party packages available in fast-moving software ecosystems, such as Node.js/npm, enables attackers to compromise applications by pushing malicious updates to their package dependencies. Studying the npm repository, we observed that many packages in the npm repository that are used in Node.js applications perform only simple computations and do not need access to filesystem or network APIs. This offers the opportunity to enforce least-privilege design per package, protecting applications and package dependencies from malicious updates. We propose a lightweight permission system that protects Node.js applications by enforcing package permissions at runtime. We discuss the design space of solutions and show that our system makes a large number of packages much harder to be exploited, almost for free.",2021,763
Graph Mining for Cybersecurity: A Survey,"Yan, Bo and Yang, Cheng and Shi, Chuan and Fang, Yong and Li, Qi and Ye, Yanfang and Du, Junping","The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.",2023,764
CSCW '23 Companion: Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing,,,2023,765
Sentiment Analysis for the Natural Environment: A Systematic Review,"Ibrohim, Muhammad Okky and Bosco, Cristina and Basile, Valerio","In this systematic review, Kitchenham’s framework is used to explore what tasks, techniques, and benchmarks for Sentiment Analysis have been developed for addressing topics about the natural environment. We comprehensively analyze seven dimensions including contribution, topical focus, data source and query, annotation, language, detail of the task, and technology/algorithm used. By showing how this research area has grown during the last few years, our investigation provides important findings about the results achieved and the challenges that need to be still addressed for making this technology actually helpful for stakeholders such as policymakers and governments.",2023,766
BSCI '23: Proceedings of the 5th ACM International Symposium on Blockchain and Secure Critical Infrastructure,,,2023,767
ICISDM '22: Proceedings of the 6th International Conference on Information System and Data Mining,,,2022,768
On the Relation between Code Elements and Accessibility Issues in Android Apps,"da Silva, Henrique Neves and Endo, Andre Takeshi and Eler, Marcelo Medeiros and Vergilio, Silvia Regina and Durelli, Vinicius H. S.","Mobile apps have gone mainstream and become part of our daily lives. Currently, many efforts have been made to make apps more accessible to people with disabilities. However, little is still known on how to implement more accessible apps. In the Android API, there are (code) elements that may be employed to (in)directly improve the app's accessibility. This paper aims to investigate the prevalence of accessibility code elements and their relation to potential accessibility issues. First, we identified code elements of the native Android API that may be related to accessibility features, and mapped them to principles and success criteria of the Web Content Accessibility Guidelines (WCAG) 2.1. Using a sample of 111 open source mobile apps available in Google Play, we conducted a characterization study to examine the prevalence of accessibility code elements. We also analyzed how these code elements are related to issues detected by the static analyzer Android Lint and the accessibility testing tool MATE. Our results indicate that code elements are not widely used; the ones directly related to accessibility are present in only a few apps. Additionally, our results would seem to suggest that apps that adopt accessibility code elements, tend to have less accessibility issues. By analyzing our results from the standpoint of the WCAG principles, we conclude that there is room for improvement in terms of how both the Android API and automated testing tools deal with accessibility-related issues.",2020,769
Data-Driven Governance in Crises: Topic Modelling for the Identification of Refugee Needs,"Sprenkamp, Kilian and Zavolokina, Liudmila and Angst, Mario and Dolata, Mateusz","The war in Ukraine and the following refugee crisis have recently again highlighted the need for effective refugee management across European countries. Refugee management contemporarily mostly relies on top-down management approaches by governments. These often lead to suboptimal policies for refugees and highlight a need to better identify and integrate refugee needs into management. Here, we show that modern applications of Natural Language Processing (NLP) allow for the effective analysis of large text corpora linked to refugee needs, making it possible to complement top-down approaches with bottom-up knowledge centered around the current needs of the refugee population. By following a Design Science Research Methodology, we utilize 58 semi-structured stakeholder interviews within Switzerland to develop design requirements for NLP applications for refugee management. Based on the design requirements, we developed R2G – “Refugees to Government”, an application based on state-of-the-art topic modeling to identify refugee needs bottom-up through Telegram data. We evaluate R2G with a dedicated workshop held with stakeholders from the public sector and civil society. Thus, we contribute to the ongoing discourse on how to design refugee management applications and showcase how topic modeling can be utilized for data-driven governance during refugee crises.",2023,770
HT '23: Proceedings of the 34th ACM Conference on Hypertext and Social Media,,,2023,771
Tracking Counterfeit Cryptocurrency End-to-end,"Gao, Bingyu and Wang, Haoyu and Xia, Pengcheng and Wu, Siwei and Zhou, Yajin and Luo, Xiapu and Tyson, Gareth","The production of counterfeit money has a long history. It refers to the creation of imitation currency that is produced without the legal sanction of government. With the growth of the cryptocurrency ecosystem, there is expanding evidence that counterfeit cryptocurrency has also appeared. In this paper, we empirically explore the presence of counterfeit cryptocurrencies on Ethereum and measure their impact. By analyzing over 190K ERC-20 tokens (or cryptocurrencies) on Ethereum, we have identified $2,117$ counterfeit tokens that target 94 of the 100 most popular cryptocurrencies. We perform an end-to-end characterization of the counterfeit token ecosystem, including their popularity, creators and holders, fraudulent behaviors and advertising channels. Through this, we have identified two types of scams related to counterfeit tokens and devised techniques to identify such scams. We observe that over 7,104 victims were deceived in these scams, and the overall financial loss sums to a minimum of $ 17 million (74,271.7 ETH). Our findings demonstrate the urgency to identify counterfeit cryptocurrencies and mitigate this threat.",2020,772
Wireless Latency Shift Keying,"Johnson, Jacob and Palacios, Ashton and Arvonen, Cody and Lundrigan, Philip","IEEE 802.11 (WiFi) only has two modes of trust---complete trust or complete untrust. The lack of nuance leaves no room for sensors that a user does not fully trust but wants to connect to their network, such as a WiFi sensor. Solutions exist, but they require advanced knowledge of network administration. We solve this problem by introducing a new way of modulating data in the latency of the network, called Latency Shift Keying. We use specific characteristics of the WiFi protocol to carefully control the latency of just one device on the network. We build a transmitter, receiver, and modulation scheme that is designed to encode data in the latency of a network. We develop an application, Wicket, that solves the WiFi trust issue using Latency Shift Keying to create a new security association between an untrusted WiFi sensor and a wired device on the trusted network. We evaluate its performance and show that it works in many network conditions and environments.",2024,773
StateDiver: Testing Deep Packet Inspection Systems with State-Discrepancy Guidance,"Zhang, Zhechang and Yuan, Bin and Yang, Kehan and Zou, Deqing and Jin, Hai","Deep Packet Inspection (DPI) systems are essential for securing modern networks (e.g., blocking or logging abnormal network connections). However, DPI systems are known to be vulnerable in their implementations, which could be exploited for evasion attacks. Due to the critical role DPI systems play, many efforts have been made to detect vulnerabilities in the DPI systems through manual inspection, symbolic execution, and fuzzing, which suffer from either poor scalability, path explosion, or inappropriate feedback. In this paper, based on our observation that a DPI system usually reaches an abnormal internal state before a forbidden packet passes through it, we propose a fuzzing framework that prioritizes inputs/mutations which could trigger the DPI system’s abnormal internal states. Further, to avoid deep understanding of the DPI systems under inspection (e.g., to identify the abnormal states), we feed one pair of inputs to multiple DPI systems and check whether the state changes of these DPI systems are consistent — an inconsistent internal state change/transference in one of the DPI systems indicates a new abnormal state is reached in the corresponding DPI system. Naturally, inputs that trigger new abnormal states are preferentially selected for mutations to generate new inputs. Following this idea, we develop StateDiver, the first fuzzing framework that uses the state discrepancy between different DPI systems as feedback to find more bypassing strategies. We make StateDiver publicly available online. With the help of StateDiver, we tested 3 famous open-source DPI systems (Snort, Snort++, and Suricata) and discovered 16 bypass strategies (8 new and 8 previously known). We have reported all the vulnerabilities to the vendors and received one CVE by the time of paper writing. We also compared StateDiver with Geneva, the state-of-the-art fuzzing tool for detecting DPI bugs. Results showed that StateDiver outperformed Geneva at the number and speed of finding vulnerabilities, indicating the ability of StateDiver to detect strategies bypassing DPI systems effectively.",2022,774
DeepPower: Non-intrusive and Deep Learning-based Detection of IoT Malware Using Power Side Channels,"Ding, Fei and Li, Hongda and Luo, Feng and Hu, Hongxin and Cheng, Long and Xiao, Hai and Ge, Rong","The vulnerability of Internet of Things (IoT) devices to malware attacks poses huge challenges to current Internet security. The IoT malware attacks are usually composed of three stages: intrusion, infection and monetization. Existing approaches for IoT malware detection cannot effectively identify the executed malicious activities at intrusion and infection stages, and thus cannot help stop potential attacks timely. In this paper, we present DeepPower, a non-intrusive approach to infer malicious activities of IoT malware via analyzing power side-channel signals using deep learning. DeepPower first filters raw power signals of IoT devices to obtain suspicious signals, and then performs a fine-grained analysis on these signals to infer corresponding executed activities inside the devices. DeepPower determines whether there exists an ongoing malware infection by conducting a correlation analysis on these identified activities. We implement a prototype of DeepPower leveraging low-cost sensors and devices and evaluate the effectiveness of DeepPower against real-world IoT malware using commodity IoT devices. Our experimental results demonstrate that DeepPower is able to detect infection activities of different IoT malware with a high accuracy without any changes to the monitored devices.",2020,775
ICIIT '23: Proceedings of the 2023 8th International Conference on Intelligent Information Technology,,,2023,776
The Effect of Alter Ego Accounts on A/B Tests in Social Networks,"Avery, Katherine and Houmansadr, Amir and Jensen, David","Social network users often maintain multiple active accounts, sometimes referred to as alter egos. Examples of alter egos include personal and professional accounts or named and anonymous accounts. If alter egos are common on a platform, they can affect the results of A/B testing because a user's alter egos can influence each other. For a single user, one account may be assigned treatment, while another is assigned control. Alter-ego bias is relevant when the treatment affects the individual user rather than the account. Through experimentation and theoretical analysis, we examine the worst and expected case bias for different numbers of alter egos and for a variety of network structures and peer effect strengths. We show that alter egos moderately bias the results of simulated A/B tests on several network structures, including a real-world Facebook subgraph and several types of synthetic networks: small world networks, forest fire networks, stochastic block models, and a worst-case structure. We also show that bias increases with the number of alter egos and that different network structures have different upper bounds on bias.",2024,777
Medical and health systems,"Sonntag, Daniel",,2019,778
ICBTA '22: Proceedings of the 2022 5th International Conference on Blockchain Technology and Applications,,,2022,779
Measuring the Prevalence of Anti-Social Behavior in Online Communities,"Park, Joon Sung and Seering, Joseph and Bernstein, Michael S.","With increasing attention to online anti-social behaviors such as personal attacks and bigotry, it is critical to have an accurate accounting of how widespread anti-social behaviors are. In this paper, we empirically measure the prevalence of anti-social behavior in one of the world's most popular online community platforms. We operationalize this goal as measuring the proportion of unmoderated comments in the 97 most popular communities on Reddit that violate eight widely accepted platform norms. To achieve this goal, we contribute a human-AI pipeline for identifying these violations and a bootstrap sampling method to quantify measurement uncertainty. We find that 6.25% (95% Confidence Interval [5.36%, 7.13%]) of all comments in 2016, and 4.28% (95% CI [2.50%, 6.26%]) in 2020, are violations of these norms. Most anti-social behaviors remain unmoderated: moderators only removed one in twenty violating comments in 2016, and one in ten violating comments in 2020. Personal attacks were the most prevalent category of norm violation; pornography and bigotry were the most likely to be moderated, while politically inflammatory comments and misogyny/vulgarity were the least likely to be moderated. This paper offers a method and set of empirical results for tracking these phenomena as both the social practices (e.g., moderation) and technical practices (e.g., design) evolve.",2022,780
When testing meets code review: why and how developers review tests,"Spadini, Davide and Aniche, Maur\'{\i}cio and Storey, Margaret-Anne and Bruntink, Magiel and Bacchelli, Alberto","Automated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research.",2018,781
ICEEL '22: Proceedings of the 2022 6th International Conference on Education and E-Learning,,,2022,782
ICPE '24 Companion: Companion of the 15th ACM/SPEC International Conference on Performance Engineering,,"It is our great pleasure to present the ICPE 2024 workshops program. ICPE workshops extend the main conference by providing a forum to foster discussion on hot and emerging topics from the broad field of performance engineering. They offer a highly dynamic venue to exchange ideas, establish new collaborations, and bootstrap debates on novel techniques, methodologies, and their associated early research results. Workshops feature various presentation formats, including research paper presentations, panel discussions, and keynote talks. Through these presentations and discussions with peer researchers, ICPE workshops help shape future research and identify promising research directions for performance engineering.",2024,783
WWW '19: The World Wide Web Conference,,"It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.",2019,784
"UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization",,,2023,785
What All the PHUZZ Is About: A Coverage-guided Fuzzer for Finding Vulnerabilities in PHP Web Applications,"Neef, Sebastian and Kleissner, Lorenz and Seifert, Jean-Pierre","Coverage-guided fuzz testing has received significant attention from the research community, with a strong focus on binary applications, greatly disregarding other targets, such as web applications. The importance of the World Wide Web in everyone's life cannot be overstated, and to this day, many web applications are developed in PHP. In this work, we address the challenges of applying coverage-guided fuzzing to PHP web applications and introduce Phuzz, a modular fuzzing framework for PHP web applications. Phuzz uses novel approaches to detect more client-side and server-side vulnerability classes than state-of-the-art related work, including SQL injections, remote command injections, insecure deserialization, path traversal, external entity injection, cross-site scripting, and open redirection. We evaluate Phuzz on a diverse set of artificial and real-world web applications with known and unknown vulnerabilities, and compare it against a variety of state-of-the-art fuzzers. In order to show Phuzz' effectiveness, we fuzz over 1,000 API endpoints of the 115 most popular WordPress plugins, resulting in over 20 security issues and 2 new CVE-IDs. Finally, we make the framework publicly available to motivate and encourage further research on web application fuzz testing.",2024,786
An Investigation into an Always Listening Interface to Support Data Exploration,"Tabalba, Roderick S and Kirshenbaum, Nurit and Leigh, Jason and Bhattacharya, Abari and Grosso, Veronica and Di Eugenio, Barbara and Johnson, Andrew E and Zellner, Moira","Natural Language Interfaces that facilitate data exploration tasks are rapidly gaining in interest in the research community because they enable users to focus their attention on the task of inquiry rather than the mechanics of chart construction. Yet, current systems rely solely on processing the user’s explicit commands to generate the user’s intended chart. These commands can be ambiguous due to natural language tendencies such as speech disfluency and underspecification. In this paper, we developed and studied how an always listening interface can help contextualize imprecise queries. Our study revealed that an always listening interface is able to use an on-going conversation to fill in missing properties for imprecise commands, disambiguate inaccurate commands without asking the user for clarification, as well as generate charts without being explicitly asked.",2023,787
Personality-affected Emotion Generation in Dialog Systems,"Wen, Zhiyuan and Cao, Jiannong and Shen, Jiaxing and Yang, Ruosong and Liu, Shuaiqi and Sun, Maosong","Generating appropriate emotions for responses is essential for dialogue systems to provide human-like interaction in various application scenarios. Most previous dialogue systems tried to achieve this goal by learning empathetic manners from anonymous conversational data. However, emotional responses generated by those methods may be inconsistent, which will decrease user engagement and service quality. Psychological findings suggest that the emotional expressions of humans are rooted in personality traits. Therefore, we propose a new task, Personality-affected Emotion Generation, to generate emotion based on the personality given to the dialogue system and further investigate a solution through the personality-affected mood transition. Specifically, we first construct a daily dialogue dataset, Personality EmotionLines Dataset (PELD), with emotion and personality annotations. Subsequently, we analyze the challenges in this task, i.e., (1) heterogeneously integrating personality and emotional factors and (2) extracting multi-granularity emotional information in the dialogue context. Finally, we propose to model the personality as the transition weight by simulating the mood transition process in the dialogue system and solve the challenges above. We conduct extensive experiments on PELD for evaluation. Results suggest that by adopting our method, the emotion generation performance is improved by 13% in macro-F1 and 5% in weighted-F1 from the BERT-base model.",2024,788
Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,"Khurana, Anjali and Subramonyam, Hariharan and Chilana, Parmit K","Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to understand how the prompt’s text related to the LLM’s responses and often followed the LLM’s suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM’s advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM’s responses, indicating a gap between their lack of software expertise and their ability to evaluate the LLM’s assistance. With the growing push for designing domain-specific LLM assistants, we emphasize the importance of incorporating explainable, context-aware cues into LLMs to help users understand prompt-based interactions, identify biases, and maximize the utility of LLM assistants.",2024,789
ICCCM '23: Proceedings of the 2023 11th International Conference on Computer and Communications Management,,,2023,790
Redistributing leadership in online creative collaboration,"Luther, Kurt and Fiesler, Casey and Bruckman, Amy","In this paper, we integrate theories of distributed leadership and distributed cognition to account for the roles of people and technology in online leadership. When leadership is distributed effectively, the result can be success stories like Wikipedia and Linux. However, finding a successful distribution is challenging. In the online community Newgrounds, hundreds of collaborative animation projects called ""collabs"" are started each year, but less than 20% are completed. We suggest that many collabs fail because leaders are overburdened and lack adequate technological support. We introduce Pipeline, a collaboration tool designed to support and transform leadership, with the goal of easing the burden on leaders of online creative projects. Through a case study of a six-week, 30-artist collaboration called Holiday Flood, we show how Pipeline supported redistributed leadership. We conclude with implications for theory and the design of social computing systems.",2013,791
Uncovering causality from multivariate hawkes integrated cumulants,"Achab, Massil and Bacry, Emmanuel and Ga\""{\i}ffas, St\'{e}phane and Mastromatteo, Iacopo and Muzy, Jean-Fran\c{c}ois","We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.",2017,792
Automating developer chat mining,"Pan, Shengyi and Bao, Lingfeng and Ren, Xiaoxue and Xia, Xin and Lo, David and Li, Shanping","Online chatrooms are gaining popularity as a communication channel between widely distributed developers of Open Source Software (OSS) projects. Most discussion threads in chatrooms follow a Q&amp;A format, with some developers (askers) raising an initial question and others (respondents) joining in to provide answers. These discussion threads are embedded with rich information that can satisfy the diverse needs of various OSS stakeholders. However, retrieving information from threads is challenging as it requires a thread-level analysis to understand the context. Moreover, the chat data is transient and unstructured, consisting of entangled informal conversations. In this paper, we address this challenge by identifying the information types available in developer chats and further introducing an automated mining technique. Through manual examination of chat data from three chatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories and create a labeled dataset with 2,959 threads. We propose a classification approach (named F2Chat) to structure the vast amount of threads based on the information type automatically, helping stakeholders quickly acquire their desired information. F2Chat effectively combines handcrafted non-textual features with deep textual features extracted by neural models. Specifically, it has two stages with the first one leveraging the siamese architecture to pretrain the textual feature encoder, and the second one facilitating an in-depth fusion of two types of features. Evaluation results suggest that our approach achieves an average F1-score of 0.628, which improves the baseline by 57%. Experiments also verify the effectiveness of our identified non-textual features under both intra-project and cross-project validations.",2022,793
SBQS '22: Proceedings of the XXI Brazilian Symposium on Software Quality,,,2022,794
Container Orchestration Honeypot: Observing Attacks in the Wild,"Spahn, Noah and Hanke, Nils and Holz, Thorsten and Kruegel, Christopher and Vigna, Giovanni","Containers, a mechanism to package software and its dependencies into a single artifact, have helped fuel the rapid pace of technological advancements in the last few years. However, it is not always clear what the potential security risk of moving to the cloud and container-based technologies is. In this paper, we investigate exposed container orchestration services on the Internet: how many there are, and the attacks against them. We considered three groups of container-based software: Docker, Kubernetes, and workflow tools. In a measurement study, we scanned the Internet to identify vulnerable container and container-orchestration services running on default ports. Considering the scan data, we then designed a high-interaction honeypot to reveal where attackers tend to strike and what is being done against exposed instances. The honeypot is based on container orchestration tools installed on Ubuntu servers, behind a carefully constructed gateway, and using the default ports. Our honeypot attracted attackers within minutes of launch. In total, we collected 94 days of attack data and extracted associated indicators of compromise (IOCs), which are provided to the research community to enable further insights. Our empirical study measures the risk associated with container and container orchestration systems exposed on the Internet. The assessment is performed by leveraging a novel design for a high-interaction honeypot. Using the observed data, we extract fresh insights into malicious tools, tactics, and procedures used against exposed host systems. In addition, we make available to the research community a rich dataset of unencrypted malicious traffic.",2023,795
CSET '23: Proceedings of the 16th Cyber Security Experimentation and Test Workshop,,,2023,796
Dissecting Click Fraud Autonomy in the Wild,"Zhu, Tong and Meng, Yan and Hu, Haotian and Zhang, Xiaokuan and Xue, Minhui and Zhu, Haojin","Although the use of pay-per-click mechanisms stimulates the prosperity of the mobile advertisement network, fraudulent ad clicks result in huge financial losses for advertisers. Extensive studies identify click fraud according to click/traffic patterns based on dynamic analysis. However, in this study, we identify a novel click fraud, named humanoid attack, which can circumvent existing detection schemes by generating fraudulent clicks with similar patterns to normal clicks. We implement the first tool ClickScanner to detect humanoid attacks on Android apps based on static analysis and variational AutoEncoders (VAEs) with limited knowledge of fraudulent examples. We define novel features to characterize the patterns of humanoid attacks in the apps' bytecode level. ClickScanner builds a data dependency graph (DDG) based on static analysis to extract these key features and form a feature vector. We then propose a classification model only trained on benign datasets to overcome the limited knowledge of humanoid attacks.We leverage ClickScanner to conduct the first large-scale measurement on app markets (i.e., 120,000 apps from Google Play and Huawei AppGallery) and reveal several unprecedented phenomena. First, even for the top-rated 20,000 apps, ClickScanner still identifies 157 apps as fraudulent, which shows the prevalence of humanoid attacks. Second, it is observed that the ad SDK-based attack (i.e., the fraudulent codes are in the third-party ad SDKs) is now a dominant attack approach. Third, the manner of attack is notably different across apps of various categories and popularities. Finally, we notice there are several existing variants of the humanoid attack. Additionally, our measurements demonstrate the proposed ClickScanner is accurate and time-efficient (i.e., the detection overhead is only 15.35% of those of existing schemes).",2021,797
MuC '22: Proceedings of Mensch und Computer 2022,,,2022,798
UIST '23 Adjunct: Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,,,2023,799
Exploring the Potential for Generative AI-based Conversational Cues for Real-Time Collaborative Ideation,"Rayan, Jude and Kanetkar, Dhruv and Gong, Yifan and Yang, Yuewen and Palani, Srishti and Xia, Haijun and Dow, Steven P.","What is the potential value and role for AI to facilitate real-time creative discussions? The paper explores principles for Generative-AI based conversational support by investigating how humans – playing the role of an AI agent – generate contextual conversational cues to guide an ideation session. We studied n=42 people (14 triads) brainstorming through a remote meeting design probe that allows a wizard facilitator to oversee the ideation and send text-based cues that appear real-time in the ideator interface. Thematic analysis of conversations, cues and post-hoc reflections by facilitators uncovered focal points, strategies and challenges. Notably, 44% of the cues sent out by the facilitators were either dismissed or ignored because they did not notice the cue update. When ideators did notice cues, certain facilitator strategies impacted the conversation more than others. Based on our analysis, we present design opportunities to improve generative AI-based systems to better support real-time creative collaborations.",2024,800
Learning to Recognize Handwriting Input with Acoustic Features,"Yin, Huanpu and Zhou, Anfu and Su, Guangyuan and Chen, Bo and Liu, Liang and Ma, Huadong","For mobile or wearable devices with a small touchscreen, handwriting input (instead of typing on the touchscreen) is highly desirable for efficient human-computer interaction. Previous passive acoustic-based handwriting solutions mainly focus on print-style capital input, which is inconsistent with people's daily habits and thus causes inconvenience. In this paper, we propose WritingRecorder, a novel universal text entry system that enables free-style lowercase handwriting recognition. WritingRecorder leverages the built-in microphone of the smartphones to record the handwritten sound, and then designs an adaptive segmentation method to detect letter fragments in real-time from the recorded sound. Then we design a neural network named Inception-LSTM to extract the hidden and unique acoustic pattern associated with the writing trajectory of each letter and thus classify each letter. Moreover, we adopt a word selection method based on language model, so as to recognize legislate words from all possible letter combinations. We implement WritingRecorder as an APP on mobile phones and conduct the extensive experimental evaluation. The results demonstrate that WritingRecorder works in real-time and can achieve 93.2% accuracy even for new users without collecting and training on their handwriting samples, under a series of practical scenarios.",2020,801
Quality assurance of generative dialog models in an evolving conversational agent used for Swedish language practice,"Borg, Markus and Bengtsson, Johan and \""{O}sterling, Harald and Hagelborn, Alexander and Gagner, Isabella and Tomaszewski, Piotr","Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting.",2022,802
EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering,,,2024,803
Information Needs in Contemporary Code Review,"Pascarella, Luca and Spadini, Davide and Palomba, Fabio and Bruntink, Magiel and Bacchelli, Alberto","Contemporary code review is a widespread practice used by software engineers to maintain high software quality and share project knowledge. However, conducting proper code review takes time and developers often have limited time for review. In this paper, we aim at investigating the information that reviewers need to conduct a proper code review, to better understand this process and how research and tool support can make developers become more effective and efficient reviewers. Previous work has provided evidence that a successful code review process is one in which reviewers and authors actively participate and collaborate. In these cases, the threads of discussions that are saved by code review tools are a precious source of information that can be later exploited for research and practice. In this paper, we focus on this source of information as a way to gather reliable data on the aforementioned reviewers' needs. We manually analyze 900 code review comments from three large open-source projects and organize them in categories by means of a card sort. Our results highlight the presence of seven high-level information needs, such as knowing the uses of methods and variables declared/modified in the code under review. Based on these results we suggest ways in which future code review tools can better support collaboration and the reviewing task.",2018,804
SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2,,"Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is ""Blazing New Trails in CS Education."" This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.",2024,805
CHIIR '24: Proceedings of the 2024 Conference on Human Information Interaction and Retrieval,,,2024,806
Evolving Roles and Workflows of Creative Practitioners in the Age of Generative AI,"Palani, Srishti and Ramos, Gonzalo","Creative practitioners (like designers, software developers, and architects) have started to employ Generative AI models (GenAI) to produce text, images, and assets comparable to those made by people. While HCI research explores specific GenAI models and creativity support tools, little is known about practitioners’ evolving roles and workflows with GenAI models across a project’s stages. This knowledge is key to guide the development of the new generation of Creativity Support Tools. We contribute to this knowledge by employing a triangulated method to capture interviews, videos, and survey responses of creative practitioners reflecting on projects they completed with GenAI. Our observations let us derive a set of factors that capture practitioners’ perceived roles, challenges, benefits, and interaction patterns when creating with GenAI. From these factors, we offer insights and propose design opportunities and priorities that serve to encourage reflection from the wider community of Creativity Support Tools and GenAI stakeholders such as systems creators, researchers, and educators on how to develop systems that meet the needs of creatives in human-centered ways.",2024,807
CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems,,,2023,808
"SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse",,,2023,809
ITiCSE-WGR '23: Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education,,"In these proceedings, we present papers from the Working Groups that worked in the context of the 28th Annual Conference on Innovation &amp; Technology in Computer Science Education (ITiCSE), held in Turku Finland, and hosted by University of Turku from the 10th to the 12th of July 2023.The concept of Working Groups has been a unique feature of the ITiCSE conference series since its inception, with CompEd adopting the Working Group practice in 2019. A Working Group typically comprises 5 to 10 researchers who work together on a project related to computing education. Working Groups provide a wonderful opportunity to work intensively on a topic of interest with an international group of computing education researchers. This unique experience is one that, in our opinion, each Computer Science Educator should strive to participate in at least once.In 2023, 13 proposals for Working Groups were received and six Working Groups were selected by the Working Group chairs to recruit members and proceed for ITiCSE 2023. There were over 100 member applications to Working Groups, with 67 being accepted across the six Working Groups.",2023,810
Normalisation of 16th and 17th century texts in French and geographical named entity recognition,"Kogkitsidou, Eleni and Gambette, Philippe","Both statistical and rule-based methods for named entity recognition are quite sensitive to the type of language used in the analysed texts. Former studies have shown for example that it was harder to detect named entities in SMS or microblog messages where words are abridged or changed to lowercase. In this article, we focus on old French texts to evaluate the impact of manual and automatic normalisation before applying five geographical named entity recognition tools, as well as an improved version of one of them, in order to help building maps displaying the locations mentioned in ancient texts. Our results show that manual normalisation leads to better results for all methods and that automatic normalisation performs differently depending on the tool used to extract geographical named entities, but with a significant improvement on most methods.",2020,811
Exploratory study of slack Q&amp;A chats as a mining source for software engineering tools,"Chatterjee, Preetha and Damevski, Kostadin and Pollock, Lori and Augustine, Vinay and Kraft, Nicholas A.","Modern software development communities are increasingly social. Popular chat platforms such as Slack host public chat communities that focus on specific development topics such as Python or Ruby-on-Rails. Conversations in these public chats often follow a Q&amp;A format, with someone seeking information and others providing answers in chat form. In this paper, we describe an exploratory study into the potential usefulness and challenges of mining developer Q&amp;A conversations for supporting software maintenance and evolution tools. We designed the study to investigate the availability of information that has been successfully mined from other developer communications, particularly Stack Overflow. We also analyze characteristics of chat conversations that might inhibit accurate automated analysis. Our results indicate the prevalence of useful information, including API mentions and code snippets with descriptions, and several hurdles that need to be overcome to automate mining that information.",2019,812
Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI,"Alam, Md Tanvirul and Bhusal, Dipkamal and Park, Youngja and Rastogi, Nidhi","Public and commercial organizations extensively share cyberthreat intelligence (CTI) to prepare systems to defend against existing and emerging cyberattacks. However, traditional CTI has primarily focused on tracking known threat indicators such as IP addresses and domain names, which may not provide long-term value in defending against evolving attacks. To address this challenge, we propose to use more robust threat intelligence signals called attack patterns. LADDER is a knowledge extraction framework that can extract text-based attack patterns from CTI reports at scale. The framework characterizes attack patterns by capturing the phases of an attack in Android and enterprise networks and systematically maps them to the MITRE ATT&amp;CK pattern framework. LADDER can be used by security analysts to determine the presence of attack vectors related to existing and emerging threats, enabling them to prepare defenses proactively. We also present several use cases to demonstrate the application of LADDER in real-world scenarios. Finally, we provide a new, open-access benchmark malware dataset to train future cyberthreat intelligence models.",2023,813
When Push Comes to Ads: Measuring the Rise of (Malicious) Push Advertising,"Subramani, Karthika and Yuan, Xingzi and Setayeshfar, Omid and Vadrevu, Phani and Lee, Kyu Hyung and Perdisci, Roberto","The rapid growth of online advertising has fueled the growth of ad-blocking software, such as new ad-blocking and privacy-oriented browsers or browser extensions. In response, both ad publishers and ad networks are constantly trying to pursue new strategies to keep up their revenues. To this end, ad networks have started to leverage the Web Push technology enabled by modern web browsers. As web push notifications (WPNs) are relatively new, their role in ad delivery has not yet been studied in depth. Furthermore, it is unclear to what extent WPN ads are being abused for malvertising (i.e., to deliver malicious ads). In this paper, we aim to fill this gap. Specifically, we propose a system called PushAdMiner that is dedicated to (1) automatically registering for and collecting a large number of web-based push notifications from publisher websites, (2) finding WPN-based ads among these notifications, and (3) discovering malicious WPN-based ad campaigns.Using PushAdMiner, we collected and analyzed 21,541 WPN messages by visiting thousands of different websites. Among these, our system identified 572 WPN ad campaigns, for a total of 5,143 WPN-based ads that were pushed by a variety of ad networks. Furthermore, we found that 51% of all WPN ads we collected are malicious, and that traditional ad-blockers and URL filters were mostly unable to block them, thus leaving a significant abuse vector unchecked.",2020,814
Attribute-aware non-linear co-embeddings of graph features,"Rashed, Ahmed and Grabocka, Josif and Schmidt-Thieme, Lars","In very sparse recommender data sets, attributes of users such as age, gender and home location and attributes of items such as, in the case of movies, genre, release year, and director can improve the recommendation accuracy, especially for users and items that have few ratings. While most recommendation models can be extended to take attributes of users and items into account, their architectures usually become more complicated. While attributes for items are often easy to be provided, attributes for users are often scarce for reasons of privacy or simply because they are not relevant to the operational process at hand. In this paper, we address these two problems for attribute-aware recommender systems by proposing a simple model that co-embeds users and items into a joint latent space in a similar way as a vanilla matrix factorization, but with non-linear latent features construction that seamlessly can ingest user or item attributes or both (GraphRec). To address the second problem, scarce attributes, the proposed model treats the user-item relation as a bipartite graph and constructs generic user and item attributes via the Laplacian of the user-item co-occurrence graph that requires no further external side information but the mere rating matrix. In experiments on three recommender datasets, we show that GraphRec significantly outperforms existing state-of-the-art attribute-aware and content-aware recommender systems even without using any side information.",2019,815
Jaal: Towards Network Intrusion Detection at ISP Scale,"Aqil, Azeem and Khalil, Karim and Atya, Ahmed O.F. and Papalexakis, Evangelos E. and Krishnamurthy, Srikanth V. and Jaeger, Trent and Ramakrishnan, K. K. and Yu, Paul and Swami, Ananthram","We have recently seen an increasing number of attacks that are distributed, and span an entire wide area network (WAN). Today, typically, intrusion detection systems (IDSs) are deployed at enterprise scale and cannot handle attacks that cover a WAN. Moreover, such IDSs are implemented at a single entity that expects to look at all packets to determine an intrusion. Transferring copies of raw packets to centralized engines for analysis in a WAN can significantly impact both network performance and detection accuracy. In this paper, we propose Jaal, a framework for achieving accurate network intrusion detection at scale. The key idea in Jaal is to monitor traffic and construct in-network packet summaries. The summaries are then processed centrally to detect attacks with high accuracy. The main challenges that we address are (a) creating summaries that are concise, but sufficient to draw highly accurate inferences and (b) transforming traditional IDS rules to handle summaries instead of raw packets. We implement Jaal on a large scale SDN testbed. We show that on average Jaal yields a detection accuracy of about 98%, which is the highest reported for ISP scale network intrusion detection. At the same time, the overhead associated with transferring summaries to the central inference engine is only about 35% of what is consumed if raw packets are transferred.",2017,816
Measuring the Effects of Gender on Online Social Conformity,"Wijenayake, Senuri and van Berkel, Niels and Kostakos, Vassilis and Goncalves, Jorge","Social conformity occurs when an individual changes their behaviour in line with the majority's expectations. Although social conformity has been investigated in small group settings, the effect of gender - of both the individual and the majority/minority - is not well understood in online settings. Here we systematically investigate the impact of groups' gender composition on social conformity in online settings. We use an online quiz in which participants submit their answers and confidence scores, both prior to and following the presentation of peer answers that are dynamically fabricated. Our results show an overall conformity rate of 39%, and a significant effect of gender that manifests in a number of ways: gender composition of the majority, the perceived nature of the question, participant gender, visual cues of the system, and final answer correctness. We conclude with a discussion on the implications of our findings in designing online group settings, accounting for the effects of gender on conformity.",2019,817
"GADTs meet their match: pattern-matching warnings that account for GADTs, guards, and laziness","Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton","For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.",2015,818
AAIA '23: Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications,,,2023,819
NivaDuck - A Scalable Pipeline to Build a Database of Political Twitter Handles for India and the United States,"Panda, Anmol and Gonawela, A’ndre and Acharyya, Sreangsu and Mishra, Dibyendu and Mohapatra, Mugdha and Chandrasekaran, Ramgopal and Pal, Joyojeet","We present a scalable methodology to identify Twitter handles of politicians in a given region and test our framework in the context of Indian and US politics. The main contribution of our work is the list of the curated Twitter handles of 18500 Indian and 8000 US politicians. Our work leveraged machine learning-based classification and human verification to build a data set of Indian politicians on Twitter. We built NivaDuck, a highly precise, two-staged classification pipeline that leverages Twitter description text and tweet content to identify politicians. For India, we tested NivaDuck’s recall using Twitter handles of the members of the Indian parliament while for the US we used state and local level politicians in California state and San Diego county respectively. We found that while NivaDuck has lower recall scores, it produces large, diverse sets of politicians with precision exceeding 90 percent for the US dataset. We discuss the need for an ML-based, scalable method to compile such a dataset and its myriad use cases for the research community and its wide-ranging utilities for research in political communication on social media.",2020,820
CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,,,2021,821
The Chameleon Attack: Manipulating Content Display in Online Social Media,"Elyashar, Aviad and Uziel, Sagi and Paradise, Abigail and Puzis, Rami","Online social networks (OSNs) are ubiquitous attracting millions of users all over the world. Being a popular communication media OSNs are exploited in a variety of cyber-attacks. In this article, we discuss the chameleon attack technique, a new type of OSN-based trickery where malicious posts and profiles change the way they are displayed to OSN users to conceal themselves before the attack or avoid detection. Using this technique, adversaries can, for example, avoid censorship by concealing true content when it is about to be inspected; acquire social capital to promote new content while piggybacking a trending one; cause embarrassment and serious reputation damage by tricking a victim to like, retweet, or comment a message that he wouldn’t normally do without any indication for the trickery within the OSN. An experiment performed with closed Facebook groups of sports fans shows that (1) chameleon pages can pass by the moderation filters by changing the way their posts are displayed and (2) moderators do not distinguish between regular and chameleon pages. We list the OSN weaknesses that facilitate the chameleon attack and propose a set of mitigation guidelines.",2020,822
ICER '23: Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1,,,2023,823
Qualifying software engineers undergraduates in DevOps - challenges of introducing technical and non-technical concepts in a project-oriented course,"Alves, Isaque and Rocha, Carla","The constant changes in the software industry, practices, and methodologies impose challenges to teaching and learning current software engineering concepts and skills. DevOps is particularly challenging because it covers technical concepts, such as pipeline automation, and non-technical ones, such as team roles and project management. The present study investigates a course setup to introduce these concepts to software engineering undergraduates. We designed the course by employing coding to associate DevOps concepts to Agile, Lean, and Open source practices and tools. We present the main aspects of this project-oriented DevOps course, with 240 students enrolled it since its first offering in 2016. We conducted an empirical study, with both a quantitative and qualitative analysis, to evaluate this project-oriented course setup. We collected the data from the projects repository and students' perceptions from a questionnaire. We mined 148 repositories (corresponding to 72 projects) and obtained 86 valid responses to the questionnaire. We also mapped the concepts which are more challenging to students learn from experience. The results evidence that first-hand experience facilitates the comprehension of DevOps concepts and enriches classes discussions. we present a set of lessons learned, which may help professors better design and conduct project-oriented courses to cover DevOps concepts.",2021,824
Deep Learning in Sentiment Analysis: Recent Architectures,"Abdullah, Tariq and Ahmet, Ahmed","Humans are increasingly integrated with devices that enable the collection of vast unstructured opinionated data. Accurately analysing subjective information from this data is the task of sentiment analysis (an actively researched area in NLP). Deep learning provides a diverse selection of architectures to model sentiment analysis tasks and has surpassed other machine learning methods as the foremast approach for performing sentiment analysis tasks. Recent developments in deep learning architectures represent a shift away from Recurrent and Convolutional neural networks and the increasing adoption of Transformer language models. Utilising pre-trained Transformer language models to transfer knowledge to downstream tasks has been a breakthrough in NLP.This survey applies a task-oriented taxonomy to recent trends in architectures with a focus on the theory, design and implementation. To the best of our knowledge, this is the only survey to cover state-of-the-art Transformer-based language models and their performance on the most widely used benchmark datasets. This survey paper provides a discussion of the open challenges in NLP and sentiment analysis. The survey covers five years from 1st July 2017 to 1st July 2022.",2022,825
Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology,,,2022,826
FOSSIL: A Resilient and Efficient System for Identifying FOSS Functions in Malware Binaries,"Alrabaee, Saed and Shirani, Paria and Wang, Lingyu and Debbabi, Mourad","Identifying free open-source software (FOSS) packages on binaries when the source code is unavailable is important for many security applications, such as malware detection, software infringement, and digital forensics. This capability enhances both the accuracy and the efficiency of reverse engineering tasks by avoiding false correlations between irrelevant code bases. Although the FOSS package identification problem belongs to the field of software engineering, conventional approaches rely strongly on practical methods in data mining and database searching. However, various challenges in the use of these methods prevent existing function identification approaches from being effective in the absence of source code. To make matters worse, the introduction of obfuscation techniques, the use of different compilers and compilation settings, and software refactoring techniques has made the automated detection of FOSS packages increasingly difficult. With very few exceptions, the existing systems are not resilient to such techniques, and the exceptions are not sufficiently efficient.To address this issue, we propose FOSSIL, a novel resilient and efficient system that incorporates three components. The first component extracts the syntactical features of functions by considering opcode frequencies and applying a hidden Markov model statistical test. The second component applies a neighborhood hash graph kernel to random walks derived from control-flow graphs, with the goal of extracting the semantics of the functions. The third component applies z-score to the normalized instructions to extract the behavior of instructions in a function. The components are integrated using a Bayesian network model, which synthesizes the results to determine the FOSS function. The novel approach of combining these components using the Bayesian network has produced stronger resilience to code obfuscation.We evaluate our system on three datasets, including real-world projects whose use of FOSS packages is known, malware binaries for which there are security and reverse engineering reports purporting to describe their use of FOSS, and a large repository of malware binaries. We demonstrate that our system is able to identify FOSS packages in real-world projects with a mean precision of 0.95 and with a mean recall of 0.85. Furthermore, FOSSIL is able to discover FOSS packages in malware binaries that match those listed in security and reverse engineering reports. Our results show that modern malware binaries contain 0.10--0.45 of FOSS packages.",2018,827
One Size Does Not Fit All: A Longitudinal Analysis of Brazilian Financial Malware,"Botacin, Marcus and Aghakhani, Hojjat and Ortolani, Stefano and Kruegel, Christopher and Vigna, Giovanni and Oliveira, Daniela and Geus, Paulo L\'{\i}cio De and Gr\'{e}gio, Andr\'{e}","Malware analysis is an essential task to understand infection campaigns, the behavior of malicious codes, and possible ways to mitigate threats. Malware analysis also allows better assessment of attackers’ capabilities, techniques, and processes. Although a substantial amount of previous work provided a comprehensive analysis of the international malware ecosystem, research on regionalized, country-, and population-specific malware campaigns have been scarce. Moving towards addressing this gap, we conducted a longitudinal (2012-2020) and comprehensive (encompassing an entire population of online banking users) study of MS Windows desktop malware that actually infected Brazilian banks’ users. We found that the Brazilian financial desktop malware has been evolving quickly: it started to make use of a variety of file formats instead of typical PE binaries, relied on native system resources, and abused obfuscation techniques to bypass detection mechanisms. Our study on the threats targeting a significant population on the ecosystem of the largest and most populous country in Latin America can provide invaluable insights that may be applied to other countries’ user populations, especially those in the developing world that might face cultural peculiarities similar to Brazil’s. With this evaluation, we expect to motivate the security community/industry to seriously consider a deeper level of customization during the development of next-generation anti-malware solutions, as well as to raise awareness towards regionalized and targeted Internet threats.",2021,828
SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing,,,2023,829
Testing RESTful APIs: A Survey,"Golmohammadi, Amid and Zhang, Man and Arcuri, Andrea","In industry, RESTful APIs are widely used to build modern Cloud Applications. Testing them is challenging, because not only do they rely on network communications, but also they deal with external services like databases. Therefore, there has been a large amount of research sprout in recent years on how to automatically verify this kind of web services. In this article, we present a comprehensive review of the current state-of-the-art in testing RESTful APIs based on the analysis of 92 scientific articles. These articles were gathered by utilizing search queries formulated around the concept of RESTful API testing on seven popular databases. We eliminated irrelevant articles based on our predefined criteria and conducted a snowballing phase to minimize the possibility of missing any relevant paper. This survey categorizes and summarizes the existing scientific work on testing RESTful APIs and discusses the current challenges in the verification of RESTful APIs. This survey clearly shows an increasing interest among researchers in this field, from 2017 onward. However, there are still a lot of open research challenges to overcome.",2023,830
CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,,,2023,831
Interacci\'{o}n '23: Proceedings of the XXIII International Conference on Human Computer Interaction,,,2023,832
ABScribe: Rapid Exploration &amp; Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models,"Reza, Mohi and Laundry, Nathan M and Musabirov, Ilya and Dushniku, Peter and Yu, Zhi Yuan “Michael” and Mittal, Kashish and Grossman, Tovi and Liut, Michael and Kuzminykh, Anastasia and Williams, Joseph Jay","Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art Large Language Models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new variations without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers’ flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration and organization of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly modify variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text fields for rapid in-place comparisons using mouse-over interactions on a popup toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions of the revision process (d = 2.41, p &lt; 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.",2024,833
MG-ShopDial: A Multi-Goal Conversational Dataset for e-Commerce,"Bernard, Nolwenn and Balog, Krisztian","Conversational systems can be particularly effective in supporting complex information seeking scenarios with evolving information needs. Finding the right products on an e-commerce platform is one such scenario, where a conversational agent would need to be able to provide search capabilities over the item catalog, understand and make recommendations based on the user's preferences, and answer a range of questions related to items and their usage. Yet, existing conversational datasets do not fully support the idea of mixing different conversational goals (i.e., search, recommendation, and question answering) and instead focus on a single goal. To address this, we introduce MG-ShopDial: a dataset of conversations mixing different goals in the domain of e-commerce. Specifically, we make the following contributions. First, we develop a coached human-human data collection protocol where each dialogue participant is given a set of instructions, instead of a specific script or answers to choose from. Second, we implement a data collection tool to facilitate the collection of multi-goal conversations via a web chat interface, using the above protocol. Third, we create the MG-ShopDial collection, which contains 64 high-quality dialogues with a total of 2,196 utterances for e-commerce scenarios of varying complexity. The dataset is additionally annotated with both intents and goals on the utterance level. Finally, we present an analysis of this dataset and identify multi-goal conversational patterns.",2023,834
Pervasive Chatbots: Investigating Chatbot Interventions for Multi-Device Applications,"Olapade, Mayowa and Hasanli, Tarlan and Ottun, Abdul-Rasheed and Akintola, Adeyinka and Liyanage, Mohan and Flores, Huber","The inherent social characteristics of humans make them prone to adopting distributed and collaborative applications easily. Although fundamental methods and technologies have been defined and developed over the years to construct these applications, their adoption in practice is uncommon because end-users may be puzzled about how to use them without much hassle. Indeed, commonly, these applications require a certain level of technical expertise and awareness to use them correctly. Fortunately, AI-chatbot interventions are envisioned to assist and support various human tasks. In this paper, we contribute pervasive chatbots as a solution that fosters a more transparent and user-friendly interconnection of devices in distributed and collaborative environments. Through two rigorous user studies, firstly, we quantify the perception of users toward distributed and collaborative applications (N = 56 participants). Secondly, we analyze the benefits of adopting pervasive chatbots when compared with the chatbot reference model designed for assistance and recommendations (N = 24 participants). Our results suggest that pervasive chatbots can significantly enhance the practicability of distributed and collaborative applications, reducing the time and effort needed for collaboration with surrounding devices by 57%. With this information, we then provide design and development implications to integrate pervasive chatbot interventions in distributed and collaborative environments. Moreover, challenges and opportunities are also provided to highlight the remaining issues that need to be addressed to realize the full vision of pervasive chatbots for any multi-device application. Our work paves the way towards the proliferation of sophisticated and highly decentralized computing environments that are easily interconnected.",2024,835
A Survey of Figurative Language and Its Computational Detection in Online Social Networks,"Abulaish, Muhammad and Kamal, Ashraf and Zaki, Mohammed J.","The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category.",2020,836
ICCMB '24: Proceedings of the 2024 7th International Conference on Computers in Management and Business,,,2024,837
MAD '24: Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation,,,2024,838
Testing Multi-Subroutine Quantum Programs: From Unit Testing to Integration Testing,"Long, Peixun and Zhao, Jianjun","Quantum computing has emerged as a promising field with the potential to revolutionize various domains by harnessing the principles of quantum mechanics. As quantum hardware and algorithms continue to advance, developing high-quality quantum software has become crucial. However, testing quantum programs poses unique challenges due to the distinctive characteristics of quantum systems and the complexity of multi-subroutine programs. This article addresses the specific testing requirements of multi-subroutine quantum programs. We begin by investigating critical properties by surveying existing quantum libraries and providing insights into the challenges of testing these programs. Building upon this understanding, we focus on testing criteria and techniques based on the whole testing process perspective, spanning from unit testing to integration testing. We delve into various aspects, including IO analysis, quantum relation checking, structural testing, behavior testing, integration of subroutine pairs, and test case generation. We also introduce novel testing principles and criteria to guide the testing process. We conduct comprehensive testing on typical quantum subroutines, including diverse mutants and randomized inputs, to evaluate our proposed approach. The analysis of failures provides valuable insights into the effectiveness of our testing methodology. Additionally, we present case studies on representative multi-subroutine quantum programs, demonstrating the practical application and effectiveness of our proposed testing principles and criteria.",2024,839
FTSCS 2022: Proceedings of the 8th ACM SIGPLAN International Workshop on Formal Techniques for Safety-Critical Systems,,"This volume contains the proceedings of the Eighth ACM SIGPLAN International Workshop on Formal Techniques for Safety-Critical Systems (FTSCS 2022), held in Auckland, New Zealand, on December 7, 2022, as a satellite event of SPLASH 2022: The ACM SIGPLAN Conference on Systems, Programming, Languages, and Applications: Software for Humanity.  

The aim of this workshop is to bring together researchers and engineers who are interested in the application of formal and semi-formal methods to improve the quality of safety-critical computer systems. FTSCS strives to promote research and development of formal methods and tools for industrial applications, and is particularly interested in industrial applications of formal methods.",2022,840
What’s the Norm Around Here? Individuals’ Responses Can Mitigate the Effects of Misinformation Prevalence in Shaping Perceptions of a Community,"Aghajari, Zhila and Baumer, Eric P. S. and DiFranzo, Dominic","Social norms play a significant role in how conspiratorial content and related misinformation impact online communities. However, less is understood about the mechanisms by which particular aspects of a community may drive perceptions of social norms in the community. Using anti-vaccine conspiracies as a testbed, this paper experimentally examines three such features and their relationships : prevalence of conspiratorial content, community response, and explicit community rules. Results show that prevalence of content has a significant effect on norm perceptions, while the results did not support the effects of explicit rule on norm perceptions. However, these effects can be mitigated by the way a community responds to such content. Furthermore, perceived norms also influence other expectations about the community, from escalated behaviors to belief in other conspiracy theories. The paper concludes by highlighting the implications of these findings for online platform design, for community governance, and for future research about the relationships among conspiratorial content and norm perceptions.",2023,841
"Network Hygiene, Incentives, and Regulation: Deployment of Source Address Validation in the Internet","Luckie, Matthew and Beverly, Robert and Koga, Ryan and Keys, Ken and Kroll, Joshua A. and claffy, k","The Spoofer project has collected data on the deployment and characteristics of IP source address validation on the Internet since 2005. Data from the project comes from participants who install an active probing client that runs in the background. The client automatically runs tests both periodically and when it detects a new network attachment point. We analyze the rich dataset of Spoofer tests in multiple dimensions: across time, networks, autonomous systems, countries, and by Internet protocol version. In our data for the year ending August 2019, at least a quarter of tested ASes did not filter packets with spoofed source addresses leaving their networks. We show that routers performing Network Address Translation do not always filter spoofed packets, as 6.4% of IPv4/24 tested in the year ending August 2019 did not filter. Worse, at least two thirds of tested ASes did not filter packets entering their networks with source addresses claiming to be from within their network that arrived from outside their network. We explore several approaches to encouraging remediation and the challenges of evaluating their impact. While we have been able to remediate 352 IPv4/24, we have found an order of magnitude more IPv4/24 that remains unremediated, despite myriad remediation strategies, with 21% unremediated for more than six months. Our analysis provides the most complete and confident picture of the Internet's susceptibility to date of this long-standing vulnerability. Although there is no simple solution to address the remaining long-tail of unremediated networks, we conclude with a discussion of possible non-technical interventions, and demonstrate how the platform can support evaluation of the impact of such interventions over time.",2019,842
"RAID '23: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses",,,2023,843
Modelling Knowledge about Software Processes using Provenance Graphs and its Application to Git-based Version Control Systems,"Schreiber, Andreas and de Boer, Claas","Using the W3C PROV data model, we present a general provenance model for software development processes and---as an example---specialized models for git services, for which we generate provenance graphs. Provenance graphs are knowledge graphs, since they have defined semantics, and can be analyzed with graph algorithms or semantic reasoning to get insights into processes.",2020,844
Seven years in the life of Hypergiants' off-nets,"Gigis, Petros and Calder, Matt and Manassakis, Lefteris and Nomikos, George and Kotronis, Vasileios and Dimitropoulos, Xenofontas and Katz-Bassett, Ethan and Smaragdakis, Georgios","Content Hypergiants deliver the vast majority of Internet traffic to end users. In recent years, some have invested heavily in deploying services and servers inside end-user networks. With several dozen Hypergiants and thousands of servers deployed inside networks, these off-net (meaning outside the Hypergiant networks) deployments change the structure of the Internet. Previous efforts to study them have relied on proprietary data or specialized per-Hypergiant measurement techniques that neither scale nor generalize, providing a limited view of content delivery on today's Internet.In this paper, we develop a generic and easy to implement methodology to measure the expansion of Hypergiants' off-nets. Our key observation is that Hypergiants increasingly encrypt their traffic to protect their customers' privacy. Thus, we can analyze publicly available Internet-wide scans of port 443 and retrieve TLS certificates to discover which IP addresses host Hypergiant certificates in order to infer the networks hosting off-nets for the corresponding Hypergiants. Our results show that the number of networks hosting Hypergiant off-nets has tripled from 2013 to 2021, reaching 4.5k networks. The largest Hypergiants dominate these deployments, with almost all of these networks hosting an off-net for at least one -- and increasingly two or more -- of Google, Netflix, Facebook, or Akamai. These four Hypergiants have off-nets within networks that provide access to a significant fraction of end user population.",2021,845
LADC '22: Proceedings of the 11th Latin-American Symposium on Dependable Computing,,,2022,846
AIMLSystems '23: Proceedings of the Third International Conference on AI-ML Systems,,,2023,847
Are We Equal Online?: An Investigation of Gendered Language Patterns and Message Engagement on Enterprise Communication Platforms,"Ferguson, Sharon A. and Olechowski, Alison","It was previously hypothesized that gender differences -- and thus gender discrimination -- would disappear if communication was no longer in person, and instead was transmitted and received in the same format for all. Yet, even online, researchers have identified gendered language styles in written communication that reveal gender cues and can lead to unequal treatment. In this work, we revisit these past findings and ask whether the same gendered patterns can be found on modern communication platforms, which present a new set of engagement features and mixed synchronous capabilities. We quantitatively analyze 335,000 Slack messages sent by 845 individuals as part of 46 teams, collected over six years of a product design capstone course. We found little evidence of traditionally gendered communication styles (characterized as elaborate, uncertain, and supportive) from the minority-gender participants. We did identify relationships between message author gender, communication style, and message engagement --- women and minority genders were more likely to have their messages engaged with, but only when using certain communication styles --- suggesting complex power dynamics exist on these platforms. We contribute the first study of gendered language styles on Enterprise Communication Platforms, adding to the community's understanding of how new settings and emerging technology relate to team collaborative dynamics, and motivating future tool development to support collaboration in diverse teams.",2023,848
On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study,"Yin, Likang and Zhang, Xiyu and Filkov, Vladimir","Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability?From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.",2023,849
EICC '23: Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference,,,2023,850
Spears Against Shields: Are Defenders Winning the Phishing War?,"El Aassal, Ayman and Verma, Rakesh","Phishing is the act of using deceptive methods to lure users into taking harmful and dangerous actions against themselves and/or the company they work for. Hackers have been using this method to bypass various security systems, steal personal information, make sensitive data public, and all of this while raking millions of dollars. On the attack side, hackers are using phishing kits and employ all possible phishing techniques to build a successful campaign. On the defense side, we find spam/phishing filters and malicious website detection in addition to anti-phishing simulation training. In this study, we assess the current phishing landscape by testing the tools used on both sides of the war. We generated 1,000 phishing emails containing phishing links semi-automatically using natural language generation technology and tested five popular tools with anti-phishing modules. The number of undetected emails ranged from 77 to 927. We also evaluate several anti-phishing training technologies and reveal their shortcomings. Our results suggest that both anti-phishing filters and current training tools have a long way to go and thus improving these defense mechanisms is still essential.",2019,851
TrustBoost: Boosting Trust among Interoperable Blockchains,"Sheng, Peiyao and Wang, Xuechao and Kannan, Sreeram and Nayak, Kartik and Viswanath, Pramod","Currently there exist many blockchains with weak trust guarantees, limiting applications and participation. Existing solutions to boost the trust using a stronger blockchain, e.g., via checkpointing, requires the weaker blockchain to give up sovereignty. In this paper, we propose a family of protocols in which multiple blockchains interact to create a combined ledger with boosted trust. We show that even if several of the interacting blockchains cease to provide security guarantees, the combined ledger continues to be secure - our Trustboost protocols achieve the optimal threshold of tolerating the insecure blockchains. This optimality, along with the necessity of blockchain interactions, is formally shown within the classic shared memory model, tackling the long standing open challenge of solving consensus in the presence of both Byzantine objects and processes. Furthermore, our proposed construction of Trustboost simply operates via smart contracts and require no change to the underlying consensus protocols of the participating blockchains, a form of ""consensus on top of consensus''. The protocols are lightweight and can be used on specific (e.g., high value) transactions; we demonstrate the practicality by implementing and deploying Trustboost as cross-chain smart contracts in the Cosmos ecosystem using approximately 3,000 lines of Rust code, made available as open source [52]. Our evaluation shows that using 10 Cosmos chains in a local testnet, Trustboost has a gas cost of roughly $2 with a latency of 2 minutes per request, which is in line with the cost on a high security chain such as Bitcoin or Ethereum.",2023,852
Browser Fingerprinting: A Survey,"Laperdrix, Pierre and Bielova, Nataliia and Baudry, Benoit and Avoine, Gildas","With this article, we survey the research performed in the domain of browser fingerprinting, while providing an accessible entry point to newcomers in the field. We explain how this technique works and where it stems from. We analyze the related work in detail to understand the composition of modern fingerprints and see how this technique is currently used online. We systematize existing defense solutions into different categories and detail the current challenges yet to overcome.",2020,853
SACMAT 2024: Proceedings of the 29th ACM Symposium on Access Control Models and Technologies,,"It is our great pleasure to welcome you to the 29th ACM Symposium on Access Control Models and Technologies (SACMAT 2024). This year's symposium continues its tradition of being the premier venue for presenting research results and experience reports on cutting edge advances on access control, including models, systems, applications, and theory, while also embracing an expanded focus on the general area of computer and information security and privacy. The overarching goal of the symposium is to share novel access control and computer security solutions that fulfill the needs of emerging applications and environments, and also to identify new directions for future research and development. ACM SACMAT provides researchers and also practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and computer security.",2024,854
SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies,,"It is our great pleasure to welcome you to the ACM Symposium on Access Control Models and Technologies (SACMAT 2021). This year's symposium continues its tradition of being the premier forum for the presentation of research results and experience reports on leading-edge issues of access control, including models, systems, applications, and theory, while also embracing a renovated focus on the general area of security.The aim of the symposium is to share novel access control and security solutions that fulfill the needs of heterogeneous applications and environments, and to identify new directions for future research and development.SACMAT provides researchers and practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and security.",2021,855
K-CAP '23: Proceedings of the 12th Knowledge Capture Conference 2023,,"It is our great pleasure to welcome you to the 12th ACM International Conference on Knowledge Capture: K-CAP 2023, held in person on December 5th - 7th in Pensacola, Florida, US.Driven by the increasing demands for knowledge-based applications and the unprecedented availability of information from heterogeneous data sources, the study of knowledge capture is of crucial importance. Knowledge capture involves the extraction of useful knowledge from vast and diverse data sources as well as its acquisition directly from human experts.Nowadays knowledge is derived from an increasingly diverse set of data resources that differ with regard to their domain, format, quality, coverage, specificity, viewpoint, bias, and most importantly, consumers and producers of data. The heterogeneity, amount and complexity of data allow us to answer complex questions that could not be answered in isolation, requiring the interaction of different scientific fields and technologies. A goal of K-CAP is to develop such synergies using systematic and rigorous methodologies.The call for papers attracted 105 submissions from all over the world, covering a diverse range of topics spanning knowledge mining, large language models for information extraction, neuro-symbolic approaches for knowledge capture, knowledge engineering, question-answering, knowledge graphs, natural language processing, reasoning, entity linking, querying and knowledge-based applications. From a competitive set of high-quality submissions, we accepted 27 long research papers, 5 short papers, and 1 vision paper. The high-quality program is divided into 7 research sessions, in addition to 3 tutorials reflecting novel topics of interest in Knowledge Capture.We encourage everyone to attend the keynote talks that we have planned for K-CAP 2023. The highly anticipated talks by Dr. Robert R. Hoffman (Florida Institute for Human and Machine Cognition) and Dr. Jane Pinelis (Johns Hopkins University Applied Physics Laboratory) will guide us to a better understanding of the future of knowledge capture and explainable, resilient AI ecosystems, as they become commonplace in real world applications.",2023,856
ICSE-SEIS'24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society,,,2024,857
Crowd-Machine Collaboration for Item Screening,"Krivosheev, Evgeny and Harandizadeh, Bahareh and Casati, Fabio and Benatallah, Boualem","In this paper we describe how crowd and machine classifier can be efficiently combined to screen items that satisfy a set of predicates. We show that this is a recurring problem in many domains, present machine-human (hybrid) algorithms that screen items efficiently and estimate the gain over human-only or machine-only screening in terms of performance and cost.",2018,858
IUI '24 Companion: Companion Proceedings of the 29th International Conference on Intelligent User Interfaces,,,2024,859
The ad wars: retrospective measurement and analysis of anti-adblock filter lists,"Iqbal, Umar and Shafiq, Zubair and Qian, Zhiyun","The increasing popularity of adblockers has prompted online publishers to retaliate against adblock users by deploying anti-adblock scripts, which detect adblock users and bar them from accessing content unless they disable their adblocker. To circumvent anti-adblockers, adblockers rely on manually curated anti-adblock filter lists for removing anti-adblock scripts. Anti-adblock filter lists currently rely on informal crowdsourced feedback from users to add/remove filter list rules. In this paper, we present the first comprehensive study of anti-adblock filter lists to analyze their effectiveness against anti-adblockers. Specifically, we compare and contrast the evolution of two popular anti-adblock filter lists. We show that these filter lists are implemented very differently even though they currently have a comparable number of filter list rules. We then use the Internet Archive's Wayback Machine to conduct a retrospective coverage analysis of these filter lists on Alexa top-5K websites over the span of last five years. We find that the coverage of these filter lists has considerably improved since 2014 and they detect anti-adblockers on about 9% of Alexa top-5K websites. To improve filter list coverage and speedup addition of new filter rules, we also design and implement a machine learning based method to automatically detect anti-adblock scripts using static JavaScript code analysis.",2017,860
Security Analysis of the Estonian Internet Voting System,"Springall, Drew and Finkenauer, Travis and Durumeric, Zakir and Kitcat, Jason and Hursti, Harri and MacAlpine, Margaret and Halderman, J. Alex","Estonia was the first country in the world to use Internet voting nationally, and today more than 30% of its ballots are cast online. In this paper, we analyze the security of the Estonian I-voting system based on a combination of in-person election observation, code review, and adversarial testing. Adopting a threat model that considers the advanced threats faced by a national election system---including dishonest insiders and state-sponsored attacks---we find that the I-voting system has serious architectural limitations and procedural gaps that potentially jeopardize the integrity of elections. In experimental attacks on a reproduction of the system, we demonstrate how such attackers could target the election servers or voters' clients to alter election results or undermine the legitimacy of the system. Our findings illustrate the practical obstacles to Internet voting in the modern world, and they carry lessons for Estonia, for other countries considering adopting such systems, and for the security research community.",2014,861
The Impact of Displaying Diversity Information on the Formation of Self-assembling Teams,"G\'{o}mez-Zar\'{a}, Diego and Guo, Mengzi and DeChurch, Leslie A. and Contractor, Noshir","Despite the benefits of team diversity, individuals often choose to work with similar others. Online team formation systems have the potential to help people assemble diverse teams. Systems can connect people to collaborators outside their networks, and features can quantify and raise the salience of diversity to users as they search for prospective teammates. But if we build a feature indicating diversity into the tool, how will people react to it? Two experiments manipulating the presence or absence of a ""diversity score"" feature within a teammate recommender demonstrate that, when present, individuals avoid collaborators who would increase team diversity in favor of those who lower team diversity. These results have important practical implications. Though the increased access to diverse teammates provided by recommender systems may benefit diversity, designers are cautioned against creating features that raise the salience of diversity as this information may undermine diversity.",2020,862
Detecting Covert Disruptive Behavior in Online Interaction by Analyzing Conversational Features and Norm Violations,"Paakki, Henna and Veps\""{a}l\""{a}inen, Heidi and Salovaara, Antti and Zafar, Bushra","Disruptive behavior is a prevalent threat to constructive online engagement. Covert behaviors, such as trolling, are especially challenging to detect automatically, because they utilize deceptive strategies to manipulate conversation. We illustrate a novel approach to their detection: analyzing conversational structures instead of focusing only on messages in isolation. Building on conversation analysis, we demonstrate that (1) conversational actions and their norms provide concepts for a deeper understanding of covert disruption, and that (2) machine learning, natural language processing and structural analysis of conversation can complement message-level features to create models that surpass earlier approaches to trolling detection. Our models, developed for detecting overt (aggression) as well as covert (trolling) behaviors using prior studies’ message-level features and new conversational action features, achieved high accuracies (0.90 and 0.92, respectively). The findings offer a theoretically grounded approach to computationally analyzing social media interaction and novel methods for effectively detecting covert disruptive conversations online.",2024,863
CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems,,,2021,864
“Finding the Magic Sauce”: Exploring Perspectives of Recruiters and Job Seekers on Recruitment Bias and Automated Tools,"Lashkari, Mitra and Cheng, Jinghui","Automated recruitment tools are proliferating. While having the promise of improving efficiency, various risks, including bias, challenges the potential of these tools. An in-depth understanding of the perceived risk factors and needs from the perspective of both recruiters and job seekers is needed. We address this through an interview study in the high-tech industry to compare and contrast the concerns of these two roles. We found that the importance of clarifying position requirements and assessing candidates as “whole individuals” are commonly discussed by both recruiters and job seekers. In contrast, while recruiters tended to be more aware of cognitive bias and desired more tool support during interviews, job seekers voiced more desire towards a healthy candidate-company relationship. Additionally, both roles considered the uncertainty of the current technology capability and reduced human contact as concerns for using automated tools. Based on these results, we provided design implications for automated recruitment tools and related decision-support technologies.",2023,865
Predicting Member Productivity and Withdrawal from Pre-Joining Attachments in Online Production Groups,"Yu, Bowen and Ren, Yuqing and Terveen, Loren and Zhu, Haiyi","Productive and dedicated members are critical to the success of online production communities like Wikipedia. Many communities organize in subgroups where members voluntarily work on projects of shared interest. In this paper, we investigate how members' pre-joining connections with the subgroup predict their productivity and withdrawal after joining. Drawing insights from attachment theories in social psychology, we examine two types of pre-joining connections: textit{identity-based} attachment (how much members' interests were aligned with the subgroup's topics) and textit{bonds-based} attachment (how much members had interacted with other members of the subgroup). Analyses of 79,704 editors in 1,341 WikiProjects show that 1) both identity-based and bonds-based attachment increased editors' post-joining productivity and reduced their likelihood of withdrawal; 2) identity-based attachment had a stronger effect on boosting direct contributions to articles while bonds-based attachment had a stronger effect on increasing article and project coordination, and reducing member withdrawal.",2017,866
OpenSym '22: Proceedings of the 18th International Symposium on Open Collaboration,,,2022,867
PPDP '22: Proceedings of the 24th International Symposium on Principles and Practice of Declarative Programming,,,2022,868
Is historical data an appropriate benchmark for reviewer recommendation systems? a case study of the gerrit community,"Gauthier, Ian X. and Lamothe, Maxime and Mussbacher, Gunter and McIntosh, Shane","Reviewer recommendation systems are used to suggest community members to review change requests. Like several other recommendation systems, it is customary to evaluate recommendations using held out historical data. While history-based evaluation makes pragmatic use of available data, historical records may be: (1) overly optimistic, since past assignees may have been suboptimal choices for the task at hand; or (2) overly pessimistic, since ""incorrect"" recommendations may have been equal (or even better) choices.In this paper, we empirically evaluate the extent to which historical data is an appropriate benchmark for reviewer recommendation systems. We replicate the cHRev and WLRRec approaches and apply them to 9,679 reviews from the Gerrit open source community. We then assess the recommendations with members of the Gerrit reviewing community using quantitative methods (personalized questionnaires about their comfort level with tasks) and qualitative methods (semi-structured interviews).We find that history-based evaluation is far more pessimistic than optimistic in the context of Gerrit review recommendations. Indeed, while 86% of those who had been assigned to a review in the past felt comfortable handling the review, 74% of those labelled as incorrect recommendations also felt that they would have been comfortable reviewing the changes. This indicates that, on the one hand, when reviewer recommendation systems recommend the past assignee, they should indeed be considered correct. Yet, on the other hand, recommendations labelled as incorrect because they do not match the past assignee may have been correct as well.Our results suggest that current reviewer recommendation evaluations do not always model the reality of software development. Future studies may benefit from looking beyond repository data to gain a clearer understanding of the practical value of proposed recommendations.",2022,869
Analyzing Organizational Routines in Online Knowledge Collaborations: A Case for Sequence Analysis in CSCW,"Keegan, Brian C. and Lev, Shakked and Arazy, Ofer","Research into socio-technical systems like Wikipedia has overlooked important structural patterns in the coordination of distributed work. This paper argues for a conceptual reorientation towards sequences as a fundamental unit of analysis for understanding work routines in online knowledge collaboration. We outline a research agenda for researchers in computer-supported cooperative work (CSCW) to understand the relationships, patterns, antecedents, and consequences of sequential behavior using methods already developed in fields like bio-informatics. Using a data set of 37,515 revisions from 16,616 unique editors to 96 Wikipedia articles as a case study, we analyze the prevalence and significance of different sequences of editing patterns. We illustrate the mixed method potential of sequence approaches by interpreting the frequent patterns as general classes of behavioral motifs. We conclude by discussing the methodological opportunities for using sequence analysis for expanding existing approaches to analyzing and theorizing about co-production routines in online knowledge collaboration.",2016,870
ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference,,,2024,871
Research on DDoS Attacks in IPv6,"Zhong, Jie and Chen, Xiangning","With the gradual replacement of IPv4 by IPv6, Distributed Denial-of-Service (DDoS) attacks that have plagued IPv4 appear in IPv6 more or less, and affect the normal operation of IPv6. However, the current research on how to construct a secure DDoS attack defense system under ipv6 is far away from satisfactory. This paper makes a detailed analysis of DDoS attacks from the aspect of causes, specific performance, architecture, classifies DDoS attacks according to the exploited vulnerabilities and analyzes the typical attack methods respectively. At the same time, the improvements and defects of IPv6 are further studied, along with the way to defend, summarizes how to defend against DDoS attacks in IPv6 from two aspects, and analyze the advantages and disadvantages of each scheme. Finally, how to build a more secure network security defense system in IPv6 is prospected.",2020,872
How statically-typed functional programmers write code,"Lubin, Justin and Chasins, Sarah E.","How working statically-typed functional programmers write code is largely understudied. And yet, a better understanding of developer practices could pave the way for the design of more useful and usable tooling, more ergonomic languages, and more effective on-ramps into programming communities. The goal of this work is to address this knowledge gap: to better understand the high-level authoring patterns that statically-typed functional programmers employ. We conducted a grounded theory analysis of 30 programming sessions of practicing statically-typed functional programmers, 15 of which also included a semi-structured interview. The theory we developed gives insight into how the specific affordances of statically-typed functional programming affect domain modeling, type construction, focusing techniques, exploratory and reasoning strategies, and expressions of intent. We conducted a set of quantitative lab experiments to validate our findings, including that statically-typed functional programmers often iterate between editing types and expressions, that they often run their compiler on code even when they know it will not successfully compile, and that they make textual program edits that reliably signal future edits that they intend to make. Lastly, we outline the implications of our findings for language and tool design. The success of this approach in revealing program authorship patterns suggests that the same methodology could be used to study other understudied programmer populations.",2021,873
"The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions",,"The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.",2019,874
Reusing software engineering knowledge from developer communication,"Costa Silva, Camila Mariane","Software development requires many different types of knowledge, such as knowledge about software development processes, practices and techniques, and about the domain of an application. Software, developers often share knowledge in informal communication channels (e.g., instant messaging tools, e-mails, or online forums). Considering that this informal communication contains knowledge that may be potentially relevant for other developers and given that this knowledge is not necessarily captured and formally documented for reuse, in this work we propose (a) exploring whether developer communication (via instant messaging) is a suitable source of reusable software engineering knowledge; (b) investigating how to identify that knowledge using data mining; (c) and analysing through action research how to present it to developers in a useful way for reuse. The envisioned theories and solutions approaches will analyze existing software development data captured in communication, rather than data that were captured and stored specifically to be reused.",2020,875
A Time-Aware Exploration of RecSys15 Challenge Dataset,"Pamp\'{\i}n, Humberto Jes\'{u}s Corona and Peleteiro, Ana","E-commerce is currently one of the main applications of recommender systems, since it generates vast amounts of data that can be used to make predictions and analyse users's behaviour. In this paper we present an overview of the public dataset used for the RecSys Challenge 2015. We describe the basic statistical properties of this dataset and how events (clicks and purchases) are distributed over products (items) and users (sessions). We also present a time-aware analysis of the dataset, with the aim to better understand the change of user behaviour within time cycles, and how it affects the activity in user purchases. We further study the relation between categories, the solely type of metadata present in this completely anonymised dataset. We are interested both in how these categories are distributed and how users and items interact with them. Finally, along the paper we explain the implications that the results obtained from our analysis may have when building models for the challenge.",2016,876
"SC-W '23: Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis",,,2023,877
Websci Companion '24: Companion Publication of the 16th ACM Web Science Conference,,,2024,878
"SPLASH Companion 2022: Companion Proceedings of the 2022 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity",,"Welcome to the SPLASH 2022! After two years of virtual only (SPLASH 2020), closed borders USA only (SPLASH 2021), we finally feel the reopening and going back to the pre-Covid in person vibe of the 37th OOPSLA/SPLASH. I am especially proud of having SPLASH outside of the USA/Canada region for the 3rd time in its history and the first time it is held in the Asia Pacific. We invited the Asian Symposium on Programming Languages and Systems (APLAS) to co-locate with us for the 3rd year in a row to celebrate this occasion.",2022,879
Analysis of Editors' Languages in Wikidata,"Kaffee, Lucie-Aim\'{e}e and Simperl, Elena","Wikidata is unique as a knowledge base as well as a community given its users contribute together to one cross-lingual project. To create a truly multilingual knowledge base, a variety of languages of contributors is needed. In this paper, we investigate the language distribution in Wikidata's editors, how it relates to Wikidata's content and the users' label editing. This gives us an insight into its community that can help supporting users working on multilingual projects.",2018,880
"ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3",,"Welcome to the third volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is mostly dedicated to the 2024 fall cycle but also provides some statistics summarizing all three cycles.We introduced several notable changes to ASPLOS this year, most of which were discussed in our previous messages from program chairs in Volume 1 and 2, including: (1) significantly increasing the program committee size to over 220 members (more than twice the size of last year); (2) foregoing synchronous program committee (PC) meetings and instead making all decisions online; (3) overhauling the review assignment process; (4) developing an automated submission format violation identifier script that uncovers, e.g., disallowed vertical space manipulations that ""squeeze"" space; (5) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee; and (6) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it and highlighting how we believe that it should be handled in the future.Assuming readers have read our previous messages, here, we will only describe differences between the current cycle and the previous ones. These include: (1) Finally unifying submission and acceptance paper formatting instructions (forgoing the `jpaper' class) to rid authors of accepted papers from the need to reformat; (2) Describing the methodology we employed to select best papers, which we believe ensures quality and hope will persist; and (3) Reporting the ethical incidents we encountered and how we handled them. In the final, fourth volume, when the outcome of the ASPLOS'24 fall major revisions will become known, we plan to conduct a broader analysis of all the data we have gathered throughout the year.Following are some key statistics of the fall cycle: 340 submissions were finalized (43% more than last year's fall count and 17% less than our summer cycle) of which 111 are related to accelerators/FPGAs/GPUs, 105 to machine learning, 54 to security, 50 to datacenter/cloud and 50 to storage/memory; 183 (54%) submissions were promoted to the second review round; 39 (11.5%) papers were accepted (of which 19 were awarded artifact evaluation badges); 33 (9.7%) submissions were allowed to submit major revisions and are currently under review (these will be addressed in the fourth volume of ASPLOS'24 and will be presented in ASPLOS'25 if accepted); 1,368 reviews were uploaded; and 4,949 comments were generated during online discussions, of which 4,070 were dedicated to the submissions that made it to the second review round.This year, in the submission form, we asked authors to specify which of the three ASPLOS research areas are related to their submitted work. Analyzing this data revealed that 80%, 39%, and 29% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, generating the highest difference we have observed across the cycles between architecture and the other two. About 46% of the fall submissions are ""interdisciplinary,"" namely, were associated with two or more of the three areas.Overall, throughout all the ASPLOS'24 cycles, we received 922 submissions, constituting a 1.54x increase compared to last year. Our reviewers submitted a total of 3,634 reviews containing more than 2.6 million words, and we also generated 12,655 online comments consisting of nearly 1.2 million words. As planned, PC members submitted an average of 15.7 reviews and a median of 15, and external review committee (ERC) members submitted an average of 4.7 and a median of 5.We accepted 170 papers thus far, written by 1100 authors, leading to an 18.4% acceptance rate, with the aforementioned 33 major revisions still under review. Assuming that the revision acceptance rate will be similar to that of previous cycles, we estimate that ASPLOS'24 will accept nearly 200 (!) papers, namely, 21%–22% of the submissions.The ASPLOS'24 program consists of 193 papers: the 170 papers we accepted thus far and, in addition, 23 major revisions from the fall cycle of ASPLOS'23, which were re-reviewed and accepted. The full details are available in the PDF of the front matter.",2024,881
iFixR: bug report driven program repair,"Koyuncu, Anil and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and Le Traon, Yves","Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and- validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).",2019,882
WWW '22: Companion Proceedings of the Web Conference 2022,,,2022,883
iLFQA: A Platform for Efficient and Accurate Long-Form Question Answering,"Butler, Rhys and Duggirala, Vishnu Dutt and Banaei-Kashani, Farnoush","We present an efficient and accurate long-form question-answering platform, dubbed iLFQA (i.e., short for intelligent Long-Form Question Answering). The purpose of iLFQA is to function as a platform which accepts unscripted questions and efficiently produces semantically meaningful, explanatory, and accurate long-form responses. iLFQA consists of a number of modules for zero-shot classification, text retrieval, and text generation to generate answers to questions based on an open-domain knowledge base. iLFQA is unique in the question answering space because it is an example of a deployable and efficient long-form question answering system. Question answering systems exist in many forms, but long-form question answering remains relatively unexplored, and to the best of our knowledge none of the existing long-form question answering systems are shown to be sufficiently efficient to be deployable. We have made the source code and implementation details of iLFQA available for the benefit of researchers and practitioners in this field. With this demonstration, we present iLFQA as an open-domain, deployable, and accurate open-source long-form question answering platform.",2022,884
MUM '22: Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia,,,2022,885
An Empirical Study of Refactorings and Technical Debt in Machine Learning Systems,"Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita","Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.",2021,886
Wolf at the Door: Preventing Install-Time Attacks in npm with Latch,"Wyss, Elizabeth and Wittman, Alexander and Davidson, Drew and De Carli, Lorenzo","The npm software ecosystem allows developers to easily import code written by others. However, manual vetting of every individual installed component is made difficult in many cases by the number of transitive dependencies brought in by installing popular packages. This has enabled attackers to propagate malicious code by hiding it deep into the dependency chains of popular packages. A particularly dangerous form of attack comes from malicious code embedded into package install scripts.We tackle the problem of preventing undesirable install-time behavior by proposing Latch, a system for mediating install-time capabilities of npm packages. Latch generates permission manifests summarizing each package's install-time behavior and checks them against user-defined policies to ensure compliance. Policies in Latch are expressed in a rich formal policy language that covers a broad range of use cases. Our key insight is that expressive Latch policies empower users to define and enforce their own individualized security needs.Evaluation of practical Latch policies on all publicly available npm packages and on a number of real-world attack packages demonstrates that our approach is effective in identifying and stopping unwanted behavior while minimizing disruption due to undesired alerts.",2022,887
SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems,,,2023,888
ASSETS '23: Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility,,,2023,889
IVA '23: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents,,"This volume contains the papers presented at the 23nd International Conference on Intelligent Virtual Agents (IVA 2023) located in W\""{u}rzburg, Germany, from 19. to 22.09.2023.",2023,890
Neural Language Models as What If? -Engines for HCI Research,"H\""{a}m\""{a}l\""{a}inen, Perttu and Tavast, Mikke and Kunnari, Anton","Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) and user experience (UX) research. In this poster paper, we explore and critically evaluate the potential of large-scale neural language models like GPT-3 in generating synthetic research data such as participant responses to interview questions. We observe that in the best case, GPT-3 can create plausible reflections of video game experiences and emotions, and adapt its responses to given demographic information. Compared to real participants, such synthetic data can be obtained faster and at a lower cost. On the other hand, the quality of generated data has high variance, and future work is needed to rigorously quantify the human-likeness, limitations, and biases of the models in the HCI domain.",2022,891
Using Digital Trace Data to Identify Regions and Cities,"Brelsford, Christa and Thakur, Gautam and Arthur, Rudy and Williams, Hywel","A greater understanding of human dynamics as they play out in both physical space and through interpersonal communication is vital for the design and development of intelligent and resilient cities. Physical context provides insight into the space-time distribution of population and their activity patterns, while interpersonal communication can now be measured at the population scale through digital interactions. In this work, we propose a novel method to discover these dynamics. We use a dataset of 72 million tweets to develop a spatially embedded network of communication, and then use community detection algorithms to explore regional and urban delineation in the United States. We compare these results to US census regions and economic and infrastructural networks. We find that the broad spatial delineation of communities and sub-communities is consistent with United States regions, states, and major metropolitan areas. We describe how these methods could be extended to generate a measure of social regions that can be consistently applied anywhere there is a sufficiently rich data source. A deeper understanding of urban social structure measured by spatially embedded communication networks can enable a better understanding of the interactions between urban social and physical contexts. This, in turn, may enable urban managers and policy makers to identify strategies for supporting urban resilience.",2019,892
Educator and Student Perspectives on the Impact of Generative AI on Assessments in Higher Education,"Smolansky, Adele and Cram, Andrew and Raduescu, Corina and Zeivots, Sandris and Huber, Elaine and Kizilcec, Rene F.","The sudden popularity and availability of generative AI tools, such as ChatGPT that can write compelling essays on any topic, code in various programming languages, and ace standardized tests across domains, raises questions about the sustainability of traditional assessment practices. To seize this opportunity for innovation in assessment practice, we conducted a survey to understand both the educators' and students' perspectives on the issue. We measure and compare attitudes of both stakeholders across various assessment scenarios, building on an established framework for examining the quality of online assessments along six dimensions. Responses from 389 students and 36 educators across two universities indicate moderate usage of generative AI, consensus for which types of assessments are most impacted, and concerns about academic integrity. Educators prefer adapted assessments that assume AI will be used and encourage critical thinking, but students' reaction is mixed, in part due to concerns about a loss of creativity. The findings show the importance of engaging educators and students in assessment reform efforts to focus on the process of learning over its outputs, higher-order thinking, and authentic applications.",2023,893
Understanding Website Behavior based on User Agent,"Pham, Kien and Santos, A\'{e}cio and Freire, Juliana","Web sites have adopted a variety of adversarial techniques to prevent web crawlers from retrieving their content. While it is possible to simulate users behavior using a browser to crawl such sites, this approach is not scalable. Therefore, understanding existing adversarial techniques is important to design crawling strategies that can adapt to retrieve the content as efficiently as possible. Ideally, a web crawler should detect the nature of the adversarial policies and select the most cost-effective means to defeat them.In this paper, we discuss the results of a large-scale study of web site behavior based on their responses to different user-agents. We issued over 9 million HTTP GET requests to 1.3 million unique web sites from DMOZ using six different user-agents and the TOR network as an anonymous proxy. We observed that web sites do change their responses depending on user-agents and IP addresses. This suggests that probing sites for these features can be an effective means to detect adversarial techniques.",2016,894
IoT '23: Proceedings of the 13th International Conference on the Internet of Things,,,2023,895
ICER '23: Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2,,,2023,896
Distributed Denial of Service (DDoS) Mitigation: Security Awareness Domain and Resources (SADAR),"Hidayat, Taufik Safar and Fadillah, Yoga and Lubis, Muharman and Lubis, Arif Ridho","Information security is a constantly evolving field to safeguard confidentiality, integrity and assets with defensive measures should be developed and planned to improve information security. Therefore, it is very important for a company to update its security system and defense techniques from attacks that occur. Furthermore, Distributed Denial of Service (DDoS) attack is a major threat that can disrupt and hinder legitimate service requests on a network. They were first reported in 1996 and until now the complexity and sophistication of these attacks is increasing, which are usually used to force a site or service to go offline, that is, by passing high data traffic until the server cannot work. Meanwhile, In August 2021, Microsoft claimed to have succeeded in overcoming the largest DDoS attack reaching 2 TBps. Moreover, with the increasing number of DDoS attacks, a mitigation scenario is needed to deal with the complexity and technical development of this DDoS. In the network world, detecting a DDoS somehow, is extremely difficult because the attempt should distinguish between normal traffic and DDoS attacks. Thus, some recommended security solutions will be compiled in this paper to deliver solution involving the mitigation.",2022,897
HRI '24: Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,"Welcome one and all to the 19th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI)!We are so pleased to re-welcome the HRI community to Boulder, Colorado, where HRI 2021 would have been held, had the COVID pandemic not interfered. Following up on the successful in-person conference held last year in Sweden, this year's theme is ""HRI in the Real World,"" and focuses on advances that aim to bring human-robot interaction out of the lab and into everyday life.One aspect of this that we are very excited about is the introduction of a robot challenge to the conference activities, where teams from around the world will showcase their research and development via actual, interactive robots in the ""real world"" of an academic conference. It is our hope that this feature will grow and develop over the coming years into a staple of the HRI conference.This year's HRI conference saw an impressive surge in global interest, with 352 full paper submissions from around the world, marking a significant 40% increase compared to the previous year. These papers were categorized under relevant thematic subcommittees and underwent a double-blind review process, a rebuttal phase, and selective shepherding by the HRI program committee. From this process, 87 outstanding papers (24.7%) were chosen for full presentation at the conference. Reflecting our joint sponsorship with IEEE and ACM, all accepted papers will be accessible in the ACM Digital Library and IEEE Xplore.",2024,898
Koli Calling '23: Proceedings of the 23rd Koli Calling International Conference on Computing Education Research,,,2023,899
"TickEth, a ticketing system built on ethereum","Corsi, Pietro and Lagorio, Giovanni and Ribaudo, Marina","We propose TickEth, a system that aims at mitigating some of the problems encountered by the ticketing industry. The name TickEth is built by combining the words ticket, its application domain, and Ethereum, the underlying platform we adopted for the development of the prototype.Nowadays, ticketing ecosystem is wide and fragmented, and faces several problems. TickEth exploits Ethereum smart contracts to tackle the inability of checking the authenticity of tickets sold online (that can be fake or duplicates of a real ticket), the wild price range of resold tickets in the secondary market, and the unwieldy refund procedures.TickEth is an open specification, where different third-party clients can freely interact, and can be instantiated by different vendors as they like. We implemented a proof-of-concept prototype, available at: https://github.com/H221/TickEth",2019,900
IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces,,,2023,901
Scrutinizing Privacy Policy Compliance of Virtual Personal Assistant Apps,"Xie, Fuman and Zhang, Yanjun and Yan, Chuan and Li, Suwan and Bu, Lei and Chen, Kai and Huang, Zi and Bai, Guangdong","A large number of functionality-rich and easily accessible applications have become popular among various virtual personal assistant&nbsp;(VPA) services such as Amazon Alexa. VPA applications&nbsp;(or VPA apps for short) are accompanied by a privacy policy document that informs users of their data handling practices. These documents are usually lengthy and complex for users to comprehend, and developers may intentionally or unintentionally fail to comply with them. In this work, we conduct the first systematic study on the privacy policy compliance issue of VPA apps. We develop Skipper, which targets Amazon Alexa skills. It automatically depicts the skill into the declared privacy profile by analyzing their privacy policy documents with Natural Language Processing&nbsp;(NLP) and machine learning techniques, and derives the behavioral privacy profile of the skill through a black-box testing. We conduct a large-scale analysis on all skills listed on Alexa store, and find that a large number of skills suffer from the privacy policy noncompliance issues.",2023,902
WayPop Machine: A Wayback Machine to Investigate Popularity and Root Out Trolls,"Elmas, Tu\u{g}rulcan and Ibanez, Thomas Romain and Hutter, Alexandre and Overdorf, Rebekah and Aberer, Karl","Contrary to celebrities who owe their popularity online to their activity offline, malicious users such as trolls have to gain fame on social media through the social media itself. The exact reasons that a certain user has become popular are often obscure especially when the popularity was gained illicitly through means such as fake amplification of content. In this paper, we develop a methodology for uncovering why an account has become popular and present an open source tool that encapsulates this methodology. This tool aims to aid others in uncovering malicious accounts which have artificially gained many followers and to distinguish such accounts from those which gained followers and popularity honestly.",2023,903
Automated approach for system-level testing of unmanned aerial systems,"Sartaj, Hassan","Unmanned aerial systems (UAS) have a large number of applications in civil and military domains. UAS rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics systems, including software systems. The current industrial practice is to manually create test scenarios, manually or automatically execute these scenarios using simulators, and manual evaluation of the outcomes. A fundamental part of system-level testing of such systems is the simulation of environmental context. The test scenarios typically consist of setting certain environment conditions and testing the system under test in these settings. The state-of-the-art approaches available for this purpose also require manual test scenario development and manual test evaluation. In this research work, we propose an approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test scenarios are generated on the fly, i.e., during test execution based on the environmental context at runtime. We develop a toolset to support automation. We perform a pilot experiment using a widely-used open-source autopilot, ArduPilot. The preliminary results show that the AITester is effective and efficient in violating autopilot expected behavior.",2022,904
Genetic Improvement @ ICSE 2020,"Langdon, William B. and Weimer, Westley and Petke, Justyna and Fredericks, Erik and Lee, Seongmin and Winter, Emily and Basios, Michail and Cohen, Myra B. and Blot, Aymeric and Wagner, Markus and Bruce, Bobby R. and Yoo, Shin and Gerasimou, Simos and Krauss, Oliver and Huang, Yu and Gerten, Michael","Following Prof. Mark Harman of Facebook's keynote and formal presentations (which are recorded in the proceed- ings) there was a wide ranging discussion at the eighth inter- national Genetic Improvement workshop, GI-2020 @ ICSE (held as part of the International Conference on Software En- gineering on Friday 3rd July 2020). Topics included industry take up, human factors, explainabiloity (explainability, jus- tifyability, exploitability) and GI benchmarks. We also con- trast various recent online approaches (e.g. SBST 2020) to holding virtual computer science conferences and workshops via the WWW on the Internet without face to face interac- tion. Finally we speculate on how the Coronavirus Covid-19 Pandemic will a ect research next year and into the future.",2020,905
Crowd_Frame: A Simple and Complete Framework to Deploy Complex Crowdsourcing Tasks Off-the-shelf,"Soprano, Michael and Roitero, Kevin and Bombassei De Bona, Francesco and Mizzaro, Stefano","Due to their relatively low cost and ability to scale, crowdsourcing based approaches are widely used to collect a large amount of human annotated data. To this aim, multiple crowdsourcing platforms exist, where requesters can upload tasks and workers can carry them out and obtain payment in return. Such platforms share a task design and deploy workflow that is often counter-intuitive and cumbersome. To address this issue, we propose Crowd_Frame, a simple and complete framework which allows to develop and deploy diverse types of complex crowdsourcing tasks in an easy and customizable way. We show the abilities of the proposed framework and we make it available to researchers and practitioners.",2022,906
IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces,,,2023,907
Detecting Authentication-Bypass Flaws in a Large Scale of IoT Embedded Web Servers,"Jiang, Yikun and Xie, Wei and Tang, Yong","With the rapid development of network and communication technologies, everything is able to be connected to the Internet. IoT devices, which include home routers, IP cameras, wireless printers and so on, are crucial parts facilitating to build pervasive and ubiquitous networks. As the number of IoT devices around the world increases, the security issues become more and more serious.To handle with the security issues and protect the IoT devices from being compromised, the firmware of devices needs to be strengthened by discovering and repairing vulnerabilities. Current vulnerability detection tools can only help strengthening traditional software, nevertheless these tools are not practical enough for IoT device firmware, because of the peculiarity in firmware's structure and embedded device's architecture. Therefore, new vulnerability detection framework is required for analyzing IoT device firmware.This paper reviews related works on vulnerability detection in IoT firmware, proposes and implements a framework to automatically detect authentication-bypass flaws in a large scale of Linux-based firmware. The proposed framework is evaluated with a data set of 2351 firmware images from several target vendors, which is proved to be capable of performing large-scale and automated analysis on firmware, and 1 known and 10 unknown authentication-bypass flaws are found by the analysis.",2018,908
ASIA CCS '23: Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security,,,2023,909
ICSIM '23: Proceedings of the 2023 6th International Conference on Software Engineering and Information Management,,,2023,910
BIOTC '23: Proceedings of the 2023 5th Blockchain and Internet of Things Conference,,,2023,911
"Computers do not make art, people do","Hertzmann, Aaron",The continually evolving relationship between artistic technologies and artists.,2020,912
"Pick, Click, Flick! The Story of Interaction Techniques","Myers, Brad A.","This book provides a comprehensive study of the many ways to interact with computers and computerized devices. An “interaction technique” starts when the user performs an action that causes an electronic device to respond, and includes the direct feedback from the device to the user. Examples include physical buttons and switches, on-screen menus and scrollbars operated by a mouse, touchscreen widgets, gestures such as flick-to-scroll, text entry on computers and touchscreens, input for virtual reality systems, interactions with conversational agents such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, and adaptations of all of these for people with disabilities. Pick, Click, Flick! is written for anyone interested in interaction techniques, including computer scientists and designers working on human-computer interaction, as well as implementers and consumers who want to understand and get the most out of their digital devices.REVIEWS“Pick, Click, Flick! is an impressive reference manual of the many years of interaction design development. It is a reference book, invaluable when questions arise, whether while you are busy designing something, or learning, or teaching, where assigning sections of the reference will be a valuable resource and learning tool for students. Brad Myers has provided a great service to the interaction community.” ‐ Don Norman, Distinguished Prof. Emeritus, Design Lab, University of California, San Diego“Every UX professional should immerse themselves in this book. Not only does it unravel the fascinating and complex history of GUI widgets that will captivate any user interface nerd, but it also stands as the definitive guide to an incredibly diverse array of interaction techniques. This is not just an engaging read; it’s an essential toolkit.” ‐ Jakob Nielsen, Principal, Nielsen Design Group",2024,913
NordiCHI '22: Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference,,,2022,914
Supervised detection of IoT botnet attacks,"Alazzam, Hadeel and Alsmady, Abdulsalam and Shorman, Amaal Al","Nowadays, more and more people start using IoT devices, which raise the threats of compromising these devices, since it's easily manipulated and hacked than desktop devices. This fact increased the number of cyberattacks that relay on IoT-based Botnet attacks. In this paper, we investigate the using of a supervised technique for detecting anomalies in IoT networks. The proposed model used a random forest classifier, the training data consider only 4 types of attacks while testing considers 10 types of attacks. The proposed model was effective in detection the new attacks and achieved 99% in terms of TPR, 100% in terms of TNR, and near-zero false alarms.",2019,915
Formally Verified Loop-Invariant Code Motion and Assorted Optimizations,"Monniaux, David and Six, Cyril","We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination. This approach is lightweight: each pass comes with a simple and independent proof of correctness. Experiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers. Our static analysis employs an efficient yet verified hashed set structure, resulting in the fast compilation.",2022,916
RecSys '23: Proceedings of the 17th ACM Conference on Recommender Systems,,,2023,917
Code review for newcomers: is it different?,"Kovalenko, Vladimir and Bacchelli, Alberto","Onboarding is a critical stage in the tenure of software developers with a project, because meaningful contribution requires familiarity with the codebase. Some software teams employ practices, such as mentoring, to help new developers get accustomed faster. Code review, i.e., the manual inspection of code changes, is an opportunity for sharing knowledge and helping with onboarding.In this study, we investigate whether and how contributions from developers with low experience in a project do receive a different treatment during code review. We compare reviewers' experience, metrics of reviewers' attention, and change merge rate between changes from newcomers and from more experienced authors in 60 active open source projects. We find that the only phenomenon that is consistent across the vast majority of projects is a lower merge rate for newcomers' changes.",2018,918
"OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications",,,2016,919
Multi-Objective Blended Ensemble For Highly Imbalanced Sequence Aware Tweet Engagement Prediction,"Felicioni, Nicol\`{o} and Donati, Andrea and Conterio, Luca and Bartoccioni, Luca and Hu, Davide Yi Xian and Bernardis, Cesare and Ferrari Dacrema, Maurizio","In this paper we provide a description of the methods we used as team BanaNeverAlone for the ACM RecSys Challenge 2020, organized by Twitter. The challenge addresses the problem of user engagement prediction: the goal is to predict the probability of a user engagement (Like, Reply, Retweet or Retweet with comment), based on a series of past interactions on the Twitter platform. Our proposed solution relies on several features that we extracted from the original dataset, as well as on consolidated models, such as gradient boosting for decision trees and neural networks. The ensemble model, built using blending, and a multi-objective optimization allowed our team to rank in position 4.",2020,920
NordiCHI '22: Nordic Human-Computer Interaction Conference,,,2022,921
"FU Covid-19 AI Agent built on Attention algorithm using a combination of Transformer, ALBERT model, and RASA framework","Quy Tran, Ban and Van Nguyen, Thai and Duc Phung, Thang and Tan Nguyen, Viet and Duy Tran, Dat and Tung Ngo, Son","Potentialized by Natural Language Processing (NLP) technology, we can build a chatbot or an AI Agent to automatically address the need to automatically get credible and timely information, especially in the fight against epidemics. However, Vietnamese understanding is still a big challenge for NLP. This paper introduces an AI Agent using the Attention algorithm and Albert model to implement the question/answering task in the Covid-19 field for the Vietnamese language. In the end, we also built two other modules, one for Vietnamese diacritic auto-correction and another for updating Covid-19 statistics (using RASA framework), to deploy a Covid-19 chatbot application on mobile devices.",2021,922
Mass Digitization of Early Modern Texts With Optical Character Recognition,"Christy, Matthew and Gupta, Anshul and Grumbach, Elizabeth and Mandell, Laura and Furuta, Richard and Gutierrez-Osuna, Ricardo","Optical character recognition (OCR) engines work poorly on texts published with premodern printing technologies. Engaging the key technological contributors from the IMPACT project, an earlier project attempting to solve the OCR problem for early modern and modern texts, the Early Modern OCR Project (eMOP) of Texas A8M received funding from the Andrew W. Mellon Foundation to improve OCR outputs for early modern texts from the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) proprietary database products—or some 45 million pages. Added to print problems are the poor quality of the page images in these collections, which would be too time consuming and expensive to reimage. This article describes eMOP's attempts to OCR 307,000 documents digitized from microfilm to make our cultural heritage available for current and future researchers. We describe the reasoning behind our choices as we undertook the project based on other relevant studies; discoveries we made; the data and the system we developed for processing it; the software, algorithms, training procedures, and tools that we developed; and future directions that should be taken for further work in developing OCR engines for cultural heritage materials.",2017,923
Towards Theory for Real-World Data,"Martens, Wim","Fundamental research on data manipulation languages is often motivated by the search for balance between desirable properties, such as expressiveness, robustness, compositionality, the existence of efficient algorithms, etc. Real-world data can be helpful for this search in many different respects. Data sets may exhibit common structures that efficient algorithms can exploit. Query logs and schemas can give us an idea of single features that are used very often, or groups of features that are frequently used together. In this sense, they can guide us towards features or fragments of data manipulation languages that are common in practice and may therefore be worthy of deeper study. In other cases, we may even get a glimpse on features that are not well-understood by users, which may inspire us to redesign them or develop tools that increase their ease-of-use. This tutorial aims to provide, first of all, an overview on several practical studies that have been conducted in the areas of tree-structured and graph-structured data, with a focus on cases with strong interaction between analysis of the data and fundamental research. Second, it aims to provide a set of lessons learned after the investigation of some large-scale logs consisting of more than 850 million queries.",2022,924
WiSec '24: Proceedings of the 17th ACM Conference on Security and Privacy in Wireless and Mobile Networks,,"Welcome to the 2024 ACM Conference on Security and Privacy in Wireless and Mobile Networks (ACM WiSec)!Now in its 17th year, WiSec continues to be the premier venue for research on all aspects of security and privacy in wireless and mobile networks, their systems, and their applications. We are hosted by the Korea Institute of Information Security &amp; Cryptology, located in the city center of Seoul, Korea - a city known for its dynamic mix of 600-year-old palaces and the contemporary urban landscape characterized by towering skyscrapers.We begin our exciting three-day main conference program on May 27th with single-track technical paper sessions, a poster and demo session, two excellent keynotes from telecommunication security expert Prof. Jean-Pierre Seifert (TU Berlin) and wireless security expert Mathy Vanhoef (KU Leuven), and a panel on wireless security and AI. Three invited talks named ""Vision Talk"" discuss the future of wireless and mobile security issues. The WiseML Workshop follows the main program on May 30th. We invite participants to attend the exciting paper presentations and keynotes, interact with the presenters during the Q&amp;A sessions after each talk, network during the coffee breaks and lunches each day, and socialize during the banquet dinner.",2024,925
A Study of Editor Features in a Creative Coding Classroom,"Mcnutt, Andrew M and Outkine, Anton and Chugh, Ravi","Creative coding is a rapidly expanding domain for both artistic expression and computational education. Numerous libraries and IDEs support creative coding, however there has been little consideration of how the environments themselves might be designed to serve these twin goals. To investigate this gap, we implemented and used an experimental editor to teach a sequence of college and high-school creative coding courses. In the first year, we conducted a log analysis of student work (n=39) and surveys regarding prospective features (n=25). These guided our implementation of common enhancements (e.g. color pickers) as well as uncommon ones (e.g. bidirectional shape editing). In the second year, we studied the effects of these features through logging (n=39+) and survey (n=23) studies. Reflecting on the results, we identify opportunities to improve creativity- and novice-focused IDEs and highlight tensions in their design—as in tools that augment artistry or efficiency but may be perceived as hindering learning.",2023,926
"RAID '22: Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses",,,2022,927
FDG '24: Proceedings of the 19th International Conference on the Foundations of Digital Games,,,2024,928
Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference,,,2022,929
Social Machines for All,"Papapanagiotou, Petros and Davoust, Alan and Murray-Rust, Dave and Manataki, Areti and Van Kleek, Max and Shadbolt, Nigel and Robertson, Dave","In today's interconnected world, people interact to a unprecedented degree through the use of digital platforms and services, forming complex 'social machines'. These are now homes to autonomous agents as well as people, providing an open space where human and computational intelligence can mingle---a new frontier for distributed agent systems. However, participants typically have limited autonomy to define and shape the machines they are part of. In this paper, we envision a future where individuals are able to develop their own Social Machines, enabling them to interact in a trustworthy, decentralized way. To make this possible, development methods and tools must see their barriers-to-entry dramatically lowered. People should be able to specify the agent roles and interaction patterns in an intuitive, visual way, analyse and test their designs and deploy them as easy to use systems. We argue that this is a challenging but realistic goal, which should be tackled by navigating the trade-off between the accessibility of the design methods --primarily the modelling formalisms-- and their expressive power. We support our arguments by drawing ideas from different research areas including electronic institutions, agent-based simulation, process modelling, formal verification, and model-driven engineering.",2018,930
GoodIT '23: Proceedings of the 2023 ACM Conference on Information Technology for Social Good,,,2023,931
dg.o '24: Proceedings of the 25th Annual International Conference on Digital Government Research,,,2024,932
A Meta-Study of Software-Change Intentions,"Kr\""{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia","Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system, many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state-of-the-art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs; and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.",2024,933
CallMine: Fraud Detection and Visualization of Million-Scale Call Graphs,"Cazzolato, Mirela and Vijayakumar, Saranya and Lee, Meng-Chieh and Vajiac, Catalina and Park, Namyong and Fidalgo, Pedro and Traina, Agma J.M. and Faloutsos, Christos","Given a million-scale dataset of who-calls-whom data containing imperfect labels, how can we detect existing and new fraud patterns? We propose CallMine, with carefully designed features and visualizations. Our CallMine method has the following properties: (a) Scalable, being linear on the input size, handling about 35 million records in around one hour on a stock laptop; (b) Effective, allowing natural interaction with human analysts; (c) Flexible, being applicable in both supervised and unsupervised settings; (d) Automatic, requiring no user-defined parameters.In the real world, in a multi-million-scale dataset, CallMine was able to detect fraudsters 7,000x faster, namely in a matter of hours, while expert humans took over 10 months to detect them.CIKM-ARP Categories: Application; Analytics and machine learning; Data presentation.",2023,934
Tamaglitchi: A Pilot Study of Anthropomorphism and Non-Verbal Sound,"Collins, Karen and Dockwray, Ruth","In this paper, we present an overview of the current state of research in anthropomorphism as it relates specifically to product design, and then present a short pilot study of nonverbal sound's influence on anthropomorphism, through two short experiments, one qualitative and one quantitative. These experiments use an online variation of a virtual pet similar to the Tamagotchi, which we have called ""Tamaglitchi"". Results show that non-verbal sound increased the tendency to anthropomorphize a virtual pet.",2018,935
Trans-centered moderation: Trans technology creators and centering transness in platform and community governance,"Thach, Hibby and Mayworm, Samuel and Thomas, Michaelanne and Haimson, Oliver L.","Mainstream platforms’ content moderation systems typically employ generalized “one-size-fits-all” approaches, intended to serve both general and marginalized users. Thus, transgender people must often create their own technologies and moderation systems to meet their specific needs. In our interview study of transgender technology creators (n=115), we found that creators face issues of transphobic abuse and disproportionate content moderation. Trans tech creators address these issues by carefully moderating and vetting their userbases, centering trans contexts in content moderation systems, and employing collective governance and community models. Based on these findings, we argue that trans tech creators’ approaches to moderation offer important insights into how to better design for trans users, and ultimately, marginalized users in the larger platform ecology. We introduce the concept of trans-centered moderation – content moderation that reviews and successfully vets transphobic users, appoints trans moderators to effectively moderate trans contexts, considers the limitations and constraints of technology for addressing social challenges, and employs collective governance and community models. Trans-centered moderation can help to improve platform design for trans users while reducing the harm faced by trans people and marginalized users more broadly.",2024,936
ACE '24: Proceedings of the 26th Australasian Computing Education Conference,,,2024,937
Sensible and Sensitive AI for Worker Wellbeing: Factors that Inform Adoption and Resistance for Information Workers,"Das Swain, Vedant and Gao, Lan and Mondal, Abhirup and Abowd, Gregory D. and De Choudhury, Munmun","Algorithmic estimations of worker behavior are gaining popularity. Passive Sensing–enabled AI (PSAI) systems leverage behavioral traces from workers’ digital tools to infer their experience. Despite their conceptual promise, the practical designs of these systems elicit tensions that lead to workers resisting adoption. This paper teases apart the monolithic representation of PSAI by investigating system components that maximize value and mitigate concerns. We conducted an interactive online survey using the Experimental Vignette Method. Using Linear Mixed-effects Models we found that PSAI systems were more acceptable when sensing digital time use or physical activity, instead of visual modes. Inferences using language were only acceptable in work-restricted contexts. Compared to insights into performance, workers preferred insights into mental wellbeing. However, they resisted systems that automatically forwarded these insights to others. Our findings provide a template to reflect on existing systems and plan future implementations of PSAI to be more worker-centered.",2024,938
Facilitating Serverless Match-based Online Games with Novel Blockchain Technologies,"Wu, Feijie and Yuen, Ho Yin and Chan, Henry and Leung, Victor C. M. and Cai, Wei","Applying peer-to-peer (P2P) architecture to online video games has already attracted both academic and industrial interests, since it removes the need for expensive server maintenance. However, there are two major issues preventing the use of a P2P architecture, namely how to provide an effective distributed data storage solution, and how to tackle potential cheating behaviors. Inspired by emerging blockchain techniques, we propose a novel consensus model called Proof-of-Play (PoP) to provide a decentralized data storage system that incorporates an anti-cheating mechanism for P2P games, by rewarding players that interact with the game as intended, along with consideration of security measures to address the Nothing-at-stake Problem and the Long-range Attack. To validate our design, we utilize a game-theory model to show that under certain assumptions, the integrity of the PoP system would not be undermined due to the best interests of any user. Then, as a proof-of-concept, we developed a P2P game (Infinity Battle) to demonstrate how a game can be integrated with PoP in practice. Finally, experiments were conducted to study PoP in comparison with Proof-of-Work (PoW) to show its advantages in various aspects.",2023,939
WSC '23: Proceedings of the Winter Simulation Conference,,,2023,940
ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference,,,2022,941
A Method to Analyze Multiple Social Identities in Twitter Bios,"Pathak, Arjunil and Madani, Navid and Joseph, Kenneth","Twitter users signal social identity in their profile descriptions, or bios, in a number of important but complex ways that are not well-captured by existing characterizations of how identity is expressed in language. Better ways of defining and measuring these expressions may therefore be useful both in understanding how social identity is expressed in text, and how the self is presented on Twitter. To this end, the present work makes three contributions. First, using qualitative methods, we identify and define the concept of a personal identifier, which is more representative of the ways in which identity is signaled in Twitter bios. Second, we propose a method to extract all personal identifiers expressed in a given bio. Finally, we present a series of validation analyses that explore the strengths and limitations of our proposed method. Our work opens up exciting new opportunities at the intersection between the social psychological study of social identity and the study of how we compose the self through markers of identity on Twitter and in social media more generally.",2021,942
EC '23: Proceedings of the 24th ACM Conference on Economics and Computation,,"Over the course of two decades, EC has established itself as one of the few truly successful interdisciplinary conferences, attracting papers and participants with a broad range of interests in economics and computer science, and fostering work in the intersection.",2023,943
"Programming '23: Companion Proceedings of the 7th International Conference on the Art, Science, and Engineering of Programming",,,2023,944
"ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",,"Welcome to the second volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is dedicated to the 2024 summer review cycle.We introduced several notable changes to ASPLOS this year, many of which were discussed in the previous message from program chairs in Volume 1. Here, to avoid repetition, we assume that readers have already read the latter message and will only describe differences between the current cycle and the previous one. These include: (1) developing and utilizing an automated format violation identifier script focused on uncovering disallowed vertical space manipulations that ""squeeze"" space; (2) incorporating authors-declared best-matching topics into our review assignment process; (3) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee (PC) meetings, which necessitated additional managerial involvement in online dissensions; and (4) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it, and highlighting how we believe that it should be handled in the future.Key statistics of the ASPLOS'24 summer cycle include: 409 submissions were finalized (about 1.5x more than last year's summer count and nearly 2.4x more than our spring cycle), with 107 related to accelerators/FPGAs/GPUs, 97 to machine learning, 88 to storage/memory, 80 to security, and 69 to datacenter/cloud; 179 (44%) submissions were promoted to the second review round; 54 (13.2%) papers were accepted (with 20 awarded one or more artifact evaluation badges); 33 (8.1%) submissions were allowed to submit major revisions, of which 27 were subsequently accepted during the fall cycle (with 13 awarded one or more artifact evaluation badges); 1,499 reviews were uploaded; and 5,557 comments were generated during online discussions.Analyzing the per-submission most-related broader areas of research, which we asked authors to associate with their work in the submission form, revealed that 71%, 47%, and 28% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, with about 45% being ""interdisciplinary"" submissions (associated with more than one area). The full details are available in the PDF of the front matter.",2024,945
UIST '22 Adjunct: Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology,,,2022,946
AINTEC '23: Proceedings of the 18th Asian Internet Engineering Conference,,,2023,947
"""We Don't Do That Here"": How Collaborative Editing with Mentors Improves Engagement in Social Q&amp;A Communities","Ford, Denae and Lustig, Kristina and Banks, Jeremy and Parnin, Chris","Online question-and-answer (Q&amp;A) communities like Stack Overflow have norms that are not obvious to novice users. Novices create and post programming questions without feedback, and the community enforces site norms through public downvoting and commenting. This can leave novices discouraged from further participation. We deployed a month long, just-in-time mentorship program to Stack Overflow in which we redirected novices in the process of asking a question to an on-site Help Room. There, novices received feedback on their question drafts from experienced Stack Overflow mentors. We present examples and discussion of various question improvements including: question context, code formatting, and wording that adheres to on-site cultural norms. We find that mentored questions are substantially improved over non-mentored questions, with average scores increasing by 50%. We provide design implications that challenge how socio-technical communities onboard novices across domains.",2018,948
ACM SE '24: Proceedings of the 2024 ACM Southeast Conference,,"We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.",2024,949
"LCTES 2024: Proceedings of the 25th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",,"Welcome to the ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES 2024), the 25th edition of this longstanding conference! This year’s conference is co-located with PLDI 2024, bringing together affiliated research conferences and workshops into a week-long joint meeting in Copenhagen, Denmark. The mission of LCTES is to provide a link between languages, compilers, and tools for embedded systems, bringing together scientists and engineers from these communities. LCTES offers a forum for researchers and developers from either area to come together, interact, share insights, and collaborate on developing novel solutions.",2024,950
EuroUSEC '23: Proceedings of the 2023 European Symposium on Usable Security,,,2023,951
C&amp;C '24: Proceedings of the 16th Conference on Creativity &amp; Cognition,,,2024,952
"Artificial intelligence: the societal responsibility to inform, educate, and regulate","Hilton, Alexander D.","Artificial Intelligence (AI) is a rapidly growing field; one that is mysterious to the general public. The mention of the word AI fills the imaginations of many with thoughts of talking robots, jobs being replaced, and possibly even the destruction of mankind. Perhaps imaginations are running wild due to, perhaps driven by the loose definition of AI as systems able to perform tasks that normally require human intelligence that allows Hollywood to take some creative license. The experts in the field tend to work directly with AI and often for large companies, allowing for the imagination and news headlines to be where the public gets their information. Many wonder if this new technology is going to be an overall benefit to society or if it will bring unmitigated disaster. When the imagination runs wild, instead of understanding, news stories can perpetuate concerns and anxieties rather than hope and optimism.",2019,953
Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models,"Blyth, Scott and Treude, Christoph and Wagner, Markus","AI foundation models have the capability to produce a wide array of responses to a single prompt, a feature that is highly beneficial in software engineering to generate diverse code solutions. However, this advantage introduces a significant trade-off between diversity and correctness. In software engineering tasks, diversity is key to exploring design spaces and fostering creativity, but the practical value of these solutions is heavily dependent on their correctness. Our study systematically investigates this trade-off using experiments with HumanEval tasks, exploring various parameter settings and prompting strategies. We assess the diversity of code solutions using similarity metrics from the code clone community. The study identifies combinations of parameters and strategies that strike an optimal balance between diversity and correctness, situated on the Pareto front of this trade-off space. These findings offer valuable insights for software engineers on how to effectively use AI foundation models to generate code solutions that are diverse and accurate.",2024,954
WebMedia '23: Proceedings of the 29th Brazilian Symposium on Multimedia and the Web,,,2023,955
SLE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering,,,2016,956
"Onward! 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software",,"Welcome to the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2023), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet fully proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.  

Onward! 2023 is co-located with SPLASH 2023, running from Sunday 22nd of October till Friday 27th of October, in Cascais, Portugal. We are delighted to have Felienne Hermans giving the Onward! keynote, on Wednesday 25th of October, on ""Creating a learnable and inclusive programming language"".  

All papers and essays that lie here before you received at least three reviews, leading to a decision of accept, reject, or conditional accept. Authors of conditionally accepted papers were provided with explicit requirements for acceptance, and were carefully re-reviewed in the second phase. The essays track received six submissions, out of which four were accepted. The papers track accepted nine out of nineteen submissions.  

We hope that the papers and essays in these proceedings will stimulate and challenge your thinking about programming and software engineering, and we are looking forward to many discussions at the conference.",2023,957
Sec2vec: Anomaly Detection in HTTP Traffic and Malicious URLs,"Gniewkowski, Mateusz and Maciejewski, Henryk and Surmacz, Tomasz and Walentynowicz, Wiktor","In this paper, we show how methods known from Natural Language Processing (NLP) can be used to detect anomalies in HTTP requests and malicious URLs. Most of the current solutions focusing on a similar problem are either rule-based or trained using manually selected features. Modern NLP methods, however, have great potential in capturing a deep understanding of samples and therefore improving the classification results. Other methods, which rely on a similar idea, often ignore the interpretability of the results, which is so important in machine learning. We are trying to fill this gap. In addition, we show to what extent the proposed solutions are resistant to concept drift. In our work, we compare three different vectorization methods: simple BoW, fastText, and the current state-of-the-art language model RoBERTa. The obtained vectors are later used in the classification task. In order to explain our results, we utilize the SHAP method. We evaluate the feasibility of our methods on four different datasets: CSIC2010, UNSW-NB15, MALICIOUSURL, and ISCX-URL2016. The first two are related to HTTP traffic, the other two contain malicious URLs. The results we show are comparable to others or better, and most importantly - interpretable.",2023,958
Empirical Evidence on Conversational Control of GUI in Semantic Automation,"Weidele, Daniel Karl I. and Martino, Mauro and Valente, Abel N. and Rossiello, Gaetano and Strobelt, Hendrik and Franke, Loraine and Alvero, Kathryn and Misko, Shayenna and Auer, Robin and Bagchi, Sugato and Mihindukulasooriya, Nandana and Chowdhury, Faisal and Bramble, Gregory and Samulowitz, Horst and Gliozzo, Alfio and Amini, Lisa","This research explores integration of a Large Language Model (LLM) fine-tuned to conversationally control the user interface (UI) for a Semantic Automation Layer (SAL). We condense SAL capabilities from prior work and prioritize with business analysts and data engineers via a Kano model, before implementing a prototypical UI. We augment the UI with our conversational engine and propose In-situ Prompt Engineering and learn from Human Feedback to smoothen the interaction and manipulation of UI through natural language commands. To evaluate the efficacy and usability of conversational control in various use-case scenarios, we conduct and report on an empirical interaction design user study. Our findings provide evidence supporting enhanced user engagement and satisfaction. We also observe significant increase of trust in AI after working with our conversational UI. This work generates areas for further refinement and research towards more intelligent, highly-integrated conversational UIs even beyond our application within Semantic Automation. We discuss our findings and point out next steps paving the way for future research and development in creating more intuitive and adaptive user interfaces.",2024,959
CHIWORK '24: Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work,,,2024,960
PolicyKit: Building Governance in Online Communities,"Zhang, Amy X. and Hugh, Grant and Bernstein, Michael S.","The software behind online community platforms encodes a governance model that represents a strikingly narrow set of governance possibilities focused on moderators and administrators. When online communities desire other forms of government, such as ones that take many members? opinions into account or that distribute power in non-trivial ways, communities must resort to laborious manual effort. In this paper, we present PolicyKit, a software infrastructure that empowers online community members to concisely author a wide range of governance procedures and automatically carry out those procedures on their home platforms. We draw on political science theory to encode community governance into policies, or short imperative functions that specify a procedure for determining whether a user-initiated action can execute. Actions that can be governed by policies encompass everyday activities such as posting or moderating a message, but actions can also encompass changes to the policies themselves, enabling the evolution of governance over time. We demonstrate the expressivity of PolicyKit through implementations of governance models such as a random jury deliberation, a multi-stage caucus, a reputation system, and a promotion procedure inspired by Wikipedia's Request for Adminship (RfA) process.",2020,961
On sentiment of online fake news,"Zaeem, Razieh Nokhbeh and Li, Chengjing and Barber, K. Suzanne","The presence of disinformation and fake news on the Internet and especially social media has become a major concern. Prime examples of such fake news surged in the 2016 U.S. presidential election cycle and the COVID-19 pandemic. We quantify sentiment differences between true and fake news on social media using a diverse body of datasets from the literature that contains about 100K previously labeled true and fake news. We also experiment with a variety of sentiment analysis tools. We model the association between sentiment and veracity as conditional probability and also leverage statistical hypothesis testing to uncover the relationship between sentiment and veracity. With a significance level of 99.999%, we observe a statistically significant relationship between negative sentiment and fake news and between positive sentiment and true news. The degree of association, as measured by Goodman and Kruskal's gamma, ranges between .037 to .475. Finally, we make our data and code publicly available to support reproducibility. Our results assist in the development of automatic fake news detectors.",2021,962
FLEXI: A Robust and Flexible Social Robot Embodiment Kit,"Alves-Oliveira, Patr\'{\i}cia and Bavier, Matthew and Malandkar, Samrudha and Eldridge, Ryan and Sayigh, Julie and Bj\""{o}rling, Elin A. and Cakmak, Maya","The social robotics market is appealing yet challenging. Though social robots are built few remain on the market for long. Many reasons account for their short lifespan with costs and context-specificity ranking high amount them. In this work, we designed, fabricated, and developed FLEXI, a social robot embodiment kit that enabled unlimited customization, making it applicable for a broad range of use cases. The hardware and software of FLEXI were entirely developed by this research team from scratch. FLEXI includes a rich set of materials and attachment pieces to allow for a diverse range of hardware customizations that ensure the embodiment is appropriate for specific customer/researcher projects. It also includes an open-source end-user programming interface to lower the barrier of robotics access to interdisciplinary teams that populate the field of Human-Robot Interaction. We present an iterative development of this cost-effective kit through the lenses of case studies, conceptual research, and soft deployment of FLEXI in three application scenarios: community-support, mental health, and education. Additionally, we provide in open-access the full list of materials and a tutorial to fabricate FLEXI, making it accessible to any maker space, research lab, or workshop space interested in working with or learning about social robots.",2022,963
Sequential Action Patterns in Collaborative Ontology-Engineering Projects: A Case-Study in the Biomedical Domain,"Walk, Simon and Singer, Philipp and Strohmaier, Markus","Within the last few years the importance of collaborative ontology-engineering projects, especially in the biomedical domain, has drastically increased. This recent trend is a direct consequence of the growing complexity of these structured data representations, which no single individual is able to handle anymore. For example, the World Health Organization is currently actively developing the next revision of the International Classification of Diseases (ICD), using an OWL-based core for data representation and Web 2.0 technologies to augment collaboration. This new revision of ICD consists of roughly 50,000 diseases and causes of death and is used in many countries around the world to encode patient history, to compile health-related statistics and spendings. Hence, it is crucial for practitioners to better understand and steer the underlying processes of how users collaboratively edit an ontology. Particularly, generating predictive models is a pressing issue as these models may be leveraged for generating recommendations in collaborative ontology-engineering projects and to determine the implications of potential actions on the ontology and community. In this paper we approach this task by (i) exploring whether regularities and common patterns in user action sequences, derived from change-logs of five different collaborative ontology-engineering projects from the biomedical domain, exist. Based on this information we (ii) model the data using Markov chains of varying order, which are then used to (iii) predict user actions in the sequences at hand.",2014,964
IHC '23: Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems,,,2023,965
COVID-19 Concerns in US: Topic Detection in Twitter,"Comito, Carmela","COVID-19 pandemic is affecting the lives of the citizens worldwide. Epidemiologists, policy makers and clinicians need to understand public concerns and sentiment to make informed decisions and adopt preventive and corrective measures to avoid critical situations. In the last few years, social media become a tool for spreading the news, discussing ideas and comments on world events. In this context, social media plays a key role since represents one of the main source to extract insight into public opinion and sentiment. In particular, Twitter has been already recognized as an important source of health-related information, given the amount of news, opinions and information that is shared by both citizens and official sources. However, it is a challenging issue identifying interesting and useful content from large and noisy text-streams. The study proposed in the paper aims to extract insight from Twitter by detecting the most discussed topics regarding COVID-19. The proposed approach combines peak detection and clustering techniques. Tweets features are first modeled as time series. After that, peaks are detected from the time series, and peaks of textual features are clustered based on the co-occurrence in the tweets. Results, performed over real-world datasets of tweets related to COVID-19 in US, show that the proposed approach is able to accurately detect several relevant topics of interest, spanning from health status and symptoms, to government policy, economic crisis, COVID-19-related updates, prevention, vaccines and treatments.",2021,966
AFT '22: Proceedings of the 4th ACM Conference on Advances in Financial Technologies,,,2022,967
Ethereum's Proposer-Builder Separation: Promises and Realities,"Heimbach, Lioba and Kiffer, Lucianna and Ferreira Torres, Christof and Wattenhofer, Roger","With Ethereum's transition from Proof-of-Work to Proof-of-Stake in September 2022 came another paradigm shift, the Proposer-Builder Separation (PBS) scheme. PBS was introduced to decouple the roles of selecting and ordering transactions in a block (i.e., the builder), from those validating its contents and proposing the block to the network as the new head of the blockchain (i.e., the proposer). In this landscape, proposers are the validators in the Proof-of-Stake consensus protocol, while now relying on specialized block builders for creating blocks with the highest value for the proposer. Additionally, relays act as mediators between builders and proposers. We study PBS adoption and show that the current landscape exhibits significant centralization amongst the builders and relays. Further, we explore whether PBS effectively achieves its intended objectives of enabling hobbyist validators to maximize block profitability and preventing censorship. Our findings reveal that although PBS grants validators the opportunity to access optimized and competitive blocks, it tends to stimulate censorship rather than reduce it. Additionally, we demonstrate that relays do not consistently uphold their commitments and may prove unreliable. Specifically, proposers do not always receive the complete promised value, and the censorship or filtering capabilities pledged by relays exhibit significant gaps.",2023,968
Matching Ukrainian Wikipedia Red Links with English Wikipedia’s Articles,"Liubonko, Kateryna and S\'{a}ez-Trumper, Diego","This work tackles the problem of matching Wikipedia red links with existing articles. Links in Wikipedia pages are considered red when lead to nonexistent articles. In other Wikipedia editions could exist articles that correspond to such red links. In our work, we propose a way to match red links in one Wikipedia edition to existent pages in another edition. We define the task as a Named Entity Linking problem because red link titles are mostly named entities. We solve it in a context of Ukrainian red links and English existing pages. We created a dataset of 3171 most frequent Ukrainian red links and a dataset of almost 3 million pairs of red links and the most probable candidates for the correspondent pages in English Wikipedia. This dataset is publicly released1. In this work we define conceptual characteristics of the data — word and graph properties — based on its analysis and exploit these properties in entity resolution. BabelNet knowledge base was applied to this task and was regarded as a baseline for our approach (F1 score = 32 %). To improve the result we introduced several similarity metrics based on mentioned red links characteristics. Combined in a linear model they resulted in F1 score = 85 %. To the best of our knowledge, we are the first to state the problem and propose a solution for red links in Ukrainian Wikipedia edition.",2020,969
CBMI '22: Proceedings of the 19th International Conference on Content-based Multimedia Indexing,,,2022,970
CF '24 Companion: Proceedings of the 21st ACM International Conference on Computing Frontiers: Workshops and Special Sessions,,"This is the companion proceedings of the 21st ACM International Conference on Computing Frontiers (Volume 2) collecting the papers from co-located workshops and invited papers from special sessions. For papers from the main track, as well as keynote abstract and poster abstracts, see Volume 1.",2024,971
PETRA '23: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments,,,2023,972
"FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,2023,973
FDG '22: Proceedings of the 17th International Conference on the Foundations of Digital Games,,,2022,974
"A critical review of ""automatic patch generation learned from human-written patches"": essay on the problem statement and the evaluation of automatic software repair","Monperrus, Martin","At ICSE'2013, there was the first session ever dedicated to automatic program repair. In this session, Kim et al. presented PAR, a novel template-based approach for fixing Java bugs. We strongly disagree with key points of this paper. Our critical review has two goals. First, we aim at explaining why we disagree with Kim and colleagues and why the reasons behind this disagreement are important for research on automatic software repair in general. Second, we aim at contributing to the field with a clarification of the essential ideas behind automatic software repair. In particular we discuss the main evaluation criteria of automatic software repair: understandability, correctness and completeness. We show that depending on how one sets up the repair scenario, the evaluation goals may be contradictory. Eventually, we discuss the nature of fix acceptability and its relation to the notion of software correctness.",2014,975
Inter-Session Modeling for Session-Based Recommendation,"Ruocco, Massimiliano and Skrede, Ole Steinar Lillest\o{}l and Langseth, Helge","In recent years, research has been done on applying Recurrent Neural Networks (RNNs) as recommender systems. Results have been promising, especially in the session-based setting where RNNs have been shown to outperform state-of-the-art models. In many of these experiments, the RNN could potentially improve the recommendations by utilizing information about the user's past sessions, in addition to its own interactions in the current session. A problem for session-based recommendation, is how to produce accurate recommendations at the start of a session, before the system has learned much about the user's current interests. We propose a novel approach that extends an RNN recommender to be able to process the user's recent sessions, in order to improve recommendations. This is done by using a second RNN to learn from recent sessions, and predict the user's interest in the current session. By feeding this information to the original RNN, it is able to improve its recommendations. Our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions, compared to a single RNN working only on the current session. The proposed model especially improves recommendations at the start of sessions, and is therefore able to deal with the cold start problem within sessions.",2017,976
One General Teacher for Multi-Data Multi-Task: A New Knowledge Distillation Framework for Discourse Relation Analysis,"Jiang, Congcong and Qian, Tieyun and Liu, Bing","Automatically identifying the discourse relations can help many downstream NLP tasks such as reading comprehension and machine translation. It can be categorized into explicit and implicit discourse relation recognition (EDRR and IDRR). Due to the lack of connectives, IDRR remains to be a big challenge. A good number of methods have been developed to combine explicit data with implicit ones under the multi-task learning framework. However, the difference in linguistic property and class distribution makes it hard to directly optimize EDRR and IDRR with multi-task learning. In this paper, we take the first step to exploit the knowledge distillation (KD) technique for discourse relation analysis. Our target is to train &lt;italic&gt;a focused single-data single-task student&lt;/italic&gt; with the help of &lt;italic&gt;a general multi-data multi-task teacher&lt;/italic&gt;. Specifically, we first train one teacher for both the top and second level relation classification tasks with explicit and implicit data. We then transfer the feature embeddings and soft labels from the teacher network to the student network. Moreover, we develop an adaptive knowledge distillation module to reduce the number of hyper-parameters and also to stimulate the potential of the student on autonomous learning. Extensive experimental results on the popular PDTB dataset proves that our model achieves a new state-of-the-art performance. We also show the effectiveness of our proposed KD architecture through detailed analysis.",2023,977
How Online Forums Complement Task Documentation in Software Crowdsourcing,"Machado, Leticia S. and Steinmacher, Igor and Marczak, Sabrina and de Souza, Cleidson R. B.","An issue in software crowdsourcing is the quality of the task documentation and the high number of registered crowd workers to solve tasks but few submitted solutions only. This happens because uncommunicated or misunderstood requirements can lead crowd workers to deliver a solution that does not meet the customers' requirements or, worse, to give up submitting a solution. In this paper, we present an empirical study in which we analyzed task documentation and online forums messages associated with 25 Software Crowdsourcing (SW CS) challenges. The findings corroborate that weak documentation is a challenge in SW CS. Meanwhile, online forums allow crowd workers to gather additional technical and operational information that is not present in the official task documentation. We provide a stepping stone towards understanding the interplay between requirements and communication, to make it possible to improve SW CS development processes, practices, and tools.",2020,978
FDG '23: Proceedings of the 18th International Conference on the Foundations of Digital Games,,,2023,979
ETRA '23: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications,,,2023,980
Fast Linking of Mathematical Wikidata Entities in Wikipedia Articles Using Annotation Recommendation,"Scharpf, Philipp and Schubotz, Moritz and Gipp, Bela","Mathematical information retrieval (MathIR) applications such as semantic formula search and question answering systems rely on knowledge-bases that link mathematical expressions to their natural language names. For database population, mathematical formulae need to be annotated and linked to semantic concepts, which is very time-consuming. In this paper, we present our approach to structure and speed up this process by using an application-driven strategy and AI-aided system. We evaluate the quality and time-savings of AI-generated formula and identifier annotation recommendations on a test selection of Wikipedia articles from the physics domain. Moreover, we evaluate the community acceptance of Wikipedia formula entity links and Wikidata item creation and population to ground the formula semantics. Our evaluation shows that the AI guidance was able to significantly speed up the annotation process by a factor of 1.4 for formulae and 2.4 for identifiers. Our contributions were accepted in 88% of the edited Wikipedia articles and 67% of the Wikidata items. The &gt;&gt;AnnoMathTeX&lt;&lt; annotation recommender system is hosted by Wikimedia at annomathtex.wmflabs.org. In the future, our data refinement pipeline will be integrated seamlessly into the Wikimedia user interfaces.",2021,981
CPP 2024: Proceedings of the 13th ACM SIGPLAN International Conference on Certified Programs and Proofs,,"Welcome to the 13th ACM SIGPLAN International Conference on Certified 
Programs and Proofs (CPP 2024). CPP covers the practical and 
theoretical topics in all areas that consider formal verification and 
certification as an essential paradigm for their work. CPP spans 
topics in computer science, mathematics, logic, and education. CPP 
2024 will be held on 15-16 January 2024 in London, UK. The conference is co-located with POPL 2024, and is sponsored by ACM SIGPLAN in cooperation with ACM SIGLOG.",2024,982
Monitoring Screen Time or Redesigning It? Two Approaches to Supporting Intentional Social Media Use,"Zhang, Mingrui Ray and Lukoff, Kai and Rao, Raveena and Baughan, Amanda and Hiniker, Alexis","Existing designs helping people manage their social media use include: 1) external supports that monitor and limit use; 2) internal supports that change the interface itself. Here, we design and deploy Chirp, a mobile Twitter client, to independently examine how users experience external and internal supports. To develop Chirp, we identified 16 features that influence users’ sense of agency on Twitter through a survey of 129 participants and a design workshop. We then conducted a four-week within-subjects deployment with 31 participants. Our internal supports (including features to filter tweets and inform users when they have exhausted new content) significantly increased users’ sense of agency, while our external supports (a usage dashboard and nudges to close the app) did not. Participants valued our internal supports and said that our external supports were for “other people.” Our findings suggest that design patterns promoting agency may serve users better than screen time tools.",2022,983
ICIEI '22: Proceedings of the 7th International Conference on Information and Education Innovations,,,2022,984
Predicting Decisions in Language Based Persuasion Games,"Apel, Reut and Erev, Ido and Reichart, Roi and Tennenholtz, Moshe","Sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. However, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. This paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions.
For this purpose, we conduct an online repeated interaction experiment. At each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. While the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. The expert’s payoff, in turn, depends on the number of times the decision-maker chooses the hotel. We also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews’ text, and observe substantial differences which can be explained through an equilibrium analysis of the game.
We consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network (DNN) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (DNN-based vs. hand-crafted). Our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. Further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup.",2022,985
Computational challenges in modeling &amp; simulation of complex systems,"Fujimoto, Richard M. and Carothers, Christopher and Ferscha, Alois and Jefferson, David and Loper, Margaret and Marathe, Madhav and Taylor, Simon J. E.","Modeling and simulation faces many new computational challenges in the design of complex engineered systems. The systems that need to be modeled are increasingly interconnected and interdependent, achieving unprecedented levels of complexity. The computational platforms upon which simulations execute have undergone dramatic changes in recent years. Position statements by leading researchers are presented concerning important computational challenges and opportunities facing the M&amp;S community.",2017,986
C&amp;T '23: Proceedings of the 11th International Conference on Communities and Technologies,,,2023,987
SSOLogin: A framework for automated web privacy measurement with SSO logins,"Pham, Tien-Huy and Vo, Quoc-Huy and Dao, Ha and Fukuda, Kensuke","Single Sign-On (SSO) enables users to access multiple websites and applications using a single set of login credentials. Undoubtedly, SSO makes it easy for users to log in to multiple websites without remembering credentials. However, it is also important to consider the potential risk of users being unaware of how the identity of their account will be utilized, the development of online tracking techniques, and any potential exchange of information with third parties without the user’s knowledge. In this paper, we propose SSOLogin, which enables us to perform large-scale automation of website login through an SSO account. We confirm that SSOLogin automatically logins to 91.8% of SSO account available websites in Tranco Top 500 sites. Next, by crawling Tranco Top 10K websites with SSOLogin, we show that 1,420 sites (14.2%) contain SSO login (Google and Facebook) as of July 2023, primarily on Information Technology and News/Media websites in the United States and the United Kingdom. We then shed light on the characteristics of privacy leakage of websites with SSO logins by setting up measurements in Japan. We find that 99% of websites have third-party online tracking activities, which may pose risks to user privacy. After SSO login, the target website shows an increase in third-party tracking domains. Logging in with Google adds 81 new tracking domains, while Facebook adds 33 new domains. Despite the convenience of logging in with an SSO account, it is important to be aware of the potential privacy risks associated with this practice.",2023,988
Extracting Threat Intelligence From Cheat Binaries For Anti-Cheating,"Anwar, Md Sakib and Zuo, Chaoshun and Yagemann, Carter and Lin, Zhiqiang","Rampant cheating remains a serious concern for game developers who fear losing loyal customers and revenue. While numerous anti-cheating techniques have been proposed, cheating persists in a vibrant (and profitable) illicit market. Inspired by novel insights into the economics behind cheat development and recent techniques for defending against advanced persistent threats (APTs), we propose a fully automated methodology for extracting “cheat intelligence” from widely distributed cheat binaries to produce a “memory access graph” that guides selective data randomization to yield immune game clients. We have implemented a prototype system for Android and Windows games, CheatFighter, and evaluated it on 86 cheats collected from a variety of real-world sources, including Telegram channels and online forums. CheatFighter successfully counteracts 80 of the real-world cheats in under a minute, demonstrating practical end-to-end protection against widespread cheating.",2023,989
Investigations of Top-Level Domain Name Collisions in Blockchain Naming Services,"Ito, Daiki and Takata, Yuta and Kumagai, Hiroshi and Kamizono, Masaki","Traditionally, top-level domains (TLDs) are managed by the Internet corporation for assigned names and numbers (ICANN), and the domain names under them are managed by registrars. Against such a centralized management, a blockchain naming service (BNS) has been proposed to manage TLDs on blockchains without authority intervention. BNS users can register TLD strings as non-fungible tokens and manage the TLD root zone. However, such a decentralized management results in the introduction of a new security issue, BNS TLD name collision, wherein the same TLD is registered in several different BNSs. In this study, we investigated BNS TLD name collisions by analyzing TLDs registered on two BNSs: Handshake and Decentraweb. Specifically, we collected TLDs registered in Handshake and Decentraweb and the associated data, and analyzed the data registration status of BNS TLDs and BNS TLD name collisions. The analysis of 11,595,406 Handshake and 11,889 Decentraweb TLDs revealed 6,973 BNS TLD name collisions. In particular, lastname TLDs, which are intended for use as person names, yielded a large number of registered domain names. In addition, the analysis identified 10 name collisions between the BNS and operational ICANN TLDs. Further, the ICANN TLD candidates under review also had name collisions against the BNS TLDs. Consequently, based on the characteristics of these name collisions and discussions in BNS communities, we considered countermeasures against BNS TLD name collisions. For the further development of BNSs, we believe that it is essential to discuss with the existing Internet communities and coexist with the existing Internet.",2024,990
Designing User Interface Elements to Improve the Quality and Civility of Discourse in Online Commenting Behaviors,"Seering, Joseph and Fang, Tianmi and Damasco, Luca and Chen, Mianhong 'Cherie' and Sun, Likang and Kaufman, Geoff","Ensuring high-quality, civil social interactions remains a vexing challenge in many online spaces. In the present work, we introduce a novel approach to address this problem: using psychologically ""embedded'' CAPTCHAs containing stimuli intended to prime positive emotions and mindsets. An exploratory randomized experiment (N = 454 Mechanical Turk workers) tested the impact of eight new CAPTCHA designs implemented on a simulated, politically charged comment thread. Results revealed that the two interventions that were the most successful at activating positive affect also significantly increased the positivity of tone and analytical complexity of argumentation in participants' responses. A focused follow-up experiment (N = 120 Mechanical Turk workers) revealed that exposure to CAPTCHAs featuring image sets previously validated to evoke low-arousal positive emotions significantly increased the positivity of sentiment and the levels of complexity and social connectedness in participants' posts. We offer several explanations for these results and discuss the practical and ethical implications of designing interfaces to influence discourse in online forums.",2019,991
CODASPY '24: Proceedings of the Fourteenth ACM Conference on Data and Application Security and Privacy,,"It is our great pleasure to welcome you to the fourteenth edition of the ACM Conference on Data and Application Security and Privacy (CODASPY 2024), for the first time held outside United States of America. This conference series has been founded to foster novel and exciting research in the data and application security and privacy arena and to help generate new directions for further research and development. The initial concept was established by the two co-founders, Elisa Bertino and Ravi Sandhu, and sharpened by subsequent discussions with several fellow data security and privacy researchers. Their enthusiastic encouragement persuaded the co-founders to move ahead with the always daunting task of creating a high-quality conference. CODASPY has become a leading forum for presentation of research results and experience reports on hardware and software security. The conference gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of data and applications security and privacy.",2024,992
ASSE '23: Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference,,,2023,993
CPSS '23: Proceedings of the 9th ACM Cyber-Physical System Security Workshop,,,2023,994
ICMLC '23: Proceedings of the 2023 15th International Conference on Machine Learning and Computing,,,2023,995
Exposing Bot Attacks Using Machine Learning and Flow Level Analysis,"Faek, Rana and Al-Fawa'reh, Mohammad and Al-Fayoumi, Mustafa","Botnets represent a major threat to Internet security that have continuously developed in scale and complexity. Command-and-control servers (C&amp;C) send commands to bots that execute and perform these commands, thereby implementing attacks such as distributed denial-of-service (DDoS), spam campaigns, or the scanning of compromised hosts. The detection of volumetric attacks in large and complex networks requires an efficient mechanism. Botnet behavior should be analyzed in order to save the network from attack, and preventive measures should be implemented in time. Anomalous botnet tracking strategies are more efficient than signature-based ones, since botnet detection methods rely on anomalies and do not need pre-constructed botnet signatures, therefore they can detect new or unidentified botnets. We use Netflow and machine learning algorithms in this paper to also improve the detection process for intrusion detection algorithms with a novel dataset. We implemented a number of algorithms in our lightweight model to show that Random Forests get the highest accuracy for the algorithms used.",2021,996
Impact of developer reputation on code review outcomes in OSS projects: an empirical investigation,"Bosu, Amiangshu and Carver, Jeffrey C.","&lt;u&gt;Context:&lt;/u&gt; Gaining an identity and building a good reputation are important motivations for Open Source Software (OSS) developers. It is unclear whether these motivations have any actual impact on OSS project success. &lt;u&gt;Goal:&lt;/u&gt; To identify how an OSS developer's reputation affects the outcome of his/her code review requests. &lt;u&gt;Method:&lt;/u&gt; We conducted a social network analysis (SNA) of the code review data from eight popular OSS projects. Working on the assumption that core developers have better reputation than peripheral developers, we developed an approach, Core Identification using K-means (CIK) to divide the OSS developers into core and periphery groups based on six SNA centrality measures. We then compared the outcome of the code review process for members of the two groups. &lt;u&gt;Results:&lt;/u&gt; The results suggest that the core developers receive quicker first feedback on their review request, complete the review process in shorter time, and are more likely to have their code changes accepted into the project codebase. Peripheral developers may have to wait 2 - 19 times (or 12 - 96 hours) longer than core developers for the review process of their code to complete. &lt;u&gt;Conclusion:&lt;/u&gt; We recommend that projects allocate resources or create tool support to triage the code review requests to motivate prospective developers through quick feedback.",2014,997
BOTS: a constraint-based component system for synthesizing scalable software systems,"Pandey, Raju and Wu, Jeffrey","Embedded application developers create applications for a wide range of devices with different resource constraints. Developers want to maximize the use of the limited resources available on the device while still not exceeding the capabilities of the device. To do this, the developer must be able to scale his software for different platforms. In this paper, we present a software engineering methodology that automatically scales software to different platforms. We intend to have the application developer write high level functional specifications of his software and have tools that automatically scale the underlying runtime. These tools will use the functional and non-functional constraints of both the hardware and client application to produce an appropriate runtime. Our initial results show that the proposed approach can scale operating systems and virtual machines that satisfy the constraints of varying hardware/application combinations.",2006,998
"Analysis of a ""/0"" stealth scan from a botnet","Dainotti, Alberto and King, Alistair and Claffy, kc and Papale, Ferdinando and Pescap\`{e}, Antonio","Botnets are the most common vehicle of cyber-criminal activity. They are used for spamming, phishing, denial of service attacks, brute-force cracking, stealing private information, and cyber warfare. Botnets carry out network scans for several reasons, including searching for vulnerable machines to infect and recruit into the botnet, probing networks for enumeration or penetration, etc. We present the measurement and analysis of a horizontal scan of the entire IPv4 address space conducted by the Sality botnet in February of last year. This 12-day scan originated from approximately 3 million distinct IP addresses, and used a heavily coordinated and unusually covert scanning strategy to try to discover and compromise VoIP-related (SIP server) infrastructure. We observed this event through the UCSD Network Telescope, a /8 darknet continuously receiving large amounts of unsolicited traffic, and we correlate this traffic data with other public sources of data to validate our inferences. Sality is one of the largest botnets ever identified by researchers, its behavior represents ominous advances in the evolution of modern malware: the use of more sophisticated stealth scanning strategies by millions of coordinated bots, targeting critical voice communications infrastructure. This work offers a detailed dissection of the botnet's scanning behavior, including general methods to correlate, visualize, and extrapolate botnet behavior across the global Internet.",2012,999
Work-to-rule: the emergence of algorithmic governance in Wikipedia,"M\""{u}ller-Birn, Claudia and Dobusch, Leonhard and Herbsleb, James D.","Research has shown the importance of a functioning governance system for the success of peer production communities. It particularly highlights the role of human coordination and communication within the governance regime. In this article, we extend this line of research by differentiating two categories of governance mechanisms. The first category is based primarily on communication, in which social norms emerge that are often formalized by written rules and guidelines. The second category refers to the technical infrastructure that enables users to access artifacts, and that allows the community to communicate and coordinate their collective actions to create those artifacts. We collected qualitative and quantitative data from Wikipedia in order to show how a community's consensus gradually converts social mechanisms into algorithmic mechanisms. In detail, we analyze algorithmic governance mechanisms in two embedded cases: the software extension ""flagged revisions"" and the bot ""xqbot"". Our insights point towards a growing relevance of algorithmic governance in the realm of governing large-scale peer production communities. This extends previous research, in which algorithmic governance is almost absent. Further research is needed to unfold, understand, and also modify existing interdependencies between social and algorithmic governance mechanisms.",2013,1000
Portable DoS Test Tool for IoT Devices,"Nagara, Keigo and Aoki, Katsunori and Matsubara, Yutaka and Takada, Hiroaki","In the recent years, internet-of-things (IoT) devices have attracted an increasing share of attention, and the vulnerability of IoT devices has been clarified. For example, the IoT malware {¥it Mirai} constructs a bot using the vulnerability of IoT equipment embedded with Linux and exploits it for distributed denial of service (DDoS) attacks. Meanwhile, as reported in papers, examples of denial of service (DoS) attacks targeting IoT/embedded devices have emerged. Therefore, the DoS test at the stage of product design and development stage is very important. We then created an open source software (OSS) based portable DoS test tool for IoT devices.",2017,1001
A theory of the engagement in open source projects via summer of code programs,"Silva, Jefferson and Wiese, Igor and German, Daniel M. and Treude, Christoph and Gerosa, Marco Aur\'{e}lio and Steinmacher, Igor","Summer of code programs connect students to open source software (OSS) projects, typically during the summer break from school. Analyzing consolidated summer of code programs can reveal how college students, who these programs usually target, can be motivated to participate in OSS, and what onboarding strategies OSS communities adopt to receive these students. In this paper, we study the well-established Google Summer of Code (GSoC) and devise an integrated engagement theory grounded in multiple data sources to explain motivation and onboarding in this context. Our analysis shows that OSS communities employ several strategies for planning and executing student participation, socially integrating the students, and rewarding student’s contributions and achievements. Students are motivated by a blend of rewards, which are moderated by external factors. We presented these rewards and the motivation theory to students who had never participated in a summer of code program and collected their shift in motivation after learning about the theory. New students can benefit from the former students' experiences detailed in our results, and OSS stakeholders can leverage both the insight into students’ motivations for joining such programs as well as the onboarding strategies we identify to devise actions to attract and retain newcomers.",2020,1002
Social network analysis in open source software peer review,"Yang, Xin","Software peer review (aka. code review) is regarded as one of the most important approaches to keep software quality and productivity. Due to the distributed collaborations and communication nature of Open Source Software (OSS), OSS review differs from traditional industry review. Unlike other related works, this study investigated OSS peer review pro- cesses from social perspective by using social network anal- ysis (SNA). We analyzed the review history from three typi- cal OSS projects. The results provide hints on relationships among the OSS reviewers which can help to understand how developers work and communicate with each other.",2014,1003
Corporate dominance in open source ecosystems: a case study of OpenStack,"Zhang, Yuxia and Stol, Klaas-Jan and Liu, Hui and Zhou, Minghui","Corporate participation plays an increasing role in Open Source Software (OSS) development. Unlike volunteers in OSS projects, companies are driven by business objectives. To pursue corporate interests, companies may try to dominate the development direction of OSS projects. One company's domination in OSS may 'crowd out' other contributors, changing the nature of the project, and jeopardizing the sustainability of the OSS ecosystem. Prior studies of corporate involvement in OSS have primarily focused on predominately positive aspects such as business strategies, contribution models, and collaboration patterns. However, there is a scarcity of research on the potential drawbacks of corporate engagement. In this paper, we investigate corporate dominance in OSS ecosystems. We draw on the field of Economics and quantify company domination using a dominance measure; we investigate the prevalence, patterns, and impact of domination in the evolution of the OpenStack ecosystem. We find evidence of company domination in over 73% of the repositories in OpenStack, and approximately 25% of companies dominate one or more repositories per version. We identify five patterns of corporate dominance: Early incubation, Full-time hosting, Growing domination, Occasional domination, and Last remaining. We find that domination has a significantly negative relationship with the survival probability of OSS projects. This study provides insights for building sustainable relationships between companies and the OSS ecosystems in which they seek to get involved.",2022,1004
Factors in the performance of the AN1 computer network,"Owicki, Susan S. and Karlin, Anna R.","AN1 (formerly known as Autonet) is a local area network composed of crossbar switches interconnected by 100Mbit/second, full-duplex links. In this paper, we evaluate the performance impact of certain choices in the AN1 design. These include the use of FIFO input buffering in the crossbar switch, the deadlock-avoidance mechanism, cut-through routing, back-pressure for flow control, and multi-path routing. AN1's performance goals were to provide low latency and high bandwidth in a lightly loaded network. In this it is successful. Under heavy load, the most serious impediment to good performance is the use of FIFO input buffers. The deadlock-avoidance technique has an adverse effect on the performance of some topologies, but it seems to be the best alternative, given the goals and  constraints of the AN1 design. Cut-through switching performs well relative to store-and-forward switching, even under heavy load. Back-pressure deals adequately with congestion in a lightly-loaded network; under moderate load, performance is acceptable when coupled with end-to-end flow control for bursts. Multi-path routing successfully exploits redundant paths between hosts to improve performance in the face of congestion.",1992,1005
Improving code review effectiveness through reviewer recommendations,"Thongtanunam, Patanamon and Kula, Raula Gaikovina and Cruz, Ana Erika Camargo and Yoshida, Norihiro and Iida, Hajimu","Effectively performing code review increases the quality of software and reduces occurrence of defects. However, this requires reviewers with experiences and deep understandings of system code. Manual selection of such reviewers can be a costly and time-consuming task. To reduce this cost, we propose a reviewer recommendation algorithm determining file path similarity called FPS algorithm. Using three OSS projects as case studies, FPS algorithm was accurate up to 77.97%, which significantly outperformed the previous approach.",2014,1006
Experience: advanced network operations in (Un)-connected remote communities,"Perino, Diego and Yang, Xiaoyuan and Serra, Joan and Lutu, Andra and Leontiadis, Ilias","The Internet Para Todos program is working to provide sustainable mobile broadband to 100 M unconnected people in Latin America. In this paper we present our commercial deployment in thousands remote small communities and describe the unique experience of maintaining this infrastructure. We describe the challenges related to managing operations containing the cost in these extreme geographical conditions. We also analyze operational data to understand outage patterns and present typical operational issues in this unique remote community environment. Finally, we present an extension of the operations support system (OSS) leveraging advanced analytics and machine learning with the goal of optimizing network maintenance while reducing costs.",2020,1007
Mining Decision-Making Processes in Open Source Software Development: A Study of Python Enhancement Proposals (PEPs) using Email Repositories,"Sharma, Pankajeshwara and Savarimuthu, Bastin Tony Roy and Stanger, Nigel","Open source software (OSS) communities are often able to produce high quality software comparable to proprietary software. The success of an open source software development (OSSD) community is often attributed to the underlying governance model, and a key component of these models is the decision-making (DM) process. While there have been studies on the decision-making processes publicized by OSS communities (e.g., through published process diagrams), little has been done to study decision-making processes that can be extracted using a bottom-up, data-driven approach, which can then be used to assess whether the publicized processes conform to the extracted processes. To bridge this gap, we undertook a large-scale data-driven study to understand how decisions are made in an OSSD community, using the case study of Python Enhancement Proposals (PEPs), which embody decisions made during the evolution of the Python language. Our main contributions are:(a) the design and development of a framework using information retrieval and natural language processing techniques to analyze the Python email archives (comprising 1.48 million emails), and(b) the extraction of decision-making processes that reveal activities that are neither explicitly mentioned in documentation published by the Python community nor identified in prior research work. Our results provide insights into the actual decision-making process employed by the Python community.",2020,1008
A Systematic Review of Computational Thinking Approach for Programming Education in Higher Education Institutions,"Agbo, Friday Joseph and Oyelere, Solomon Sunday and Suhonen, Jarkko and Adewumi, Sunday","This study examined how computational thinking (CT) has been used to teach problem-solving skills and programming education in the recent past. This study specifically (i) identified articles that discussed CT approach for programming education at higher education institutions (HEIs), (ii) classified the different CT approaches and tools employed for programming education at HEIs, (iii) synthesised and discussed results that are reported by relevant studies that utilized CT for teaching programming at HEIs. A systematic literature review methodology was adopted in this study. Out of 161 articles retrieved, 33 of them that met the inclusion criteria were reviewed. Our study revealed that the use of CT at HEIs for programming education began in 2010; many studies did not specify the context of use, but the use of CT is found to be gaining grounds in many contexts, especially the developed countries; course design approach was mostly employed by educators to introduce CT at HEIs for programming education. Furthermore, this study pointed out how CT approach can be explored for designing a smart learning environment to support students in learning computer programming.",2019,1009
High speed switch scheduling for local area networks,"Anderson, Thomas E. and Owicki, Susan S. and Saxe, James B. and Thacker, Charles P.",,1992,1010
Hardware/software IP integration using the ROSES design environment,"Wagner, Fl\'{a}vio R. and Ces\'{a}rio, Wander and Jerraya, Ahmed A.","Considering current time-to-market pressures, IP reuse is mandatory for the design of complex embedded systems-on-chip (SoC). The integration of IP components into a given design is the most complex task in the whole reuse process. This paper describes the IP integration approach implemented in the ROSES design environment, which presents a unique combination of features that enhance IP reuse: automatic assembly of interfaces between heterogeneous software and hardware IP components; easy adaptation to different on-chip communication structures and bus and core standards; generation of customized and minimal OSs for programmable components; and an architecture-independent high-level API embedded into SystemC that makes application software independent from system implementation. Application code is written by using communication functions available in this API. ROSES automatically assembles wrappers that implement these functions, such that the application code does not need to be modified in order to run in the final synthesized system.",2007,1011
Robustly secure computer systems: a new security paradigm of system discontinuity,"Solworth, Jon A.","For over 30 years, system software has been bound by compatibility with legacy applications. The system software base, whether proprietary or open source, is dominated by the programming language C and the POSIX operating system specification. Even when commercial operating systems stray from this model, they don't go very far.Unfortunately, the POSIX/C base was constructed in a more benign environment than today and before many security issues were widely understood. Rather than fix these issues, compatibility has been deemed more important than security, and so this base has been kept intact with all its flaws. As a result, programmers routinely create software with security holes---even in the most security critical software---and today's systems are easily attacked.We propose a new paradigm of system discontinuity which emphasizes security over compatibility by removing those constructs in our system software which lead to security holes in applications. Of course, removing parts of the interface will break applications, and hence the discontinuity. To deal with this situation, we advocate the use of virtual machines to enable multiple operating systems to run concurrently. Thus high security OSs can be used for the most security sensitive applications. Compatibility is maintained for less security sensitive applications using legacy operating systems. Over time, legacy applications can migrate to a more secure OS, thus raising the security of all applications.",2008,1012
Why Security Defects Go Unnoticed during Code Reviews? A Case-Control Study of the Chromium OS Project,"Paul, Rajshakhar and Turzo, Asif Kamal and Bosu, Amiangshu","Peer code review has been found to be effective in identifying security vulnerabilities. However, despite practicing mandatory code reviews, many Open Source Software (OSS) projects still encounter a large number of post-release security vulnerabilities, as some security defects escape those. Therefore, a project manager may wonder if there was any weakness or inconsistency during a code review that missed a security vulnerability. Answers to this question may help a manager pinpointing areas of concern and taking measures to improve the effectiveness of his/her project's code reviews in identifying security defects. Therefore, this study aims to identify the factors that differentiate code reviews that successfully identified security defects from those that missed such defects.With this goal, we conduct a case-control study of Chromium OS project. Using multi-stage semi-automated approaches, we build a dataset of 516 code reviews that successfully identified security defects and 374 code reviews where security defects escaped. The results of our empirical study suggest that the are significant differences between the categories of security defects that are identified and that are missed during code reviews. A logistic regression model fitted on our dataset achieved an AUC score of 0.91 and has identified nine code review attributes that influence identifications of security defects. While time to complete a review, the number of mutual reviews between two developers, and if the review is for a bug fix have positive impacts on vulnerability identification, opposite effects are observed from the number of directories under review, the number of total reviews by a developer, and the total number of prior commits for the file under review.",2021,1013
Hershel: Single-Packet OS Fingerprinting,"Shamsi, Zain and Nandwani, Ankur and Leonard, Derek and Loguinov, Dmitri and Shamsi, Zain and Nandwani, Ankur and Leonard, Derek and Loguinov, Dmitri","Traditional TCP/IP fingerprinting tools e.g., nmap are poorly suited for Internet-wide use due to the large amount of traffic and intrusive nature of the probes. This can be overcome by approaches that rely on a single SYN packet to elicit a vector of features from the remote server. However, these methods face difficult classification problems due to the high volatility of the features and severely limited amounts of information contained therein. Since these techniques have not been studied before, we first pioneer stochastic theory of single-packet OS fingerprinting, build a database of 116 OSs, design a classifier based on our models, evaluate its accuracy in simulations, and then perform OS classification of 37.8M hosts from an Internet-wide scan.",2016,1014
Dynamic Analysis of Web Objects,"Weldemariam, Komminist and Shahriar, Hossain and Devendran, VamsheeKrishna","Various reports show that web browsers are known for being insecure, with growing amount of flaws that make them vulnerable to various attacks. Such attacks can be used to execute arbitrary procedures on the victims' computer and silently install malicious software, turning them into bots. In addition, browsers are complex and typically incorporate third-party libraries installed on-demand. This makes it very difficult for security experts to analyze the causes of such flaws or devise countermeasures. In this paper, we present an approach to detect and prevent attacks against a browser by intercepting the interactions between its core libraries and the underlying operating system. We then build mathematical models that capture the behavior of the browser during the rendering of web objects. Finally, we show that such models can be leveraged to automatically classify web objects as malicious or benign using real-world malicious websites.",2014,1015
Articulations of wikiwork: uncovering valued work in wikipedia through barnstars,"Kriplean, Travis and Beschastnikh, Ivan and McDonald, David W.","Successful online communities have complex cooperative arrangements, articulations of work, and integration practices. They require technical infrastructure to support a broad division of labor. Yet the research literature lacks empirical studies that detail which types of work are valued by participants in an online community. A content analysis of Wikipedia barnstars -- personalized tokens of appreciation given to participants -- reveals a wide range of valued work extending far beyond simple editing to include social support, administrative actions, and types of articulation work. Our analysis develops a theoretical lens for understanding how wiki software supports the creation of articulations of work. We give implications of our results for communities engaged in large-scale collaborations.",2008,1016
Request for comments: conversation patterns in issue tracking systems of open-source projects,"Rath, Michael and M\""{a}der, Patrick","Issue tracking systems play an important role in developing software systems. They provide a central place to store and maintain different development artifacts. Various studies are concerned with the contained bug reports, features, the relations among them and traces to the projects code base. However, an issue tracker can also be used as a communication channel between project contributors by attaching comments to issues. Less is known on how users actually utilize this functionality. In this paper, we study more than 270,000 comments from twelve open-source projects. We analyze to what extend comments are used and then study the structure occurring in threads of comments. Based on the order of comments and participating contributors, we identified three patterns of conversation: monolog, feedback, and collaboration. Our results show that most conversations are collaborations among two or more developers discussing the issue.",2020,1017
Pros and cons of operating system transactions for data base systems,"Weikum, Gerhard",,1986,1018
A Systematic Mapping Study of the Onboarding Process in Software Development Organizations,"Perp\'{e}tua, Sueli Pereira and Vieira, S\'{a}vio Luiz and Portela, Carlos dos Santos and Souza, Maur\'{\i}cio Ronny de Almeida","The onboarding process refers to the organizational integration or socialization of new employees to the company and their role. This includes training, orientation, support, and communication, seeking to help newcomers to adapt to the new culture and work environment. Since software development is a human activity that requires collaboration and teamwork, especially in the professional environment, onboarding is a critical process to ensure the proper integration of new employees in software projects. Thus, this paper aims to identify the main approaches, practices, trends, and challenges related to the onboarding process in the Software Engineering context. We conducted a Systematic Mapping Study, which resulted in the selection of 23 primary studies. The main findings include the observation that onboarding process is context-sensitive, affected by characterisctics such as organization size, work model, and development methodologies. We also found that there is a lack of studies evaluating models and tools. Finally, there is a heavy focus on oboarding activities related to learning and socialization, in contrast to developing the confidence of newcomers.",2023,1019
Studying and Improving Software License Compliance in Practice,"Wintersgill, Nathan","As the process of software development has matured, the reuse of open-source software has become commonplace. Open-source software licenses can both provide permissions and impose restrictions regarding software's distribution, modification, and reuse. Modern systems can have many such licensed components, complicating the task of license compliance and compounding the risk associated with reusing open source components. To address these issues, this dissertation seeks to identify weaknesses in current processes and automated tools, such as in handling license conflicts, exceptions, and interpretations, in order to develop new compliance tools and resources grounded in the realities of software compliance as revealed by software developers and legal practitioners.",2024,1020
Using Virtual Machine Introspection for Operating Systems Security Education,"Bhatt, Manish and Ahmed, Irfan and Lin, Zhiqiang","Historically, hands-on cybersecurity exercises helped reinforce the basic cybersecurity concepts. However, most of them focused on the user level attacks and defenses and did not provide a convenient way of studying the kernel level security. Since OS kernels provide foundations for applications, any compromise to OS kernels will lead to a computer that cannot be trusted. Moreover, there has been a great interest in using virtualization to profile, characterize, and observe kernel events including security incidents. Virtual Machine Introspection (VMI) is a technique that has been deeply investigated in intrusion detection, malware analysis, and memory forensics. Inspired by the great success of VMI, we used it to develop hands-on labs for teaching kernel level security. In this work, we present three VMI-based labs on (1) stack-based buffer over-flow, (2) direct kernel object manipulation (DKOM), and (3) kernel integrity checker which have been made available online. Then, we analyze the differences in approaches taken by VMI-based labs and traditional labs and conclude that VMI-based labs are better as opposed to traditional labs from a teaching standpoint because they provide more visibility than the traditional labs and superior ability to manipulate kernel memory which provides more insight into kernel security concepts.",2018,1021
Record and replay for Android: are we there yet in industrial cases?,"Lam, Wing and Wu, Zhengkai and Li, Dengfeng and Wang, Wenyu and Zheng, Haibing and Luo, Hui and Yan, Peng and Deng, Yuetang and Xie, Tao","Mobile applications, or apps for short, are gaining popularity. The input sources (e.g., touchscreen, sensors, transmitters) of the smart devices that host these apps enable the apps to offer a rich experience to the users, but these input sources pose testing complications to the developers (e.g., writing tests to accurately utilize multiple input sources together and be able to replay such tests at a later time). To alleviate these complications, researchers and practitioners in recent years have developed a variety of record-and-replay tools to support the testing expressiveness of smart devices. These tools allow developers to easily record and automate the replay of complicated usage scenarios of their app. Due to Android's large share of the smart-device market, numerous record-and-replay tools have been developed using a variety of techniques to test Android apps. To better understand the strengths and weaknesses of these tools, we present a comparison of popular record-and-replay tools from researchers and practitioners, by applying these tools to test three popular industrial apps downloaded from the Google Play store. Our comparison is based on three main metrics: (1) ability to reproduce common usage scenarios, (2) space overhead of traces created by the tools, and (3) robustness of traces created by the tools (when being replayed on devices with different resolutions). The results from our comparison show which record-and-replay tools may be the best for developers and identify future directions for improving these tools to better address testing complications of smart devices.",2017,1022
May I help you? Design of Human-like Polite Approaching Behavior,"Kato, Yusuke and Kanda, Takayuki and Ishiguro, Hiroshi","When should service staff initiate interaction with a visitor? Neither simply-proactive (e.g. talk to everyone in a sight) nor passive (e.g. wait until being talked to) strategies are desired. This paper reports our modeling of polite approaching behavior. In a shopping mall, there are service staff members who politely approach visitors who need help. Our analysis revealed that staff members are sensitive to ""intentions"" of nearby visitors. That is, when a visitor intends to talk to a staff member and starts to approach, the staff member also walks a few steps toward the visitors in advance to being talked. Further, even when not being approached, staff members exhibit ""availability"" behavior in the case that a visitor's intention seems uncertain. We modeled these behaviors that are adaptive to pedestrians' intentions, occurred prior to initiation of conversation. The model was implemented into a robot and tested in a real shopping mall. The experiment confirmed that the proposed method is less intrusive to pedestrians, and that our robot successfully initiated interaction with pedestrians.",2015,1023
Touchscreen Device Size Suitable for Icon Search by Blind Users,"Asakura, Rei and Watanabe, Tetsuya","Blind smartphones users are on the rise and they need to know which device is easy to use for them. Thus, to explore the optimal touchscreen device size for blind people, we conducted an experiment in which six blind participants searched for the target icons with four different-sized touchscreen devices. We analyzed the search time, search strategies and subjective evaluations. As a result, devices with a screen size of 4.7 inches had the shortest search time and obtained the highest subjective evaluation among the four devices. This finding is useful for blind people at the stage of purchasing smartphones.",2017,1024
Embedded out-of-distribution detection on an autonomous robot platform,"Yuhas, Michael and Feng, Yeli and Ng, Daniel Jun Xian and Rahiminasab, Zahra and Easwaran, Arvind","Machine learning (ML) is actively finding its way into modern cyber-physical systems (CPS), many of which are safety-critical real-time systems. It is well known that ML outputs are not reliable when testing data are novel with regards to model training and validation data, i.e., out-of-distribution (OOD) test data. We implement an unsupervised deep neural network-based OOD detector on a real-time embedded autonomous Duckiebot and evaluate detection performance. Our OOD detector produces a success rate of 87.5% for emergency stopping a Duckiebot on a braking test bed we designed. We also provide case analysis on computing resource challenges specific to the Robot Operating System (ROS) middleware on the Duckiebot.",2021,1025
When the robot criticizes you...: self-serving bias in human-robot interaction,"You, Sangseok and Nie, Jiaqi and Suh, Kiseul and Sundar, S. Shyam","This study explores how human users respond to feedback and evaluation from a robot. A between-subjects experiment was conducted using the Wizard of Oz method, with 63 participants randomly assigned to one of three evaluations (good evaluation vs. neutral evaluation vs. bad evaluation) following a training session. When participants attempted to reproduce the physical motion taught by the robot, they were given a verbal evaluation of their performance by the robot. They showed a strong negative response to the robot when it gave a bad evaluation, while showing positive attraction when it gave a good or neutral evaluation. Participants tended to dismiss criticism from the robot and attributed blame to the robot, while claiming credit to themselves when their performance was rated positively. These results have theoretical implications for the psychology of self-serving bias and practical implications for designing and deploying trainer robots as well as conducting user studies of such robots.",2011,1026
Social capital increases efficiency of collaboration among Wikipedia editors,"Nemoto, Keiichi and Gloor, Peter and Laubacher, Robert","In this study we measure the impact of pre-existing social capital on the efficiency of collaboration among Wikipedia editors. To construct a social network among Wikipedians we look to mutual interaction on the user talk pages of Wikipedia editors. As our data set, we analyze the communication networks associated with 3085 featured articles - the articles of highest quality in the English Wikipedia, comparing it to the networks of 80154 articles of lower quality. As the metric to assess the quality of collaboration, we measure the time of quality promotion from when an article is started until it is promoted to featured article. The study finds that the higher pre-existing social capital of editors working on an article is, the faster the articles they work on reach higher quality status, such as featured articles. The more cohesive and more centralized the collaboration network, and the more network members were already collaborating before starting to work together on an article, the faster the article they work on will be promoted or featured.",2011,1027
"Developer turnover in global, industrial open source projects: insights from applying survival analysis","Lin, Bin and Robles, Gregorio and Serebrenik, Alexander","Large open source software projects often have a globally distributed development team. Studies have shown developer turnover has a significant impact on the project success. Frequent developer turnover may lead to loss of productivity due to lacking relevant knowledge and spending extra time learning how projects work. Thus, lots of attention has been paid to which factors are related to developer retention; however, few of them focus on the impact of activities of individual developers.In this paper, we study five open source projects from different organizations and examine whether developer turnover is affected by when they start contributing and what types of contributions they are making. Our study reveals that developers have higher chances to survive in software projects when they 1) start contributing to the project earlier; 2) mainly modify instead of creating files; 3) mainly code instead of dealing with documentations. Our results also shed lights on the potential approaches to improving developer retention.",2017,1028
Report of the International Workshop on Distributed Systems: operations &amp; management,,,1992,1029
A Comparative study of Open Source IDSs according to their Ability to Detect Attacks,"Bouziani, Ossama and Benaboud, Hafssa and Chamkar, Achraf Samir and Lazaar, Saiida","In this paper, we focus on the important role of intrusion detection systems for detecting unauthorized actions initiated from both internal and external network by collecting and monitoring network traffic. We give a study of the open source Next-Generation of IDS (SNORT, SURICATA, BRO). We test and compare their ability to detect attacks and performance by implementing the three IDSs individually.",2019,1030
An effective defense mechanism against DoS/DDoS attacks in flow-based routers,"Park, PyungKoo and Yi, HeeKyoung and Hong, SangJin and Ryu, JaeCheul","Due to proliferation of diverse network applications, DoS/DDoS attacks are evolving. Many studies have been performed and implemented in on/off-line network devices such as routers and IDS/IPS. While IDS/IPS is powerful enough to handle deep packet inspection (DPI) tasks, routers are better suited in real-time and line-speed processing requirements. Since the routers are designed to handle IP packet header information, if one can devise an DoS/DDoS detection/prevention methods that utilizes the router specific features it will be best for the in-line and real-time processing. We introduce a Flow based DoS/DDoS detection algorithm(FDDA) that detects Distributed Denial of Service (DDoS) attacks by monitoring TTL and ID fields of incoming packet's IP header. As DDoS attacks are based on IP source address spoofing, the TTL and ID fields may have abnormal behavior. The device keeps track of 8-tuple flow table. The behavior of these two fields is monitored to determine DoS/DDoS attack situation. The effectiveness of our method is such that it is implemented flow-based routers and devices.",2010,1031
Modeling the Centrality of Developer Output with Software Supply Chains,"Mockus, Audris and Rigby, Peter C. and Abreu, Rui and Suresh, Parth and Chen, Yifen and Nagappan, Nachiappan","Raw developer output, as measured by the number of changes a developer makes to the system, is simplistic and potentially misleading measure of productivity as new developers tend to work on peripheral and experienced developers on more central parts of the system. In this work, we use Software Supply Chain (SSC) networks and Katz centrality and PageRank on these networks to suggest a more nuanced measure of developer productivity. Our SSC is a network that represents the relationships between developers and artifacts that make up a system. We combine author-to-file, co-changing files, call hierarchies, and reporting structure into a single SSC and calculate the centrality of each node. The measures of centrality can be used to better understand variations in the impact of developer output at Meta. We start by partially replicating prior work and show that the raw number of developer commits plateaus over a project-specific period. However, the centrality of developer work grows for the entire period of study, but the growth slows after one year. This implies that while raw output might plateau, more experienced developers work on more central parts of the system. Finally, we investigate the incremental contribution of SSC attributes in modeling developer output. We find that local attributes such as the number of reports and the specific project do not explain much variation (𝑅2 = 5.8%). In contrast, adding Katz centrality or PageRank produces a model with an 𝑅2 above 30%. SSCs and their centrality provide valuable insights into the centrality and importance of a developer’s work.",2023,1032
Understanding code snippets in code reviews: a preliminary study of the OpenStack community,"Fu, Liming and Liang, Peng and Zhang, Beiqi","Code review is a mature practice for software quality assurance in software development with which reviewers check the code that has been committed by developers, and verify the quality of code. During the code review discussions, reviewers and developers might use code snippets to provide necessary information (e.g., suggestions or explanations). However, little is known about the intentions and impacts of code snippets in code reviews. To this end, we conducted a preliminary study to investigate the nature of code snippets and their purposes in code reviews. We manually collected and checked 10,790 review comments from the Nova and Neutron projects of the OpenStack community, and finally obtained 626 review comments that contain code snippets for further analysis. The results show that: (1) code snippets are not prevalently used in code reviews, and most of the code snippets are provided by reviewers. (2) We identified two high-level purposes of code snippets provided by reviewers (i.e., Suggestion and Citation) with six detailed purposes, among which, Improving Code Implementation is the most common purpose. (3) For the code snippets in code reviews with the aim of suggestion, around 68.1% was accepted by developers. The results highlight promising research directions on using code snippets in code reviews.",2022,1033
"Download malware? no, thanks: how formal methods can block update attacks","Mercaldo, Francesco and Nardone, Vittoria and Santone, Antonella and Visaggio, Corrado Aaron","In mobile malware landscape there are many techniques to inject malicious payload in a trusted application: one of the most common is represented by the so-called update attack. After an apparently innocuous application is installed on the victim's device, the user is asked to update the application, and a malicious behavior is added to the application. In this paper we propose a static method based on model checking able to identify this kind of attack. In addiction, our method is able to localize the malicious payload at method-level. We obtain an accuracy very close to 1 in identifying families implementing update attack using a real Android dataset composed by 2,581 samples.",2016,1034
Working for the Invisible Machines or Pumping Information into an Empty Void? An Exploration of Wikidata Contributors' Motivations,"Zhang, Charles Chuankai and Houtti, Mo and Smith, C. Estelle and Kong, Ruoyan and Terveen, Loren","Structured data peer production (SDPP) platforms like Wikidata play an important role in knowledge production. Compared to traditional peer production platforms like Wikipedia, Wikidata data is more structured and intended to be used by machines, not (directly) by people; end-user interactions with Wikidata often happen through intermediary ""invisible machines."" Given this distinction, we wanted to understand Wikidata contributor motivations and how they are affected by usage invisibility caused by the machine intermediaries. Through an inductive thematic analysis of 15 interviews, we find that: (i) Wikidata editors take on two archetypes---Architects who define the ontological infrastructure of Wikidata, and Masons who build the database through data entry and editing; (ii) the structured nature of Wikidata reveals novel editor motivations, such as an innate drive for organizational work; (iii) most Wikidata editors have little understanding of how their contributions are used, which may demotivate some. We synthesize these insights to help guide the future design of SDPP platforms in supporting the engagement of different types of editors.",2022,1035
Memorialising day-to-day content: bushfire affected communities,"Mori, Joji","Memorialising allows communities to commemorate or honour anything of significance, especially after tragic events which result in a large number of fatalities. This project will explore the use of day-to-day content of a digital nature to determine the role it can play in memorialising for communities. This could be content created of and by the deceas ed or survivors. It may also have been created be fore, during or after the disaster. Examples of relevant content could include digital photos, emails, mobile phone content and even social networking pages. The research approach will usea ""Black Saturday"" bus hfire affected township in Victoria. Australia as a vehicle to 1. develop and deploy a memorial and 2. explore the role this day-to-day content may have in memorialising for the community.",2010,1036
Towards Porting Operating Systems with Program Synthesis,"Hu, Jingmei and Lu, Eric and Holland, David A. and Kawaguchi, Ming and Chong, Stephen and Seltzer, Margo","The end of Moore’s Law has ushered in a diversity of hardware not seen in decades. Operating system (OS) (and system software) portability is accordingly becoming increasingly critical. Simultaneously, there has been tremendous progress in program synthesis. We set out to explore the feasibility of using modern program synthesis to generate the machine-dependent parts of an operating system. Our ultimate goal is to generate new ports automatically from descriptions of new machines.One of the issues involved is writing specifications, both for machine-dependent operating system functionality and for instruction set architectures. We designed two domain-specific languages: Alewife for machine-independent specifications of machine-dependent operating system functionality and Cassiopea for describing instruction set architecture semantics. Automated porting also requires an implementation. We developed a toolchain that, given an Alewife specification and a Cassiopea machine description, specializes the machine-independent specification to the target instruction set architecture and synthesizes an implementation in assembly language with a customized symbolic execution engine. Using this approach, we demonstrate the successful synthesis of a total of 140 OS components from two pre-existing OSes for four real hardware platforms. We also developed several optimization methods for OS-related assembly synthesis to improve scalability.The effectiveness of our languages and ability to synthesize code for all 140 specifications is evidence of the feasibility of program synthesis for machine-dependent OS code. However, many research challenges remain; we also discuss the benefits and limitations of our synthesis-based approach to automated OS porting.",2023,1037
On capturing malware dynamics in mobile power-law networks,"Bose, Abhijit and Shin, Kang G.","The increasing convergence of power-law networks such as social networking and peer-to-peer sites, web applications and mobile platforms makes today's users highly vulnerable to entirely new generations of malware that exploit vulnerabilities in web applications and mobile platforms for new infections, while using the power-law connectivity for finding new victims. The traditional epidemic models based on assumptions of homogeneity, averagedegree distributions, and perfect-mixing are inadequate to model this type of malware propagation. In this paper, we study three aspects crucial to modeling malware propagation in such environments: application-level interactions among users of such networks, local network structure, and user mobility.Since closed-form solutions of malware propagation in such environments are difficult to obtain, we describe an open-source, flexible agent-based emulation framework that can be used by malware researchers for studying today's complex malware. The framework, called Agent-Based Malware Modeling (AMM), allows different applications, network structure and user mobility in either a geographic or a logical domain to study various infection and propagation scenarios. The majority of the parameters used in the framework can be derived from real-life network traces collected from these networks, and therefore, represent realistic malware propagation and infection scenarios. As representative examples, we examine two well-known malware spreading mechanisms: (i) a malicious virus such as Cabir spreading among the subscribers of a cellular network using Bluetooth, and (ii) a hybrid worm that exploit email and file-sharing to infect users of a social network. In both cases, we identify the parameters most important to the spread of the epidemic based upon our extensive simulation results.",2008,1038
Text Mining-based Social-Psychological Vulnerability Analysis of Potential Victims To Cybergrooming: Insights and Lessons Learned,"Guo, Zhen and Wang, Pei and Cho, Jin-Hee and Huang, Lifu","Cybergrooming is a serious cybercrime that primarily targets youths through online platforms. Although reactive predator detection methods have been studied, proactive victim protection and crime prevention can also be achieved through vulnerability analysis of potential youth victims. Despite its significance, vulnerability analysis has not been thoroughly studied in the data science literature, while several social science studies used survey-based methods. To address this gap, we investigate humans’ social-psychological traits and quantify key vulnerability factors to cybergrooming by analyzing text features in the Linguistic Inquiry and Word Count (LIWC). Through pairwise correlation studies, we demonstrate the degrees of key vulnerability dimensions to cybergrooming from youths’ conversational features. Our findings reveal that victims have negative correlations with family and community traits, contrasting with previous social survey studies that indicated family relationships or social support as key vulnerability factors. We discuss the current limitations of text mining analysis and suggest cross-validation methods to increase the validity of research findings. Overall, this study provides valuable insights into understanding the vulnerability factors to cybergrooming and highlights the importance of adopting multidisciplinary approaches.",2023,1039
Harnessing the wisdom of crowds in wikipedia: quality through coordination,"Kittur, Aniket and Kraut, Robert E.","Wikipedia's success is often attributed to the large numbers of contributors who improve the accuracy, completeness and clarity of articles while reducing bias. However, because of the coordination needed to write an article collaboratively, adding contributors is costly. We examined how the number of editors in Wikipedia and the coordination methods they use affect article quality. We distinguish between explicit coordination, in which editors plan the article through communication, and implicit coordination, in which a subset of editors structure the work by doing the majority of it. Adding more editors to an article improved article quality only when they used appropriate coordination techniques and was harmful when they did not. Implicit coordination through concentrating the work was more helpful when many editors contributed, but explicit coordination through communication was not. Both types of coordination improved quality more when an article was in a formative stage. These results demonstrate the critical importance of coordination in effectively harnessing the ""wisdom of the crowd"" in online production environments.",2008,1040
A TOCTOU Attack on DICE Attestation,"Hristozov, Stefan and Wettermann, Moritz and Huber, Manuel","A major security challenge for modern IoT deployments is to ensure that the devices run legitimate firmware free from malware. This challenge can be addressed through a security primitive called attestation which allows a remote backend to verify the firmware integrity of the devices it manages. In order to accelerate broad attestation adoption in the IoT domain the Trusted Computing Group (TCG) has introduced the Device Identifier Composition Engine (DICE) series of specifications. DICE is a hardware-software architecture for constrained, e.g., microcontroller-based IoT devices where the firmware is divided into successively executed layers. In this paper, we demonstrate a remote Time-Of-Check Time-Of-Use (TOCTOU) attack on DICE-based attestation. We demonstrate that it is possible to install persistent malware in the flash memory of a constrained microcontroller that cannot be detected through DICE-based attestation. The main idea of our attack is to install malware during runtime of application logic in the top firmware layer. The malware reads the valid attestation key and stores it on the device's flash memory. After reboot, the malware uses the previously stored key for all subsequent attestations to the backend. We conduct the installation of malware and copying of the key through Return-Oriented Programming (ROP). As a platform for our demonstration, we use the Cortex-M-based nRF52840 microcontroller. We provide a discussion of several possible countermeasures which can mitigate the shortcomings of the DICE specifications.",2022,1041
Determining an organization's information requirements: a state of the art survey,"Yadav, Surya B.","The issue of information requirements of an organization and their specifications span two isolated territories. One territory is that of organization and management and the other belongs to technicians. There is a considerable gap between these two territories. Research in requirements engineering (technician's side) has primarily concentrated on designing and developing formal languages to document and analyze user requirements, once they have been determined. This research has ignored the organizational issues involved in information requirements determination. Research in the field of organization and management has addressed the organizational issues which affect information requirements of an organization. Various frameworks reported in the literature provide insights, but they cannot be considered as methods of determining requirements. Little work has been done on the process of determining requirements. This process must start with the understanding of an organization and end with a formal specification of information requirements. Here, it is worth emphasizing the fact that the process of determining and specifying information requirements of an organization is very different from the process of specifying design requirements of an information system. Therefore, program design methodologies, which are helpful in designing a system are not suitable for the process of determining and specifying information requirements of an organization.This paper discusses the state of the art in information requirements determination methodologies. Excluded are those methodologies which emphasize system design and have little to offer for requirements determination of an organization.",1983,1042
Automatic Stub Generation for Dynamic Symbolic Execution of ARM binary,"Nguyen, Anh T. V. and Ogawa, Mizuhito","Recently, dynamic symbolic execution (DSE) tools for binary codes (mostly for x86) have been developed, e.g., McVeto, CoDisasm, MAYHEM, KLEE-MC, BE-PUM, angr, BINSEC and CORANA. When a process stays in the uniform context, DSE simply obeys the formal semantics of an instruction set. Practical binary code like malware often uses system functions, which are beyond the user-level context, which requires stubs to simulate them. This paper proposed an automatic generation of Linux API Stub from the C function interface, which extends the DSE tool CORANA for ARM. The API stub is an under-approximation of a system function call by spawning its execution in the real Linux environment, and its Hoare logic deduction rule is defined and justified in terms of awareness. We implement CORANA/API and its experiments on real-world IoT malware, such as Mirai, is reported.",2022,1043
Sensors Know When to Interrupt You in the Car: Detecting Driver Interruptibility Through Monitoring of Peripheral Interactions,"Kim, SeungJun and Chun, Jaemin and Dey, Anind K.","Interruptions while driving can be quite dangerous, whether these are self-interruptions or external interruptions. They increase driver workload and reduce performance on the primary driving task. Being able to identify when a driver is interruptible is critical for building systems that can mediate these interruptions. In this paper, we collect sensor and human-annotated data from 15 drivers, including vehicle motion, traffic states, physiological responses and driver motion. We demonstrate that this data can be used to build a machine learning classifier that can determine interruptibility every second with a 94% accuracy. We present both population and individual models and discuss the features that contribute to the high performance of this system. Such a classifier can be used to build systems that mediate when drivers use technology to self-interrupt and when drivers are interrupted by technology.",2015,1044
Does the initial environment impact the future of developers?,"Zhou, Minghui and Mockus, Audris","Software developers need to develop technical and social skills to be successful in large projects. We model the relative sociality of developer as a ratio between the size of her communication network and the number of tasks she participates in. We obtain both measures from the problem tracking systems. We use her workflow peer network to represent her social learning, and the issues she has worked on to represent her technical learning. Using three open source and three traditional projects we investigate how the project environment reflected by the sociality measure at the time a developer joins, affects her future participation. We find: a) the probability that a new developer will become one of long-term and productive developers is highest when the project sociality is low; b) times of high sociality are associated with a higher intensity of new contributors joining the project; c) there are significant differences between the social learning trajectories of the developers who join in low and in high sociality environments; d) the open source and commercial projects exhibit different nature in the relationship between developer's tenure and the project's environment at the time she joins. These findings point out the importance of the initial environment in determining the future of the developers and may lead to better training and learning strategies in software organizations.",2011,1045
Evaluating artefacts with children: age and technology effects in the reporting of expected and experienced fun,"Read, Janet C.","In interaction design, there are several metrics used to gather user experience data. A common approach is to use surveys with the usual method being to ask users after they have experienced a product as to their opinion and satisfaction. This paper describes the use of the Smileyometer (a product from the Fun Toolkit) to test for user experience with children by asking for opinions in relation to expected as well as experienced fun.Two studies looked at the ratings that children, from two different age groups and in two different contexts, gave to a set of varied age-appropriate interactive technology installations. The ratings given before use (expectations) are compared with ratings given after use (experience) across the age groups and across installations.The studies show that different ratings were given for the different installations and that there were age-related differences in the use of the Smileyometer to rate user experience; these firstly evidence that children can, and do, discriminate between different experiences and that children do reflect on user experience after using technologies. In most cases, across both age groups, children expected a lot from the technologies and their after use (experienced) rating confirmed that this was what they had got.The paper concludes by considering the implications of the collective findings for the design and evaluation of technologies with children",2012,1046
United Universe: A Second Screen Transmedia Experience,"Eversman, Dillon and Major, Timothy and Tople, Mithila and Schaffer, Lauren and Murray, Janet","United Universe is a second screen transmedia experience aimed at supporting understanding of a complex storyworld presented across media artifacts. Using the highly interconnected and allusive Marvel Cinematic Universe as a primary example, United Universe abstracts a story into the fundamental elements of characters, events, items, and locations, and presents them in a ""glanceable"" manner to the interactor. As significant story elements are referenced, the application provides explanatory information on the second screen. Drawing from the larger story world made up of multiple comic books, movies, games, and television shows, United Universe aims to provide clarity and background for the novice, and depth and engagement for more knowledgeable viewers.",2015,1047
Careful-Packing: A Practical and Scalable Anti-Tampering Software Protection enforced by Trusted Computing,"Toffalini, Flavio and Ochoa, Mart\'{\i}n and Sun, Jun and Zhou, Jianying","Ensuring the correct behaviour of an application is a critical security issue. One of the most popular ways to modify the intended behaviour of a program is to tamper its binary. Several solutions have been proposed to solve this problem, including trusted computing and anti-tampering techniques. Both can substantially increase security, and yet both have limitations. In this work, we propose an approach which combines trusted computing technologies and anti-tampering techniques, and that synergistically overcomes some of their inherent limitations. In our approach critical software regions are protected by leveraging on trusted computing technologies and cryptographic packing, without introducing additional software layers. To illustrate our approach we implemented a secure monitor which collects user activities, such as keyboard and mouse events for insider attack detection. We show how our solution provides a strong anti-tampering guarantee with a low overhead: around 10 lines of code added to the entire application, an average execution time overhead of 5.7% and only 300KB of memory allocated for the trusted module.",2019,1048
Approximate recognition of non-regular languages by finite automata,"Eisman, Gerry and Ravikumar, B.","Approximate computation is a central concept in algorithms and computation theory. Our notion of approximation is that the algorithm perform correctly on most of the inputs. We propose some finite automata models to study the question of how well a finite automaton can approximately recognize a non-regular language. On the one hand, we show that there are natural problems for which a DFA can correctly solve almost all the instances. The design of these DFA's leads to a linear time randomized algorithm for approximate integer multiplication. On the other hand, we show that some languages (such as Lmajority = {x ∈ (0 + 1)* | x has more 1's than 0's}) can't be approximated by any regular language in a strong sense. We also present results comparing different models of approximation.",2005,1049
Threat-modeling-guided Trust-based Task Offloading for Resource-constrained Internet of Things,"Bradbury, Matthew and Jhumka, Arshad and Watson, Tim and Flores, Denys and Burton, Jonathan and Butler, Matthew","There is an increasing demand for Internet of Things (IoT) networks consisting of resource-constrained devices executing increasingly complex applications. Due to these resource constraints, IoT devices will not be able to execute expensive tasks. One solution is to offload expensive tasks to resource-rich edge nodes, which requires a framework that facilitates the selection of suitable edge nodes to perform task offloading. Therefore, in this article, we present a novel trust-model-driven system architecture, based on behavioral evidence, that is suitable for resource-constrained IoT devices and supports computation offloading. We demonstrate the viability of the proposed architecture with an example deployment of the Beta Reputation System trust model on real hardware to capture node behaviors. The open environment of edge-based IoT networks means that threats against edge nodes can lead to deviation from expected behavior. Hence, we perform a threat modeling to identify such threats. The proposed system architecture includes threat handling mechanisms that provide security properties such as confidentiality, authentication, and non-repudiation of messages in required scenarios and operate within the resource constraints. We evaluate the efficacy of the threat handling mechanisms and identify future work for the standards used.",2022,1050
A methodology on extracting reusable software candidate components from open source games,"Ampatzoglou, Apostolos and Stamelos, Ioannis and Gkortzis, Antonios and Deligiannis, Ignatios","Component-Based Software Engineering (CBSE) focuses on the development of reusable components in order to enable their reuse in more systems, rather than only to be used to the original ones for which they have been implemented in the first place (i.e. development for reuse) and the development of new systems with reusable components (i.e. development with reuse). This paper aims at introducing a methodology for the extraction of candidate reusable software components from open source games. The extracted components have been empirically evaluated through a case study. Additionally, the component candidates that have been extracted are available for reuse through a web service.",2012,1051
Automatic clustering of languages,"Batagelj, Vladimir and Pisanski, Toma\v{z} and Ker\v{z}i\v{c}, Damijana",Automatic clustering of languages seems to be one possible application that arose during our study of mathematical methods for computing dissimilarities between strings. The results of this experiment are discussed.,1992,1052
Automated program repair,"Goues, Claire Le and Pradel, Michael and Roychoudhury, Abhik",Automated program repair can relieve programmers from the burden of manually fixing the ever-increasing number of programming mistakes.,2019,1053
"Code Review of Build System Specifications: Prevalence, Purposes, Patterns, and Perceptions","Nejati, Mahtab and Alfadel, Mahmoud and McIntosh, Shane","Build systems automate the integration of source code into executables. Maintaining build systems is known to be challenging. Lax build maintenance can lead to costly build breakages or unexpected software behaviour. Code review is a broadly adopted practice to improve software quality. Yet, little is known about how code review is applied to build specifications.In this paper, we present the first empirical study of how code review is practiced in the context of build specifications. Through quantitative analysis of 502,931 change sets from the Qt and Eclipse communities, we observe that changes to build specifications are at least two times less frequently discussed during code review when compared to production and test code changes. A qualitative analysis of 500 change sets reveals that (i) comments on changes to build specifications are more likely to point out defects than rates reported in the literature for production and test code, and (ii) evolvability and dependency-related issues are the most frequently raised patterns of issues. Follow-up interviews with nine developers with 1--40 years of experience point out social and technical factors that hinder rigorous review of build specifications, such as a prevailing lack of understanding of and interest in build systems among developers, and the lack of dedicated tooling to support the code review of build specifications.",2023,1054
A Model for Context in the Design of Open Production Communities,"Ziaie, Pujan","Open production communities (OPCs) provide technical features and social norms for a vast but dispersed and diverse crowd to collectively accumulate content. In OPCs, certain mechanisms, policies, and technologies are provided for voluntary users to participate in community-related activities including content generation, evaluation, qualification, and distribution and in some cases even community governance. Due to the known complexities and dynamism of online communities, designing a successful community is deemed more an art than a science. Numerous studies have investigated different aspects of certain types of OPCs. Most of these studies, however, fall short of delivering a general view or prescription due to their narrow focus on a certain type of OPCs. In contribution to theories on technology-mediated social participation (TMSP), this study synthesizes the streams of research in the particular area of OPCs and delivers a theoretical framework as a baseline for adapting findings from one specific type of community on another. This framework consists of four primary dimensions, namely, platform features, content, user, and community. The corresponding attributes of these dimensions and the existing interdependencies are discussed in detail. Furthermore, a decision diagram for selecting features and a design guideline for “decontextualizing” findings are introduced as possible applications of the framework. The framework also provides a new and reliable foundation on which future research can extend its findings and prescriptions in a systematic way.",2014,1055
SMASheD: Sniffing and Manipulating Android Sensor Data,"Mohamed, Manar and Shrestha, Babins and Saxena, Nitesh","The current Android sensor security model either allows only restrictive read access to sensitive sensors (e.g., an app can only read its own touch data) or requires special install-time permissions (e.g., to read microphone, camera or GPS). Moreover, Android does not allow write access to any of the sensors. Sensing-based security applications therefore crucially rely upon the sanity of the Android sensor security model.In this paper, we show that such a model can be effectively circumvented. Specifically, we build SMASheD, a legitimate framework under the current Android ecosystem that can be used to stealthily sniff as well as manipulate many of the Android's restricted sensors (even touch input). SMASheD exploits the Android Debug Bridge (ADB) functionality and enables a malicious app with only the INTERNET permission to read, and write to, multiple different sensor data files at will. SMASheD is the first framework, to our knowledge, that can sniff and manipulate protected sensors on unrooted Android devices, without user awareness, without constant device-PC connection and without the need to infect the PC.The primary contributions of this work are two-fold. First, we design and develop the SMASheD framework. Second, as an offensive implication of the SMASheD framework, we introduce a wide array of potentially devastating attacks. Our attacks against the touchsensor range from accurately logging the touchscreen input (TouchLogger) to injecting touch events for accessing restricted sensors and resources, installing and granting special permissions to other malicious apps, accessing user accounts, and authenticating on behalf of the user --- essentially almost doing whatever the device user can do (secretively). Our attacks against various physical sensors (motion, position and environmental) can subvert the functionality provided by numerous existing sensing-based security applications, including those used for(continuous) authentication, and authorization.",2016,1056
Convergent contemporary software peer review practices,"Rigby, Peter C. and Bird, Christian","Software peer review is practiced on a diverse set of software projects that have drastically different settings, cultures, incentive systems, and time pressures. In an effort to characterize and understand these differences we examine two Google-led projects, Android and Chromium OS, three Microsoft projects, Bing, Office, and MS SQL, and projects internal to AMD. We contrast our findings with data taken from traditional software inspection conducted on a Lucent project and from open source software peer review on six projects, including Apache, Linux, and KDE. Our measures of interest include the review interval, the number of developers involved in review, and proxy measures for the number of defects found during review. We find that despite differences among projects, many of the characteristics of the review process have independently converged to similar values which we think indicate general principles of code review practice. We also introduce a measure of the degree to which knowledge is shared during review. This is an aspect of review practice that has traditionally only had experiential support. Our knowledge sharing measure shows that conducting peer review increases the number of distinct files a developer knows about by 66% to 150% depending on the project. This paper is one of the first studies of contemporary review in software firms and the most diverse study of peer review to date.",2013,1057
"Process out-grafting: an efficient ""out-of-VM"" approach for fine-grained process execution monitoring","Srinivasan, Deepa and Wang, Zhi and Jiang, Xuxian and Xu, Dongyan","Recent rapid malware growth has exposed the limitations of traditional in-host malware-defense systems and motivated the development of secure virtualization-based out-of-VM solutions. By running vulnerable systems as virtual machines (VMs) and moving security software from inside the VMs to outside, the out-of-VM solutions securely isolate the anti-malware software from the vulnerable system. However, the presence of semantic gap also leads to the compatibility problem in not supporting existing defense software. In this paper, we present process out-grafting, an architectural approach to address both isolation and compatibility challenges in out-of-VM approaches for fine-grained process-level execution monitoring. Specifically, by relocating a suspect process from inside a VM to run side-by-side with the out-of-VM security tool, our technique effectively removes the semantic gap and supports existing user-mode process monitoring tools without any modification. Moreover, by forwarding the system calls back to the VM, we can smoothly continue the execution of the out-grafted process without weakening the isolation of the monitoring tool. We have developed a KVM-based prototype and used it to natively support a number of existing tools without any modification. The evaluation results including measurement with benchmark programs show it is effective and practical with a small performance overhead.",2011,1058
Active objects: actions for entity-centric search,"Lin, Thomas and Pantel, Patrick and Gamon, Michael and Kannan, Anitha and Fuxman, Ariel","We introduce an entity-centric search experience, called Active Objects, in which entity-bearing queries are paired with actions that can be performed on the entities. For example, given a query for a specific flashlight, we aim to present actions such as reading reviews, watching demo videos, and finding the best price online. In an annotation study conducted over a random sample of user query sessions, we found that a large proportion of queries in query logs involve actions on entities, calling for an automatic approach to identifying relevant actions for entity-bearing queries. In this paper, we pose the problem of finding actions that can be performed on entities as the problem of probabilistic inference in a graphical model that captures how an entity bearing query is generated. We design models of increasing complexity that capture latent factors such as entity type and intended actions that determine how a user writes a query in a search box, and the URL that they click on. Given a large collection of real-world queries and clicks from a commercial search engine, the models are learned efficiently through maximum likelihood estimation using an EM algorithm. Given a new query, probabilistic inference enables recommendation of a set of pertinent actions and hosts. We propose an evaluation methodology for measuring the relevance of our recommended actions, and show empirical evidence of the quality and the diversity of the discovered actions.",2012,1059
Meta-stasis of the Internet,"Desai, Bipin C.","This paper offers a brief history of the information age in order to demonstrate how the loss of user control and the increase in certain forms of automation have metastasized into imminent and ongoing threats to social order and the democratic way of life. The internet was established after a number of developments which included the interconnection of computers without extensive need of action by the users. It led to the introduction of user communication sub-systems such as text, email, file sharing and systems for searching for files. The so called information age is said to be marked by the adaption of a hypertext transport protocol in the last decade of the twentieth century. The information age was marked by a number of meetings which included the first of the world wide web conference in April 1994 followed by the second (Oct. 1994) and the third(April 1995) in quick succession. Other, by invitation only, meetings which dealt with issue of this era were held in Denver, OH(Metadata) and (America in the Age of Information)Bethesda, MD. However, in just under three decades this information age has meta-stasis-ed into a form that is a threat to our social order and democratic way of life while fostering division. Enormous wealth has been garnered by just a few corporations and individuals at the expense of the harm it is doing to people all over the globe. This is the result of the spreading of fake-news and favouring angry content that result in civil strife and loss of lives. It has led to divisiveness and autocratic governments. Some so called democracies are in name only with the same people continuing in their ’elected’ position from term to term, ad infinitum. Just as in the metastasis of a cancer, until it is checked, this transformed internet will destroy some vital parts of our everyday existence: our privacy and liberty while promoting an inegalitarian spirit.",2022,1060
A comparative study of information system curriculum in U.S. and foreign universities,"Goslar, Martin D. and Deans, P. Candace","The continued shift toward a more integrated world economy and the need to be more informed regarding business events outside the domestic (U.S.) marketplace have contributed to a need for better collaboration among IS academicians in both U.S. and foreign schools of business. IS educators are developing new curriculum initiatives, broadening the scope to incorporate international business environment forces.This research provides the IS academic community with insights into IS curriculumin business schools worldwide in order to better understand differences and similarities across programs. These insights will prove valuable as efforts are made to broaden the scope of IS curriculum content to an international dimension. Theoretical foundations for this study are derived primarily from the international business discipline. The methodology incorporates a questionnaire targeted for a representative sample of U.S. and foreign schools of business. The results of this study provide IS academicians with a reference point from which to proceed in developing a more refined international IS component. Insights derived from IS curriculum approaches currently in place in foreign schools of business will contribute to an understanding of international IS issues and encourage collaboration with academicians outside the United States.",1994,1061
Exploring the State-of-Receptivity for mHealth Interventions,"K\""{u}nzler, Florian and Mishra, Varun and Kramer, Jan-Niklas and Kotz, David and Fleisch, Elgar and Kowatsch, Tobias","Recent advancements in sensing techniques for mHealth applications have led to successful development and deployments of several mHealth intervention designs, including Just-In-Time Adaptive Interventions (JITAI). JITAIs show great potential because they aim to provide the right type and amount of support, at the right time. Timing the delivery of a JITAI such as the user is receptive and available to engage with the intervention is crucial for a JITAI to succeed. Although previous research has extensively explored the role of context in users' responsiveness towards generic phone notifications, it has not been thoroughly explored for actual mHealth interventions. In this work, we explore the factors affecting users' receptivity towards JITAIs. To this end, we conducted a study with 189 participants, over a period of 6 weeks, where participants received interventions to improve their physical activity levels. The interventions were delivered by a chatbot-based digital coach -Ally - which was available on Android and iOS platforms.We define several metrics to gauge receptivity towards the interventions, and found that (1) several participant-specific characteristics (age, personality, and device type) show significant associations with the overall participant receptivity over the course of the study, and that (2) several contextual factors (day/time, phone battery, phone interaction, physical activity, and location), show significant associations with the participant receptivity, in-the-moment. Further, we explore the relationship between the effectiveness of the intervention and receptivity towards those interventions; based on our analyses, we speculate that being receptive to interventions helped participants achieve physical activity goals, which in turn motivated participants to be more receptive to future interventions. Finally, we build machine-learning models to detect receptivity, with up to a 77% increase in F1 score over a biased random classifier.",2020,1062
"Data-driven Curation, Learning and Analysis for Inferring Evolving IoT Botnets in the Wild","Pour, Morteza Safaei and Mangino, Antonio and Friday, Kurt and Rathbun, Matthias and Bou-Harb, Elias and Iqbal, Farkhund and Shaban, Khaled and Erradi, Abdelkarim","The insecurity of the Internet-of-Things (IoT) paradigm continues to wreak havoc in consumer and critical infrastructure realms. Several challenges impede addressing IoT security at large, including, the lack of IoT-centric data that can be collected, analyzed and correlated, due to the highly heterogeneous nature of such devices and their widespread deployments in Internet-wide environments. To this end, this paper explores macroscopic, passive empirical data to shed light on this evolving threat phenomena. This not only aims at classifying and inferring Internet-scale compromised IoT devices by solely observing such one-way network traffic, but also endeavors to uncover, track and report on orchestrated ""in the wild"" IoT botnets. Initially, to prepare the effective utilization of such data, a novel probabilistic model is designed and developed to cleanse such traffic from noise samples (i.e., misconfiguration traffic). Subsequently, several shallow and deep learning models are evaluated to ultimately design and develop a multi-window convolution neural network trained on active and passive measurements to accurately identify compromised IoT devices. Consequently, to infer orchestrated and unsolicited activities that have been generated by well-coordinated IoT botnets, hierarchical agglomerative clustering is deployed by scrutinizing a set of innovative and efficient network feature sets. By analyzing 3.6 TB of recent darknet traffic, the proposed approach uncovers a momentous 440,000 compromised IoT devices and generates evidence-based artifacts related to 350 IoT botnets. While some of these detected botnets refer to previously documented campaigns such as the Hide and Seek, Hajime and Fbot, other events illustrate evolving threats such as those with cryptojacking capabilities and those that are targeting industrial control system communication and control services.",2019,1063
Stealthy malware detection and monitoring through VMM-based “out-of-the-box” semantic view reconstruction,"Jiang, Xuxian and Wang, Xinyuan and Xu, Dongyan","An alarming trend in recent malware incidents is that they are armed with stealthy techniques to detect, evade, and subvert malware detection facilities of the victim. On the defensive side, a fundamental limitation of traditional host-based antimalware systems is that they run inside the very hosts they are protecting (“in-the-box”), making them vulnerable to counter detection and subversion by malware. To address this limitation, recent solutions based on virtual machine (VM) technologies advocate placing the malware detection facilities outside of the protected VM (“out-of-the-box”). However, they gain tamper resistance at the cost of losing the internal semantic view of the host, which is enjoyed by “in-the-box” approaches. This poses a technical challenge known as the semantic gap.In this article, we present the design, implementation, and evaluation of VMwatcher—an “out-of-the-box” approach that overcomes the semantic gap challenge. A new technique called guest view casting is developed to reconstruct internal semantic views (e.g., files, processes, and kernel modules) of a VM nonintrusively from the outside. More specifically, the new technique casts semantic definitions of guest OS data structures and functions on virtual machine monitor (VMM)-level VM states, so that the semantic view can be reconstructed. Furthermore, we extend guest view casting to reconstruct details of system call events (e.g., the process that makes the system call as well as the system call number, parameters, and return value) in the VM, enriching the semantic view. With the semantic gap effectively narrowed, we identify three unique malware detection and monitoring capabilities: (i) view comparison-based malware detection and its demonstration in rootkit detection; (ii) “out-of-the-box” deployment of off-the-shelf anti malware software with improved detection accuracy and tamper-resistance; and (iii) nonintrusive system call monitoring for malware and intrusion behavior observation. We have implemented a proof-of-concept VMwatcher prototype on a number of VMM platforms. Our evaluation experiments with real-world malware, including elusive kernel-level rootkits, demonstrate VMwatcher's practicality and effectiveness.",2010,1064
"Configuring global software teams: a multi-company analysis of project productivity, quality, and profits","Ramasubbu, Narayan and Cataldo, Marcelo and Balan, Rajesh Krishna and Herbsleb, James D.","In this paper, we examined the impact of project-level configurational choices of globally distributed software teams on project productivity, quality, and profits. Our analysis used data from 362 projects of four different firms. These projects spanned a wide range of programming languages, application domain, process choices, and development sites spread over 15 countries and 5 continents. Our analysis revealed fundamental tradeoffs in choosing configurational choices that are optimized for productivity, quality, and/or profits. In particular, achieving higher levels of productivity and quality require diametrically opposed configurational choices. In addition, creating imbalances in the expertise and personnel distribution of project teams significantly helps increase profit margins. However, a profit-oriented imbalance could also significantly affect productivity and/or quality outcomes. Analyzing these complex tradeoffs, we provide actionable managerial insights that can help software firms and their clients choose configurations that achieve desired project outcomes in globally distributed software development.",2011,1065
A fully reusable class of objects for synchronization and communication in Ada 95,"de Bondeli, Patrick","This paper presents a very general class which can be reused to specify and implement any type exporting synchronization or communication properties. The new Ada 95 features modelling inheritance, polymorphism and hierarchies of library units are used extensively in describing the architecture of the class and other new features (access to subprograms, protected types, …) are used for the specification and implementation of the components of the class.Section 2 presents the general architecture of our class. Sections 3, 4, 5 respectively give examples of specification, use and implementation of its components. Section 6 concludes on recalling the full role of formal techniques in our approach (they appear in the present paper only to show that the semantics of our class is defined at a more abstract level than its implementation) and discussing a few interesting points about the way Ada 95 is used here.",1999,1066
Lock-step simulation is child's play (experience report),"Breitner, Joachim and Smith, Chris","Implementing multi-player networked games by broadcasting the player’s input and letting each client calculate the game state -- a scheme known as *lock-step simulation* – is an established technique. However, ensuring that every client in this scheme obtains a consistent state is infamously hard and in general requires great discipline from the game programmer. The thesis of this pearl is that in the realm of functional programming – in particular with Haskell's purity and static pointers – this hard problem becomes almost trivially easy.  We support this thesis by implementing lock-step simulation under very adverse conditions. We extended the educational programming environment CodeWorld, which is used to teach math and programming to middle school students, with the ability to create and run interactive, networked multi-user games. Despite providing a very abstract and high-level interface, and without requiring any discipline from the programmer, we can provide consistent lock-step simulation with client prediction.",2017,1067
Rationale Dataset and Analysis for the Commit Messages of the Linux Kernel Out-of-Memory Killer,"Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis","Code commit messages can contain useful information on why a developer has made a change. However, the presence and structure of rationale in real-world code commit messages is not well studied. Here, we detail the creation of a labelled dataset to analyze the code commit messages of the Linux Kernel Out-Of-Memory Killer component. We study aspects of rationale information, such as presence, temporal evolution, and structure. We find that 98.9% of commits in our dataset contain sentences with rationale information, and that experienced developers report rationale in about 60% of the sentences in their commits. We report on the challenges we faced and provide examples for our labelling.",2024,1068
Understanding Human-Machine Networks: A Cross-Disciplinary Survey,"Tsvetkova, Milena and Yasseri, Taha and Meyer, Eric T. and Pickering, J. Brian and Engen, Vegard and Walland, Paul and L\""{u}ders, Marika and F\o{}lstad, Asbj\o{}rn and Bravos, George","In the current hyperconnected era, modern Information and Communication Technology (ICT) systems form sophisticated networks where not only do people interact with other people, but also machines take an increasingly visible and participatory role. Such Human-Machine Networks (HMNs) are embedded in the daily lives of people, both for personal and professional use. They can have a significant impact by producing synergy and innovations. The challenge in designing successful HMNs is that they cannot be developed and implemented in the same manner as networks of machines nodes alone, or following a wholly human-centric view of the network. The problem requires an interdisciplinary approach. Here, we review current research of relevance to HMNs across many disciplines. Extending the previous theoretical concepts of socio-technical systems, actor-network theory, cyber-physical-social systems, and social machines, we concentrate on the interactions among humans and between humans and machines. We identify eight types of HMNs: public-resource computing, crowdsourcing, web search engines, crowdsensing, online markets, social media, multiplayer online games and virtual worlds, and mass collaboration. We systematically select literature on each of these types and review it with a focus on implications for designing HMNs. Moreover, we discuss risks associated with HMNs and identify emerging design and development trends.",2017,1069
The Sound of Silence: Mining Security Vulnerabilities from Secret Integration Channels in Open-Source Projects,"Ramsauer, Ralf and Bulwahn, Lukas and Lohmann, Daniel and Mauerer, Wolfgang","Public development processes are a key characteristic of open source projects. However, fixes for vulnerabilities are usually discussed privately among a small group of trusted maintainers, and integrated without prior public involvement. This is supposed to prevent early disclosure, and cope with embargo and non-disclosure agreement (NDA) rules. While regular development activities leave publicly available traces, fixes for vulnerabilities that bypass the standard process do not.We present a data-mining based approach to detect code fragments that arise from such infringements of the standard process. By systematically mapping public development artefacts to source code repositories, we can exclude regular process activities, and infer irregularities that stem from non-public integration channels. For the Linux kernel, the most crucial component of many systems, we apply our method to a period of seven months before the release of Linux 5.4. We find 29 commits that address 12 vulnerabilities. For these vulnerabilities, our approach provides a temporal advantage of 2 to 179 days to design exploits before public disclosure takes place, and fixes are rolled out.Established responsible disclosure approaches in open development processes are supposed to limit premature visibility of security vulnerabilities. However, our approach shows that, instead, they open additional possibilities to uncover such changes that thwart the very premise. We conclude by discussing implications and partial countermeasures.",2020,1070
Analyzing and improving Linux kernel memory protection: a model checking approach,"Liakh, Siarhei and Grace, Michael and Jiang, Xuxian","Code injection continues to pose a serious threat to computer systems. Among existing solutions, W ⊕ X is a notable approach to prevent the execution of injected code. In this paper, we focus on the Linux kernel memory protection and systematically check for possible W ⊕ X violations in the Linux kernel design and implementation. In particular, we have developed a Murphi-based abstract model and used it to discover several serious shortcomings in the current Linux kernel that violate the W ⊕ X property. We have confirmed with the Linux community the presence of these problems and accordingly developed five Linux kernel patches. (Four of them are in the process of being integrated into the mainline Linux kernel.) Our evaluation with these patches indicate that they involve only minimal changes to the existing code base and incur negligible performance overhead.",2010,1071
Designing New Operating Primitives to Improve Fuzzing Performance,"Xu, Wen and Kashyap, Sanidhya and Min, Changwoo and Kim, Taesoo","Fuzzing is a software testing technique that finds bugs by repeatedly injecting mutated inputs to a target program. Known to be a highly practical approach, fuzzing is gaining more popularity than ever before. Current research on fuzzing has focused on producing an input that is more likely to trigger a vulnerability.In this paper, we tackle another way to improve the performance of fuzzing, which is to shorten the execution time of each iteration. We observe that AFL, a state-of-the-art fuzzer, slows down by 24x because of file system contention and the scalability of fork() system call when it runs on 120 cores in parallel. Other fuzzers are expected to suffer from the same scalability bottlenecks in that they follow a similar design pattern. To improve the fuzzing performance, we design and implement three new operating primitives specialized for fuzzing that solve these performance bottlenecks and achieve scalable performance on multi-core machines. Our experiment shows that the proposed primitives speed up AFL and LibFuzzer by 6.1 to 28.9x and 1.1 to 735.7x, respectively, on the overall number of executions per second when targeting Google's fuzzer test suite with 120 cores. In addition, the primitives improve AFL's throughput up to 7.7x with 30 cores, which is a more common setting in data centers. Our fuzzer-agnostic primitives can be easily applied to any fuzzer with fundamental performance improvement and directly benefit large-scale fuzzing and cloud-based fuzzing services.",2017,1072
Constructive Code Review: Managing the Impact of Interpersonal Conflicts in Practice,"Wurzel Goncalves, Pavlina and S. V. Goncalves, Joao and Bacchelli, Alberto","Code review is an activity where developers receive feedback on their code contributions from other developers. The frequent and potentially negative feedback developers receive makes code review prone to interpersonal conflicts. There is a consensus about such behavior being anti-social and leading to negative outcomes for the code, team, project, and even the company. However, these conflicts are a naturally occurring phenomenon that can lead to reaping the benefits of code review if managed well. Interpersonal conflicts in code review are not necessarily an issue to avoid, but rather to be managed.In this study, we survey developers in two companies - Adnovum - working predominantly on closed-source projects - and in Red Hat open source projects. Based on a set of 154 respondents, we have found that 77% of developers sometimes experience interpersonal conflicts in code review, even though mostly not very frequently. These conflicts pose some degree of a problem to 64% of developers. However, developers are rather successful in deriving constructive outcomes in the face of conflicts - 24% of developers report that conflicts have more positive than negative outcomes. While they are highly successful in deriving positive outcomes for code quality and maintainability, the motivation of developers and their communication and collaboration in a team has the most potential to be harmed by conflicts. The most effective strategy to have constructive rather than destructive conflicts is managing work effort and re-assigning tasks in the team to reduce stress in review. Data and materials: https://zenodo.org/records/10477537.",2024,1073
RACS '23: Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems,,"With the expansion of both the Internet and the advanced information technology development profession, reliable and convergent computing has attracted increasing interest in both academia and industry. To cope with this important problem, the Research in Adaptive and Convergent Systems (RACS) provides a forum for exchanging highly original ideas about an important class of computing systems. The RACS aims primarily at researchers who have experience in reliable and convergent computing systems and are engaged in the design and implementation of new computing applications. Each year RACS brings together engineers and scientists from diverse communities with interests in practical computing technologies and creates an environment for them to discuss and report experimental results, novel designs, work-in-progress, experiences, case studies, and trend-setting ideas.",2023,1074
Dead Code Removal at Meta: Automatically Deleting Millions of Lines of Code and Petabytes of Deprecated Data,"Shackleton, Will and Cohn-Gordon, Katriel and Rigby, Peter C. and Abreu, Rui and Gill, James and Nagappan, Nachiappan and Nakad, Karim and Papagiannis, Ioannis and Petre, Luke and Megreli, Giorgi and Riggs, Patrick and Saindon, James","Software constantly evolves in response to user needs: new features are built, deployed, mature and grow old, and eventually their usage drops enough to merit switching them off. In any large codebase, this feature lifecycle can naturally lead to retaining unnecessary code and data. Removing these respects users’ privacy expectations, as well as helping engineers to work efficiently. In prior software engineering research, we have found little evidence of code deprecation or dead-code removal at industrial scale. We describe Systematic Code and Asset Removal Framework (SCARF), a product deprecation system to assist engineers working in large codebases. SCARF identifies unused code and data assets and safely removes them. It operates fully automatically, including committing code and dropping database tables. It also gathers developer input where it cannot take automated actions, leading to further removals. Dead code removal increases the quality and consistency of large codebases, aids with knowledge management and improves reliability. SCARF has had an important impact at Meta. In the last year alone, it has removed petabytes of data across 12.8 million distinct assets, and deleted over 104 million lines of code.",2023,1075
"Make it work, make it right, make it fast: building a platform-neutral whole-system dynamic binary analysis platform","Henderson, Andrew and Prakash, Aravind and Yan, Lok Kwong and Hu, Xunchao and Wang, Xujiewen and Zhou, Rundong and Yin, Heng","Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, lacking APIs, etc. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection combined with a novel TCG instruction-level tainting at bit granularity, backed by a plugin based, simple-to-use event driven programming interface. DECAF exercises fine control over the TCG instructions to accomplish on-the-fly optimizations. We present 3 platform-neutral plugins - Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9550 lines of C++ code and 10270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605% for system wide tainting and 12% for VMI.",2014,1076
Secure the Cloud: From the Perspective of a Service-Oriented Organization,"Roy, Arpan and Sarkar, Santonu and Ganesan, Rajeshwari and Goel, Geetika","In response to the revival of virtualized technology by Rosenblum and Garfinkel [2005], NIST defined cloud computing, a new paradigm in service computing infrastructures. In cloud environments, the basic security mechanism is ingrained in virtualization—that is, the execution of instructions at different privilege levels. Despite its obvious benefits, the caveat is that a crashed virtual machine (VM) is much harder to recover than a crashed workstation. When crashed, a VM is nothing but a giant corrupt binary file and quite unrecoverable by standard disk-based forensics. Therefore, VM crashes should be avoided at all costs. Security is one of the major contributors to such VM crashes. This includes compromising the hypervisor, cloud storage, images of VMs used infrequently, and remote cloud client used by the customer as well as threat from malicious insiders. Although using secure infrastructures such as private clouds alleviate several of these security problems, most cloud users end up using cheaper options such as third-party infrastructures (i.e., private clouds), thus a thorough discussion of all known security issues is pertinent. Hence, in this article, we discuss ongoing research in cloud security in order of the attack scenarios exploited most often in the cloud environment. We explore attack scenarios that call for securing the hypervisor, exploiting co-residency of VMs, VM image management, mitigating insider threats, securing storage in clouds, abusing lightweight software-as-a-service clients, and protecting data propagation in clouds. Wearing a practitioner's glasses, we explore the relevance of each attack scenario to a service company like Infosys. At the same time, we draw parallels between cloud security research and implementation of security solutions in the form of enterprise security suites for the cloud. We discuss the state of practice in the form of enterprise security suites that include cryptographic solutions, access control policies in the cloud, new techniques for attack detection, and security quality assurance in clouds.",2015,1077
Improving activity in communities of practice through software release management,"van Ingen, Kevin and van Ommen, Job and Jansen, Slinger","Keystone players, the company that occupies the crucial hubs, need to nurture its inhabitants to keep the ecosystem active. Many modern ecosystems have communities of practice where knowledge is transferred by collaborative problem solving, sharing of ideas, software components or configurations. In recent years a widely spread medium for communities of practice is the use of online discussion boards. This research proposes a method to analyze the relationship between software releases and activity in a community. This paper explains how software ecosystem Keystone companies can use software product release management to cultivate communities of practice using an illustrative case study. In this case study a comparison is made between two communities in the Android ecosystem. The results show peaks in community activity coinciding with the software releases. The release not only revitalizes the activity of the developers but the heterogeneous community in its entirety.",2011,1078
"TECHNOLOGY THAT EDUCATORS OF COMPUTING HAIL (TECH)Using Cloud9, a powerful cloud-based IDE in the classroom","Solin, Jeff","Welcome to the latest TECH column, a column that features a guest columnist, discussing technologies that educators find useful in their classrooms. The guest columnist is selected from those who have presented at a SIGCSE Technical Symposium session. This issue's guest columnist, Jeff Solin, offers his ideas on Cloud9.",2017,1079
The Evolution of Android Malware and Android Analysis Techniques,"Tam, Kimberly and Feizollah, Ali and Anuar, Nor Badrul and Salleh, Rosli and Cavallaro, Lorenzo","With the integration of mobile devices into daily life, smartphones are privy to increasing amounts of sensitive information. Sophisticated mobile malware, particularly Android malware, acquire or utilize such data without user consent. It is therefore essential to devise effective techniques to analyze and detect these threats. This article presents a comprehensive survey on leading Android malware analysis and detection techniques, and their effectiveness against evolving malware. This article categorizes systems by methodology and date to evaluate progression and weaknesses. This article also discusses evaluations of industry solutions, malware statistics, and malware evasion techniques and concludes by supporting future research paths.",2017,1080
Coverage-based Greybox Fuzzing as Markov Chain,"B\""{o}hme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik","Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few ""high-frequency"" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule.We implemented the exponential schedule by extending AFL. In 24 hours, AFLFAST exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFAST produces at least an order of magnitude more unique crashes than AFL.",2016,1081
A Comprehensive Review of the State-of-the-Art on Security and Privacy Issues in Healthcare,"L\'{o}pez Mart\'{\i}nez, Antonio and Gil P\'{e}rez, Manuel and Ruiz-Mart\'{\i}nez, Antonio","Currently, healthcare is critical environment in our society, which attracts attention to malicious activities and has caused an important number of damaging attacks. In parallel, the recent advancements in technologies, computing systems, and wireless communications are changing healthcare environment by adding different improvements and complexity to it. This article reviews the current state of the literature and provides a holistic view of cybersecurity in healthcare. With this purpose in mind, the article enumerates the main stakeholders and architecture implemented in the healthcare environment, as well as the main security issues (threats, attacks, etc.) produced in healthcare. In this context, this work maps the threats collected with a widely used knowledge-based framework, MITRE ATT&amp;CK, building a contribution not seen so far. This article also enumerates the security mechanisms created to protect healthcare, identifying the principal research lines addressed in the literature, and listing the available public security-focused datasets used in machine-learning to provide security in the medical domain. To conclude, the research challenges that need to be addressed for future research works in this area are presented.",2023,1082
"B.Y.O.C (1,342 times and counting)","Kamp, Poul-Henning",Why can't we all use standard libraries for commonly needed algorithms?,2011,1083
User-defined actions for SDN,"Farhadi, Hamid and Du, Ping and Nakao, Akihiro","In Software-Defined Networking (SDN), the control plane can program the data plane via SDN open APIs such as OpenFlow. An OpenFlow-like data plane applies &lt;match, action&gt; rules to every packet. However, it only supports a few actions that are all predefined and hardcoded to a piece of hardware in SDN switch. We argue that we should extend the programmability and flexibility of SDN to the data plane to allow network owners to add their custom network functions while keeping the programability of existing SDN. Since current OpenFlow actions are not sufficient and flexible, we posit we need user-defined actions deployed within the switch box rather than an external equipment (e.g., Fire-wall). Finally, we study the feasibility of two sample user-defined actions (i.e., Portscan detector and Botminer detector) using two different underlying mechanisms: OpenFlow and our previous work, TagFlow. Our evaluations show that user-defined actions are capable of handling traffic at line speed. Moreover, we also indicate that TagFlow user-defined actions are 33% faster than OpenFlow. We concluded that extending SDN features to include user-defined actions is lightweight and feasible.",2014,1084
Verified gaming,"Kiniry, Joseph R. and Zimmerman, Daniel M.","In recent years, several Grand Challenges (GCs) of computing have been identified and expounded upon by various professional organizations in the U.S. and England. These GCs are typically very difficult problems that will take many hundreds, or perhaps thousands, of man-years to solve. Researchers involved in identifying these problems are not going to solve them. That task will fall to our students, and our students' students. Unfortunately for GC6, the Grand Challenge focusing on Dependable Systems Evolution, interest in formal methods--both by students and within computer science faculties - falls every year and any mention of mathematics in the classroom seems to frighten students away. So the question is: How do we attract new students in computing to the area of dependable software systems?Over the past several years at three universities we have experimented with the use of computer games as a target domain for software engineering project courses that focus on reliable systems engineering. This position paper summarizes our experiences in incorporating rigorous software engineering into courses with computer game projects.",2011,1085
Advancing Web 3.0: Making Smart Contracts Smarter on Blockchain,"Huang, Junqin and Kong, Linghe and Cheng, Guanjie and Xiang, Qiao and Chen, Guihai and Huang, Gang and Liu, Xue","Blockchain and smart contracts are one of the key technologies promoting Web 3.0. However, due to security considerations and consistency requirements, smart contracts currently only support simple and deterministic programs, which significantly hinders their deployment in intelligent Web 3.0 applications. To enhance smart contracts intelligence on the blockchain, we propose SMART, a plug-in smart contract framework that supports efficient AI model inference while being compatible with existing blockchains. To handle the high complexity of model inference, we propose an on-chain and off-chain joint execution model, which separates the SMART contract into two parts: the deterministic code still runs inside an on-chain virtual machine, while the complex model inference is offloaded to off-chain compute nodes. To solve the non-determinism brought by model inference, we leverage Trusted Execution Environments (TEEs) to endorse the integrity and correctness of the off-chain execution. We also design distributed attestation and secret key provisioning schemes to further enhance the system security and model privacy. We implement a SMART prototype and evaluate it on a popular Ethereum Virtual Machine (EVM)-based blockchain. Theoretical analysis and prototype evaluation show that SMART not only achieves the security goals of correctness, liveness, and model privacy, but also has approximately 5 orders of magnitude faster inference efficiency than existing on-chain solutions.",2024,1086
Tea and sympathy: crafting positive new user experiences on wikipedia,"Morgan, Jonathan T. and Bouterse, Siko and Walls, Heather and Stierch, Sarah","We present the Teahouse, a pilot project for supporting and socializing new Wikipedia editors. Open collaboration systems like Wikipedia must continually recruit and retain new members in order to sustain themselves. Wikipedia's editor decline presents unique exigency for evaluating novel strategies to support newcomers and increase new user retention in such systems, particularly among demographics that are currently underrepresented in the user community. In this paper, we describe the design and deployment of Teahouse, and present preliminary findings. Our findings highlight the importance of intervening early in the editor lifecycle, providing user-friendly tools, creating safe spaces for newcomers, and facilitating positive interactions between newcomers and established community members.",2013,1087
Teaching Computer Science Students to Communicate Scientific Findings More Effectively,"Wyrich, Marvin and Wagner, Stefan","Science communication forms the bridge between computer science researchers and their target audience. Researchers who can effectively draw attention to their research findings and communicate them comprehensibly not only help their target audience to actually learn something, but also benefit themselves from the increased visibility of their work and person. However, the necessary skills for good science communication must also be taught, and this has so far been neglected in the field of software engineering education.We therefore designed and implemented a science communication seminar for bachelor students of computer science curricula. Students take the position of a researcher who, shortly after publication, is faced with having to draw attention to the paper and effectively communicate the contents of the paper to one or more target audiences. Based on this scenario, each student develops a communication strategy for an already published software engineering research paper and tests the resulting ideas with the other seminar participants.We explain our design decisions for the seminar, and combine our experiences with responses to a participant survey into lessons learned. With this experience report, we intend to motivate and enable other lecturers to offer a similar seminar at their university. Collectively, university lecturers can prepare the next generation of computer science researchers to not only be experts in their field, but also to communicate research findings more effectively.",2023,1088
Rationale in development chat messages: an exploratory study,"Alkadhi, Rana and La\c{t}a, Teodora and Guzman, Emitza and Bruegge, Bernd","Chat messages of development teams play an increasingly significant role in software development, having replaced emails in some cases. Chat messages contain information about discussed issues, considered alternatives and argumentation leading to the decisions made during software development. These elements, defined as rationale, are invaluable during software evolution for documenting and reusing development knowledge. Rationale is also essential for coping with changes and for effective maintenance of the software system. However, exploiting the rationale hidden in the chat messages is challenging due to the high volume of unstructured messages covering a wide range of topics. This work presents the results of an exploratory study examining the frequency of rationale in chat messages, the completeness of the available rationale and the potential of automatic techniques for rationale extraction. For this purpose, we apply content analysis and machine learning techniques on more than 8,700 chat messages from three software development projects. Our results show that chat messages are a rich source of rationale and that machine learning is a promising technique for detecting rationale and identifying different rationale elements.",2017,1089
Leveraging Change Intents for Characterizing and Identifying Large-Review-Effort Changes,"Wang, Song and Bansal, Chetan and Nagappan, Nachiappan and Philip, Adithya Abraham","Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. In most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis.In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes regarding review effort---changes with large review effort. Specifically, we first propose a feedback-driven and heuristics-based approach to obtain change intents. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on a large-scale project from Microsoft and three large-scale open source projects, i.e., Qt, Android, and OpenStack. Our results show that, (i) code changes with some intents are more likely to be LRE changes, (ii) machine learning based prediction models can efficiently help identify LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. The tool developed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process.",2019,1090
"Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence","Cao, Jacky and Lam, Kit-Yung and Lee, Lik-Hang and Liu, Xiaoli and Hui, Pan and Su, Xiang","Mobile Augmented Reality (MAR) integrates computer-generated virtual objects with physical environments for mobile devices. MAR systems enable users to interact with MAR devices, such as smartphones and head-worn wearables, and perform seamless transitions from the physical world to a mixed world with digital entities. These MAR systems support user experiences using MAR devices to provide universal access to digital content. Over the past 20 years, several MAR systems have been developed, however, the studies and design of MAR frameworks have not yet been systematically reviewed from the perspective of user-centric design. This article presents the first effort of surveying existing MAR frameworks (count: 37) and further discusses the latest studies on MAR through a top-down approach: (1) MAR applications; (2) MAR visualisation techniques adaptive to user mobility and contexts; (3) systematic evaluation of MAR frameworks, including supported platforms and corresponding features such as tracking, feature extraction, and sensing capabilities; (4) and underlying machine learning approaches supporting intelligent operations within MAR systems. Finally, we summarise the development of emerging research fields and the current state-of-the-art and discuss the important open challenges and possible theoretical and technical directions. This survey aims to benefit both researchers and MAR system developers alike.",2023,1091
Evolution of Conversations in the Age of Email Overload,"Kooti, Farshad and Aiello, Luca Maria and Grbovic, Mihajlo and Lerman, Kristina and Mantrach, Amin","Email is a ubiquitous communications tool in the workplace and plays an important role in social interactions. Previous studies of email were largely based on surveys and limited to relatively small populations of email users within organizations. In this paper, we report results of a large-scale study of more than 2 million users exchanging 16 billion emails over several months. We quantitatively characterize the replying behavior in conversations within pairs of users. In particular, we study the time it takes the user to reply to a received message and the length of the reply sent. We consider a variety of factors that affect the reply time and length, such as the stage of the conversation, user demographics, and use of portable devices. In addition, we study how increasing load affects emailing behavior. We find that as users receive more email messages in a day, they reply to a smaller fraction of them, using shorter replies. However, their responsiveness remains intact, and they may even reply to emails faster. Finally, we predict the time to reply, length of reply, and whether the reply ends a conversation. We demonstrate considerable improvement over the baseline in all three prediction tasks, showing the significant role that the factors that we uncover play, in determining replying behavior. We rank these factors based on their predictive power. Our findings have important implications for understanding human behavior and designing better email management applications for tasks like ranking unread emails.",2015,1092
"Content is King, Leadership Lags: Effects of Prior Experience on Newcomer Retention and Productivity in Online Production Groups","Karumur, Raghav Pavan and Yu, Bowen and Zhu, Haiyi and Konstan, Joseph A.","Organizers of online groups often struggle to recruit members who can most effectively carry out the group's activities and remain part of the group over time. In a study of a sample of 30,000 new editors belonging to 1,054 English WikiProjects, we empirically examine the effects of generalized prior work-productivity experience (measured by overall prior article edits), prior leadership experience (measured by overall prior project edits), and localized prior work-productivity experience (measured by pre-joining article edits on a project) on early retention and productivity. We find that (1)generalized prior work-productivity experience is positively associated with retention, but negatively associated with productivity (2) prior leadership experience is negatively associated with both retention and productivity, and (3) localized prior work-productivity experience is positively associated with both retention and productivity within that focal project. We then discuss implications to inform the designs of early interventions aimed at group success.",2018,1093
Fuzzing@Home: Distributed Fuzzing on Untrusted Heterogeneous Clients,"Jang, Daehee and Askar, Ammar and Yun, Insu and Tong, Stephen and Cai, Yiqin and Kim, Taesoo","Fuzzing is a practical technique to automatically find vulnerabilities in software. It is well-suited to running at scale with distributed computing platforms thanks to its parallelizability. Therefore, individual researchers and companies typically setup fuzzing platforms on multiple servers and run fuzzers in parallel. However, as such resources are private, they suffer from financial and physical limits. In this paper, we propose Fuzzing@Home; the first public collaborative fuzzing network, based on heterogeneous machines owned by potentially untrusted users. Using our system, multiple organizations (or individuals) can easily collaborate to fuzz a software of common interest in an efficient way. One can participate and earn economic benefits if the fuzzing network is tied to a bug-bounty program, or simply donate spare computing power as a volunteer. If the network compensates collaborators, system fairness becomes an issue. In this light, we devise a system to make the fuzzing results verifiable and devise cheat detection techniques to ensure integrity and fairness in collaboration. In terms of performance, we devise a technique to effectively sync the global coverage state, hence minimizing the overhead for verifying computation results. Finally, to increase participation, Fuzzing@Home uses WebAssembly to run fuzzers inside the web browser engine, allowing anyone to instantly join a fuzzing network with a single click on their mobile phone, tablet, or any modern computing device. To evaluate our system, we bootstrapped Fuzzing@Home with 72 open-source projects and ran experimental fuzzing networks for 330 days with 826 collaborators as beta testers.",2022,1094
Collaborative Security: A Survey and Taxonomy,"Meng, Guozhu and Liu, Yang and Zhang, Jie and Pokluda, Alexander and Boutaba, Raouf","Security is oftentimes centrally managed. An alternative trend of using collaboration in order to improve security has gained momentum over the past few years. Collaborative security is an abstract concept that applies to a wide variety of systems and has been used to solve security issues inherent in distributed environments. Thus far, collaboration has been used in many domains such as intrusion detection, spam filtering, botnet resistance, and vulnerability detection. In this survey, we focus on different mechanisms of collaboration and defense in collaborative security. We systematically investigate numerous use cases of collaborative security by covering six types of security systems. Aspects of these systems are thoroughly studied, including their technologies, standards, frameworks, strengths and weaknesses. We then present a comprehensive study with respect to their analysis target, timeliness of analysis, architecture, network infrastructure, initiative, shared information and interoperability. We highlight five important topics in collaborative security, and identify challenges and possible directions for future research. Our work contributes the following to the existing research on collaborative security with the goal of helping to make collaborative security systems more resilient and efficient. This study (1) clarifies the scope of collaborative security, (2) identifies the essential components of collaborative security, (3) analyzes the multiple mechanisms of collaborative security, and (4) identifies challenges in the design of collaborative security.",2015,1095
An Intimate Laboratory? Prostheses as a Tool for Experimenting with Identity and Normalcy,"Bennett, Cynthia L. and Cen, Keting and Steele, Katherine M. and Rosner, Daniela K.","This paper is about the aspects of ability, selfhood, and normalcy embodied in people's relationships with prostheses. Drawing on interviews with 14 individuals with upper-limb loss and diverse experiences with prostheses, we find people not only choose to use and not use prosthesis throughout their lives but also form close and complex relationships with them. The design of ""assistive"" technology often focuses on enhancing function; however, we found that prostheses played important roles in people's development of identity and sense of normalcy. Even when a prosthesis failed functionally, such as was the case with 3D-printed prostheses created by an on-line open-source maker community (e-NABLE), we found people still praised the design and initiative because of the positive impacts on popular culture, identity, and community building. This work surfaces crucial questions about the role of design interventions in identity production, the promise of maker communities for accelerating innovation, and a broader definition of ""assistive"" technology.",2016,1096
Middleware '23: Proceedings of the 24th International Middleware Conference: Industrial Track,,,2023,1097
SIGGRAPH '15: SIGGRAPH 2015: Studio,,"The world is becoming more malleable by the day, with new tools, applications, and methods to create, craft, build, and share. At SIGGRAPH 2015, the Studio focuses on disruptive practices in the world of content creation. Along with a renewed emphasis on technology, it presents projects from alternative fields that utilize and build new foundations in computer graphics - particularly those that extend beyond traditional screens and into the physical world - including themed entertainment, location-based installations, projection mapping, and advancements in augmented and virtual reality.",2015,1098
CHI '24: Proceedings of the CHI Conference on Human Factors in Computing Systems,,,2024,1099
ICCSIE '23: Proceedings of the 8th International Conference on Cyber Security and Information Engineering,,,2023,1100
EuroUSEC '22: Proceedings of the 2022 European Symposium on Usable Security,,,2022,1101
Session details: An introduction to sketch-based interfaces,"LaViola, Joseph",,2006,1102
EUROSEC '23: Proceedings of the 16th European Workshop on System Security,,,2023,1103
IDEAS '22: Proceedings of the 26th International Database Engineered Applications Symposium,,,2022,1104
LADC '23: Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing,,,2023,1105
SA '18: SIGGRAPH Asia 2018 Courses,,"At SIGGRAPH Asia 2018 hundreds of visitors will attend its courses to broaden and deepen their knowledge and to learn the secrets of new directions. The Crossover can also be offered via the presenters being from different backgrounds (e.g., computer science, art, animation, medicine, science and others), with each giving their unique perspective on the topic.",2018,1106
SoICT '22: Proceedings of the 11th International Symposium on Information and Communication Technology,,,2022,1107
ASPDAC '24: Proceedings of the 29th Asia and South Pacific Design Automation Conference,,"ASP-DAC is a high-quality and premium conference on Electronic Design Automation (EDA) area like other sister conferences such as Design Automation Conference (DAC), Design, Automation &amp; Test in Europe (DATE), International Conference on Computer-Aided Design (ICCAD), and Embedded Systems Week (ESWEEK). ASP-DAC started in 1995 and has continuously offered opportunity to know the recent advanced technologies on LSI design and design automation areas, and to communicate each other for researchers and designers around Asia and South Pacific regions.",2024,1108
SIGGRAPH '14: ACM SIGGRAPH 2014 Studio,,,2014,1109
ICIMMI '22: Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence,,,2022,1110
UIST '23: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,,,2023,1111
HASP '23: Proceedings of the 12th International Workshop on Hardware and Architectural Support for Security and Privacy,,,2023,1112
CNSM '22: Proceedings of the 18th International Conference on Network and Service Management,,"CNSM 2022 focuses on the theme ""Intelligent Management of Disruptive Network Technologies and Services"", that aims at capturing emerging approaches and intelligent solutions for dealing with disruptive network technologies, as well as associated services and applications.",2022,1113
Middleware '22: Proceedings of the 23rd ACM/IFIP International Middleware Conference,,,2022,1114
SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles,,"Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!",2023,1115
ISCA '23: Proceedings of the 50th Annual International Symposium on Computer Architecture,,,2023,1116
DIS '24 Companion: Companion Publication of the 2024 ACM Designing Interactive Systems Conference,,,2024,1117
ICSIM '24: Proceedings of the 2024 7th International Conference on Software Engineering and Information Management,,,2024,1118
EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering,,,2022,1119
SoCC '22: Proceedings of the 13th Symposium on Cloud Computing,,"SoCC 2022 is the thirteenth annual ACMSymposium on Cloud Computing, the premier conference on cloud computing. It brings together researchers, software developers, end-users, and practitioners interested in wide-ranging aspects of cloud computing, and it is the only conference co-sponsored by the ACM Special Interest Groups on Management of Data (SIGMOD) and on Operating Systems (SIGOPS).",2022,1120
ECSEE '23: Proceedings of the 5th European Conference on Software Engineering Education,,,2023,1121
SA '16: SIGGRAPH ASIA 2016 Courses,,"The SIGGRAPH Asia Courses program will feature a variety of instructional sessions catered to the different levels of expertise of our attendees. Sessions from introductory to advanced topics in computer graphics and interactive techniques will be conducted by speakers from renowned organizations and academic research institutions from over the world.The program has been the premier source for practitioners, developers, researchers, artists, and students who want to learn about the state-of-the-art technologies in computer graphics and their related topics.",2016,1122
"Seminal graphics: pioneering efforts that shaped the field, Volume 1",,,1998,1123
AutomotiveUI '22: Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications,,,2022,1124
ICDEL '23: Proceedings of the 2023 8th International Conference on Distance Education and Learning,,,2023,1125
"CNIOT '23: Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things",,,2023,1126
CGO 2023: Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization,,"Welcome to the 21st ACM/IEEE International Symposium on Code Generation and Optimization (CGO ’23), and to Montreal. After two years of virtual CGO, we are finally back in person! 

CGO provides a premier venue to bring together researchers and practitioners working at the interface of hardware and software on a wide range of optimization and code generation techniques and related issues. The conference spans the spectrum from purely static to fully dynamic approaches, and from pure software-based methods to specific architectural features and support for code generation and optimization.",2023,1127
Middleware '23: Proceedings of the 24th International Middleware Conference,,,2023,1128
"TEI '23: Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction",,,2023,1129
MICRO '23: Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture,,,2023,1130
UbiComp/ISWC '22 Adjunct: Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers,,,2022,1131
"IoTAAI '23: Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence",,,2023,1132
"MOBISYS '24: Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services",,,2024,1133
ICMLCA '23: Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application,,,2023,1134
EITCE '23: Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering,,,2023,1135
Aide-mémoire: Improving a Project’s Collective Memory via Pull Request–Issue Links,"Pârţachi, Profir-Petru and White, David R. and Barr, Earl T.","Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-mémoire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-mémoire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project’s lifetime and continuously improve traceability. Aide-mémoire is tailored for two specific instances of the general traceability problem—namely, commit to issue and pull request to issue links, with a focus on the latter—and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-mémoire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers.",2023,1136
Interacción '23: Proceedings of the XXIII International Conference on Human Computer Interaction,,,2023,1137
Sparse Shield: Social Network Immunization vs. Harmful Speech,"Petrescu, Alexandru and Truic\u{a}, Ciprian-Octavian and Apostol, Elena-Simona and Karras, Panagiotis","With the rise of social media users and the general shift of communication from traditional media to online platforms, the spread of harmful content (e.g., hate speech, misinformation, fake news) has been exacerbated. Harmful content in the form of hate speech causes a person distress or harm, having a negative impact on the individual mental health, with even more detrimental effects on the psychology of children and teenagers. In this paper, we propose an end-to-end solution with real-time capabilities to detect harmful content in real-time and mitigate its spread over the network. Our main contribution is Sparse Shield, a novel method that out-scales existing state-of-the-art methods for network immunization. We also propose a novel architecture for harmful speech mitigation that maximizes the impact of immunization. Our solution aims to identify a set of users for which to move harmful content at the bottom of the user feed, rather than censoring users. By immunizing certain network nodes in this manner, we minimize the negative impact on the network and minimize the interference with and limitation of individual freedoms: the information is not hidden but rather not as easy to reach without an explicit search. Our analysis is based on graphs built on real-world data collected from Twitter; these graphs reflect real user behavior. We perform extensive scalability experiments to prove the superiority of our method over existing state-of-the-art network immunization techniques. We also perform extensive experiments to showcase that Sparse Shield outperforms existing techniques on the task of harmful speech mitigation on a real-world dataset.",2021,1138
CloudArcade: A Blockchain Empowered Cloud Gaming System,"Zhao, Juntao and Chi, Yuanfang and Wang, Zehua and Leung, Victor C.M. and Cai, Wei","By rendering game scenes on the remote cloud and delivering real-time video to end devices via the Internet, cloud gaming enables players to access game services anytime anywhere despite the hardware capacity of their terminals. However, as a commercial service, the state-of-the-art payment models for cloud gaming are still in their preliminary stages. In this paper, we reveal the shortages of existing cloud gaming pricing models and propose CloudArcade, a token-based cloud gaming system that employs blockchain-empowered cryptocurrency as a payment method for the players using the cloud gaming services. By using cryptocurrency, CloudArcade provides a transparent and resource-aware pricing method, it also enables a time irrelevant silent payment on the floating price to protects users' payment. These features eliminate the quality of experience degradation caused by the spot price in the traditional dynamic pricing model on the QoE-aware service pricing. We also employ the payment channel in CloudArcade to improve the system performance. Discussions on service pricing criteria are put forward, open issues about token issuing and malicious resource speculation are also reviewed. We believe the design of CloudArcade can show a good generality on other QoE-aware and human-centered applications.",2020,1139
ICDAR '24: Proceedings of the 5th ACM Workshop on Intelligent Cross-Data Analysis and Retrieval,,,2024,1140
AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics,"Ghafouri, Vahid and Agarwal, Vibhor and Zhang, Yong and Sastry, Nishanth and Such, Jose and Suarez-Tangil, Guillermo","The introduction of ChatGPT and the subsequent improvement of Large Language Models (LLMs) have prompted more and more individuals to turn to the use of ChatBots, both for information and assistance with decision-making. However, the information the user is after is often not formulated by these ChatBots objectively enough to be provided with a definite, globally accepted answer.Controversial topics, such as ""religion"", ""gender identity"", ""freedom of speech"", and ""equality"", among others, can be a source of conflict as partisan or biased answers can reinforce preconceived notions or promote disinformation. By exposing ChatGPT to such debatable questions, we aim to understand its level of awareness and if existing models are subject to socio-political and/or economic biases. We also aim to explore how AI-generated answers compare to human ones. For exploring this, we use a dataset of a social media platform created for the purpose of debating human-generated claims on polemic subjects among users, dubbed Kialo.Our results show that while previous versions of ChatGPT have had important issues with controversial topics, more recent versions of ChatGPT (gpt-3.5-turbo) are no longer manifesting significant explicit biases in several knowledge areas. In particular, it is well-moderated regarding economic aspects. However, it still maintains degrees of implicit libertarian leaning toward right-winged ideals which suggest the need for increased moderation from the socio-political point of view. In terms of domain knowledge on controversial topics, with the exception of the ""Philosophical"" category, ChatGPT is performing well in keeping up with the collective human level of knowledge. Finally, we see that sources of Bing AI have slightly more tendency to the center when compared to human answers. All the analyses we make are generalizable to other types of biases and domains.",2023,1141
ICAIF '23: Proceedings of the Fourth ACM International Conference on AI in Finance,,,2023,1142
CHCHI '23: Proceedings of the Eleventh International Symposium of Chinese CHI,,,2023,1143
"UMAP '24: Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization",,,2024,1144
GoodIT '22: Proceedings of the 2022 ACM Conference on Information Technology for Social Good,,,2022,1145
SAW-BOT: Proposing Fixes for Static Analysis Warnings with GitHub Suggestions,D. Serban; B. Golsteijn; R. Holdorp; A. Serebrenik,"In this experience report we present SAW-BOT, a bot proposing fixes for static analysis warnings. The bot has been evaluated with five professional software developers by means of a Wizard of Oz experiment, semi-structured interviews and the mTAM questionnaire. We have observed that developers prefer GitHub suggestions to two baseline operation modes. Our study indicates that GitHub suggestions are a viable mechanism for implementing bots proposing fixes for static analysis warnings.",2021,1146
Leveraging Predictions From Multiple Repositories to Improve Bot Detection,N. Chidambaram; A. Decan; M. Golzadeh,"Contemporary social coding platforms such as GitHub facilitate collaborative distributed software development. Developers engaged in these platforms often use machine accounts (bots) for automating effort-intensive or repetitive activities. Determining whether a contributor corresponds to a bot or a human account is important in socio-technical studies, for example to assess the positive and negative impact of using bots, analyse the evolution of bots and their usage, identify top human contributors, and so on. BoDeGHa is one of the bot detection tools that have been proposed in the literature. It relies on comment activity within a single repository to predict whether an account is driven by a bot or by a human. This paper presents preliminary results on how the effectiveness of BoDeGHa can be improved by combining the predictions obtained from many repositories at once. We found that doing this not only increases the number of cases for which a prediction can be made, but that many diverging predictions can be fixed this way. These promising, albeit preliminary, results suggest that the “wisdom of the crowd” principle can improve the effectiveness of bot detection tools.",2022,1147
Identifying bot activity in GitHub pull request and issue comments,M. Golzadeh; A. Decan; E. Constantinou; T. Mens,"Development bots are used on Github to automate repetitive activities. Such bots communicate with human actors via issue comments and pull request comments. Identifying such bot comments allows to prevent bias in socio-technical studies related to software development. To automate their identification, we propose a classification model based on natural language processing. Starting from a balanced ground-truth dataset of 19,282 PR and issue comments, we encode the comments as vectors using a combination of the bag of words and TF-IDF techniques. We train a range of binary classifiers to predict the type of comment (human or bot) based on this vector representation. A multinomial Naive Bayes classifier provides the best results. Its performance on a test set containing 50% of the data achieves an average precision, recall, and F1 score of 0.88. Although the model shows a promising result on the pull request and issue comments, further work is required to generalize the model on other types of activities, like commit messages and code reviews.",2021,1148
On the Accuracy of Bot Detection Techniques,M. Golzadeh; A. Decan; N. Chidambaram,"Development bots are often used to automate a wide variety of repetitive tasks in collaborative software development. Such bots are commonly among the most active project contributors in terms of commit activity. As such, tools that analyse contributor activity (e.g., for recognizing and giving credit to project members for their contributions) need to take into account the bots and exclude their activity. While there are a few techniques to detect bots in software repositories, these techniques are not perfect and may miss some bots or may wrongly identify some human accounts as bots. In this paper, we present an exploratory study on the accuracy of bot detection techniques on a set of 540 accounts from 27 GitHub projects. We show that none of the bot detection techniques are accurate enough to detect bots among the 20 most active contributors of each project. We show that combining these techniques drastically increases the accuracy and recall of bot detection. We also highlight the importance of considering bots when attributing contributions to humans, since bots are prevalent among the top contributors and responsible for large proportions of commits.",2022,1149
On the Adoption of a TODO Bot on GITHuB: A Preliminary Study,H. Mohayeji; F. Ebert; E. Arts; E. Constantinou; A. Serebrenik,"Bots support different software maintenance and evolution activities, such as code review or executing tests. Recently, several bots have been proposed to help developers to keep track of postponed activities, expressed by means of TODO comments: e.g., TODO Bot automatically creates a GITHuB issue when a TODO comment is added to a repository, increasing visibility of TODO comments. In this work, we perform a preliminary evaluation of the impact of the TODO Bot on software development practice. We conjecture that the introduction of the TODO Bot would facilitate keeping track of the TODO comments, and hence encourage developers to use more TODO comments in their code changes. To evaluate this conjecture, we analyze all the 2,208 repositories which have at least one GITHuB issue created by the TODO Bot. Firstly, we investigate to what extent the bot is being used and describe the repositories using the bot. We observe that the majority (54%) of the repositories which adopted the TODO Bot are new, i.e., were created within less than one month of first issue created by the bot, and from those, more than 60% have the issue created within three days. We observe a statistically significant increase in the number of the TODO comments after the adoption of the bot, however with a small effect size. Our results suggest that the adoption of the TODO Bot encourages developers to introduce TODO comments rendering the postponed decisions more visible. Nevertheless, it does not speed up the process of addressing TODO comments or corresponding GITHuB issues.",2022,1150
Bot With Interactions: Improving GitHub Pull-Request Feedback Through Two-Way Communication,Z. Hu; E. Gehringer,"Projects in our software-engineering course require students to submit GitHub pull requests to an open-source software project containing more than 30,000 lines of source code. Once submitted, code is checked by a static code analyzer, as well as a bot named Danger Bot. The Danger Bot is able to detect more than 40 programmable system-specific guideline violations (which are different from static analysis rules). Although use of the Danger Bot was associated with a decrease of 40% in guideline violations, it also emitted some false positives. Neither staff nor students could change the feedback given by the Danger Bot on pull-request pages, because there was only one-way communication, from bot to humans. In this paper, we discuss how we bypass the limitations of the Danger Bot by introducing the Danger Bot 2.0 (hereinafter, ”the bot”) with two-way communication between humans and the bot. In this way, if students or teaching staff find some false positives produced by the bot, they can tell the bot about this. After teaching staff cancel a particular message as a false positive, the bot will not report that message again until it is re-enabled by the staff. We conducted a pilot study for the bot with two-way communication. Results showed that the bot with two-way communication was associated with a significant 70% decrease of unresolved guideline violations and the elimination of 100% false positives in them. The majority of two-way communications happened between the teaching staff and the bot, especially when teaching staff cancelled all the false positive violations after inspection. There was only one communication between students and the bot.",2023,1151
Do Bots Modify the Workflow of GitHub Teams?,S. Saadat; N. Colmenares; G. Sukthankar,"The ever-increasing complexity of modern software engineering projects makes the usage of automated assistants imperative. Bots can be used to complete repetitive tasks during development and testing, as well as promoting communication between team members through issue reporting and documentation. Although the ultimate aim of these automated assistants is to speed taskwork completion, their inclusion into GitHub repositories may affect teamwork as well. This paper studies the question of how bots modify the team workflow. We examined the event sequences of repositories with bots and without bots using a contrast motif discovery method to detect subsequences that are more prevalent in one set of event sequences vs. the other. Our study reveals that teams with bots are more likely to intersperse comments throughout their coding activities, while not actually being more prolific commenters.",2021,1152
Software Bots in Software Engineering: Benefits and Challenges,M. Wessel; M. A. Gerosa; E. Shihab,"Software bots are becoming increasingly popular in software engineering (SE). In this tutorial, we define what a bot is and present several examples. We also discuss the many benefits bots provide to the SE community, including helping in development tasks (such as pull request review and integration) and onboarding newcomers to a project. Finally, we discuss the challenges related to interacting with and developing software bots.",2022,1153
"Bots Don’t Mind Waiting, Do They? Comparing the Interaction With Automatically and Manually Created Pull Requests",M. Wyrich; R. Ghit; T. Haller; C. Müller,"As a maintainer of an open source software project, you are usually happy about contributions in the form of pull requests that bring the project a step forward. Past studies have shown that when reviewing a pull request, not only its content is taken into account, but also, for example, the social characteristics of the contributor. Whether a contribution is accepted and how long this takes therefore depends not only on the content of the contribution. What we only have indications for so far, however, is that pull requests from bots may be prioritized lower, even if the bots are explicitly deployed by the development team and are considered useful. One goal of the bot research and development community is to design helpful bots to effectively support software development in a variety of ways. To get closer to this goal, in this GitHub mining study, we examine the measurable differences in how maintainers interact with manually created pull requests from humans compared to those created automatically by bots. About one third of all pull requests on GitHub currently come from bots. While pull requests from humans are accepted and merged in 72.53% of all cases, this applies to only 37.38% of bot pull requests. Furthermore, it takes significantly longer for a bot pull request to be interacted with and for it to be merged, even though they contain fewer changes on average than human pull requests. These results suggest that bots have yet to realize their full potential.",2021,1154
An Exploratory Study of Reactions to Bot Comments on GitHub,J. C. Farah; B. Spaenlehauer; X. Lu; S. Ingram; D. Gillet,"The widespread use of bots to support software development makes social coding platforms such as GitHub a particularly rich source of data for the study of human-bot interaction. Software development bots are used to automate repetitive tasks, interacting with their human counterparts via comments posted on the various discussion interfaces available on such platforms. One type of interaction supported by GitHub involves reacting to comments using predefined emoji. To investigate how users react to bot comments, we conducted an observational study comprising 54 million GitHub comments, with a particular focus on comments that elicited the laugh reaction. The results from our analysis suggest that some reaction types are not equally distributed across human and bot comments and that a bot's design and purpose influence the types of reactions it receives. Furthermore, while the laugh reaction is not exclusively used to express laughter, it can be used to convey humor when a bot behaves unexpectedly. These insights could inform the way bots are designed and help developers equip them with the ability to recognize and recover from unanticipated situations. In turn, bots could better support the communication, collaboration, and productivity of teams using social coding platforms.",2022,1155
Towards an Autonomous Bot for Automatic Source Code Refactoring,M. Wyrich; J. Bogner,"Continuous refactoring is necessary to maintain source code quality and to cope with technical debt. Since manual refactoring is inefficient and error-prone, various solutions for automated refactoring have been proposed in the past. However, empirical studies have shown that these solutions are not widely accepted by software developers and most refactorings are still performed manually. For example, developers reported that refactoring tools should support functionality for reviewing changes. They also criticized that introducing such tools would require substantial effort for configuration and integration into the current development environment. In this paper, we present our work towards the Refactoring-Bot, an autonomous bot that integrates into the team like a human developer via the existing version control platform. The bot automatically performs refactorings to resolve code smells and presents the changes to a developer for asynchronous review via pull requests. This way, developers are not interrupted in their workflow and can review the changes at any time with familiar tools. Proposed refactorings can then be integrated into the code base via the push of a button. We elaborate on our vision, discuss design decisions, describe the current state of development, and give an outlook on planned development and research activities.",2019,1156
Bot Detection in GitHub Repositories,N. Chidambaram; P. R. Mazrae,"Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the GrimoireLab software development analytics platform. In this work, we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using Kibana.",2022,1157
A Dataset of Bot and Human Activities in GitHub,N. Chidambaram; A. Decan; T. Mens,"Software repositories hosted on GitHub frequently use development bots to automate repetitive, effort intensive and error-prone tasks. To understand and study how these bots are used, state-of-the-art bot identification tools have been developed to detect bots based on their comments in commits, issues and pull requests. Given that bots can be involved in many other activity types, there is a need to consider more activities that they are carrying out in the software repositories they are involved in. We therefore propose a curated dataset of such activities carried out by bots and humans involved in GitHub repositories. The dataset was constructed by identifying 24 high-level activity types that could be extracted from 15 lower-level event types that were queried from GitHub’s event stream API for all considered bots and humans. The proposed dataset contains around 834K activities performed by 385 bots and 616 humans involved in GitHub repositories, during an observation period ranging from 25 November 2022 to 9 March 2023. By analysing the activity patterns of bots and humans, this dataset could lead to better bot identification tools and empirical studies on how bots play a role in collaborative software development.",2023,1158
Should I Stale or Should I Close? An Analysis of a Bot That Closes Abandoned Issues and Pull Requests,M. Wessel; I. Steinmacher; I. Wiese; M. A. Gerosa,"On GitHub, projects use bots to automate predefined and repetitive tasks related to issues and pull requests. Our research investigates the adoption of the stale bot, which helps maintainers triaging abandoned issues and pull requests. We analyzed the bots' configuration settings and their modifications over time. These settings define the time for tagging issues and pull request as stale and closing them. We collected data from 765 OSS projects hosted on GitHub. Our results indicate that most of the studied projects made no more than three modifications in the configurations file, issues tagged as bug reports are exempt from being considered stale, while the same occurs with pull requests that need some input to be processed.",2019,1159
Detecting Bot on GitHub Leveraging Transformer-based Models: A Preliminary Study,J. Zhang; X. Wu; Y. Zhang; S. Xu,"Bots are prevalent contributors in collaborative software development, necessitating accurate detection techniques. This preliminary study aims to leveraging public datasets and Transformer-based models (i.e., BERT, CodeBERT, RoBERTa, BART, and PLBART) for the bot detection task. Our experimental result reveals that CodeBERT achieves the highest performance, with an impressive accuracy score of 94.1%.",2023,1160
BotHunter: An Approach to Detect Software Bots in GitHub,A. Abdellatif; M. Wessel; I. Steinmacher; M. A. Gerosa; E. Shihab,"Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level.",2022,1161
How Do Software Developers Use GitHub Actions to Automate Their Workflows?,T. Kinsman; M. Wessel; M. A. Gerosa; C. Treude,"Automated tools are frequently used in social coding repositories to perform repetitive activities that are part of the distributed software development process. Recently, GitHub introduced GitHub Actions, a feature providing automated work-flows for repository maintainers. Although several Actions have been built and used by practitioners, relatively little has been done to evaluate them. Understanding and anticipating the effects of adopting such kind of technology is important for planning and management. Our research is the first to investigate how developers use Actions and how several activity indicators change after their adoption. Our results indicate that, although only a small subset of repositories adopted GitHub Actions to date, there is a positive perception of the technology. Our findings also indicate that the adoption of GitHub Actions increases the number of monthly rejected pull requests and decreases the monthly number of commits on merged pull requests. These results are especially relevant for practitioners to understand and prevent undesirable effects on their projects.",2021,1162
Suggestion Bot: Analyzing the Impact of Automated Suggested Changes on Code Reviews,N. Palvannan; C. Brown,"Peer code reviews are crucial for maintaining the quality of the code in software repositories. Developers have introduced a number of software bots to help with the code review process. Despite the benefits of automating code review tasks, many developers face challenges interacting with these bots due to non-comprehensive feedback and disruptive notifications. In this paper, we analyze how incorporating a bot in software development cycle will decrease turnaround time of pull request. We created a bot called “SUGGESTION BOT” to automatically review the code base using GitHub’s suggested changes functionality in order to solve this issue. A preliminary comparative empirical investigation between the utilization of this bot and manual review procedures was also conducted in this study. We evaluate SUGGESTION BOT concerning its impact on review time and also analyze whether the comments given by the bot are clear and useful for users. Our results provide implications for the design of future systems and improving human-bot interactions for code review.",2023,1163
On Twitter Bots Behaving Badly: A Manual and Automated Analysis of Python Code Patterns on GitHub,A. Millimaggi; F. Daniel,"Bots, i.e., algorithmically driven entities that behave like humans in online communications, are increasingly infiltrating social conversations on the Web. If not properly prevented, this presence of bots may cause harm to the humans they interact with. This article aims to understand which types of abuse may lead to harm and whether these can be considered intentional or not. We manually review a dataset of 60 Twitter bot code repositories on GitHub, derive a set of potentially abusive actions, characterize them using a taxonomy of abstract code patterns, and assess the potential abusiveness of the patterns. The article then describes the design and implementation of a code pattern recognizer and uses the pattern recognizer to automatically analyze a dataset of 786 Python bot code repositories. The study does not only reveal the existence of 28 communication-specific code patterns - which could be used to assess the harmfulness of bot code - but also their consistent presence throughout all studied repositories.",2019,1164
Together or Apart? Investigating a mediator bot to aggregate bot’s comments on pull requests,E. Ribeiro; R. Nascimento; I. Steinmacher; L. Xavier; M. Gerosa; H. de Paula; M. Wessel,"Software bots connect users and tools, streamlining the pull request review process in social coding platforms. However, bots can introduce information overload into developers’ communication. Information overload is especially problematic for newcomers, who are still exploring the project and may feel overwhelmed by the number of messages. Inspired by the literature of other domains, we designed and evaluated FunnelBot, a bot that acts as a mediator between developers and other bots in the repository. We conducted a within-subject study with 25 newcomers to capture their perceptions and preferences. Our results provide insights for bot developers who want to mitigate noise and create bots for supporting newcomers, laying a foundation for designing better bots.",2022,1165
Classifying Issues into Custom Labels in GitBot,D. Park; H. Cho; S. Lee,"GitBots are bots in Git repositories to automate repetitive tasks that occur in software development, testing and maintenance. Git-Bots are expected to perform the repetitive tasks that are normally done by humans, such as feedback on issue reports and answers to questions. However, studies on GitBots for labeling issue reports fall short of replacing developers' labeling tasks. Developers still manually attach labels to issues. In this paper, we introduce an issue labeling bot classifying issue reports into custom labels that developers define by themselves so that our bot could attach labels in a similar way to human behavior.",2022,1166
"Bots for Pull Requests: The Good, the Bad, and the Promising",M. Wessel; A. Abdellatif; I. Wiese; T. Conte; E. Shihab; M. A. Gerosa; I. Steinmacher,"Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (“the good”). However, their interactions can be disruptive and noisy and lead to information overload (“the bad”). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (“the promising”). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",2022,1167
Improving Feedback on GitHub Pull Requests: A Bots Approach,Z. Hu; E. F. Gehringer,"Rising enrollments make it difficult for instructors and teaching assistants to give adequate feedback on each student's work. Our course projects require students to submit GitHub pull requests as deliverables for their open-source software (OSS) projects. We have set up a static code analyzer and a continuous integration service on GitHub to help students check different aspects of the code. However, these tools have some limitations. In this paper, we discuss how we bypass the limitations of existing tools by implementing three Internet bots. These bots are either open source or free for OSS projects and can be easily integrated with any GitHub repositories. One-hundred one Computer Science and Computer Engineering masters students participated in our study. The survey results showed that more than 84% of students thought bots can help them to contribute code with better quality. We analyzed 396 pull requests. Results revealed that bots can provide more timely feedback than teaching staff. The Danger Bot is associated with a significant reduction system-specific guideline violations (by 39%), and the Code Climate Bot is associated with a significant 60% decrease of code smells in student contributions. However, we found that the Travis CI Bot did not help student contributions pass automated tests.",2019,1168
Effects of Adopting Code Review Bots on Pull Requests to OSS Projects,M. Wessel; A. Serebrenik; I. Wiese; I. Steinmacher; M. A. Gerosa,"Software bots, which are widely adopted by Open Source Software (OSS) projects, support developers on several activities, including code review. However, as with any new technology adoption, bots may impact group dynamics. Since understanding and anticipating such effects is important for planning and management, we investigate how several activity indicators change after the adoption of a code review bot. We employed a regression discontinuity design on 1,194 software projects from GitHub. Our results indicate that the adoption of code review bots increases the number of monthly merged pull requests, decreases monthly non-merged pull requests, and decreases communication among developers. Practitioners and maintainers may leverage our results to understand, or even predict, bot effects on their projects' social interactions.",2020,1169
Understanding the Impact of Bots on Developers Sentiment and Project Progress,A. Gao; S. Chen; T. Wang; J. Deng,"Software developers advance the project process by contributing and discussing on the code platform. Software bot acts as an assistant to help developers deal with repetitive tasks. In this paper, we explore whether the adoption of bots has an impact on developer sentiment and projects progress. We collected issues, pull requests and comments from GitHub popular projects. And we found that human users had significantly reduced positive sentiment in bot-created issues and PRs. The average merging time of the bot-created PRs is significantly shorter than human-created ones. The average solving time of bot-commented issues is significantly longer than human-created ones.",2022,1170
BDGOA: A bot detection approach for GitHub OAuth Apps,Z. Liao; X. Huang; B. Zhang; J. Wu; Y. Cheng,"As various software bots are widely used in open source software repositories, some drawbacks are coming to light, such as giving newcomers non-positive feedback and misleading empirical studies of software engineering researchers. Several techniques have been proposed by researchers to perform bot detection, but most of them are limited to identifying bots performing specific activities, let alone distinguishing between GitHub App and OAuth App. In this paper, we propose a bot detection technique for OAuth App, named BDGOA. 24 features are used in BDGOA, which can be divided into three dimensions: account information, account activity, and text similarity. To better explore the behavioral features, we define a fine-grained classification of behavioral events and introduce self-similarity to quantify the repeatability of behavioral sequence. We leverage five machine learning classifiers on the benchmark dataset to conduct bot detection, and finally choose random forest as the classifier, which achieves the highest F1-score of 95.83%. The experimental results comparing with the state-of-the-art approaches also demonstrate the superiority of BDGOA.",2023,1171
Recognizing Bot Activity in Collaborative Software Development,M. Golzadeh; T. Mens; A. Decan; E. Constantinou; N. Chidambaram,"Using popular open source projects on GitHub, we provide evidence that bots are regularly among the most active contributors, even though GitHub does not explicitly acknowledge their presence. This poses a problem for techniques that analyze human contributor activity.",2022,1172
"Human, bot or both? A study on the capabilities of classification models on mixed accounts",N. Cassee; C. Kitsanelis; E. Constantinou; A. Serebrenik,"Several bot detection algorithms have recently been discussed in the literature, as software bots that perform maintenance tasks have become more popular in recent years. State-of-the-art techniques detect bots based on a binary classification, where a GitHub account is either a human or a bot. However, this conceptualisation of bot detection as an account-level binary classification problem fails to account for ‘mixed accounts’, accounts that are shared between a human and a bot, and that therefore exhibit both bot and human activity. By using binary classification models for bot detection, researchers might hence mischaracterize both human and bot behavior in software maintenance. This calls for conceptualisation of bot detection through a comment-level classification. However, the single such approach solely investigates a small number of mixed account comments. The nature of mixed accounts on GitHub is thus yet unknown, and the absence of appropriate datasets make this a difficult problem to study. In this paper, we investigate three comment-level classification models and we evaluate these classifiers on a manually labeled dataset of mixed accounts. We find that the best classifiers based on these classification models achieve a precision and recall between 88% and 96%. However, even the most accurate comment-level classifier cannot accurately detect mixed accounts; rather, we find that textual content alone, or textual content combined with templates used by bots, are very effective features for the detection of both bot and mixed accounts. Our study calls for more accurate bot detection techniques capable of identifying mixed accounts, and as such supporting more refined insights in software maintenance activities performed by humans and bots on social coding sites.",2021,1173
Interactive Refactoring Documentation Bot,S. Rebai; O. Ben Sghaier; V. Alizadeh; M. Kessentini; M. Chater,"The documentation of code changes is significantly important but developers ignore it, most of the time, due to the pressure of the deadlines. While developers may document the most important features modification or bugs fixing, recent empirical studies show that the documentation of quality improvements and/or refactoring is often omitted or not accurately described. However, the automated or semi-automated documentation of refactorings has not been yet explored despite the extensive work on the remaining steps of refactoring including the detection, prioritization and recommendation. In this paper, we propose a semi-automated refactoring documentation bot that helps developers to interactively check and validate the documentation of the refactorings and/or quality improvements at the file level for each opened pull-request before being reviewed or merged to the master. The bot starts by checking the pullrequest if there are significant quality changes and refactorings at the file level and whether they are documented by the developer. Then, it checks the validity of the developers description of the refactorings, if any. Based on that analysis, the documentation bot will recommend a message to document the refactorings, their locations and the quality improvement for that pull-request when missing information is found. Then, the developer can modify his pull request description by interacting with the bot to accept/modify/reject part of the proposed documentation. Since refactoring do not happen in isolation most of the time, the bot is documenting the impact of a sequence of refactorings, in a pull-request, on quality and not each refactoring in isolation. We conducted a human survey with 14 active developers to manually evaluate the relevance and the correctness of our tool on different pull requests of 5 open source projects and one industrial system. The results show that the participants found that our bot facilitates the documentation of their quality-related changes and refactorings.",2019,1174
FixMe: A GitHub Bot for Detecting and Monitoring On-Hold Self-Admitted Technical Debt,S. Phaithoon; S. Wongnil; P. Pussawong; M. Choetkiertikul; C. Ragkhitwetsagul; T. Sunetnanta; R. Maipradit; H. Hata; K. Matsumoto,"Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related ""On-hold SATD"", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN_YQ.",2021,1175
Analysis ChatGPT Potential: Transforming Software Development with AI Chat Bots,J. W. Purwoko; T. Abdullah; B. Wijaya; A. A. Santoso Gunawan; K. E. Saputra,"Artificial intelligence (AI) is a technology that is constantly evolving and is being applied more frequently in many facets of society, including product and service development. Chatbots, which are computer programs that can connect with people through chat or voice apps, are one sort of AI that is evolving quickly. However, there is still much debate among scientists and professionals about whether AI advancements like ChatGPT can help software engineers with their daily tasks or even replace the work of software engineers. So, on this occasion, we conduct research on whether AI (artificial intelligence) is capable of helping software engineers and how far AI can assist software engineers. In this study, we aim to evaluate the effectiveness of ChatGPT as an AI tool for code retrieval and its potential to help or replace software engineers. Our research methodology involves using ChatGPT to refactor provided code and make a simple application from scratch. The results of this research show that AI chatbot models like ChatGPT cannot replace software developers 100",2023,1176
Guidelines for Developing Bots for GitHub,M. Wessel; A. Zaidman; M. A. Gerosa; I. Steinmacher,"Projects on GitHub rely on the automation provided by software development bots. Nevertheless, the presence of bots can be annoying and disruptive to the community. Backed by multiple studies with practitioners, this article provides guidelines for developing and maintaining software bots.",2023,1177
"Expecting the Unexpected: Distilling Bot Development, Challenges, and Motivations",A. M. Pinheiro; C. S. Rabello; L. B. Furtado; G. Pinto; C. R. B. de Souza,"Software bots are becoming an increasingly popular tool in the software development landscape, which is particularly due to their potential of use in several different contexts. More importantly, software developers interested in transitioning to bot development may have to face challenges intrinsic related to bot software development. However, so far, it is still unclear what is the profile of bot developers, what motivate them, or what challenges do they face when dealing with bot development. To shed an initial light on this direction, we conducted a survey with 43 Github users who have been involved (showing their interest or actively contributing to) in bot software projects.",2019,1178
Autonomy Is An Acquired Taste: Exploring Developer Preferences for GitHub Bots,A. Ghorbani; N. Cassee; D. Robinson; A. Alami; N. A. Ernst; A. Serebrenik; A. Wąsowski,"Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",2023,1179
How Does Bot Affect Developer’s Sentiment: An Empirical Study on GitHub Issues and PRs,A. Gao; Y. Zhang; T. Wang; S. Chen; J. Deng,"Software bots act as assistants to help developers in GitHub issues and pull requests (PRs) solution. In this paper, we explore whether software bots affect developer’s sentiment through an empirical study. We find that developers express more neutral sentiment in bot-involved projects and have more positive sentiment in bot-involved issues and PRs. Further, we find developers are less positive in issues and PRs created by bots. To Figure out how developer’s sentiment looks like when mention bots, we use a combination of quantitative and qualitative analyses to explore the different sentiment of developers toward bots and explain the reasons.",2022,1180
From Specialized Mechanics to Project Butlers: The Usage of Bots in Open Source Software Development,Z. Wang; Y. Wang; D. Redmiles,We seek to identify how open source software (OSS) projects adopt bot services from a diverse set of selections. Our empirical research examines bot usage in the most popular OSS repositories in GitHub.,2022,1181
Automating Dependency Updates in Practice: An Exploratory Study on GitHub Dependabot,R. He; H. He; Y. Zhang; M. Zhou,"Dependency management bots automatically open pull requests to update software dependencies on behalf of developers. Early research shows that developers are suspicious of updates performed by dependency management bots and feel tired of overwhelming notifications from these bots. Despite this, dependency management bots are becoming increasingly popular. Such contrast motivates us to investigate Dependabot, currently the most visible bot on GitHub, to reveal the effectiveness and limitations of state-of-art dependency management bots. We use exploratory data analysis and a developer survey to evaluate the effectiveness of Dependabot in keeping dependencies up-to-date, interacting with developers, reducing update suspicion, and reducing notification fatigue. We obtain mixed findings. On the positive side, projects do reduce technical lag after Dependabot adoption and developers are highly receptive to its pull requests. On the negative side, its compatibility scores are too scarce to be effective in reducing update suspicion; developers tend to configure Dependabot toward reducing the number of notifications; and 11.3% of projects have deprecated Dependabot in favor of other alternatives. The survey confirms our findings and provides insights into the key missing features of Dependabot. Based on our findings, we derive and summarize the key characteristics of an ideal dependency management bot which can be grouped into four dimensions: configurability, autonomy, transparency, and self-adaptability.",2023,1182
RefBot: Intelligent Software Refactoring Bot,V. Alizadeh; M. A. Ouali; M. Kessentini; M. Chater,"The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any ""open"" or ""merge"" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.",2019,1183
Bobble-Bot: An educational platform for real-time control with ROS,M. Moore; J. Sooknanan; J. Holley,"To help meet the growing needs of the robotics community, the Robot Operating System (ROS) is currently undergoing a major redesign in which one of its primary design goals is to prioritize support for real-time computing (ROS2). Real-time computing techniques are found in most industrial robots, and yet this capability is noticeably lacking from many of the open-source ROS robots in existence today. This knowledge gap is problematic for students using ROS and their future employers with realtime systems requiring development and maintenance. Bobble-Bot is an open-source ROS robot that was created to close this gap. The robot demonstrates the use of real-time control using ROS in a fun and engaging way. Bobble-Bot is a modern example of the classic inverted pendulum problem that is commonly covered in control theory. It is a controls problem that requires a real-time controller to maintain stability. This paper introduces Bobble-Bot and its accompanying simulator in order to demonstrate how to design a real-time system that uses the ROS and ROS2 frameworks. In addition, results from simulation and hardware tests are provided along with links to Bobble-Bot's open-source software and project documentation.",2019,1184
Between JIRA and GitHub: ASFBot and its Influence on Human Comments in Issue Trackers,A. Moharil; D. Orlov; S. Jameel; T. Trouwen; N. Cassee; A. Serebrenik,"Open-Source Software (OSS) projects have adopted various automations for repetitive tasks in recent years. One common type of automation in OSS is bots. In this exploratory case study, we seek to understand how the adoption of one particular bot (ASFBot) by the Apache Software Foundation (ASF) impacts the discussions in the issue-trackers of these projects. We use the SmartShark dataset to investigate whether the ASFBot affects (i) human comments mentioning pull requests and fixes in issue comments and (ii) the general human comment rate on issues. We apply a regression discontinuity design (RDD) on nine ASF projects that have been active both before and after the ASFBot adoption. Our results indicate (i) an immediate decrease in the number of median comments mentioning pull requests and fixes after the bot adoption, but the trend of a monthly decrease in this comment count is reversed, and (ii) no effect in the number of human comments after the bot adoption. We make an effort to gather first insights in understanding the impact of adopting the ASFBot on the commenting behavior of developers who are working on ASF projects.",2022,1185
How to Design a Program Repair Bot? Insights from the Repairnator Project,S. Urli; Z. Yu; L. Seinturier; M. Monperrus,"Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a ""program repair bot"" and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 523 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 15 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.",2018,1186
Bringing Automation to the Classroom: A ChatOps-Based Approach,E. Mulyana; R. Hakimi; Hendrawan,In this paper we present the design and implementation of a chatbot-based virtual assistant called LTKA-Bot. Its main function is to streamline and to automate manual and administrative tasks while supporting other course-related activities. It differs from other recent approaches in that it is based on the ChatOps paradigm instead of on some AI-based schemes. LTKA-Bot introduces a case of automation and demonstrates its potentials in the area of higher education which is steadily transformed to cope with technological progresses and administrative policy dynamics including accreditation. International accreditation body such as ABET requires fulfillment of certain criteria which in turn also requires appropriate course design and quite a lot of document works. LTKA-Bot borrows the idea of automation which is very common in the modern tech companies to cope with such challenges. It facilitates more efficient course-related activities while satisfying all document requirement with minimal effort.,2018,1187
Understanding the Time to First Response in GitHub Pull Requests,K. A. Hasan; M. Macedo; Y. Tian; B. Adams; S. Ding,"The pull-based development is widely adopted in modern open-source software (OSS) projects, where developers propose changes to the codebase by submitting a pull request (PR). However, due to many reasons, PRs in OSS projects frequently experience delays across their lifespan, including prolonged waiting times for the first response. Such delays may significantly impact the efficiency and productivity of the development process, as well as the retention of new contributors as long-term contributors.In this paper, we conduct an exploratory study on the time-to-first-response for PRs by analyzing 111,094 closed PRs from ten popular OSS projects on GitHub. We find that bots frequently generate the first response in a PR, and significant differences exist in the timing of bot-generated versus human-generated first responses. We then perform an empirical study to examine the characteristics of bot- and human-generated first responses, including their relationship with the PR’s lifetime. Our results suggest that the presence of bots is an important factor contributing to the time-to-first-response in the pull-based development paradigm, and hence should be separately analyzed from human responses. We also report the characteristics of PRs that are more likely to experience long waiting for the first human-generated response. Our findings have practical implications for newcomers to understand the factors contributing to delays in their PRs.",2023,1188
"Exploring Activity and Contributors on GitHub: Who, What, When, and Where",X. Xia; Z. Weng; W. Wang; S. Zhao,"Apart from being a code hosting platform, GitHub is the place where large-scale open collaborations and contributions happen. Every minute, thousands of developers are submitting code, having discussions of issues or pull requests, with all user behaviors recorded in the GitHub Event Stream (GES). Exploration of the activities in the GES could help understand who is active, the way they work, the time when they are active and even their location. To this end, a large-scale analysis was initially performed based on the 0.86 billion event records generated in 2020. We extracted 902K active contributors out of 14 million GitHub accounts by observing their activity distribution, then explored their behavior distribution, active time in the day and week, and estimated time zone distributions on the basis of their circadian activity rhythm. To go deeper, a case study of 79 projects in CNCF and contrast analyses of different project maturity levels were conducted. Our results showed that from a macro perspective, bots are increasingly more active and can serve numerous projects. Contributors work on weekdays, and are globally more inclined toward the daytime working hours in the Americas and Europe. The time zone distribution also reveals that UTC+2 and UTC-4 have the most active contributors. A critical discovery was the validation and quantification of a high bus factor risk exists in the OSS ecosystem. Whether from a large group point of view or within specific projects, a rather small group of OSS contributors (less than 20%) undertook the majority of the work. The GES can provide a wealth of information about open source software (OSS). Our findings provide insights into global GitHub collaboration behaviors and may be of help for researchers and practitioners to further understand modern OSS ecosystem.",2022,1189
AI-Powered Code Review Assistant for Streamlining Pull Request Merging,C. Adapa; S. S. Avulamanda; A. R. K. Anjana; A. Victor,"WatsonX, a comprehensive data and AI platform, adeptly addresses contemporary challenges by meticulously training, validating, tuning, and deploying data to drive impactful business outcomes. The intricate task of timely merging Pull Requests (PRs) poses a significant challenge for software development teams, directly influencing business operations. This paper introduces an innovative solution leveraging AI, particularly harnessing generative AI techniques with the Falcon40-B model through the platform. The AI bot facilitates an initial PR review, offering insightful feedback on code formatting, best practices, and minor issues and streamlines collaboration by automatically assigning and notifying PR reviewers. The overarching goal is the continuous evolution of this AI bot into an intelligent reviewer, capable of assessing code from a functional standpoint. The implementation of this solution holds the promise of significantly enhancing PR management and expediting the entire development workflow.",2024,1190
Recommending Good First Issues in GitHub OSS Projects,W. Xiao; H. He; W. Xu; X. Tan; J. Dong; M. Zhou,"Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for new-comers to locate suitable development tasks, while existing “Good First Issues” (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RECGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RECGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RECGFI, we collect 53,510 resolved issues among 100 GitHub projects and care-fully restore their historical states to build ground truth datasets. Our evaluation shows that RECGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals in-teresting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer.",2022,1191
JITBot: An Explainable Just-In-Time Defect Prediction Bot,C. Khanan; W. Luewichana; K. Pruktharathikoon; J. Jiarpakdee; C. Tantithamthavorn; M. Choetkiertikul; C. Ragkhitwetsagul; T. Sunetnanta,"Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explain-ability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/.",2020,1192
Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions,C. Treude,"Navigating the diverse solution spaces of non-trivial software engineering tasks requires a combination of technical knowledge, problem-solving skills, and creativity. With multiple possible solutions available, each with its own set of trade-offs, it is essential for programmers to evaluate the various options and select the one that best suits the specific requirements and constraints of a project. Whether it is choosing from a range of libraries, weighing the pros and cons of different architecture and design solutions, or finding unique ways to fulfill user requirements, the ability to think creatively is crucial for making informed decisions that will result in efficient and effective software. However, the interfaces of current chatbot tools for programmers, such as OpenAI’s ChatGPT or GitHub Copilot, are optimized for presenting a single solution, even for complex queries. While other solutions can be requested, they are not displayed by default and are not intuitive to access. In this paper, we present our work-in-progress prototype “GPTCOMPARE”, which allows programmers to visually compare multiple source code solutions generated by GPT-n models for the same programming-related query by highlighting their similarities and differences.",2023,1193
Leaky Kits: The Increased Risk of Data Exposure from Phishing Kits,B. Tejaswi; N. Samarasinghe; S. Pourali; M. Mannan; A. Youssef,"Phishing kits allow adversaries with little or no technical experience to launch phishing websites in a short time. Past research has found such phishing kits that contain backdoors (e.g., obfuscated email addresses), which are intentionally added by the kit developers to obtain the phished data. In this work, we augment on prior research by exploring several ways in which security flaws in phishing kits make the victim data accessible to a wider set of adversaries beyond the kit deployers and kit developers. We implement an automated framework for kit collection and analysis, which includes a custom command-line PHP execution tool (for dynamic analysis) along with other open-source tools. Our analysis focuses on finding backdoors (e.g., obfuscated email address, command injection), measuring the extent of disclosure of sensitive information (e.g., via exposed plaintext files, hardcoded Telegram bot tokens, hardcoded admin console passwords) and detecting security vulnerabilities in phishing kits. We analyze 4238 distinct phishing kits (from a set of 26,281 compressed files collected from several sources over a span of 15 months), each having unique SHA-1 hash value. We found that 3.9% of the analyzed kits contained at least one form of backdoor. We also found hardcoded admin console passwords and API keys used to access third party services, in 8.3% and 16% of the analyzed kits, respectively. In addition, 15.8% of the analyzed kits wrote stolen information (PII) of users in plaintext files; 5.6% kits did not restrict external access to these plaintext files, leading to exposure of sensitive phished data (e.g., 178,504 passwords, 133,248 email addresses, 1253 credit card numbers). Furthermore, 11.7% of the analyzed kits contained hardcoded Telegram bots; we obtained invite links to join Telegram chats in 0.5% kits, and found them to expose chat messages containing sensitive PII information of victims (e.g., 73,342 passwords, 141,095 email addresses, 3584 credit card numbers). We also found that 64% of the kits are affected by security vulnerabilities (e.g., insecure file operations, SQL injection), which can be abused to further expose user data. We have open-sourced our framework and other artifacts to benefit future research.",2022,1194
Conversation Clustering Adaptation for Intent Recognition,M. Lew; A. Obuchowski; E. Kacprzak; A. Pluwak,"With the increasing presence of NLU tools such as automatic dialogue systems, achieving high accuracy of intent recognition in chatbots becomes an especially important problem to tackle. This issue cannot be solved without sufficient training data, but the scarcity of labelled training data often poses a major challenge to the development of real-life chatbots. Therefore, methods utilizing unlabelled data resources have been recently gaining interest. One of most notable approaches is the use of pre-trained encoders based on language models. Trained for general purposes, they benefit from further domain adjustments. In our work we offer an approach that can increase the model’s accuracy for text classification, which can serve as an alternative for standard methods of domain adaptation. Our approach consists of a combination of methods: a clustering approach, similar to intent induction; an encoder domain adaptation on a cluster classification task, similar to intent recognition using unlabelled data; and model fine-tuning on labelled datasets. In this approach unlabelled data becomes complementary to labelled data, reducing the time needed for corpus building. We evaluate our approach on: 1) the public WebApp dataset and 2) a demanding real-life banking domain dataset, achieving 0.97 and 0.93 accuracy respectively. This approach, called Conversation Clustering Adaptation (CCA), when applied to an encoder, increases the accuracy of intent recognition up by to 12.4pp and exceeds current state-of-the-art methods while benefiting from the use of additional training data. We share our code at https://github.com/michal-lew/cca.",2021,1195
A Blockchain-Based Ticket Sales Platform,P. Sombat; P. Ratanaworachan,"Concerts or fan meetings usually attract lots of attention, generating huge demand for tickets for these events. But, existing ticket purchase systems are unable to efficiently and transparently accommodate this. We often hear scandals of bot-controlled clients gobbling up tickets within seconds after sales open or celebrities acquiring prime tickets exceeding individual quota. These problems arise because existing systems are centralized. This can be a single point of failure and the controlling authority can ban or give privileges to certain users. This work sets out to remedy these problems using a blockchain-base solution that, by nature, is highly decentralized. We have created and deployed EVM (Ethereum Virtual Machine)-based smart contracts for ticket sales platform on two EVM-compatible blockchains, Ethereum (ETH) and Avalanche (AVAX). These contracts inherit heavily from the ERC-721 standard for NFT (Non-Fungible Token). The platforms on both blockchains engender the desirable transparency. However, the platform on Avalanche is much more efficient and economical to deploy and utilize. We have open-sourced the code for our platform on Github. Visit the following link to fork or check it out: https://github.com/JesperBerben/TicketNFT In addition, you can now interact with our platform on Avalanche which has been deployed and verified at the address linked to below: https://snowtrace.io/address/0x17fd3f6cf6cbff75604b63f1ca12c6db29730a9c",2023,1196
Explaining Differences in Classes of Discrete Sequences,S. Saadat; G. Sukthankar,"While there are many machine learning methods to classify and cluster sequences, they fail to explain what are the differences in groups of sequences that make them distinguishable. Although in some cases having a black box model is sufficient, there is a need for increased explainability in research areas focused on human behaviors. For example, psychologists are less interested in having a model that predicts human behavior with high accuracy and more concerned with identifying differences between actions that lead to divergent human behavior. This paper presents techniques for understanding differences between classes of discrete sequences. Approaches introduced in this paper can be utilized to interpret black box machine learning models on sequences. The first approach compares k-gram representations of sequences using the silhouette score. The second method characterizes differences by analyzing the distance matrix of subsequences. As a case study, we trained black box supervised learning methods to classify sequences of GitHub teams and then utilized our sequence analysis techniques to measure and characterize differences between event sequences of teams with bots and teams without bots. In our second case study, we classified Minecraft event sequences to infer their high-level actions and analyzed differences between low-level event sequences of actions.",2020,1197
Improved text language identification for the South African languages,B. Duvenhage; M. Ntini; P. Ramonyai,"Virtual assistants and text chatbots have recently been gaining popularity. Given the short message nature of text-based chat interactions, the language identification systems of these bots might only have 15 or 20 characters to make a prediction. However, accurate text language identification is important, especially in the early stages of many multilingual natural language processing pipelines. This paper investigates the use of a naive Bayes classifier, to accurately predict the language family that a piece of text belongs to, combined with a lexicon based classifier to distinguish the specific South African language that the text is written in. This approach leads to a 31% reduction in the language detection error. In the spirit of reproducible research the training and testing datasets as well as the code are published on github. Hopefully it will be useful to create a text language identification shared task for South African languages.",2017,1198
Dish-ID: A neural-based method for ingredient extraction and further recipe suggestion,I. Shchuka; S. Miftakhov; V. Patrushev; M. Tikhonova; A. Fenogenova,"The paper presents a method for meal recognition, ingredient extraction and recipe suggestion in the Russian language. The proposed algorithm consists of several consecutive stages. On the first stage the model extracts a list of ingredients from a photo of the dish, based on which recipes on the second stage are selected. Two ingredient extraction architectures were tested for the first stage and three recipe matching methods for recipe suggestion are proposed. In addition, the algorithm was incorporated into the telegram-bot which provides friendly user experience. Source code is at https://github.com/Alenushldish_id_sirius.",2020,1199
Toward an Empirical Theory of Feedback-Driven Development,M. Beller,"Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are prerather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial theory on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.",2018,1200
Multi-Domain Learning and Identity Mining for Vehicle Re-Identification,S. He; H. Luo; W. Chen; M. Zhang; Y. Zhang; F. Wang; H. Li; W. Jiang,"This paper introduces our solution for the Track2 in AI City Challenge 2020 (AICITY20). The Track2 is a vehicle re-identification (ReID) task with both the real-world data and synthetic data.Our solution is based on a strong baseline with bag of tricks (BoT-BS) proposed in person ReID. At first, we propose a multi-domain learning method to joint the real-world and synthetic data to train the model. Then, we propose the Identity Mining method to automatically generate pseudo labels for a part of the testing data, which is better than the k-means clustering. The tracklet-level re-ranking strategy with weighted features is also used to post-process the results. Finally, with multiple-model ensemble, our method achieves 0.7322 in the mAP score which yields third place in the competition. The codes are available at https://github.com/heshuting555/AICITY2020_DMT_VehicleReID.",2020,1201
On the Use of Dependabot Security Pull Requests,M. Alfadel; D. E. Costa; E. Shihab; M. Mkhallalati,"Vulnerable dependencies are a major problem in modern software development. As software projects depend on multiple external dependencies, developers struggle to constantly track and check for corresponding security vulnerabilities that affect their project dependencies. To help mitigate this issue, Dependabot has been created, a bot that issues pull-requests to automatically update vulnerable dependencies. However, little is known about the degree to which developers adopt Dependabot to help them update vulnerable dependencies.In this paper, we investigate 2,904 JavaScript open-source GitHub projects that subscribed to Dependabot. Our results show that the vast majority (65.42%) of the created security-related pull-requests are accepted, often merged within a day. Through manual analysis, we identify 7 main reasons for Dependabot security pull-requests not being merged, mostly related to concurrent modifications of the affected dependencies rather than Dependabot failures. Interestingly, only 3.2% of the manually examined pull-requests suffered from build breakages. Finally, we model the time it takes to merge a Dependabot security pull-request using characteristics from projects, the fixed vulnerabilities and issued pull requests. Our model reveals 5 significant features to explain merge times, e.g., projects with relevant experience with Dependabot security pull-requests are most likely associated with rapid merges. Surprisingly, the severity of the dependency vulnerability and the potential risk of breaking changes are not strongly associated with the merge time. To the best of our knowledge, this study is the first to evaluate how developers receive Dependabot’s security contributions. Our findings indicate that Dependabot provides an effective platform for increasing awareness of dependency vulnerabilities and helps developers mitigate vulnerability threats in JavaScript projects.",2021,1202
"Mitigating Turnover with Code Review Recommendation: Balancing Expertise, Workload, and Knowledge Distribution",E. Mirsaeedi; P. C. Rigby,"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover with minimal impact on the development process. We evaluate review recommenders in the context of ensuring expertise during review, Expertise, reducing the review workload of the core team, CoreWorkload, and reducing the Files at Risk to turnover, FaR. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing risk of knowledge loss from turnover by up to 65%. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover by -29% but they unacceptably reduce the overall expertise during reviews by -26%. We develop the Sofia recommender that suggests experts when none of the files under review are hoarded by developers, but distributes knowledge when files are at risk. In this way, we are able to simultaneously increase expertise during review with a ΔExpertise of 6%, with a negligible impact on workload of ΔCoreWorkload of 0.09%, and reduce the files at risk by ΔFaR -28%. Sofia is integrated into GitHub pull requests allowing developers to select an appropriate expert or “learner” based on the context of the review. We release the Sofia bot as well as the code and data for replication purposes.",2020,1203
Robust Text CAPTCHAs Using Adversarial Examples,R. Shao; Z. Shi; J. Yi; P. -Y. Chen; C. -J. Hsieh,"CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) is a widely used technology to distinguish real users and automated users such as bots. However, the advance of AI technologies weakens many CAPTCHA tests and can induce security concerns. In this paper, we propose a user-friendly text-based CAPTCHA generation method named Robust Text CAPTCHA (RTC). At the first stage, the foregrounds and backgrounds are constructed with font and background images respectively sampled from font and image libraries, and they are then synthesized into identifiable pseudo adversarial CAPTCHAs. At the second stage, we utilize a highly transferable adversarial attack designed for text CAPTCHAs to better obstruct CAPTCHA solvers. Our experiments cover comprehensive models including shallow models such as KNN, SVM and random forest, as well as various deep neural networks and OCR models. Experiments show that our CAPTCHAs have a failure rate lower than one millionth in general and high usability. They are also robust against various defensive techniques that attackers may employ, including adversarially trained CAPTCHA solvers and solvers trained with collected RTCs using manual annotation. Codes available at https://github.com/RulinShao/RTC.",2022,1204
BreakBot: Analyzing the Impact of Breaking Changes to Assist Library Evolution,L. Ochoa; T. Degueule; J. -R. Falleri,"“If we make this change to our code, how will it impact our clients?” It is difficult for library maintainers to answer this simple—yet essential!—question when evolving their libraries. Library maintainers are constantly balancing between two opposing positions: make changes at the risk of breaking some of their clients, or avoid changes and maintain compatibility at the cost of immobility and growing technical debt. We argue that the lack of objective usage data and tool support leaves maintainers with their own subjective perception of their community to make these decisions.We introduce BreakBot, a bot that analyses the pull requests of Java libraries on GitHub to identify the breaking changes they introduce and their impact on client projects. Through static analysis of libraries and clients, it extracts and summarizes objective data that enrich the code review process by providing maintainers with the appropriate information to decide whether—and how—changes should be accepted, directly in the pull requests.",2022,1205
"GeeSolver: A Generic, Efficient, and Effortless Solver with Self-Supervised Learning for Breaking Text Captchas",R. Zhao; X. Deng; Y. Wang; Z. Yan; Z. Han; L. Chen; Z. Xue; Y. Wang,"Although text-based captcha, which is used to differentiate between human users and bots, has faced many attack methods, it remains a widely used security mechanism and is employed by some websites. Some deep learning-based text captcha solvers have shown excellent results, but the labor-intensive and time-consuming labeling process severely limits their viability. Previous works attempted to create easy-to-use solvers using a limited collection of labeled data. However, they are hampered by inefficient preprocessing procedures and inability to recognize the captchas with complicated security features.In this paper, we propose GeeSolver, a generic, efficient, and effortless solver for breaking text-based captchas based on self-supervised learning. Our insight is that numerous difficult-to-attack captcha schemes that ""damage"" the standard font of characters are similar to image masks. And we could leverage masked autoencoders (MAE) to improve the captcha solver to learn the latent representation from the ""unmasked"" part of the captcha images. Specifically, our model consists of a ViT encoder as latent representation extractor and a well-designed decoder for captcha recognition. We apply MAE paradigm to train our encoder, which enables the encoder to extract latent representation from local information (i.e., without masking part) that can infer the corresponding character. Further, we freeze the parameters of the encoder and leverage a few labeled captchas and many unlabeled captchas to train our captcha decoder with semi-supervised learning.Our experiments with real-world captcha schemes demonstrate that GeeSolver outperforms the state-of-the-art methods by a large margin using a few labeled captchas. We also show that GeeSolver is highly efficient as it can solve a captcha within 25 ms using a desktop CPU and 9 ms using a desktop GPU. Besides, thanks to latent representation extraction, we successfully break the hard-to-attack captcha schemes, proving the generality of our solver. We hope that our work will help security experts to revisit the design and availability of text-based captchas. The code is available at https://github.com/NSSL-SJTU/GeeSolver.",2023,1206
Automatic Commit Message Generation: A Critical Review and Directions for Future Work,Y. Zhang; Z. Qiu; K. -J. Stol; W. Zhu; J. Zhu; Y. Tian; H. Liu,"Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of ‘noise’; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models ‘learn’ inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.",2024,1207
A Crowdsourcing Data Annotation System For Vietnamese Scene Text Detection,B. -V. Nguyen-Thi; T. L. Phan; T. N. Nguyen; T. D. Ngo,"Most of the recent breakthroughs in AI-related research and applications are data driven. Annotated data plays the main role in improving AI/ML-based solutions. The lack of high-quality annotated data is one of the main obstacles to AI adoption, especially in Vietnamese scene text detection. Additionally, manual annotation is expensive yet not scalable. This project aims to tackle the problem by developing a crowdsourcing data annotation system at word-level annotations, vCaptcha, in the form of an anti-bot widget to integrate to some web pages. After that, results can be derived from the workers prior to evaluating and aggregating the crowdsourced labels. Also, we propose a novel human-in-the-loop approach for incorporating the state-of-the-art scene-text detection models and crowdsourcing system so that in the end, a fully annotated dataset with optimal and high quality can be generated with significant cost and time savings. Usage document to integrate vCaptcha to any web pages is available at https://bichvan2810.github.io/vCaptcha-end-user-info.",2022,1208
"Factoring Expertise, Workload, and Turnover Into Code Review Recommendation",F. Hajari; S. Malmir; E. Mirsaeedi; P. C. Rigby,"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload. We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review. Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover. Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover. Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing the risk of knowledge loss from turnover. Recent work, WhoDo, that considers developer workload, assigns developers that are not sufficiently committed to the project and we see an increase in files at risk to turnover. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover, but they unacceptably reduce the overall expertise during reviews. Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer. In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge. For the projects we study, we are able to globally increase expertise during reviews, $+3$+3%, reduce workload concentration, $-12$−12%, and reduce the files at risk, $-28$−28%. We make our scripts and data available in our replication package [1]. Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes [2].",2024,1209
Multimodal Recommendation of Messenger Channels,E. Koshchenko; E. Klimov; V. Kovalenko,"Collaboration platforms, such as GitHub and Slack, are a vital instrument in the day-to-day routine of software engineering teams. The data stored in these platforms has a significant value for datadriven methods that assist with decision-making and help improve software quality. However, the distribution of this data across different platforms leads to the fact that combining it is a very time-consuming process. Most existing algorithms for socio-technical assistance, such as recommendation systems, are based only on data directly related to the purpose of the algorithms, often originating from a single system. In this work, we explore the capabilities of a multimodal recommendation system in the context of software engineering. Using records of interaction between employees in a software company in messenger channels and repositories, as well as the organizational structure, we build several channel recommendation models for a software engineering collaboration platform, and compare them on historical data. In addition, we implement a channel recommendation bot and assess the quality of recommendations from the best models with a user study. We find that the multimodal recommender yields better recommendations than unimodal baselines, allows to mitigate the overfitting problem, and helps to deal with cold start. Our findings suggest that the multimodal approach is promising for other recommendation problems in software engineering.",2022,1210
Optimizing Workflow for Elite Developers: Perspectives on Leveraging SE Bots,Z. Wang; Y. Wang; D. Redmiles,"Small-scale automation services in Software Engineering, known as SE Bots, have gradually infiltrated every aspect of daily software development with the goal of enhancing productivity and well-being. While leading the OSS development, elite developers have often burned out from holistic responsibilities in projects and looked for automation support. Building on prior research in BotSE and our interviews with elite developers, this paper discusses how to design and implement SE bots that integrate into the workflows of elite developers and meet their expectations. We present six main design guidelines for implementing SE bots for elite developers, based on their concerns about noise, security, simplicity, and other factors. Additionally, we discuss the future directions of SE bots, especially in supporting elite developers’ increasing workload due to rising demands.",2023,1211
Android botnets on the rise: Trends and characteristics,H. Pieterse; M. S. Olivier,"Smartphones are the latest technology trend of the 21st century. Today's social expectation of always staying connected and the need for an increase in productivity are the reasons for the increase in smartphone usage. One of the leaders of the smartphone evolution is Google's Android Operating System (OS). The openness of the design and the ease of customizing are the aspects that are placing Android ahead of the other smartphone OSs. Such popularity has not only led to an increase in Android usage but also to the rise of Android malware. Although such malware is not having a significant impact on the popularity of Android smartphones, it is however creating new possibilities for threats. One such threat is the impact of botnets on Android smartphones. Recently, malware has surfaced that revealed specific characteristics relating to traditional botnet activities. Malware such as Geinimi, Pjapps, DroidDream, and RootSmart all display traditional botnet functionalities. These malicious applications show that Android botnets is a reality. From a security perspective it is important to understand the underlying structure of an Android botnet. This paper evaluates Android malware with the purpose of identifying specific trends and characteristics relating to botnet behaviour. The botnet trends and characteristics are detected by a comprehensive literature study of well-known Android malware applications. The identified characteristics are then further explored in terms of the Android Botnet Development Model and the Android Botnet Discovery Process. The common identified trends and characteristics aid the understanding of Android botnet activities as well as the possible discovery of an Android bot.",2012,1212
"The Chemical Basis of Diatom Morphogenesis††Dedicated to the memory of Judith Georgia Colburn, botanist, and her love for plants.",Richard Gordon and Ryan W. Drum,"An examination of the artificial diatoms shows that purely chemical and physical considerations will account for the varieties of pattern we notice in natural diatoms, and their living structure appears only to provide the conditions under which the silicious precipitation takes place, according to the ordinary laws of chemical action and molecular coalescence [I have tried] to make the subject more intelligible to that, I fear, very numerous class of microscopists who have not paid to Schultze's (1863a,b) artificial diatoms the attention they deserve. (Slack, 1870) The external form of crystals is prismatic, and bounded by straight surfaces which cut each other at certain angles. But the same form is seen in the skeletons of many of the protists, especially the flinty shells of the diatomes and radiolaria; their silicious coverings lend themselves to mathematical determination just as well as the inorganic crystals. (Haeckel, 1905)",1994,1213
"Hexose and hexose-phosphate metabolism in barley leaves and roots. Role of glucose 1,6-biphosphate",R.C. Sicher and D.F. Kremer,"Photosynthetic intermediate levels were measured in barley leaves (Hordeum vulgare L.) as a function of light, carbon dioxide concentration and leaf excision. The results were related to changes in starch, and hexose metabolism. Glucose 6-phosphate (Glc 6-P) and fructose 6-phosphate (Fru6-P) pools in barley primary leaves decreased when net photosynthesis was eliminated after a light-to-dark transition or when leaves were exposed to CO2-free air. However, the glucose 1-phosphate (Glc1-P) concentration remained constant during transitions in leaf metabolism. Glucose 1,6-biphosphate (Glc1,6-P2) levels were about 1 and 3 nmol (mg chlorophyll)−1 in dark and light barley leaves, respectively. This finding suggests that Glc1,6-P2 in plants functions primarily in gluconeogenic metabolic pathways. Changes of Glc1,6-P2 level were slower than the rapid adjustments observed in fructose 2,6-bisphosphate (Fru2,6-P2) during shifts in metabolism, indicating that the former compound is unlikely to be a signal metabolite in plants. Hexose-bisphosphate and hexose-phosphate levels, other than Glc1-P, decreased in 72-h old barley roots after seedlings were exposed to anoxia for 24-h. Changes in Glc1,6-P2 levels in barley leaves and roots were proportional to alterations in the hexose-phosphate pool, except during the 60–90-min transition period following an abrupt shift in metabolism. Metabolite ratios indicated that phosphoglucose isomerase was slightly displaced from equilibrium in illuminated barley leaves but was near equilibrium in darkened leaves and roots. Glc1-P to Glc1-P ratios were about 6 and 2 in light and dark barley primary leaves, respectively, indicating that phosphoglucomutase was closer to equilibrium in the light than in the dark.",1990,1214
Effect of procaine on development of ‘ridges’ and ‘domes’ in primary cell cultures from mammary glands of untreated virgin mice: The role of cell density in the occurrence of ‘domes’,G.J. Wiepjes and A.S. Visser and F.J.A. Prop,Primary cell cultures from mammary glands of virgin mice that were not pretreated with hormones were subjected to: (1) procaine; (2) insulin+ prolactin +hydrocortisone; (3) a combination of (1) and (2). Procaine caused a ‘ridge’ effect similar to that of the hormones. The combination of procaine with the hormones caused a still stronger ‘ridge’ effect as well as the formation of ‘domes’. The formation of ‘domes’ is suggested to be dependent on cell density.,1976,1215
Assessing infections at multiple levels of aggregation,M. Kadohira and J.J. McDermott and M.M. Shoukri and M.A. Thorburn,"The patterns of sero-prevalence of antibodies to four infectious diseases, representing a broad range of pathogens (bacteria: brucellosis; mycoplasma: contagious bovine pleuropneumonia; viruses: infectious bovine rhinotracheitis; protozoa: trypanosomosis) were investigated at three levels of organization (farm, area and district). Three contrasting districts in Kenya were compared: an arid and pastoral area (Samburu); a tropical highland area (Kiambu), and a tropical coastal area (Kilifi). Cattle in three districts were selected by two-stage cluster sampling between August 1991 and 1992. Schall's algorithm, a generalized linear mixed model suitable for multi-level analysis, was compared to ordinary logistic regression (OLR), which ignores clustering of responses; generalized estimating equations (GEE) or Jacknife, to account for clustering at the farm level; SAS VARCOMP, which provides normal-theory random-effects models. Schall's algorithm provided similar estimates to GEE (regression effects) and Jackknife (standard errors) for farm-level clustered data. Extending Schall's procedure for additional district and area-withindistrict random effects usually provided additional information. In general, models that included only a farm-level random effect consistently provided larger estimates of farms' variance components than did models with additional district and area random effects. The four type diseases exhibited various amounts of clustering. Brucellosis had moderate farm clustering plus some area and district clustering. Contagious bovine pleuropneumonia had only a small amount of clustering, mostly by area. Infectious bovine rhinotracheitis exhibited a large amount of clustering, primarily at the farm level. Trypanosomiasis antibody prevalence varied by district, area and farm. We believe that patterns of disease clustering identified by multi-level analysis can be used to better target high-risk units for disease control and guide research to understand disease transmission factors.",1997,1216
Ruthenium and osmium,K.R. Seddon,,1981,1217
Physics with a GeV-electron accelerator,Ingo Sick,,1980,1218
"Chapter 9 - Domain 8: Software Development Security (Understanding, Applying, and Enforcing Software Security)",Eric Conrad and Seth Misenar and Joshua Feldman,"Chapter 9 introduces Domain 8 of the CISSP, Software Development Security. The most important aspects of this domain are related to managing the development of software and applications. Approaches to software development that attempt to reduce the likelihood of defects or flaws are a key topic in this domain. In particular, the Waterfall, Spiral, and Rapid Application Development (RAD) models of the software development are considered. Another significant portion of this chapter is dedicated to understanding the principles of Object Oriented programming and design. A basic discussion of several types of software vulnerabilities and the issues surrounding disclosure of the vulnerabilities are also a topic for this domain. Finally, databases, being a key component of many applications, are considered.",2016,1219
Receptor and antibody interactions of human interleukin-3 characterized by mutational analysis.,L.C. Dorssers and M.C. Mostert and H. Burger and C. Janssen and P.J. Lemson and R. {van Lambalgen} and G. Wagemaker and R.W. {van Leen},"Human interleukin-3 (hIL-3) is a regulator of proliferation and differentiation of multipotent hemopoietic progenitor cells. Mutants of hIL-3 have been constructed by oligonucleotide-directed mutagenesis and expressed in Escherichia coli and Bacillus licheniformis. Purified muteins were assayed for induction of DNA synthesis in IL-3-dependent human cells and for binding to the IL-3 receptor. Residues at the NH2 and COOH termini together comprising one-quarter of the molecule could be removed without loss of biological function. Deletions of 6-15 residues within the central part of the molecule caused a large reduction (up to 5 logs) but no complete loss of activity. Substitution of evolutionary conserved residues resulted in a strong decrease of biological activity and demonstrated that the S-S bridge is an essential structural element in hIL-3. Interestingly, four muteins displayed a significantly higher potency of binding to the IL-3 receptor than in stimulating DNA synthesis. These results demonstrate that receptor binding may be (partly) disconnected from activation of DNA synthesis. Analysis of hIL-3 muteins demonstrated that the majority of monoclonal antibodies are directed against a small portion of the IL-3 molecule. The neutralizing potential of individual monoclonal antibodies could be increased by a combination of antibodies directed against nonoverlapping epitopes.",1991,1220
Tertiary climatic evolution and vegetation history in the Southeast Indian Ocean region,Elizabeth M. Kemp,"The broad trends of climatic evolution in the Southeast Indian Ocean region during the Tertiary have become clearer with the accumulation of data from Legs 28 and 29 of the Deep Sea Drilling Project. The same events which are reflected in the deep-sea sediments clearly influenced the major continental areas of Australia and Antarctica and the vegetation which covered those regions. In this review, data relating to sea-surface temperatures, to land and sea positions, and to the extent of the Antarctic ice-cap, are used to derive climatic models for the Palaeocene to latest Miocene interval: the models are correlated with vegetation history deciphered from palynological data. For most of the Palaeocene, Australia and Antarctica were joined, and lay in high southern latitudes. Sea-surface temperatures were relatively high, and there is no evidence for Antarctic ice. The postulated atmospheric circulation patterns suggest a zone of westerly winds confined to 60–80°S, and, north of that, a wide zone of erratic circulation, with deep inland penetration of rain-bearing winds. Palynological evidence from Australia indicates rainforest in the southeast, with an extension to presently arid inland South Australia. In the Eocene, Australia and Antarctica were separated by a wide gulf; water temperatures were high — in the vicinity of 20°C on the Campbell Plateau — and evidence for Antarctic ice development is meagre. Atmospheric circulation was probably sluggish, with patterns similar to those of the Palaeocene. Vegetational data show the spread of rainforest communities across Australia from the east to the southwest, and locally, inland to central Australia. Vegetation zonation appears to have been minimal on that continent. In Antarctica, late Eocene vegetation was poorly diversified. For the Oligocene, evidence for a pronounced temperature drop is both isotopic and sedimentological, and includes the first record of ice-rafting near Antarctica. Intensification of atmospheric circulation seems likely. The vegetation records suggest lowered floristic diversity in Australia, and the persistence of vegetation, again of low diversity, in Antarctica into the late Oligocene. For the Miocene, the marine record suggests the development of a major Antarctic ice-cap; in consequence, atmospheric circulations continued to intensify. Continued northward drift of Australia meant increasing aridity in regions to the north and northwest; the south and east were watered by rain-bearing westerly systems and by embryonic Trade Winds. Vegetation data in Australia suggest rainforest cover in the southeast and on the east coast, and, in the middle Miocene, the development of grasslands in interfluvial areas in the central part of the continent. The latest Miocene was marked by an intense and sudden chilling which is clearly documented in Southern Ocean sediments. This event must have caused marked precipitation decrease in much of Australia, and may coincide with the disappearance of pollen of the Nothofagus brassi type from regions west of the Great Dividing Range, although age control of this event is poor.",1978,1221
Effects of 25-hydroxycholesterol and aminoglutethimide in isolated rat adrenal cells. A model for congenital lipoid adrenal hyperplasia?,H.E. Falke and H.J. Degenhart and G.J.A. Abeln and H.K.A. Visser,"The production of corticosterone from 25-hydroxycholesterol by isolated rat adrenal cells is inhibited by aminoglutethimide phosphate (AGI); half-maximal inhibition is obtained at ca. 10 μM. AGI also inhibits ACTH-stimulated steroid production from endogeneous substrates; here half-maximal inhibition is obtained with ca. 40 μM AGI. In the presence of ACTH + AGI, 25-hydroxycholesterol causes additive inhibition. This effect of 25-hydroxycholesterol is dose-dependent. ACTH-stimulated steroid production from endogeneous substrates is partially inhibited by 5-cholene-3β, 24-diol. These results may just reflect substrate competition for the side-chain cleaving system or may be due to some secondary toxic effect on the cells.",1976,1222
Index,,,2018,1223
Serological study on barley stripe mosaic virus protein polymerization: II. Comparative antigenic analysis of intact virus and some stable protein intermediates,J.G. Atabekov and S.P. Dementyeva and N.D. Schaskolskaya and G.N. Sacharovskaya,"Aggregates of barley stripe mosaic virus protein (BSMVp) beginning at the level of 10 S aggregate (i.e., 10 S, 20 S, 30 S, 40 S, etc.) are antigenically identical to each other and to BSMV. The monomeric BSMVp unit is serologically related to, but not identical with, the intact BSMV. The influence of quaternary structure of BSMVp on the conformation of polypeptide chain is discussed. The multiple line formation, with antibody in excess, by the mixtures of antigenically identical BSMVp and BSMV was demonstrated in double-diffusion tests.",1968,1224
Interaction of growth-determining systems with gravity,A. Merkys and R. Laurinavičius and D. Bendoraityté and D. Švegždiené and O. Rupainiené,"The experiments have been carried out with lettuce shoots on board the Salyut-7 orbital station, the Kosmos-1667 biological satellite and under ground conditions at 180° plant inversion. By means of the centrifuge Biogravistat-1M the threshold value of gravitational sensitivity of lettuce shoots has been determined on board the Salyut-7 station. It was found to be equal to 2.9 × 10−3g for hypocotyls and 1.5 × 10−4g for roots. The following results have been received in the experiment performed on board the Kosmos-1667 satellite: a) under microgravity the proliferation of the meristem cells and the growth of roots did not differ from the control; b) the growth of hypocotyls in length was significantly enhanced in microgravity; c) under microgravity transverse growth of hypocotyls (increase in cross sectional area) was significantly increased due to enhancement of cortical parenchyma cell growth. At 180° inversion in Earth's gravity root extension growth and rate of cell division in the root apical meristem were decreased. The determination of DNA-fuchsin value in the nuclei of the cell root apexes showed that inversion affected processess of the cell cycle preceeding cytokinesis.",1986,1225
Gravity-driven countercurrent two-phase flow during filling of an unvented vessel,B. Kamboj and E.M. Ritenour and S.M. Ghiaasiaan and S.I. Abdel-Khalik,"Countercurrent two-phase flow associated with filling of sealed vessels via gravity-driven liquid injection through inclined channels was experimentally studied and analytically modeled. Experiments were performed using transparent tubular test sections connected at one end to the bottom of a large, open water tank, and at the other end to an unvented tank. The test section parameters (including the channel diameter (1.27–2.54cm), length (30.5–122 cm), angle of inclination with respect to horizontal plane (0–30°), and the empty volume in the sealed vessel) were systematically varied. Flow regimes in the test section were recorded and transient flow rates were measured during the experiments. Oscillatory, and intermittent stratified slug, were dominant flow regimes in most tests. The quasi-steady liquid superficial velocity in the test section was sensitive to the test section dimensions, and varied in the range 0.04–0.95 m s−1. These flow regimes were mechanisally modeled. The models are shown to satisfactory predict the measured hydrodynamic parameters.",1995,1226
A tetrazolium-based colorimetric MTT assay to quantitate human monocyte mediated cytotoxicity against leukemic cells from cell lines and patients with acute myeloid leukemia,A.A. {van de Loosdrecht} and R.H.J. Beelen and G.J. Ossenkoppele and M.G. Broekhoven and M.M.A.C. Langenhuijsen,"The MTT-colorimetric monocyte mediated cytotoxicity assay, based upon the ability of living cells to reduce 3-[4,5-dimethylthiazol-2-yl]-2,5 diphenyltetrazolium bromide (MTT) into formazan, was evaluated using leukemic cells from five representative human leukemic cell lines and from 28 patients with acute myeloid leukemia (AML). An excellent linearity between absorbance and leukemic cell number was observed up to 5 × 104 cells/well and 50 × 104 cells/well for all cell lines and patients samples tested, respectively, in a 96-wells microtiter culture system. A huge variability in the susceptibility of leukemic cells to purified and IFN-γ-activated human monocytes could be observed at effector-to-target cell (E:T) ratios of 1. The mean signal-to-noise ratio of the MTT assay for monocyte-leukemic cell mixtures from patients was 2.69 ± 0.39 at E:T 1. In conclusion, the MIT based monocyte mediated cytotoxicity assay should be useful for studying the susceptibility of a variety of leukemic cells from cell lines and from patients with AML to monocytes in a rapid, sensitive and semi-automated manner.",1994,1227
Biphasic expression of a Vicia faba legumin B gene in developing seeds of transgenic tobacco,Reinhard Panitz and Renate Manteuffel and Helmut Bäumlein and Ulrich Wobus,"Summary
We analysed the spatial and temporal expression of the Vicia faba legumin gene LeB4 in developing seeds of transgenic tobacco plants by means of in situ hybridization and immunohistostaining. The results indicate that the expression of the LeB4 gene is regulated in embryo and endosperm in a biphasic manner. Legumin accumulation starts during early seed development at low amounts in single cells of the integument. Accumulation proceeds successively in the proembryo, suspensor and endosperm reaching maximum levels in all cells of these organs around 10 to 12 DAP. With the beginning of the heart stage (around 13 DAP) legumin reserves disappear completely from all parts of the seed. The described early phase of legumin accumulation occurs in embryo and endosperm still before the onset of the cell expansion phase in a period of continuous mitotic activity. A second expression phase starts in embryo and endosperm around 18 DAP, as the embryo enters late torpedo stage. Within the embryo the accumulation is restricted to the parenchyma cells of cotyledons and hypocotyl, while the embryonic root and provascular tissue do not accumulate LeB4 protein. In addition, a low percentage (<10%) of cells from protoderm and calyptra was found to be involved in legumin accumulation. A comparison of LeB4 mRNA and protein distribution in embryo and endosperm revealed corresponding patterns for both expression phases. The biphasic expression of the LeB4 gene in transgenic tobacco seeds is identical to that found in V. faba for the legumin and vicilin genes during early embryogenesis (Panitz et al., 1995), but differs from that of tobacco 12S globulin genes, which are expressed in the embryo not before the late heart stage.",1997,1228
The biochemical systematics of Fagaropsis angolensis and its significance in the Rutales,Peter G. Waterman and Sami A. Khalid,"From the stem bark of Fagaropsis angolensis (Rutaceae) three alkaloids and two limonoids were isolated. The alkaloids were identified as the 6-acetonyl derivatives of the benzophenantridines, dihydrochelerythrine, dihydrosanguinarine and dihydronitidine, the last of these being reported for the first time. The alkaloids did not appear to be artefacts of the corresponding benzophenanthridines. The limonoids were identified as rutaevin and limonin diosphenol. The significance of these compounds in resolving the confused taxonomic position of F. angolensis is discussed. Attention is drawn to the presence of a small group of taxa within the Rutaceae capable of synthesizing 1-benzyltetrahydroisoquinoline-derived alkaloids and the potential of these taxa as a starting point for visualizing biochemical evolution within the order Rutales, and putative relationships between the Rutales and Ranales are examined.",1981,1229
Full Issue PDF,,,2023,1230
Bibliography of carbohydrate polymers,,,1996,1231
13C-Nuclear magnetic resonance and X-ray photoelectron spectroscopy of Cu-AMP,Ulrich Weser and Gert-Joachim Strobel and Wolfgang Voelter,,1974,1232
Intertidal “trees”: consequences of aggregation on the mechanical and photosynthetic properties of sea-palms Postelsia palmaeformis Ruprecht,N. {Michele Holbrook} and Mark W. Denny and M.A.R. Koehl,"Sea-palms Postelsia palmaeformis Ruprecht are annual brown algae that grow on wave-swept rocky shores, often forming dense stands. Unlike most macroalgae, Postelsia stands upright in air-like trees. The stipe flexibility that permits Postelsia to withstand waves is provided by the low elastic modulus (5–10 MPa) of stipe tissue; in spite of the weakness (low breaking stress, ≈ 1 MPa) of this tissue, a large amount of energy (≈ 100 kJ/m3) is required to break a stipe because they can be extended by 20–25% before breaking. Although made of such easily deformed tissue, Postelsia can stand upright in air due to the width (high second moment of area) and resilience of their stipes, but the brittleness (low work of fracture, 400–900 J/m2) that accompanies this resilience renders them susceptible to breakage if they sustain deep scratches. Although wave-induced stresses experienced by individuals in aggregations are not lower than those experienced by isolated sea-palms, photon flux densities of photosynthetically active radiation within these dense groves are less than 10% of those above Postelsia canopies. A number of morphological features differ between canopy, understory, and isolated individuals. Canopy plants in dense aggregations are taller than isolated individuals and may exceed limiting proportions for elastic stability. Postelsia shows photosynthetic characteristics of “shade-adapted” plants, understory individuals being especially effective at using low light. Despite this, blade growth rates of understory plants are lower than those of either canopy or isolated individuals.",1991,1233
Viability quantitation of leptospires after rapid and controlled rate freezing,Harry L. Torney and Dale E. Bordt,"Summary
A method novel to the quantitation of leptospiral viability was used which permitted new observations on the effect of freezing leptospires. Use of this method showed that some loss of viability occurred during freezing and/or thawing of three serotypes of leptospires with no significant additional loss during 22 months of storage in liquid nitrogen. These studies also indicated that a controlled rate of freezing was significantly better than quick freezing by immersion in liquid nitrogen. In an additional study, a hamster-virulent strain of L. canicola was frozen and gave reproducible viability and virulence titers at intervals over a 2-year period.",1969,1234
Stratigraphic evidence for eemian crustal movements and relative sea-level changes in eastern fennoscandia,Lars Forsström and Marjatta Aalto and Matti Eronen and Tuulikki Grönlund,"Palaeobotanical data show that during the last, or Eemian, interglacial, the climate in northern Europe was warmer than during the postglacial climatic optimum. The main warm period of the Eemian is identifiable in pollen and plant macrofossil analyses from the abundance of thermophilous species, many of which grew in Eemian time far beyond their Flandrian northern limits. These features, typical of the Eemian occurrences, were found in the Ollala interglacial site in Haapavesi, western Finland. The diatom flora, together with the plant macrofossils, showed a sequence from a marine to fresh-water environment, and thus the isolation of the sedimentary basin from the Eemian sea as a consequence of glacio-isostatic rebound. The marine diatom flora was rich, with many exotic species, indicating a higher salinity than ever found in the postglacial marine deposits in this region. Together with some other finds, the Ollala interglacial deposits (at about 116–117 m above the present sea level), suggest that the general patterns of Eemian and Flandrian uplift were largely similar, but that there were also differences in crustal deformations during these two interglacials. The ice load during the maximum phase of each glaciation determines the amount of crustal downwarping, but the deglaciation history and changes in the geographical distribution of the ice load during the melting phase are other important factors, responsible for the shape of the land uplift regions.",1988,1235
Expression of the major bean proteins from Theobroma cacao (cocoa) in the yeasts Hansenula polymorpha and Saccharomyces cerevisiae,M.Ö. Yavuz and S.M.V. Ashton and E.D. Deakin and M.E. Spencer and P.E. Sudbery,"The production in two yeast expression systems of recombinant forms of the major proteins from the cocoa bean is described. Three major protein species are found in the cocoa bean: an albumin of molecular mass 21 kDa (p21) and two insoluble vicilin-like proteins of molecular mass 31 kDa and 47 kDa (p31 and p47, respectively). The p31 and p47 species are known to be derived from a common 67-kDa precursor (p67) by post-translational processing that includes the deletion of a hydrophilic domain located immediately after an N-terminal signal sequence. All three proteins appear to be targeted to membrane-bound storage organelles by N-terminal signal sequences. The p21 and p67 coding sequences were expressed in Hansenula polymorpha using the powerful methanol oxidase (MOX) promoter and in Saccharomyces cerevisiae using the promoter of the pyruvate kinase (PYK) gene. The expression constructs contained the native plant signal sequence, or various yeast signals. The p21 protein was successfully expressed and secreted from both yeasts. The insoluble p67 protein proved more difficult. Species of the correct molecular mass were recovered internally and small amounts of a p47 species were secreted using a yeast leader sequence. However, proteolytic cleavage, probably due to Kex2p-like processing, led to the appearance of other protein species.",1996,1236
Alzheimer's paired helical filaments: Amyloid precursor protein epitope mapping,Frank P. Zemlan and Glenn D. Vogelsang and Lea McLaughlin and Gary E. Dean,Paired helical filaments (PHF) were electro-phoretically purified and solubilized from Alzheimer's neurofibrillary tangles and consisted of a primary 66 kDa protein on SDS-PAGE analysis. A panel of antibodies raised against restricted regions of the beta-amyloid precursor protein (APP) were employed for epitope mapping studies of this 66 kDa PHF protein. Western blot studies revealed that C-terminal APP antibodies were immunoreactive with the 66 kDa PHF protein. Further analysis revealed that only antisera raised against peptides that include the beta/A4-amyloid region within the C-terminal portion of APP were immunoreactive with PHF proteins. These data complement previous immunocytochemical studies which indicated that C-terminal APP antibodies preferentially label PHF-containing neurofibrillary tangles in Alzheimer's brain. The present data suggest a similarity of secondary or tertiary structure between beta/A4-amyloid and PHF which accounts for the cross-reactivity of beta/A4-amyloid antibodies with PHF proteins.,1994,1237
Давид Бурлюк — Лирика,Živa Benčić,,1987,1238
The VALU3S ECSEL project: Verification and validation of automated systems safety and security,J.A. Agirre and L. Etxeberria and R. Barbosa and S. Basagiannis and G. Giantamidis and T. Bauer and E. Ferrari and M. {Labayen Esnaola} and V. Orani and J. Öberg and D. Pereira and J. Proença and R. Schlick and A. Smrčka and W. Tiberti and S. Tonetta and M. Bozzano and A. Yazici and B. Sangchoolie,"Manufacturers of automated systems and their components have been allocating an enormous amount of time and effort in R&D activities, which led to the availability of prototypes demonstrating new capabilities as well as the introduction of such systems to the market within different domains. Manufacturers need to make sure that the systems function in the intended way and according to specifications. This is not a trivial task as system complexity rises dramatically the more integrated and interconnected these systems become with the addition of automated functionality and features to them. This effort translates into an overhead on the V&V (verification and validation) process making it time-consuming and costly. In this paper, we present VALU3S, an ECSEL JU (joint undertaking) project that aims to evaluate the state-of-the-art V&V methods and tools, and design a multi-domain framework to create a clear structure around the components and elements needed to conduct the V&V process. The main expected benefit of the framework is to reduce time and cost needed to verify and validate automated systems with respect to safety, cyber-security, and privacy requirements. This is done through identification and classification of evaluation methods, tools, environments and concepts for V&V of automated systems with respect to the mentioned requirements. VALU3S will provide guidelines to the V&V community including engineers and researchers on how the V&V of automated systems could be improved considering the cost, time and effort of conducting V&V processes. To this end, VALU3S brings together a consortium with partners from 10 different countries, amounting to a mix of 25 industrial partners, 6 leading research institutes, and 10 universities to reach the project goal.",2021,1239
Scanning electron microscope observations of apical surfaces of dog thyroid cells,P. Ketelbant-Balasse and F. Rodesch and P. Neve and J.M. Pasteels,"The scanning electron microscope provides good three-dimensional pictures of dog thyroid slices. Our observations have shown at the apical surface of thyroid cells, beside numerous microvilli, large pseudopods appearing after acute TSH stimulation in vivo and in vitro. Their formation was completely inhibited by cytochalasin B. In chronic stimulation in vivo, these pseudopods are not observed. Their morphogenesis is discussed.",1973,1240
Bibliography on carbohydrate polymers,,,1992,1241
Scheduling multiple virtual environments in cloud federations for distributed calculations,A.J. Rubio-Montero and E. Huedo and R. Mayo-García,"In the pool of cloud providers that are currently available there is a lack of standardised APIs and brokering tools to effectively distribute high throughput calculations among them. Moreover, the current middleware tools are not able to straightforwardly provision the ephemeral and specific environments that certain codes and simulations require. These facts prevent the massive portability of legacy applications to cloud environments. Such an issue can be overcome by effectively scheduling the distributed calculations using the basic capacities offered by cloud federations. In this work, a framework achieving such a goal is presented: a pilot system (GWpilot) that has been improved with cloud computing capabilities (GWcloud). This framework profits from the expertise acquired in grid federations and provides interesting features that make it more efficient, flexible and useable than other approaches. Thus, decentralisation, middleware independence, dynamic brokering, on-demand provisioning of specific virtual images, compatibility with legacy applications, and the efficient accomplishment of short tasks, among other features, are achieved. Not only this, the new framework is multi-user and multi-application, dynamically instantiating virtual machines depending on the available and demanded resources, i.e. allowing users to consolidate their own resource provisioning. Results presented in this work demonstrate these features by efficiently executing several legacy applications with very different requirements on the FedCloud infrastructure at the same time.",2017,1242
"Comprehensive review on intelligent security defences in cloud: Taxonomy, security issues, ML/DL techniques, challenges and future trends",Mohamad Mulham Belal and Divya Meena Sundaram,"Nowadays, machine learning and deep learning algorithms are used in recent studies as active security techniques instead of traditional ones to secure the cloud environment based on pre-trained data. In this paper, a literature review on machine and deep learning based defences against attacks and security issues in cloud computing is provided. A taxonomy of all different types of attacks and threats as per cloud security alliance (CSA) layers; and the general defences against cloud attacks is shown in this review as well as the reasons which let the traditional security techniques fail to satisfy the desired security level are discussed. Forty-two case studies are selected based on seven quality assessment standards and then, analyzed to answer seven research questions which help to protect cloud environments from various attacks, issues, and challenges. The analysis of case studies shows a description of the most common security issues in cloud; machine learning and deep learning models that are applied, datasets models, performance metrics, machine learning and deep learning based countermeasures and defences that are developed to prevent security issues. Finally, the future scope and open challenges in cloud computing security based on machine and deep learning are discussed as well.",2022,1243
Forthcoming meetings of interest to COSPAR,,,1980,1244
Visually perceived eye level and perceived elevation of objects: Linearly additive influences from visual field pitch and from gravity,Lonard Matin and Charles R. Fox,"Observing a pitched visual field (i.e. tilted around a horizontal axis in the observer's frontal plane) results in large changes in the elevation visually perceived to correspond to eye level (VPEL) and in the perceived elevation and size of stationary objects viewed against the field. With topforward pitch (top toward observer) VPEL lies above true eye level and objects appear smaller and lower; with topbackward pitch VPEL lies below true eye level and objects appear larger and higher. Oscillation of the pitched field induces synchronous perceived oscillation of elevation of a stationary target viewed against the field. Typical VPEL settings deviated from true eye level by 20° with the field pitched at 40°, although some individuals mislocalized by as much as 40°. VPEL varied linearly with visual field pitch with individual slopes for the relation between VPEL and visual field pitch ranging from +0.42 to +0.78 (avg = +0.56). The linear correlation (r) between VPEL in darkness and against an erect visual field was +0.91. The two relations—VPEL vs visual field pitch, VPEL in darkness vs VPEL in the erect illuminated visual field (slope ≈ 0.5)—are both accurately predicted by the linear model: VPEL = kvV + kbB; in which V is the influence of visual field structure and B is the influence of the body-referenced mechanism which combines information regarding the orientation of the head relative to gravity, the position of the eye in the orbit, and the vertical location of the image on the retina; kv and kb are the relative weights of V and B with kv + kb = 1. In an illuminated field kv = kb ≈ 0.5; in the dark kv = 0, kb = 1.",1989,1245
Umsetzung von metall- und metalloidverbindungen mit mehrfunktionellen molekülen: XIV. Darstellung monomerer cyanoborane,Anton Meller and Walter Maringgele and Ulrich Sicker,"Monomeric cyanoboranes are obtained by reaction of bis(amino)haloboranes and amino(halo)organylboranes with AgCN. 1H, 11B NMR, IR and mass spectra are reported together with analytical data.
Zusammenfassung
Monomere Cyanoborane werden bei der Reaktion von Bis(amino)halogenboranen und Amino(halogen)organylboranen mit AgCNerhalten. 1H- 11B-NMR, IR und Massenspektren werden zusammen mit analytischen Daten berichtet.",1977,1246
"Trade, investment, and labor: The case of Indonesia",Debora Spar,"Rapid industrialization and urbanization spurred on by a surge in foreign investment is often considered typical of newly industrializing countries (NICs). But in Indonesia, the combination of late development and an authoritarian state has created a particularly potent mix, one that has raised more questions than usual about the effects of growth, trade, and investment on labor conditions and local standards of living. The basic motives of foreign direct investment (FDI) are said to create a pattern that is inherently ripe for exploitation, since the capital, technology and market access all rest with the foreign investor. The author uses the Indonesian case to explore factors that might reduce exploitation. While conceding that foreign investment is likely to affect labor conditions in the host economy, especially in times of rapid growth, she cautions against presuming that the consequences will be mainly negative with respect to the living standards and basic human rights of the local population.",1996,1247
Different effects of cycloheximide and chloramphenicol on corticosterone production by isolated rat adrenal cells,H.E. Falke and J.G.M. Huijmans and H.J. Degenhart,"Cycloheximide and chloramphenicol both inhibit the stimulating effect of adenocorticotropic hormone (ACTH) on adrenal steroid production. To test whether these inhibitors had any effect on adrenal steroid production, independent from the mechanism of action of ACTH, we investigated their effect on the conversion of 25-hydroxycholesterol into corticosterone in isolated rat adrenal cells. Cycloheximide, both in the absence and in the presence of ACTH, had no effect on this conversion. Chloramphenicol inhibited the conversion of 25-hydroxycholesterol into corticosterone whether ACTH was present or not. The results with Cycloheximide indicate that ACTH has no direct effect on the cholesterol side-chain cleaving system. The inhibition by chloramphenicol of the ACTH-stimulated steroid production is at least partly due to inhibition of one or more of the processes involved in the conversion of 25-hydroxycholesterol into corticosterone.",1976,1248
Mass spectrometry of natural and recombinant proteins and glycoproteins,Howard R. Morris and Fiona M. Greer,"Most modern protein sequence analysis is carried out using classical, wet-chemical Edman degradation technology. However, an increasing number of studies on both natural and recombinant genetically engineered proteins demands the use of new technologies capable of assigning structural features such as glycosylation, which cannot be assigned by Edman sequence analysis. The most important alternative and complementary procedure at present is the use of high-mass mass spectrometry. This brief article introduces some of the principles and applications of the technique. Protein research laboratories, both academic and industrial will make increasing use of these techniques to complement classical gas phase sequencing, and to identify post-translational modifications including glycosylation, phosphorylation, SS bridge assignment and processing events, including the formation of ‘ragged ends’.",1988,1249
Bibliography of toxinology,,,1997,1250
Phoma and related genera,B.C. Sutton,"The type species of Cyphellopycnis Tehon & Stout and Alveophoma Alcalde, genera described as characterized by multi-ostiolate pycnidia, are compared with Phoma herbarum Westend, var. lactaria var. nov., a multi-ostiolate variety isolated from the rubber tubing of an automatic milking machine. Cyphellopycnis is shown to be congeneric with Phomopsis (Sacc.) Sacc, the type species C. pastinacae Tehon & Stout being antedated by P. diachenii Sacc. Alveophoma is distinct from Phomopsis and Phoma Sacc. on account of the sympodial development of the conidiophores. As a result of comparative studies on several hyaline-spored pycnidial fungi related to Phoma, it is suggested that separation of genera on the basis of conidium and conidiophore development will provide a more satisfactory approach to the classification of pycnidial fungi.",1964,1251
Effects of the 5-HT1A antagonist (+)-way-100135 on murine social and agonistic behavior,Robert Bell and Paul J. Mitchell and Helen Hobson,"Compounds previously identified as 5-HT1A antagonists have subsequently been demonstrated to possess partial agonistic properties in models assessing somatodendritic autoreceptor function. This study examined the influences of (+)-WAY-100135, claimed to be the first selective 5-HT1A antagonist, on offensive behaviour in male mice. Employing a resident-intruder paradigm, administration of (+)WAY-100135 (1.0–10.0 mg/kg sc) enhanced elements of resident offensive behaviour at 2.5 and 5.0 mg/kg but reduced such behaviour at 10.0 mg/kg. In comparison, resident defensive postures remained unchanged except for a significant increase in defensive sideways behaviour at 10.0 mg/kg. These effects were accompanied by reduced rearing behaviour across the dose range tested. Attend/approach behaviour was significantly reduced at the lowest, but increased at the highest, doses tested. Such results may reflect response competition rather than concomitant motor impairment. Given the dynamic behavioural interactions occurring in this paradigm, the increased offensive behaviour of the resident mice leads to enhanced defence and counter-attack by the intruder conspecifics. The results are discussed with reference to the current literature concerning the behavioural effects of other 5-HT1A antagonists.",1996,1252
"Photosynthetic activity of diimidoester-modified cells, permeaplasts, and cell-free membrane fragments of the blue-green alga Anacystis nidulans",George C. Papageorgiou,"On treating the blue-green alga Anacystis nidulans with dimethylsuberimidate up to 70% of the free NH2 of the photosynthetic membrane is amidinated, and presumably inter- and intramolecular cross-links are established in the membrane proteins. Amidination destroys the ability of A. nidulans to photoreduce HCO3− but leaves the photochemical activities of Photosystems II and I nearly intact. With added electron acceptors, photosynthetic O2 evolution can be demonstrated both with permeable cells (permeaplasts) prepared by digestion of the cell wall of dimethylsuberimidate-reacted A. nidulans with lysozyme, as well as with heavy membrane particles (36 000 × g) prepared from dimethylsuberimidate-reacted cells. Permeaplasts prepared from dimethylsuberimidate-reacted cells resist damage in hypoosmotic medium, whereas those prepared from unreacted cells are induced to release C-phycocyanin. On the other hand, the former are inactivated more easily by heat stress than the latter. On this basis, it is concluded that cross-linking with dimethylsuberimidate confers functional instability to photosynthetic membranes.",1977,1253
Annual survey of organometallic metal cluster chemistry for the year 1994,Michael G. Richmond,,1996,1254
Capillary column gas chromatography for mercury speciation,E. Bulska and D.C. Baxter and W. Frech,"The determination of methyl- and ethylmercury halides in environmental and biological samples typically involves gas chromatography with electron-capture detection. However, these organomercury halides are notorious for their poor chromatographic characteristics (severe tailing, decomposition, low column efficiencies) on packed columns. The problems can be temporarily alleviated by column passivation using a concentrated organic solution of mercury(II) chloride. Attempts to use capillary columns instead, to improve the chromatographic behaviour of organomercury halides, have met with mixed success, and the results presented generally show poorer performance than that obtained using packed columns, even after passivation. To eliminate the problem at its source (the polar mercury-halide bond), it is proposed to butylate the mercury species with a Grignard reagent to yield the non-polar dialkyl derivatives. As the electron-capturing halide moiety is absent from these derivatives, mercury-specific detection is necessary, and a microwave-induced plasma emission detector is utilized. In combination with capillary gas chromatography, unprecedented column and separation efficiencies for methyl- and ethylmercury are achieved. The practical utility of the method is illustrated in a preliminary application to the determination of mercury species in a fish tissue reference material after extraction and butylation.",1991,1255
Electrosorption of L-α-dipalmitoylphosphatidylcholine at the mercury solution interface,G.T. Runbeck and D.M. Mohilner and T.N. Solie,Electrocapillary curves were measured for L-α-dipalmitoylphosphatidylcholine in 97% (υ/υ) methanol-water solutions containing 0·01 M NH4NO3. The electrosorption isotherms are similar to those of simpler aliphatic compounds. Saturated coverage of the electrode surface with this lipid in 97% (υ/υ) methanol-water was not possible owing to the formation of micelles in the bulk solution. It is shown that it is possible from the electrocapillary curves to estimate the critical micelle concentration of lipids in non-aqueous electrolyte solutions.,1975,1256
Schistosoma mansoni: Rapid isolation and purification of schistosomula of different developmental stages by centrifugation on discontinuous density gradients of Percoll,Janis K. Lazdins and Marsha J. Stein and John R. David and Alan Sher,"Step gradients of polyvinylpyrolidone-coated colloidal silica particles (Percoll) were used to isolate and purify early development stages of Schistosoma mansoni (cercariae, skin stage, and 5-day-old schistosomula). With this method, mechanically transformed schistosomula can be isolated in higher purity and yield than that obtained with conventional procedures. In addition, use of the method revealed that schistosomula undergo a dramatic change in density during the first hours after transformation from cercariae. In other experiments, 5-day-old schistosomula were effectively purified from contaminating lung tissue by means of the Percoll gradient procedure. After purification on Percoll, schistosomula display no evidence of damage when examined by light microscopy and no loss in viability as judged by recovery of adult worms from mice.",1982,1257
Multi-classification approaches for classifying mobile app traffic,Giuseppe Aceto and Domenico Ciuonzo and Antonio Montieri and Antonio Pescapé,"The growing usage of smartphones in everyday life is deeply (and rapidly) changing the nature of traffic traversing home and enterprise networks, and the Internet. Different tools and middleboxes, such as performance enhancement proxies, network monitors and policy enforcement devices, base their functions on the knowledge of the applications generating the traffic. This requirement is tightly coupled to an accurate traffic classification, being exacerbated by the (daily) expanding set of apps and the moving-target nature of mobile traffic. On the top of that, the increasing adoption of encrypted protocols (such as TLS) makes classification even more challenging, defeating established approaches (e.g., Deep Packet Inspection). To this end, in this paper we aim to improve the performance of classification of mobile apps traffic by proposing a multi-classification (viz. fusion) approach, intelligently-combining outputs from state-of-the-art classifiers proposed for mobile and encrypted traffic classification. Under this framework, four classes of different combiners (differing in whether they accept soft or hard classifiers' outputs, the training requirements, and the learning philosophy) are taken into account and compared. The present approach enjoys modularity, as any classifier may be readily plugged-in/out to improve performance further. Finally, based on a dataset of (true) users' activity collected by a mobile solutions provider, our results demonstrate that classification performance can be improved according to all considered metrics, up to +9.5% (recall score) with respect to the best state-of-the-art classifier. The proposed system is also capitalized to validate a novel pre-processing of traffic traces, here developed, and assess performance sensitivity to traffic object (temporal) segmentation, before actual classification.",2018,1258
Serotonin and antidepressant drugs antagonize melatonin-induced behavioural changes after injection into the nucleus accumbens of rats,Odile Gaffori and J.M. {Van Ree},"Small doses of melatonin (0.1–100 ng), injected into the nucleus accumbens of rats, decreased locomotor activity and rearing, and increased grooming and sniffing behaviour when the animals were tested in small test-cages. Larger doses ofmelatonin appeared to be less effective. The action ofmelatonin is apparently not mediated by dopaminergic systems, because the behavioural changes were not antagonized by local pretreatment with haloperidol or sulpiride. Injection of serotonin antagonists (methysergide and cyproheptadine) into the nucleus accumbens resulted in similar behavioural changes as was found after treatment with melatonin. Treatment with serotonin and various antidepressant drugs (zimelidine, mianserin, nortriptyline, clomipramine, desipramine) injected into the nucleus accumbens, completely inhibited the melatonin-induced behavioural responses. The antidepressants did not significantly interfere with the decrease of locomotor activity and rearing induced by injection of small doses of the dopamine agonist, apomorphine, into the nucleus accumbens. p ]These results suggest that there is an interrelationship between melatonin and serotonin systems in the nucleus accumbens and showed that various antidepressant drugs, similar to serotonin, antagonized the behavioural effects of melatonin after injection into the nucleus accumbens.",1985,1259
Full issue PDF,,,2023,1260
Indirect impacts of soil trampling on tree growth and plant succession in the North Cascade Mountains of Washington,Roland C. {de Gouvenain},"The purpose of this study was to establish what effects soil trampling impacts may have had on tree growth and plant community succession at subalpine elevations in the North Cascade Mountains of Washington. While soil penetrability of an impacted site was not different from that of control sites, other soil characteristics were significantly different. Species diversity was greater at the impacted site, which was characterized by associations between soil characteristics and vegetation that were unlike those found at control sites. Mixed ordination analyses of the floristic and soil data indicated that past trampling impacts on soils may have had long-term effects on the successional development of the plant community. The impact of turn of the century mining operations on tree growth was significant at the time of impact but had no long-term effect after mining activities ceased. Recommendations for the management and reclamation of disturbed sties at subalpine elevations are given.",1996,1261
Functional deprivation of noradrenaline neurotransmission: effects of clonidine on brain development,Majid Mirmiran and Matthijs G.P. Feenstra and Fred A. Dijcks and Nico P.A. Bos and Frans {Van Haaren},"Publisher Summary
The purpose of this chapter is to provide foundations for a working hypothesis on drug-induced functional neuroteratology. A class of centrally acting antihypertensive drugs, including clonidine, which are still being prescribed during pregnancy in humans, is used as the model. A hypothesis is put forward in this chapter which proposes that behavioral state-dependent changes in monoaminergic neuronal firing levels and patterns play a key role during early ontogeny in regulating neuron membrane potential, neurotransmitter release and neurotransmitter receptor sensitivity patterns in adulthood. Neurons show spontaneous activity that varies as a function of the behavioral state of the organism. This has been demonstrated for noradrenergic neurons of the locus coeruleus (LC), serotonergic neurons of the dorsal raphe (DR), dopaminergic neurons of the substantia nigra (SN) and cholinergic neurons of the dorsolateral part of the pons and basal forebrain. Because centrally acting antihypertensives predominantly affect the noradrenaline (NA) system in the brain, the induced modifications of central NA neurotransmission is also emphasized in this chapter.",1988,1262
The basal ganglia on cranial computed tomography: Normal anatomy and pathology,Anne G. Osborn and Terrence Saville,"The density and configuration of the basal ganglia were analyzed on 1000 consecutive cranial computed tomography (CCT) scans performed both with and without contrast enhancement. At our standard window width (150 H) and window level (30 H), visibly increased attenuation in the arus of the basal ganglia was apparent in 5.2% of all scans examined. A statistically significant difference in attenuation between these subcortical gray nuclei and adjacent white matter was detected by computer analysis of numerical printouts even though this difference was often not visually perceptible on our first generation-type CCT scanner. Slight, but statistically significant increased attenuation occurs with contrast enhancement and affects both the basal ganglia and surrounding white matter. Three thousand consecutive CCT scans were reviewed; of these, 61 cases had lesions primarily involving the basal ganglia. Cases representative of typical abnormalities are described and discussed.",1977,1263
Manufacture of salt,,,1832,1264
Release of neuronotrophic factor from rabbit corneal epithelium during wound healing and nerve regeneration,Kwan Y. Chan and Robert R. Jones and Don H. Bark and Jay Swift and James A. Parker and Richard H. Haschke,"Epithelial neuronotropic factor (ENF) is secreted by cultured epithelial cells of rabbit cornea and conjunctiva, and is active in promoting survival and inducing neurite outgrowth of cultured trigeminal neurons. This study evaluated the relation of ENF to corneal nerve regeneration utilizing a model of heptanol-induced epithelial wounding. The organ culture technique was used to collect ENF from the intact corneal epithelium, and a neuronal bioassay was utilized to quantify ENF. The results revealed no change in ENF secretion either during initial wound closure or after 1 week, when the epithelium had regenerated. However, ENF secretion was elevated 2·4 times in 2 weeks after wounding. Morphometric analysis of corneal nerves stained by gold chloride impregnation showed that the first sign of regeneration of intraepithelial nerves was observed after 2 weeks, and the normal pattern of epithelial neural density was re-established after 3 weeks. However, the neural density was still subnormal (35–47% less than the control) in the wounded epithelium up to 4 weeks after wounding. Thus it appears that a surge in ENF secretion occurred after epithelial regeneration but before nerve regeneration. The results suggest that ENF may mediate corneal nerve regeneration.",1987,1265
"Historical, Socio-Cultural, and Conceptual Issues to Consider When Researching Mexican American Children and Families, and other Latino Subgroups*",Raymond Buriel,"In order for the field of psychology in the United States to maintain its relevance and validity, it must become more inclusive in its theory and research of Latinos, who are now the largest “minority” group in the nation. In particular, due to immigration and birth rates, Mexican Americans are the largest and fastest growing segment of the Latino population. This paper addresses some of the most significant historical and socio-cultural factors contributing to the psychological nature and wellbeing of Mexican Americans. These factors should be understood and used to guide research and theory in order to make the discipline of psychology relevant for Mexican Americans. The concept of mestizaje is used to explain the biological and cultural mixing constituting the diverse origins of the Mexican people. Immigration to the U.S. is described in terms of selective socio-cultural variables giving rise to a diverse Mexican American culture that is resistant to complete assimilation. Within a U.S. context, the constructs of generational status, acculturation, and biculturalism are used to explain the socio-cultural adaptation of Mexican Americans. The special role of children in immigrant families as language and cultural brokers are also discussed, and used to explain the adjustment of Mexican American families.
Resumen
Para que el campo de la Psicología en los Estados Unidos siga manteniendo su relevancia y validez, debe incluir en mayor medida, tanto en su teoría como en la investigación práctica, a las poblaciones hispanas, grupo que en la actualidad compone la “minoría” más numerosa de la nación. En concreto, debido a la inmigración y las tasas de natalidad, los mexicano-estadounidenses son el segmento más amplio y de mayor crecimiento dentro de la población hispana. El presente artículo aborda algunos de los factores históricos y socio-culturales más significativos en la naturaleza y bienestar psicológico de los mexicanoestadounidenses. Estos factores deben ser comprendidos y utilizados como guía en la investigación y el desarrollo teórico para que la disciplina de psicología incluya a la población mexicano-estadounidense. Se emplea el concepto de mestizaje para explicar la mezcolanza biológica y cultural que da pie a la diversidad de orígenes de la población mejicana. Se describe la inmigración a los EEUU en base a variables socioculturales selectivas que conforman una cultura mexicano-estadounidense diversa que se resiste a una total asimilación. Dentro del contexto norteamericano, los constructos de estatus generacional, aculturación y bi-culturalismo son utilizados para explicar la adaptación socio-cultural de los mexicano-estadounidenses. También se aborda el papel especial que desempeñan los niños de familias inmigrantes en cuanto al idioma y la cultura y su importancia en el proceso de adaptación de las familias mexicano-estadounidenses.",2012,1266
ProCon: An automated process-centric quality constraints checking framework,Christoph Mayr-Dorn and Michael Vierhauser and Stefan Bichler and Felix Keplinger and Jane Cleland-Huang and Alexander Egyed and Thomas Mehofer,"When dealing with safety–critical systems, various regulations, standards, and guidelines stipulate stringent requirements for certification and traceability of artifacts, but typically lack details with regards to the corresponding software engineering process. Given the industrial practice of only using semi-formal notations for describing engineering processes – with the lack of proper tool mapping – engineers and developers need to invest a significant amount of time and effort to ensure that all steps mandated by quality assurance are followed. The sheer size and complexity of systems and regulations make manual, timely feedback from Quality Assurance (QA) engineers infeasible. In order to address these issues, in this paper, we propose a novel framework for tracking, and “passively” executing processes in the background, automatically checking QA constraints depending on process progress, and informing the developer of unfulfilled QA constraints. We evaluate our approach by applying it to three case studies: a safety–critical open-source community system, a safety–critical system in the air-traffic control domain, and a non-safety–critical, web-based system. Results from our analysis confirm that trace links are often corrected or completed after the work step has been considered finished, and the engineer has already moved on to another step. Thus, support for timely and automated constraint checking has significant potential to reduce rework as the engineer receives continuous feedback already during their work step.",2023,1267
"Nuclear-matter density distribution in the neutron-rich nuclei 12,14Be from proton elastic scattering in inverse kinematics",S. Ilieva and F. Aksouh and G.D. Alkhazov and L. Chulkov and A.V. Dobrovolsky and P. Egelhof and H. Geissel and M. Gorska and A. Inglessi and R. Kanungo and A.V. Khanzadeev and O.A. Kiselev and G.A. Korolev and X.C. Le and Yu.A. Litvinov and C. Nociforo and D.M. Seliverstov and L.O. Sergeev and H. Simon and V.A. Volkov and A.A. Vorobyov and H. Weick and V.I. Yatsoura and A.A. Zhdanov,"In the present work, the differential cross sections for small-angle proton elastic scattering on the 12,14Be nuclei were measured in inverse kinematics, using secondary radioactive beams with energies near 700 MeV/u produced with the fragment separator FRS at GSI. The main part of the experimental setup was the active target IKAR, which was used simultaneously as a target and a detector for the recoil protons. Auxiliary detectors for projectile tracking and isotope identification completed the setup. The measured differential cross sections were analyzed using the Glauber multiple-scattering theory. For the evaluation of the data several phenomenological nuclear-matter density parametrizations and a sum of Gaussian parametrization were used. The nuclear-matter radii and radial density distributions of the isotopes 12,14Be were deduced. Extended nuclear-matter density distributions were observed in both isotopes, and the halo structure of 14Be was confirmed. The results were also compared with microscopic few-body and fermionic molecular dynamics model calculations concerning the structure of these neutron-rich nuclei.",2012,1268
Chapter 8 - A Study of the Effectiveness Abs Reliability of Android Free Anti-Mobile Malware Apps,J. Walls and K.-K.R. Choo,"With the increasing popularity of Android devices and the number of malware apps targeting them, ensuring the security and privacy of user data and preventing the device from being compromised are of paramount importance. In this chapter, we survey Android-related security risks and vulnerabilities, as well as conduct a systematic evaluation of 15 popular free antimalware apps using 11 known malware samples collected between Feb. 27, 2014 and Aug. 3, 2014 from the Contagio Mobile Malware Mini Dump database. By conducting a manual experiment to replicate potential day-to-day threats to users unknowingly installing malicious apps (i.e., malware samples), the evaluation produced results that varied significantly across all three Android operating systems and hardware test devices. We hope the findings will contribute to a better understanding of the effectiveness and reliability of such apps for Android devices, as well as improved antimalware apps and detection rates.",2017,1269
Stratigraphic analysis system: SAS,Brian R. Shaw and Richard Simms,"Stratigraphic Analysis System (SAS) is an on-line, interactive data-base analysis system designed for use in a subsurface laboratory. The program is written in FORTRAN and ALGOL W and presently runs under the Michigan Terminal System at the University of Michigan. The SAS system was designed to overcome several problems in geological data-base systems. Both data discontinuities and substring indexing have been considered as well as three-dimensional location of information. The system consists of four procedures; the command processor, the user aid package, the data-set loader and general data processors. The data set is composed of hierarchical records in a one-dimensional array which consists of logical flags to index an internal dictionary. Presently output contains well listings, well displays, data editing and data search capabilities.",1977,1270
"Venomous Snakebites: Current Concepts in Diagnosis, Treatment, and Management",Barry S. Gold and Robert A. Barish,,1992,1271
"Adhesion of suspension-cultured Catharanthus roseus cells to surfaces: effect of pH, ionic strength, and cation valency",Peter J. Facchini and A. {Wilhelm Neumann} and Frank DiCosmo,"The correlation between the effects of pH, ionic strength and cation valency on the electrophoretic mobility and the extent of adhesion of suspension-cultured Catharanthus roseus cells to various polymer substrates is presented. The electrophoretic mobility of cells was unaltered in the pH range of 6–8, but decreased from approximately −2.2 × 10−8 m V−1 s−1 and approached zero as the pH of the suspending liquid was decreased from 6 to 2. Similarly, the value of electrophoretic mobility decreased continuously as the ionic strength was increased from 0 to 1.0 M when cells were suspended in salt solutions of sodium chloride, calcium chloride, and aluminium chloride. However, using equimolar concentrations, the slope of the decrease in electrophoretic mobility increased following the sequence sodium chloride < calcium chloride < aluminium chloride. The electrophoretic mobility was near zero for suspensions containing 1.0 M calcium chloride or 0.1 M aluminium chloride. The extent of adhesion of the cells to the polymers sulphonated polystyrene < polyethylene terephthalate < polystyrene < fluorinated ethylene-propylene followed this sequence. These results agree with a thermodynamic model of plant cell adhesion that implicates the importance of interfacial tensions in the adhesion process. However, higher levels of adhesion were generally observed when the electrophoretic mobility for the cells in the corresponding test liquid was at a minimum absolute value. These results can be explained by considering the effects of the electrolytic properties of the suspending liquid on the electrostatic repulsive interactions between the cells and the polymer surface in terms of a double-layer phenomenon and the DLVO theory. Consideration of results presented here along with kinetic studies of plant cell adhesion indicates that cell-cell, rather than cell-substrate repulsion, may play the more important role in modifying the extent of adhesion.",1989,1272
"Clinical assessment of subdermal implants of megestrol acetate, d-norgestrel, and norethindrone as a longterm contraceptive in women",H.B. Croxatto and S. Díaz and E. Quinteros and L. Simoneti and E. Kaplan and R. Rencoret and P. Leixelard and C. Mártinez,"Megestrol acetate (MA), d-norgestrel (d-Ng), and norethindrone (NET) contained in Silastic capsules were implanted under the skin for clinical evaluation as a longterm contraceptive in women. 1509 woman-months of exposure and 4 pregnancies were recorded within the first twelve months of use in 135 women who received 6 MA implants. 1049 woman-months and 19 pregnancies were recorded within the first twelve months of use in 131 women who received 4 d-Ng implants. After twelve months use, the implants were replaced with a new set of capsules. The contraceptive effectiveness of the second and subsequent set of implants was similar to that of the first. Five NET implants failed completely to prevent pregnancy and 4 MA implants combined with 2 d-Ng implants were as effective as 6 MA implants. Other doses tested were: 5 MA, 3 d-Ng, and 4 MA plus 1 d-Ng. They were significantly less effective than the higher doses. No adverse effect upon the outcome of unplanned pregnancies was noted and prompt recovery of fertility was observed after termination of treatment. Ovulation took place in most cycles of women treated with 5 or 6 MA implants, as judged from the occurrence of LH peak in urine, pregnanediol excretion, changes of cervical mucus, BBT, and endometrial biopsy. Intermenstrual bleeding was by far the most common side effect recorded. Initially, it occurred in about 30 % of the cycles, but the incidence decreased gradually and by the end of the second year, it was below 10 %. Adnexal complications were observed in some of the treatment groups.",1975,1273
"Kinetics of the incorporation of dietary fatty acids into serum cholesteryl esters, erythrocyte membranes, and adipose tissue: an 18-month controlled study",M B Katan and J P Deslypere and A P {van Birgelen} and M Penders and M Zegwaard,"Tissue levels of n-3 fatty acids reflect dietary intake, but quantitative data about rate of incorporation and levels as a function of intake are scarce. We fed 58 men 0, 3, 6, or 9 g/d of fish oil for 12 months and monitored fatty acids in serum cholesteryl esters, erythrocytes, and subcutaneous fat during and after supplementation. Eicosapentaenoic acid (EPA) in cholesteryl esters plateaued after 4-8 weeks; the incorporation half-life was 4.8 days. Steady-state levels increased by 3.9 +/- 0.3 mass ﹪ points (+/- SE) for each extra gram of EPA eaten per day. Incorporation of docosahexaenoic acid (DHA) was erratic; plateau values were 1.1 +/- 0.1 mass ﹪ higher for every g/d ingested. Incorporation of EPA into erythrocyte membranes showed a half-life of 28 days; a steady state was reached after 180 days. Each g/d increased levels by 2.1 +/- 0.1 mass ﹪. C22:5n-3 levels increased markedly. Changes in DHA were erratic and smaller. EPA levels in adipose tissue rose also; the change after 6 months was 67﹪ of that after 12 months in gluteal and 75﹪ in abdominal fat. After 12 months each gram per day caused an 0.11 +/- 0.01 mass ﹪ rise in gluteal fat for EPA, 0.53 +/- 0.07 for C22:5n-3, and 0.14 +/- 0.03 for DHA. Thus, different (n-3) fatty acids were incorporated with different efficiencies, possibly because of interconversions or different affinities of the enzymatic pathways involved. EPA levels in cholesteryl esters reflect intake over the past week or two, erythrocytes over the past month or two, and adipose tissue over a period of years. These findings may help in assessing the intake of (n-3) fatty acids in epidemiological studies.",1997,1274
Effects of synthetic detergents on in vivo activity of tissue phosphatases and succinic dehydrogenase from Mystus vittatus,D. Mohan and S.R. Verma,"African catfish (Mystus vittatus) were exposed to three sub-lethal concentrations of Swascofix E45 (13.8, 9.2 and 4.6 mg/l) and Swascol 3L (69.3, 46.2 and 23.1 mgl) for 15 and 30 days, and their effects on alkaline and acid phosphatase,, and succinic dehydrogenase in liver, kidney and intestine were measured. The enzymes were found to be inhibited in all the tissues. Maximum inhibition (38.44%) was observed in liver alkaline phosphatase activity after 30 days with the highest concentration of Swascofix E45 and the lowest inhibition (0.118%) was found in kidney acid phosphatase activity with the lowest concentration of Swascol 3L after 15 days. Insignificant enzyme stimulation in some cases was also observed.",1981,1275
Precise ReOs determinations and systematics of iron meteorites,J.J. Shen and D.A. Papanastassiou and G.J. Wasserburg,"The ReOs system for samples of FeNi, sulphide, and phosphide from iron meteorites was investigated. Techniques were developed which yield reproducible analyses for Re/Os at the 2%‰ level and which permit complete isotopic exchange between sample and tracer, as is necessary for concentration measurements of Re and Os by isotope dilution. High precision osmium and rhenium isotope data have been obtained using negative ion thermal ionization, with ionization efficiencies of up to 10% for Os and 20% for Re, both for normals and for Re and Os extracted from the samples. Replicate analyses of Re/Os are in good agreement, within ±2.5%o. The results show a well defined correlation line on a 187Re-187Os evolution diagram for iron meteorites from groups IAB, IIAB, IIIAB, IVA, and IVB, all taken together. This correlation line yields a slope of 0.07863 ± 0.00031 (2σ) and initial 187Os/188Os = 0.09560 ± 0.00018 (2σ). If the individual groups of iron meteorites for which there is sufficient dispersion in Re/Os are considered, data on the IIAB and on the IVA irons appear to indicate a difference in age of 60 ± 45 Ma, with the IVA group being older. This age difference is qualitatively the same as obtained for PdAg data but is larger. Sulphides from two IAB iron meteorites show extremely low concentrations of Re and Os and indicate that Re and Os are not partitioned into this phase during planetary differentiation. There is evidence for recent element remobilization or contamination, corresponding to relative enrichment of Re or loss of Os in the sulphides. Schreibersites contain small but significant amounts of Re and Os, with high Re/Os relative to the metal phases and with 187Os/188Os much more radiogenic than in the metal. Model ages for the Schreibersites are relatively young (4.3–3.5 AE) and indicate that the Schreibersites were open-systems for ReOs at least 0.5–1 AE after the original formation of the iron meteorites. It now appears possible to use metal- schreibersite pairs to determine internal isochrons. Based on the schreibersite model ages, the cooling rates for the two IAB meteorites are estimated to be ∼ 1°C/Ma, more than an order of magnitude lower than the most recently determined metallographic cooling rates for IAB irons (Herpfer et al., 1994).",1996,1276
Pleistozäne Kaltzeitliche Ablagerungen In Der Sila Und Basilicata (Süd-Italien),Friderun Fuchs and Arno Semmel,"Summary
Within the regions studied deposits are to be found in the whole area between the Pleistocene marine terraces — situated only a few meters above sea-level — and the limit of the Pleistocene glaciations. These deposits can be interpreted as glacial frost-climate formations. Whilst in higher altitudes indications of great mass-movements were found, above all solifluction mantles, in middle altitudes formations of increased wash-off become prominent. In basin-areas, which are situated in the proximity of the sea, the accumulation of loess prevailed. On the basis of the data available at present, the greatest part of these formations appears to belong to the last glaciation (Wuerm). In some localities, however, deposits were found, which are likely to originate from older glaciations.
Zusammenfassung
In den untersuchten Gebieten kommen im gesamten Bereich zwischen den nur wenige Meter über dem Meeresspiegel liegenden pleistozänen marinen Terrassen und der Grenze der pleistozänen Vergletscherungen Ablagerungen vor, die als kaltzeitliche frostklimatische Bildungen gedeutet werden können. Während in den höheren Lagen Anzeichen starker Massenbewegungen, vor allem Solifluktionsdecken gefunden wurden, treten in einem mittleren Höhengürtel Bildungen verstärkter Abspülung in den Vordergrund. In den meernahen Beckenlagen dominierte die Löß-Akkumulation. Aufgrund der gegenwärtig verfügbaren Daten scheint der größte Teil dieser Bildungen in die letzte Kaltzeit (Würm) zu gehören. An einigen Stellen sind jedoch auch Ablagerungen gefunden worden, die wohl in älteren Kaltzeiten entstanden sind.",1973,1277
Interpol review of digital evidence for 2019–2022,Paul Reedy,,2023,1278
"Effect of prenatal exposure to styrene on the neurobehavioral development, activity, motor coordination, and learning behavior of rats",Reiko Kishi and Bing Qing Chen and Yohko Katakura and Toshiko Ikeda and Hirotsugu Miyake,"Maternal Wistar rats were exposed via inhalation to 0, 50, or 300 ppm styrene for 6 h/day during gestation days 7 to 21, and offspring were subsequently evaluated in several neurobehavioral tests. Preliminary results with a small number of litters revealed significant dose-dependent effects in tests performed prior to weaning (surface righting, pivoting locomotion, and bar holding), as well as in tests performed after weaning (motor coordination, open-field behavior, and motor activity). Exposure to low concentrations of styrene (50 ppm) caused disturbances in motor coordination in addition to delaying some motor and reflex developments. Large doses (300 ppm) led to changes in open-field behavior and increases in spontaneous activity in addition to the delay in neurobehavioral developments. Exposure of dams to styrene did not clearly affect the learning behavior of the offspring. It was also observed that age played a role in the differences in styrene's effects on neurobehavioral function. Only subtle effects were found in both open-field behavior and motorcoordination function when compared with control rats at 120 days of age. These results suggest that the functional neurobehavioral development of progeny of dams exposed to styrene (or other solvents) should be further investigated.",1995,1279
Lipoprotein lipase in heart cell cultures is suppressed by bacterial lipopolysaccharide: an effect mediated by production of tumor necrosis factor,G. Friedman and R. Gallily and T. Chajek-Shaul and O. Stein and E. Shiloni and J. Etienne and Y. Stein,"Exposure of rat heart cell cultures, consisting mainly of nonbeating mesenchymal cells, to 50 ng/ml of bacterial lipopolysaccharide (LPS) for 24 h resulted in a more than 80% reduction in lipoprotein lipase activity. The loss of enzymic activity was accompanied by a concomitant reduction in enzyme protein, as shown by inununoblotting. Addition of LPS to the culture medium resulted also in the production of tumor necrosis factor (TNT), and the fall in lipoprotein lipase in LPS-treated cultures could be prevented by an antibody to TNF. Addition of recombinant human TNF to the heart cell cultures also depressed lipoprotein lipase activity. LPS treatment of preadipocytes in culture resulted in a fall in lipoprotein lipase activity and TNF production. Since TNF is known as a macrophage product, the cultures were tested for phagocytic capacity, and only 0.2–1.3% of the cells were shown to engulf Staphylococcus albus. Immunofluorescent staining with monoclonal antibodies OX-1, which identify leukocyte common antigen, was negative, and only 0.1 ± 0.07% of the cells were positive after staining with OX-42 antibody to iC3 receptor. Both antibodies stained more than 98% of rat peritoneal macrophages used as controls. Since LPS treatment of macrophages at numbers comparable to or exceeding the number of phagocytic cells present in the heart cell cultures did not induce measurable amounts of TNF, it is suggested that in the heart cell cultures, TNF may be produced by cells other than macrophages.",1988,1280
Construction of refined applied theories for a shell of arbitrary shape,N.A. Bazarenko,"Basic equations of the theory of elasticity are given in a semi-orthogonal curvilinear coordinate system in which one of the families of the coordinate surfaces is parallel to the middle surface of the shell. Symbolic notation of Lur'e /1/ is used to obtain a solution of the problem of the theory of elasticity for a shell, in terms of a series in powers of the normal coordinate. The solution is then used to reduce the three-dimensional problem to two dimensions and to express all characteristic features of the stress and deformation states of the shell in terms of six functions, namely the coordinates of the displacement and stress vectors defined on the middle surface. Use of the first two terms of the series obtained yields an applied theory free of any hypotheses and intended for removing the external load from the front surface of the shell. A similar approach to the problem of constructing applied theories was first used in /2–4/ which made wide use of the resources of tensor analysis.",1980,1281
Synthesis of glycopeptides with phytoalexin elicitor activity — III. Syntheses of hexaglycosyl hexapeptides and a nonaglycosyl hexapeptide,Tadahiro Takeda and Takuya Kanemitsu and Yukio Ogihara,"A block synthesis of the model compound for the phytoalexin elicitor-active glycoprotein is described. Combination of the C-terminus free compounds, N-(9-fluorenylmethoxycarbonyl)-O-(tert-butyl)-l-seryl-l-proline (1) or N-(9-fluorenylmethoxycar bonyl)-(2,3,4,6-tetra-O-acetyl-β-d-glucopyranosyl)-(1→ 6)-(2,3,4-tri-O-acetyl-α-d-mannopyranosyl)-(1→ 6)-(2,3,4-tri-O-acetyl-α-d-mannopyranosyl)-l-seryl-l-proline (2) with the N-terminus free compounds, 2,3,4,6-tetra-O-acetyl-β-d-glucopyranosyl(1→6)-(2,3,4-tri-O-acetyl-α-d- mannopyranosyl)-(1→6)-(2,3,4-tri-O-acetyl-α-d-mannopyranosyl)-l-seryl-l- prolyl-l-seryl-l-proline methyl ester (4), O-(tert-butyl)-l-seryl-l-prolyl-(2,3,4,6-tetra-O-acetyl- β-d-glucopyranosyl)- (1→6)-(2,3,4-tri-O-acetyl-α-d-mannopyranosyl)-(1→6)-(2,3,4-tri-O- acetyl-α-d-mannopyranosyl)-l-seryl-l-proline methyl ester (6) or 2,3,4,6-tetra-O-acetyl-β-d-glucopyranosyl-(1→6)-(2,3,4-tri-O-acetyl-α-d- mannopyranosyl)-(1→6)-(2,3,4-tri-O-acetyl-α-d- mannopyranosyl)-l-seryl-l-prolyl-(2,3,4,6-tetra-O-acetyl-β-d-glucopyranosyl)-(1→6)-(2,3,4-tri-O- acetyl-α-d-mannopyranosyl)-(1→6)-(2,3,4-tri- O-acetyl-α-d-mannopyranosyl)-l-seryl-l-proline methyl ester (8), by use of N-ethoxycarbonyl-2-ethoxy-1,2-dihydroquinoline (EEDQ) gave three hexaglycosyl hexapeptides and a nonaglycosyl hexapeptide derivatives (9, 11, 14, and 17). These N-terminus free compounds were derived from triglycosyl tetrapeptides (3, and 5) or a hexaglycosyl tetrapeptide (7) on selective deblock reaction by morpholine. The hexaglycosyl hexapeptides (10, 13, and 16) and the nonaglycosyl hexapeptide (18) have been prepared by the convergent block synthesis.",1996,1282
Appendix B - Testing the DMZ,,,2006,1283
Two-pion decay of K20 at 10 GeV/c,X. {De Bouard} and D. Dekkers and B. Jordan and R. Mermod and T.R. Willitts and K. Winter and P. Scharff and L. Valentin and M. Vivargent and M. Bott-Bodenhausen,,1965,1284
"Empirical model of Skeletonema costatum photosynthetic rate, with applications in the San Francisco Bay estuary",James E. Cloern,"An empirical model of Skeletonema costatum photosynthetic rate is developed and fit to measurements of photosynthesis selected from the literature. Because the model acknowledges existence of: 1) a light-temperature interaction (by allowing optimum irradiance to vary with temperature), 2) light inhibition, 3) temperature inhibition, and 4) a salinity effect, it accurately estimates photosynthetic rates measured over a wide range of temperature, light intensity, and salinity. Integration of predicted instantaneous rate of photosynthesis with time and depth yields daily net carbon assimilation (pg C cell−1 day−1) in a mixed layer of specified depth, when salinity, temperature, daily irradiance and extinction coefficient are known. The assumption of constant carbon quota (pg C cell−1) allows for prediction of mean specific growth rate (day−1), which can be used in numerical models of Skeletonema costatum population dynamics. Application of the model to northern San Francisco Bay clearly demonstrates the limitation of growth by low light availability, and suggests that large population densities of S. costatum observed during summer months are not the result of active growth in the central deep channels (where growth rates are consistently predicted to be negative). But predicted growth rates in the lateral shallows are positive during summer and fall, thus offering a testable hypothesis that shoals are the only sites of active population growth by S. costatum (and perhaps other neritic diatoms) in the northern reach of San Francisco Bay.",1978,1285
Host effector mechanisms against parasites,W.I. Morrison,"The first part of this presentation considers some of the complexities of parasitic infections and parasite-specific effector mechanisms which have hampered the development of practical methods of immunisation against parasitic diseases. In the second part, an outline is given of the effector mechanisms involved in immunity of cattle to the protozoan parasite Theileria parva. Parasites are antigenically complex organisms which often have distinct developmental stages, sometimes with different predilection sites within the host. Antigenic polymorphism between strains is a common feature of parasites and sometimes results in strain-specific immunity. Certain parasites have also evolved mechanisms of modulating surface antigens which allow them to escape host effector mechanism. Effector mechanisms which control parasitic infections may operate by preventing establishment of the parasites, by eliminating the parasites once they have established or by affecting growth or fecundity of the parasites. In addition to specific antibody and cell-mediated immune responses, inflammatory or physiological responses play an important role in the control of some parasites. Current evidence suggests that effector mechanisms against T. parva parasites operate at two levels. First, antibodies produced against the infective stage of the parasite, the sporozoite, can, by neutralising infectivity, reduce the numbers of organisms which establish in the host. Second, cytotoxic T cells directed against parasitised lymphoblasts cause destruction of parasites following their establishment in the host. Moreover, in situations where immunity is parasite strain-specific, the cytotoxic T cell responses have also been found to be strain-specific. The elucidation of these effector mechanisms has indicated potential new strategies of immunisation against T.parva.",1987,1286
Clinical Implications of Brushite Calculi,Lawrence W. Klee and C. Gilberto Brito and James E. Lingeman,"The clinical history of 30 patients with a total of 46 proved brushite urinary calculi was reviewed. The patients were active metabolically with 87% having a history of multiple calculi. Of the brushite stones 61% appeared hyperdense on x-ray but they had no consistent shape. Of the patients who were metabolically evaluated 82% had treatable abnormalities. Treatment with percutaneous nephrostolithotomy or ureteroscopy and ureteral lithotripsy was 92% successful in rendering the patient stone-free, whereas, extracorporeal shock wave lithotripsy monotherapy resulted in a stonefree rate of only 11%. Brushite stone patients require aggressive treatment, full metabolic evaluation and close clinical followup.",1991,1287
CHAPTER 13 - Lysosomes,P.B. GAHAN,,1968,1288
On the formation of a homogeneous zygotic population in Chlamydomonas reinhardtii,Kwen-Sheng Chiang and Joseph R. Kates and Raymond F. Jones and Noboru Sueoka,"Upon transfer into a liquid medium devoid of a nitrogen source, vegetative cells of Chlamydomonas reinhardtii undergo a gametogenic differenciation process to give rise to sexually active gametes that are capable of mating with opposite mating type gametes. Striking differences have been demonstrated in the mating efficiency (0 to 100%) of gametes induced at different vegetative growth stages in the light-dark synchronized culture. This difference in mating efficiency of gametes is independent of mating-type but directly related to the growth stage of the synchronized vegetative cultures at the time of gametogenic induction. In contrast to the synchronized culture, the mating efficiency of gametes in the continuous light, nonsynchronized vegetative culture is independent of the growth stage at the time of gametogenic induction. The mating behavior of gametes appears to be rather unstable since the mating efficiency of a given clone can be altered considerably during routine maintenance of the culture. Not all the progeny of a particular strain inherit the identical mating efficiency of their parents. The stability of the mating behavior and the transmittance of the mating behavior to the vegetative progeny have been found to vary among different clones. The high frequency at which the mating behavior alters suggests that the mating behavior is a rather complex expression of a number of different factors.",1970,1289
"Geniculosporium serpens gen. et sp.nov., the imperfect state of Hypoxylon serpens",C.G.C. Chesters and G.N. Greenhalgh,"A fairly common mould on decaying branches and trunks of forest trees, formerly incorrectly known as Haplaria grisea Link, is described and Geniculosporium gen. nov. is proposed to accommodate it, with G. serpens sp.nov. as type species. Its relationship with Hypoxylon serpens (Pers. ex Fr.) Kickx. is discussed and it is briefly compared with the imperfect states of other Xylariaceae.",1964,1290
"Emergence and early growth of an epigeal seedling (Daucus carota L.): influence of soil temperature, sowing depth, soil crusting and seed weight",V. Tamet and J. Boiffin and C. Dürr and N. Souty,"This study describes and analyses how seed placement and seed weight influence the response of emergence and early growth of carrot seedlings to changes in seedbed conditions (temperature and surface structure). The first experiment was carried out in a glasshouse with different sowing conditions (1, 3 and 5 cm sowing depths, 2 and 5 mm wet or dry crusts) and two seed weight ranges. The final emergence percentages (40–94%) and times from 50% germination to 50% emergence (55–105°Cd) varied widely. Seed weights had a marked influence only in the most extreme conditions. Seedling growth was then analysed in growth chambers at two temperatures (10 and 20°C), for the same two seed weight ranges and different times of growth in the dark. Hypocotyl elongation rates and growth forces decreased when this time increased. Heavy seeds had longer final hypocotyl lengths and greater growth forces, which explained their better emergence from deep sowing and with surface obstacles. Growth after emergence both in the glasshouse and growth chamber was influenced by seed weight and time from germination to emergence: the seedling weight at emergence depended only on initial seed weight; the seedling relative growth rate was not influenced by the initial seed weight, but decreased with increase in time before emergence. This was due to a decrease in cotyledon photosynthetic efficiency. Poor seedbed structure and seed placement control affect not only emergence but also early growth. These results provide basic information for modelling the emergence and early growth of dicotyledon epigeal seedlings.",1996,1291
The effect of atmospheric screening on the visible border of noctilucent clouds,M.J Taylor and M.A Hapgood and D.A.R Simmons,"The noctiiucent cloud display of 10/11th July 1979 was observed from two sites in Scotland: Clinterty near Aberdeen, using a low light level TV camera, and Milngavie near Glasgow, using a photographic camera. Coincident observations of the display were made from 23.55 UT until 00.50 UT. By projecting the image of the noctilucent cloud structure as seen by one camera into the field of view of the other camera, the height of the clouds was found to be 82 ± 1 km. Using this result, the effect of atmospheric screening on the visible border of the noctilucent clouds was determined. Taking account of the refraction on the solar grazing rays illuminating the clouds at the visible border and the finite angular diameter of the sun's disc, the altitude of the screening layer was determined to be 7 ± 1 km. Thus, on this occasion, the screening effect of the atmosphere was confined to the troposphere and was probably caused by atmospheric haze and/or tropospheric cloud. This result contrasts markedly with the high values of screening height, ~ 30 km, deduced from measurements made earlier this century, also using the visible boundary of the clouds. The analysis presented herein indicates that these large values were probably in error, due to the poor dynamic range of the photographic films employed.",1984,1292
"Ophiobolus graminis Sacc. Var. Avenae var.n., as the cause of take all or whiteheads of oats in Wales",Elizabeth M. Turner,"Summary
Outbreaks of Take All in oats have often been reported in recent years from Wales, and occasionally from Australia, Denmark and Holland, although in most parts of the world it is commonly held that oats resist the disease. Isolates of Ophiobolus from oats grown in Wales were indistinguishable in cultural behaviour from O. graminis, but they were very pathogenic to oats, which were found to be highly resistant to ordinary O. graminis. This histology of the infection of oat plants by the fungus from Wales and by O. graminis was studied in detail. It was found that there were significant differences between the two groups of isolates in the length and septation of the ascospores, the Welsh material giving a length of 101–117µ, the English material 79–86µ The fungus from Welsh oats is therefore regarded as a new variety, Ophiobolus graminis Sacc. var. Avenae E.M. Turner. I have much pleasure in recording my grateful thanks to Mr S.D. Garrett, who suggested this problem and supervised the work, and to Miss E.M. Wakefield for her advice on the systematic status of the isolates of Ophiobolus from oats.",1940,1293
Identifying the attack surface for IoT network,Syed Rizvi and RJ Orr and Austin Cox and Prithvee Ashokkumar and Mohammad R. Rizvi,"For this research, our primary goal is to define an attack surface for networks utilizing the IoT (Internet of Things) devices. The IoT consists of systems of integrated objects, computing devices, digital, or mechanical machines that are given the ability to transmit and receive the data over a network without the need for human interaction. Each of these devices can operate independently within the existing Internet infrastructure. Issues will continue to increase as devices become more prevalent and continuously evolve to counter newer threats and schemes. The attack surface of a network sums up all penetration points, otherwise known as attack vectors. An attacker or an unauthorized user can take advantage of these attack vectors to penetrate and change or extract data from the threat environment. For this research, we define a threat model that allows us to systematically analyze the security solutions to mitigate potential risks from the beginning of the design phase. By designing an IoT architecture and breaking it down into several zones, we focus on each zone to identify any vulnerability or weaknesses within a system that allows unauthorized privileges, as well as any attacks that can target that area. We also investigate the available IoT devices across several domains (e.g., wellness, industrial, home, etc.) to provide a 1:1 and 1:n mapping across devices, vulnerabilities, and potential security threats based on the subjective assessment.",2020,1294
"THE REMOVAL OF A NEEDLE FROM THE HEART WITH ELECTROCARDIOGRAPH RECORDS BEFORE, DURING, AND AFTER OPERATION",Francis A.C. Scrimger,,1933,1295
Identification of a Boiling Channel and Comparisons with a Theoretical Model for Coolant Flow Stability Analysis,T.M. Romberg and N.W. Rees,"Traditional methods used to develop models for system identification and control studies invariably yield models which give little understanding of the physical structure of the process, and are therefore of limited value to the process design engineer. This paper demonstrates how noise analysis methods together with a comprehensive theoretical model of a boiling channel may be used to give a more detailed understanding of the channel dynamics and its coolant flow stability. Results are presented from an experimental and theoretical study carried out on a boiling channel situated in a small scale test rig.",1975,1296
Press review,,,2011,1297
A program for 2D seismic-ray tracing in Benioff zones,F {Alejandro Nava},"A simple, seismic-ray tracing computer program tailored to velocity models representing a Benioff zone is presented. The 2D model represents the overriding crust by flat, horizontal layers, and the subducted crust by flat, parallel, dipping layers. The propagation velocity within each layer may be constant or depend linearly on the distance perpendicular to the top of the layer. A seismic source, generating rays travelling in either P or S mode, may be located anywhere within the model. Whenever a ray intersects an interface, reflection or transmission and, if desired, change in mode of propagation can be specified. Travel time is computed for each ray, and propagation is terminated on encountering the free surface or a model boundary. Model and rays can be plotted. The advantages of this specialized program over well-known more general programs for ray tracing in laterally heterogeneous media are shortness, simplicity, and fast interactive operation.",1986,1298
Treatment of the Uncorrected Clubfoot by Triple Arthrodesis,Alexander Hersh and Louis A. Fuchs,,1973,1299
The origin and structure of fungal disease resistance genes in plants,Tony Pryor,Genetic and mutational analysis indicates that genes specifying resistance against obligate biotrophic fungi have a complex organization. It seems likely that new specificities are generated by rearrangement or recombinational events within the complex locus.,1987,1300
"Characterization of the chloride carrier in the plasmalemma of the alga Valonia utricularis: the inhibition by 4,4′-diisothiocyanatostilbene-2,2′-disulfonic acid",Ingo Spieβ and Jianning Wang and Roland Benz and Ulrich Zimmerman,"The effect of the anion transport inhibitor 4,4′-diisothiocyanatostilbene-2,2′-disulfonic acid (DIDS) on the Cl−-transport system located in the plasmalemma of cells of the giant marine alga Valonia utricularis was studied by using the charge pulse relaxation technique. Analysis of the biphasic relaxation patterns in terms of the kinetic model published previously (Wang, J., Wehner, G., Benz, R. and Zimmermann, U. (1991) Biophys. J. 59, 235–248) demonstrated that extracellular DIDS dramatically reduced the translocation rate, KAS, of the Cl−-carrier complex (maximal inhibition 79%). The translocation rate of the free carrier molecules, KS, as well as the total surface concentration of the carrier, No, were not affected. A Hill-plot of DIDS inhibition on KAS yielded an half-maximal inhibition concentration (IC50) of 3.9 · 10−5 M and a Hill-coefficient of 1.61, suggesting a co-operative binding of the inhibitors to the Cl−-carrier. The maximal inhibition of DIDS was dependent on the extracellular Cl−-concentration. This inhibition was not competitive to chloride, since it increased and did not decrease with increasing chloride concentration. The DIDS effect decreased with increasing pH-value (investigated pH range between 6.5 and 10). Intravacuolar DIDS or SITS (4-acetamido-4′-isothiocyanatostilbene-2,2′-disulfonic acid) had no effect on the biphasic voltage relaxation pattern. These results showed that the binding sites of DIDS must be located on the outer surface of the plasmalemma of V. utricularis and, in turn, supported previous conclusions that the Cl−-carrier (which is assumed to be part of the turgor-pressure-sensing mechanism) is only located in the outer membrane.",1993,1301
Revue de presse,,,2021,1302
"Aggregate investment, the stock market, and the Q model: Robust results for six OECD countries",Gabriel Sensenbrenner,"Modern macroeconomists prefer the Q approach for modelling aggregate investment despite the empirical superiority of Jorgenson's ‘neo classical’ approach. This paper revisits this conflict. Its theoretical and empirical results make the case that, in contrast to the original static Q investment equation, a new dynamic Q equation is as satisfactory as Jorgenson's equation from an empirical standpoint while being more satisfactory from a theoretical standpoint. The paper identifies a theoretical and an empirical line of research in resolving the conflict between theorists and empiricists. On the theory side, the possibility of generating lags in a Q investment equation via the structural Q model is explored: This period's investment cost depends on the investment level achieved last period. The interpretation of this intertemporality combines adjustment costs and gestation lags. The closed-form Q equation is an ARMAX. The explicit generation of lagged investment therefore accounts for the key stylized fact characterizing the investment data process. The empirical line of research turns to data puzzles in the specification of a viable Q equation. It reconciles inter alia the ‘non-Q evidence’ which claims a paramount link between stock prices and investment with the ‘Q non-evidence’ which denies such a link exists. More importantly, the ARMAX Q equations is systematically gauged against the Jorgenson and accelerator specifications across six OECD countries. This comparative analysis established that the superiority of the new Q equations over Jorgenson's is robust.",1991,1303
Neural control,,,1991,1304
Master listing,,,1987,1305
Bibliography Section: Liquid column chromatography,,,1984,1306
Ultrathin gate dielectric films for Si-based microelectronic devices,C. Krug and I.J.R. Baumvol,"Publisher Summary
This chapter discusses the application of ultrathin gate dielectric films for Si-based microelectronic devices. Emphasis is placed on the correlation of dielectric quality, physicochemical issues, and processing parameters. Basic requirements of ultrathin dielectric films to be used in microelectronic devices are given. The chapter discusses the film preparation methods followed by electrical and physicochemical methods of characterization of dielectric films. The significance and effects of the hydrogen presence in gate dielectrics are also presented in the chapter. Further, silicon oxide films thermally grown on single-crystalline silicon in dry oxygen are discussed in the chapter. Understanding and simulating modem-processing routes to the formation of gate dielectric films require an understanding of the atomic transport processes responsible for their growth. Atomic transport is the natural way to approach the growth of ultrathin films and is explored in the chapter.",2002,1307
A Review of Oxidation in Milk and Milk Products as Related to Flavor1,W. Carson Brown and L.M. Thurston,"Summary
Of all the factors concerned in bringing about oxidative changes in dairy products metallic contamination, particularly by copper, is at present the most important. The control of copper contamination in processing becomes increasingly more important as the sanitary quality of the products is improved. However, not all milk must have copper contamination to develop an oxidized flavor. Fortunately, the percentage of milk which develops oxidized flavor spontaneously is relatively small and when this milk is mixed with normal milk it remains normal in flavor. For this reason it is not of as great commercial importance as is milk susceptible to metal-induced oxidized flavor. Oxidized flavor in milk has been shown to be associated with milk of low bacterial count. The growth of bacteria in the milk, either by using up the oxygen or by reduction of the potential, render milk non-susceptible even in the presence of copper. Oxidized flavor in milk was originally believed to be the result of oxidation of the fat catalyzed by copper and brought about through the action of an enzyme. The recent trend in literature points to the phospholipid fraction as the source of the flavor and recent work on cooked flavor questions seriously the action of an enzyme in bringing about the defect. Grass feeding has been shown to reduce the susceptibility of milk to oxidized flavor even though the milk fat is made more susceptible to ordinary chemical oxidation. This is explained on the basis of reducing substances in the feed or in the milk, or in both. Various investigations have shown that ascorbic acid and carotene in the feed tend to reduce the susceptibility of the milk. Also a number of antioxidants have been demonstrated to have protective qualities. The mechanism whereby an oxidized flavor develops has not been completely demonstrated nor has the mechanism whereby the various factors exert their effects. Tallowy flavor in butter appears to be the result of metal-induced oxidation of the fat, the point of attack probably being the double bonds in the oleic and linoleic acid radicles. Light and oxygen have been demonstrated to be important factors favoring oxidative changes. Oxidation will take place in the absence of light but the rate of reaction is slow. The source of oxygen may be either the free or the combined form. Both strong acid and alkaline reactions have been shown to favor oxidative changes. Over-neutralization is known to favor oxidative changes while butter made from high acid cream in the presence of copper contamination and salt is prone to become fishy upon storage. Both salt and moisture appear to play a role in the development of oxidative changes but they are of minor importance when compared to metallic contamination. Temperature is important only as a regulator of the rate of oxidative change. As the temperature increases the rate of oxidative change increases, all other factors being constant. Low temperature storage favors a slow rate of oxidative change. Development of tallowy or oxidized flavor in ice cream is undoubtedly an oxidative change. In this product, however, we have the possibility that the oxidative changes may affect either the phospholipid fraction or the butter-fat or both. The present work points toward fat as the substance oxidized. If the present trend of research on oxidized flavor of milk continues, a reex-amination of tallowy flavor in ice cream would seem desirable. The elimination of copper contamination is one of the major problems confronting the dairy industry today as the flavor problems resulting from chemical reactions have a copper history in the vast majority of cases. If these problems are to be eliminated by removal of the cause, it will be necessary for copper to be eliminated from all surfaces with which milk comes in contact because of the extremely small amount of copper required to develop the flavor in many cases. The authors wish to express their appreciation to Dr. R. B. Dustman, Department of Agricultural Chemistry, West Virginia Agricultural Experiment Station, for advice and criticism in the preparation of this paper.",1940,1308
Liquid column chromatography,,,1987,1309
Complications associated with Meckel's diverticulum,Grosvenor T. Root and Charles P. Baker,,1967,1310
Bibliography,,,1987,1311
Anatomy and embryology of congenital intrinsic obstruction of the duodenum,Edward A. Boyden and John G. Cope and Alexander H. Bill,,1967,1312
Review of gas-liquid chromatography,C.J. Hardy and F.H. Pollard,,1959,1313
Author index to volumes 400–420,,,1997,1314
Résumés des communications,,,2010,1315
Abstracts for Supplement,,,2010,1316
Communications orales du dimanche 16 octobre,,,2011,1317
Cell differentiation,,,1989,1318
"Calculated wavelengths, oscillator strengths, and energy levels for n = 2−2 and 2–3 transitions in F-like ions Mg IV to Ni XX and for 3−3 and other transitions in Mg IV, Al V, and Si VI",B.C. Fawcett,"Weighted oscillator strengths, energy levels, and wavelengths are calculated for the 2s22p5-2s2p6, 2s22p5-222p43s, and 2s22p5-2s22p43d transition arrays for F-like ions in the isoelectronic sequence from Mg IV to Ni XX and in addition for n = 3−3 and other transitions in Mg IV, Al V, and Si VI. The calculation involves the computation of ab initio values of Slater radial energy integrals using a Hartree-Fock-Relativistic computer program package, which includes configuration-interaction and applies the Blume-Watson method for spin-orbit integrals. Some of the parameters are subsequently optimized on the basis of empirical data. Adopted values are tabulated along with atomic energy level compositions.",1984,1319
Posters,,,2011,1320
Electrical control of facial pain,C.Hunter Shelden and Robert H. Pudenz and James Doyle,,1967,1321
Current management of civilian thoracic trauma,Reinold J. Jones and Paul C. Samson and David J. Dugan,"A review of thoracic trauma as treated at Highland General Hospital is presented. Five hundred sixty-two cases with twenty-five deaths were analyzed. The survey encompasses an eight year span and changes in technics over this period are stressed. All phases of thoracic trauma, including blunt trauma, pneumothorax and hemopneumothorax, flail chest and cardiac and great vessel injury, esophageal injury, and diaphragmatic injury, are discussed as to emergency and definitive care. Morbidity and mortality are presented with discussion relevant to decreasing both, in the emergency care of thoracic injury.",1967,1322
Oceanographic Literature Review,,,1990,1323
Cell multiplication,,,1990,1324
Transmesenteric plication for small intestinal obstruction,A.Thomas Ferguson and Vernoy A. Reihmer and Max R. Gaspar,,1967,1325
Quartäre Klimazyklen Im Westlichen Mediterrangebiet Und Ihre Auswirkungen Auf Die Reliefund Bodenentwicklung: vorwiegend nach Untersuchungen an Kliffprofilen auf den Balearen und an der marokkanischen Atlantikküste,von H. Rohdenburg and U. Sabelberg,"Summary
According to the findings of several studies the older concept of paleoclimates, which postulates the regular alternation of humid “Pluvials” and arid “Interpluvials” in the subtropics of the northern hemisphere, and which are supposed to correspond to Glacials and Interglacials in Central Europe, can no longer be supported.1.Palynological studies performed in Spain, Italy, and Greece unanimously prove the existence of types of vegetation with sparce trees or even free of tree growth, whereas from the older point of view Glacials were marked by dense forests (increase of rainfall is taken for granted).2.From loess covers that are to be found in several parts of the Mediterranean (corresponding to the above mentioned facts) Glacial aridity can be concluded, as confirmed by the loess snail fauna.3.Lime — crusts, which are the predominant evidence for very arid conditions, are not surface formations, but CCa-horizons of soils. Their formation is based on conditions of lime removal from close-to-surface soils horizons; this presupposes a humid season, while on the other hand the considerable lime precipitation in the subsoil presupposes an arid season as well.4.Paleosols and debris covers on slopes, being analogously considered indications for increased rainfall (“Pluvial” climate), are not the product of one type of climate only. As they occur in multiple alternations they cannot be simultaneous formations, but were formed at various times under different ecological conditions. Substantial seepage and relative stagnation of erosion are requisites for the formation of soils, whereas displacement of debris is conditional on the predominance of surface runoff. The most important indications — soils (having partly lime-crust CCa-horizons) an slope debris covers — do not allow any conclusions as to the absolute amount of rainfall and its variations, but to different ecological types of effects of rainfall, which are due to a different ratio of see page to surface runoff because of variable rainfall distribution. The abandonment of the misleading terms “Pluvial” — “Interpluvial” is suggested in favour of terms not based on hypothesis, but which describe directly observable basic geoecological conditions: periods of morphodynamic activity (with predominance of slope erosion and without formation of soils) are put in contrast to the periods of morphodynamic stability (with formation of soils and a lack of or only insignificant displacement of suspended material on slopes). Transitional states are characterized by the term “partial activity”. During several excursions accumulation sequences interrupted by as few erosion phases as possible have been searched for, in order to obtain a new paleoclimatic concept. Out of these, one large scale profile (Ses Penyes Rotjes/Mallorca), which includes early and middle pleistocene series composed of alternating soils and sediments (> 30 fossil soils), as well as several low cliff profiles on the Balearic Islands and at the Moroccan Atlantic Coast with late pleistocene soil and sediment series, have been subjected to a detailed analysis up to now. We think it worthwhile to repeat the following partia results obtained: As in Central Europe, an alternation of soil formation periods (periods of morphodynamic stability) and of periods characterized by an intense morphodynamic activity on slopes, valley floors, and alluvial fans (periods of morphodynamic activity) can be observed. As in Central Europe the paleosols do not occur in isolation but as soil complexes in profiles of a more complex nature. They correspond to the interglacial and early glacial periods according to a correlation (partially even in alternating layers) with marine sediments of the interglacial high sea levels. In contrast to what is known about Central European soil complexes, the individual soils of the Mediterranean usually cannot be distinguished from each other in terms of soil typology. As the basal soil of a complex is often marked by karst formations of the lime-crust CCa-horizon, indicating a more substantial humidity and/or longer duration of formation processes, so it can be considered the correlate formation of the Central European Interglacial (or parts of it). In accordance with KUKLA's findings obtained at loess profiles in the ČSSR, the soil complexes can be subdivided into several subcycles on the basis of different types of sediments and variable intensities of individual soil formation. Depending on varying site conditions the pleniglacial sections tend to consist mainly of fine to coarse clastic alluvial sediments or dune series resulting from periods of regression and/or alternating layers of these two formations. Unlike early and interglacial series, soil sediments, even autochthonous soils diminish in the pleniglacial series. Though soil formations are not completely lacking, they are, however, of weak intensity. Soils both well- and badly-developed, in all profile sections, may have CCa-horizons, either of the cementation or the lamellar lime-crust type. This means that during all soil formation periods — during both the warm and cold periods of the quarternary Glacial-Interglacial-cycles — mediterraneanlike climate, marked by intense soil exsiccation in summer time, was prevalent. The latest sediment sequence is covered by several soils probably of holocene or late glacial origin. In the Mediterranean the profiles which are composed of alternating soils and sediments exhibit a sequence as rich as, and strikingly similar to some of the most richly differentiated Central European profiles. According to KUKLA they can be subdivided into Lower and Upper Series. The Lower Series are mainly characterized by periods of morphodynamic stability, interrupted by comparatively short periods of morphodynamic activity during which, moreover, only the state of partial activity (moderately reduced vegetal cover) was reached. The Upper Series consist of longer periods of activity during which the state of total activity (considerably reduced vegetal cover), besides states of partial activity, gained more importance. Intercalated periods of morphodynamic stability were of short duration only. The Holocene is a stratigraphic period that, under natural conditions, was predominantly marked by morphodynamic stability and intense formation of soils since rainfall distribution — though being more accentuated than in Central Europe — favoured types of vegetation rich in ligneous plants which gave sufficient soil protection. In deforested areas intact soil covers can be found only in areas with low slope gradients; in areas with steep slope gradients only in some large forested zones (e. g. cork oak forests in Catalonia). In vast areas the solum of the holocene soil is largely or fully eroded so that the solid rock or the Cca-horizon either constitute the surface or are close to it. Potent cutting or complete removal of a bulk of the holocene soil cover is to a great extent due to direct and indirect destruction of the vegetal cover by man (e.g. by grazing). This erosion caused by man — a quasi-natural erosion — is by no means as intense as erosion of the stronger pleistocene phases of activity. Partial activity is predominant, i.e. slow slope erosion due to unconcentrated surface runoff not proceeding on the piedmont plain but being changed into accumulation. Concentration of surface runoff accompanied by a revival of dry valleys or even new formation of runoff-concentration-forms — i.e. slope gullying — is frequent on very argillaceous rocks. Even then accumulation or gullying are predominant in the piedmont area. Examples of recent areal erosion in the piedmont plain and at the same time expansion of the erosional piedmont plain (= pediment), i.e. existence of recent pedimentation, are rare, and always confined to rocks highly susceptible to erosion. This is borne out by the fact that newly formed pediments have only an extremely small extent. It was observed at several locations that pedimentation was more important in subrecent periods than now. This is in conformity with the fact that post-pleistocene slope gullying forms can be observed comparatively often which, due to their slope debris and vegetal cover, prove to be inactive. Occasional observations show that to some (or even to a considerable) extent cutting or removal of the post-pleistocene soil cover is due to this (or these) subrecent period(s) of more substantial erosion caused by climatic conditions only, or by a super-imposition of climatic and anthropogenic influences. This is to say that the holocene period of predominant morphodynamic stability may be assumed — mainly in the arid areas — to have been interrupted by short phases of morphodynamic activity of climatic origin, as was proved in West Africa and North America. From a comparison of results obtained in the Central Sahara desert with those obtained in the Mediterranean area, numerous parallels are followed and also a marked difference: the morphodynamics of humid areas are characterized by alternating states of stability and activity (often states of partial activity only); whereas in deserts the state of stability is largely substituted by the state of partial activity. Well developed soils are not completely lacking, but here they are confined to the less frequent periods of highest soil-climatic humidity. Though direct comparison between the climatic sequence, as ascertained in the Mediterranean and the hitherto existing best profiles of the Sahara (Saoura Valley), is not yet practicable, it must be assumed that in the Sahara geomorphodynamically relevant climatic variations were not less frequent than in the more humid subtropics and Central Europe. The question as to whether climatic variations in the Sahara (and West Africa) ran parallel with or in opposition to those of the Mediterranean and temperate zones cannot be clarified for lack of a stratigraphy in more explicit detail. Several findings favour the hypothesis of parallelism whereas the hitherto existing “pendulation theories” are not appropriate since they are partly based on the highly ambiguous Pluvial concept.
Zusammenfassung
Aufgrund verschiedener Befunde kann die ältere paläoklimatologische Vorstellung eines regelhaften Wechsels von feuchten “Pluvialen” und trockenen “Interpluvialen”, die in den Subtropen der Nordhemisphäre mit den Glazialen und Interglazialen Mitteleuropas korrespondieren sollten, nicht mehr befriedigen:1.Pollenanalytische Untersuchungen aus Spanien, Italien und Griechenland ergaben über-einstimmend waldarme oder sogar waldfreie Vegetationstypen, während die ältere Auffassung mit geschlossener Bewaldung während der Kaltzeiten rechnete (aufgrund vorausgesetzter Niederschlagserhöhung).2.Damit korrespondiert das Auftreten von Lößdecken in verschiedenen Teilgebieten des Mediterrangebietes, die auf kaltzeitliche Aridität schließen lassen, was durch die Lößschneckenfauna bestätigt wird.3.Die überwiegend als Indikatoren für sehr aride Verhältnisse angesehenen Kalkkrusten sind keine Oberflächenbildungen, sondern CCa-Horizonte von Böden. Ihre Bildung ist an Bedingungen mit Kalkabfuhr aus einem oberflächennahen Solum gebunden und setzt demzufolge jahreszeitliche Humidität voraus; die starke Kalkausfällung im Unterboden erfordert andererseits auch eine trockene Jahreszeit.4.Die in gleicher Weise als Indikatoren für höhere Niederschläge (“Pluvial”-Klima) herangezogenen Paläo-Böden und Hang schuttdecken sind mcht Produkte eines Klimatyps. Ihr in vielfachem Wechsel alternierendes Auftreten zeigt an, daß sie nicht gleichzeitige Bildungen sind, sondern zu verschiedenen Zeiten mit unterschiedlichen ökologischen Bedingungen entstanden. Und zwar setzt Bodenbildung hohe Versickerung und relative Abtragungsruhe voraus, Schuttverlagerung am Hang erfordert dagegen Bedingungen mit Dominanz des Oberflächenabflusses. Gerade die wichtigsten Indikatoren — Böden (z. T. mit Kalkkrusten-CCa-Horizonten) und Hangschuttdecken — erlauben also primär keine Schlußfolgerungen auf die absolute Niederschlagsmenge und ihre Veränderung, sondern auf landschaftsökologisch unterschiedliche Wirkungstypen des Niederschlages infolge eines unterschiedlichen Verhältnisses von Versickerung zu Oberflächenabfluß aufgrund unterschiedlicher Niederschlagsverteilung. Es wird vorgeschlagen, das irreführende Begriffspaar “Pluvial” — “Interpluvial” ganz aufzugeben und nur Begriffe zu benutzen, die nicht auf Hypothesen beruhen, sondern die direkt beobachtbare landschaftsökologische Grundzustände beschreiben: morphodynamische Aktivitätszeiten (mit Dominanz der Hangabtragung und ohne Bodenbildung) werden den morphodynamischen Stabilitätszeiten (mit Bodenbildung und fehlender oder sehr geringer Schwebstoffverlagerung an Hängen) gegenübergestellt. Über-gangszustände werden durch den Begriff “Teilaktivität” gekennzeichnet. Zur Gewinnung einer neuen paläoklimatischen Konzeption wurde auf mehreren Reisen nach Aufschlüssen in möglichst wenig durch Abtragungsphasen unterbrochenen Akkumulationsfolgen gesucht. Davon wurden bis jetzt ein Großprofil (Ses Penyes Rotjes/Mallorca) mit alt- und mittelpleistozänen Boden-Sediment-Serien (> 30 fossile Böden) sowie mehrere niedrigere Kliffprofile auf den Balearen und an der marokkanischen Atlantikküste mit jungpleistozänen Boden-Sediment-Serien einer detaillierten Analyse unterzogen. Folgende Ergebnisse scheinen uns wert, hervorgehoben zu werden: Wie in Mitteleuropa ist auch in den Tieflagen des Mediterrangebietes ein Alternieren von Zeiten mit Bodenbildung (Stabilitätszeiten) und von Zeiten mit starker Morphodynamik an Hängen, in den Talböden und auf Schwemmfächern (Aktivitätszeiten) festzustellen. Wie in Mitteleuropa treten die Paläoböden in den reicher gegliederten Profilen nicht isoliert, sondern in Bodenkomplexen auf, die nach Korrelation (z. T. sogar in Wechsellagerung) mit marinen Sedimenten der warmzeitlichen Meeresspiegelhochstände in die Interglazial — und Frühglazialabschnitte der quartären Kaltzeiten zu stellen sind. Im Gegensatz zu den Befunden an mitteleuropäischen Bodenkomplexen sind die Einzelböden im Mediterrangebiet in der Regel bodentypologisch nicht voneinander unterscheidbar; lediglich der basale Boden eines Komplexes ist oft durch eine (synoder parapedogenetische) Verkarstung des Kalkkrusten-CCa-Horizontes ausgezeichnet, was auf größere Humidität oder größere Bildungsdauer (bzw. beides) hinweist, so daß in ihm die korrelate Bildung der mitteleuropäischen Interglaziale (oder Teilen davon) zu sehen ist. Die Bodenkomplexe können — in Entsprechung zu den von KUKLA in der ČSSR festgestellten Befunden an Lößprofilen — aufgrund unterschiedlicher Sedimenttypen und Bodenbildungsintensitäten der Einzelböden in mehrere Subzyklen eingeteilt werden. Die hochkaltzeitlichen Abschnitte bestehen — je nach Standortsverhältnissen — überwiegend aus fein — bis grobklastischen Schwemmsedimenten oder regressionszeitlichen Dünenserien bzw. einer Wechsellagerung beider Bildungen. In den hochkaltzeitlichen Serien treten im Gegensatz zu den interglazial-frühglazialen Serien neben Bodensedimenten auch autochthone Böden stark zurück. Bodenbildungen fehlen allerdings nicht vollständig, sind aber von schwacher Intensität. Sehr schwache Böden (in allen Profilabschnitten) wie auch sehr stark ausgeprägte Böden können sämtlich CCa-Horizonte in Zementations- wie auch in lamellärer Kalkkrusten-Ausbildung aufweisen. Das bedeutet, daß in allen Bodenbildungszeiten, also sowohl im warmen als auch im kalten Abschnitt der quartären Warmzeit-Kaltzeit-Zyklen, mediterraner Klimacharakter mit starker sommerlicher Bodenaustrocknung herrschte. Die jüngste Sedimentfolge wird von mehreren Böden abgeschlossen, die in das Holozän und vermutlich auch in das Spätglazial zu stellen sind. Die Boden-Sediment-Profile in der mediterranen Stufe besitzen eine ebenso reiche und auffällig gleichartige Gliederung wie die am reichsten gegliederten Profile Mitteleuropas. Mit KUKLA kann man sie in Untere und Obere Serien einteilen. Die Unteren Serien sind vor allem durch Stabilitätszeiten gekennzeichnet, die nur durch relativ kurze Aktivitätsphasen getrennt waren, in denen zudem überwiegend nur der Zustand der Teilaktivität (mäßige Vegetationsauflichtung) erreicht wurde. Die Oberen Serien bestehen aus längeren Aktivitätsabschnitten, in denen neben Phasen mit Teilaktivität auch dem Zustand der Vollaktivität (beträchtliche Vegetationsauflichtung) größere Bedeutung zukam; eingeschaltete Stabilitätsphasen waren hier nur von kurzer Dauer. Das Holozän ist ein stratigrafischer Abschnitt, der unter natürlichen Bedingungen überwiegend durch morphodynamische Stabilität mit intensiver Bodenbildung gekennzeichnet war, da die Niederschlagsverteilung — obwohl akzentuierter als in Mitteleuropa — bis in die trockensten Gebiete hinein an Holzpflanzen reiche Vegetations-typen mit ausreichendem Bodenschutz begünstigte. Geschlossene Bodendecken findet man im Offenland fast nur bei geringen Hangneigungen, im Steilrelief nur in einigen größeren Waldgebieten (Beispiel: Korkeichenwälder in Katalonien). In sehr weiten Bereichen ist das Solum des holozänen Bodens weitgehend oder vollständig abgetragen, so daß das Gestein oder der CCa-Horizont weitflächig in Oberflächennähe anstehen bzw. direkt die Oberfläche bilden. Für die starke Kappung bzw. vollständige Ausräumung des Großteils der holozänen Bodendecke ist sicher die direkt und indirekt (z. B. durch Beweidung) anthropogen bedingte Vegetationszerstörung in großem Maße verantwortlich zu machen. Diese anthropogen ausgelöste, also quasinatürliche Abtragung ist weit weniger intensiv als die Abtragung in den stärkeren pleistozänen Aktivitätszeiten. Es überwiegt Teilaktivität, d. h. langsame Hangabtragung durch unkonzentrierten Oberflächenabfluß, die sich nicht auf den Fußflächen fortsetzt, sondern dort in Akkumulation übergeht. Abflußkonzentration mit Wiederaufleben präexistenter Hohlformen bzw. sogar Neubildung von Abflußkonzentrationsformen — also Hangzerschneidung — ist nur in tonreichen Gesteinen häufiger. Aber auch dann überwiegen im Fußflächenbereich Akkumulation oder aber Zerschneidung. Fälle mit rezenter Fußflächenabtragung und somit auch Ausweitung des Areals der Abtragungsfußflächen, also Vorkommen von rezenter Pedimentation, sind sehr selten und stets auf Gesteine mit hoher Abtragungsgunst beschränkt. In die gleiche Richtung weist, daß neugebildete Pedimente im Vergleich zu pleistozänen Vorzeitformen nur eine extrem kleine Ausdehnung haben. An mehreren Lokalitäten wurde beobachtet, daß in subrezenter Zeit Pedimentation größere Bedeutung hatte als heute. Dem entspricht, daß relativ häufig postpleistozäne Hangzerschneidungsformen festgestellt werden können, die sich anhand ihrer Bedeckung mit Hangschutt und Vegetation heute als inaktiv erweisen. Einzelbeobachtungen weisen darauf hin, daß dieser bzw. diesen subrezenten Phase(n) mit stärkerer Abtragung, die möglicherweise rein klimatisch oder durch Überlagerung klimatischer und anthropogener Einflüsse bedingt sind, z.T. sogar erheblicher Anteil an der Kappung bzw. Abräumung der postpleistozänen Bodendecke zukommt. Man muß also, und zwar besonders in den Trockengebieten, damit rechnen, daß die holozäne Periode überwiegender Stabilität — wie in Westafrika und Nordamerika nachgewiesen — durch klimatisch ausgelöste kurze Aktivitätsphasen unterbrochen gewesen sein könnte. Ein Vergleich der im Mediterrangebiet gewonnenen Ergebnisse mit den Wüstengebieten der zentralen Sahara ergibt zahlreiche Parallelen, allerdings auch einen deutlichen Unterschied: In den humiden Gebieten ist die Morphodynamik durch ein Alternieren von Stabilitäts- und Aktivitätszuständen (häufig nur Teilaktivitätszuständen) gekennzeichnet. In den Wüsten wird demgegenüber der Stabilitätszustand weitgehend durch den Teilaktivitätszustand ersetzt. Kräftige Böden fehlen nicht vollständig, sind hier aber auf die weniger häufigen Zeiten mit größter bodenklimatischer Humidität beschränkt. Obwohl ein direkter Vergleich der im Mediterrangebiet festgestellten Klimafolge mit den bisher vollständigsten Profilen der Sahara (Saoura-Tal) noch nicht möglich ist, muß als sicher gelten, daß geomorphodynamisch relevante Klimaschwankungen in der Sahara nicht seltener waren als in den humideren Subtropen und in Mitteleuropa. Die Frage, ob die Klimaschwankungen in der Sahara (und Westafrika) mit denen des Mediterrangebietes und der temperierten Zone parallel oder entgegengesetzt verliefen, kann mangels einer Feinstratigrafie nicht entschieden werden. Mehrere Befunde sprechen für die Parallelitätshypothese, wohingegen die bisherigen “Pendulationstheorien” nicht zutreffen, zumal da sie z. T. auf dem mehrdeutigen Pluvialbegriff aufbauen.
Resume
Sur la base de différentes découvertes, l'ancienne conception des paléoclimats ne donne plus satisfaction. Elle supposait une alternance de périodes pluviales humides et d'interpluviales sèches qui devaient coïncidé dans la zone subtropicale de l'hémisphère du nord avec les périodes glaciaires et interglaciaires de l'Europe centrale.1.Des analyses des pollens, en Espagne, en Italie et en Grèce, ont donnés des résultats concordants: des types de végétation qui sont caractérisés par pauvrement boisés ou même pas boisés, alors que l'ancienne théorie présupposait pendant les périodes froides la présence d'une couverture forestière ininterrompue, du fait de l'augmentation supposée des précipitations.2.A ces différents états de végétation correspond l'apparition de couvertures de loess dans différentes parties de la région méditerranéenne, qui permettent de conclure à l'existence d'une aridité de climal froid, ce qui est confirmé par la faune de mollusques du loess.3.Les croûtes calcaires, considérées la plus souvent comme les indices d'une extrème aridité, ne sont pas des formations de surface, mais les horizons-CCa des sols. D'un part, leur élaboration est liée à la décalcification des couches proches de la surface, ce que suppose une saison humide, tandis que d'autre part, la forte précipitation du calcaire dans le sous-sol demande du même une saison sèche.4.Les paléosols et les couvertures de débris de pente, qui étaient également considérés comme les indices pour des précipitations augmentées (climat “pluvial”), ne sont pas les produits d'un type de climat seulement. Leur apparition alternant de changements multiples démontre, qu'il ne s'agit pas de formations simultanées, mais qu'ils se sont évolués à des époques et conditions écologiques différentes. Certes, la pédogenèse présuppose une haute infiltration des eaux et une disparition relative de l'érosion, mais un déplacement de débris de pente demande des conditions avec dominance du ruissellement. Juste les indices les plus importants — l'existence des sols (partiellement des horizons-CCa de croûte calcaire) et des couvertures de débris de pente — ne permettent aucune conclusion sur la quantité absolue des précipitations et sa variation, mais on est capable de différencier entre les differents types des effets sur l'écologie du paysage causés par des précipitations différentes du fait d'un rapport variable entre l'infiltration et le ruissellement à cause des variations dans la répartition des pluies. Nous proposons d'abandonner complètement les dénominations “Pluvial” — “Interpluvial”, qui peuvent induire en erreur, et de n'utiliser que des termes qui, au lieu de reposer sur des hypothèses, sont dérivés de l'observation des états principaux de l'écologie du paysage: Aux périodes d'activité morphodynamique, où le transport des matériaux sur les pentes domine, sans qu'il y ait pédogenèse, on opposera des périodes de stabilité morphodynamique où la pédogenèse est très importante et où la répartition du matériel en suspension sur les pentes est presque inexistante. Les phases de transition seront nommées périodes d'activité morphodynamique partielle. Avant de tenter l'élaboration d'une nouvelle thèse concernant les paléoclimats, nous avons cherché, aux cours de nombreux voyages, des profils où les séries sédimentaires soient le moins possible interrompues par des phases d'érosion. Jusqu'ici nous avons pu en soumettre à une analyse détaillée: le haut profil Ses Penyes Rotjes (situé à Majorque), qui est riche de séries avec des sols et des sédiments alternants du haut at du moyen Pléistocène (plus que 30 sols fossiles). Les autres profils, d'une hauteur plus réduite, sont répartis dans les falaises côtières des Baléares et de la façade atlantique du Maroc. Ils sonst composés de séries de sols et de sédiments en alternance du Pléistocène récent. Les résultats suivants nous semblent dignes d'être soulignés: Comme en Europe centrale, on peut constater, dans les régions méditerranéennes de basse altitude, une alternance des périodes avec pédogenèse (périodes de stabilité morphodynamique) et des périodes de forte activité morphodynamique sur les pentes, dans les fonds de vallées et sur les cônes de déjections (périodes d'activité morphodynamique). Comme en Europe centrale également, les paléosols, dans les profils les plus richement diversifiés, n'apparaissent pas isolés mais en complexes de sols, qui l'on peut attribuer, selon la corrélation (partiellement même déposé alternativement) avec des sédiments marins de l'époque chaude des transgressions marines, aux périodes interglaciaires et aux entrées des périodes glaciaires des époques glaciaires du Quaternaire. Contrairement aux découvertes faites dans les complexes de sols de l'Europe centrale, les sols un à un de la région méditerranéenne ne se différencient pas en générale entre eux quant au type du sol. Seulement, le sol de base d'un complexe est souvent caractérisé par une évolution karstique (syn- ou para-pédogénétique) de la croûte calcaire, de l'horizon'CCa, ce qui indique un degré d'humidité plus grand, une formation plus longue ou une combinaison de ces deux facteurs, de sorte qu'on peut le regarder comme une formation parallèle à l'époque interglaciaire L'Holocène est une phase stratigraphique, qui est essentiellement marquée, dans des conditions naturelles, par sa stabilité morphodynamique avec pédogenèse intensive, car la répartition des pluies, bien que plus accentuées qu'en Europe centrale, favorisait jusque dans les régions les plus sèches des formations végétales plus riches en arbres, qui protègeaient suffisamment les sols. Actuellement, dans les régions peu boisées on ne trouve une couverture pédologique étendue que sur les pentes les moins accentuées ou seulement dans les quelques forêts les plus importantes, si le relief est extrèmement accidenté (comme p. ex. dans les bois de chênes lièges en Catalogne). Le solum du sol holocène a été presque complètement érodé sur de grandes surfaces, ainsi que la roche-mère ou l'horizon-CCa se trouvent à peu de profondeur ou ils-mêmes présentent la surface. La destruction directe ou indirecte (par la mise en pâturage) de la végétation par l'homme est à rendre responsable pour la plus grande part de ces arasements et même pour le déblaiement total d'une grande partie de la couverture pédologique holocène. Cette érosion, causée par l'homme, c'est-à-dire “quasinaturelle”, est bien moins forte que celle des périodes de plus forte acitivité morphodynamique du Pléistocène. L'activité partielle domine, c'est-à-dire lente érosion de pente à cause d'un ruissellement diffus et elle ne se prolonge pas sur les glacis, mais cède la place à l'accumulation. On ne trouve plus souvent de concentration du ruissellement, qui réactive les vallons en berceau et les vallées sèches ou crée même des formes nouvelles causées par le ruissellement concentré, c'est-à-dire la dissection des pentes, que dans les roches riches d'argile. Mais même dans ce cas on a sur les glacis d'accumulation ou aussi de dissection. Des cas avec érosion récente sur des glacis et ainsi aussi élargissement des glacis d'érosion, par conséquence l'existence de formation récente des pédiments, sont très rares et toujours limités aux roches bien favorables à l'érosion. Dans le même ordre d'idée, les pédiments nouveaux n'ont qu'une extension extrèmement réduite par rapport aux formes fossiles du Pléistocène. On a observé en de nombreux endroits que la formation des pédiments eut à l'époque sousrécente une plus grande importance que maintenant. On peut par suite découvrir assez souvent des formes de dissection de pente postérieures au Pléistocène et recouvertes de débris et de végétation, ce qui prouve qu'elles sont inactives. Quelques observations sporadiques indiquent de plus que cette ou ces phases d'érosion sous-récentes plus fortes qu'elles soient probablement uniquement causées par le climat ou par une combinaison des influences climatiques et humaines, peuvent avoir une grande part de responsabilité pour avoir coupé ou même enlever la couverture pédologique postérieure au Pléistocène. Il faut par conséquent, surtout dans les régions sèches, tenir compte du fait que la prépondérance de la stabilité morphodynamique dans l'Holocène — comme on a pu le prouver en Amérique et Afrique occidentale — peut avoir été interrompue par de courtes phases d'activité causée par un changement de climat. On peut relever de nombreuses coincidences en comparant les résultats obtenus dans la région méditerranéenne avec ceux du Sahara central, avec toutefois une différence évidente: Dans les régions humides, la morphodynamique est caractérisée par une alternance des phases de stabilité et d'activité (souvent seulement des phases d'activité partielle). Par contre, l'état de stabilité morphodynamique est notablement remplacé dans les déserts par l'état d'activité morphodynamique partielle. Les sols bien développés ne manquent pas totalement, mais leur formation se restreigne ici sur les époques moins nombreuses où règne la plus grande humidité pédoclimatique. Bien qu'une comparaison directe entre la séquence climatique de la région méditerranéenne avec les profils jusqu'à maintenant les plus complète du Sahara (vallée de Saoura) ne soit pas encore possible, on doit être certain, que les variations climatiques qui ont influencées la morphodynamique ne furent pas plus rares dans le Sahara que dans les subtropicales plus humides ou qu'en Europe centrale. La question de savoir si les variations climatiques au Sahara (et en Afrique occidentale) se sont produites simultanément ou en opposition avec celles de la zone tempérée ne peut être résolue, faute d'une stratigraphie détaillée. Nombre de découvertes confirment l'hypothèse du parallélisme, alors qu'elles ne concordent pas avec les “théorieś de la pendulation”, qui d'ailleurs sont basées sur le terme ambigu de “Pluvial”.
Resumen
Debido a diferentes resultados, ya no puede satisfacer la antigua concepción paleoclimatológica en cuanto a un cambio regular entre húmedo “pluvial” y seco “interpluvial”, que deberían corresponder en el subtrópico del hemisferio norte a glacial e interglacial en Centroeuropa:1.Investigaciones analíticas de polen en España, Italia y Grecia han dado análogamente tipos de vegetación pobres en bosques ó hasta sin bosques, mientras que la idea antigua presuponía bosques con vegetación cerrada durante los períodos glaciares (debido a un supuesto aumento de precipitaciones).2.Con esto corresponden las apariciones de capas de loess en diferentes partes de la región mediterránea, que dejan deducir un clima glacial árido, lo que se comprueba con la fauna de caracoles encontrada en el loess.3.Las costras calcáreas, tomadas predominantemente como indicadores para condiciones muy áridas no son una formación superficial, sino horizontes CCa de suelos. Su formación está ligada a condiciones con una escurrida de caliza de un solum cercano a la superficie y supone con esto una humedad estacional, el intenso precipitado de cal en el subsuelo requiere por otra parte una estación anual seca.4.Los paleo-sueolos y capas de escombros de falda tomadas igualmente como indicadores para precipitaciones mayores, no son producto de u n tipo de clima. Su aparición alternada de cambio múltiple demuestra, que no son de una formación a un mismo tiempo, sino que se formaron en distintos períodos de diferentes condiciones ecológicas. La formación de suelos presupone un alto rezumen y una relativa calma en cuanto a erosión, mientras que una dislocación de escombros de falda presupone condiciones con dominio de desagüe superficial. O sea que justamente los indicadores más importantes — suelos (en parte con horizontes CCa en forma de costra calcárea) y capas de escombros de falda — no permiten una deducción en cuanto a la cantidad absoluta de precipitaciones y su transformación, sino que tipos de los efectos diferentes de la precipitación en la ecología de paisaje, debido a una relación diferenciada de rezumen a desagüe superficial, por un reparto diferente de las precipitaciones. Se propone no usar los términos “pluvial” e “interpluvial”, que pueden llevar a errores, sino que usar sólo términos no basados en hipótesis, y que describan estados fundamentales ecológicos directamente observables: tiempos de actividad morfodinámica (con dominancia de erosión de falda y sin formación de suelos) se ponen en contraposición a tiempos de estabilidad morfodinámica (con formación de suelos y sin ó con muy poca dislocación de materia en suspensión en faldas). Estados de transición se caracterizan con el término “actividad parcial”. Para obtener un nuevo concepto paleoclimático se han buscado en varios viajes afloramientos posiblemente en series de acumulación que no se hayan interrumpido por fases de erosión. De estos se han sometido a un análisis detallado hasta ahora un perfil grande (Ses Penyes Rotjes/Mallorca) con series de suelos y sedimentos del Pleistoceno inferior y medio (más de 30 suelos fósiles), como varios perfiles de acantilados más bajos en las islas Baleares y en la costa atlántica de Marruecos con series de suelos y sedimentos del Pleistoceno superior. Habría que hacer resaltar los siguientes resultados: Se han comprobado en las partes bajas de la región mediterránea, como en Centroeuropa, un alternar de períodos con formación de suelos (períodes de estabilidad) y períodos con fuerte morfodinámica en faldas, vaguadas y sobre abanicos de deyección (periodos de actividad). Como en Centroeuropa no aparecen aislados los paleosuelos en los perfiles más divididos sino en complejos, que según su correlación (en parte hasta en estratos alternantes) con sedimentos marinos de las transgresiones interglaciares del mar, hay que atribuirlos a períodos interglaciares y periodos prematuros de los glaciales del Cuaternario. En contraposición a los resultados en los complejos de suelos centroeuropeos, no se pueden en regla general diferenciar tipológicamente los suelos singulares en la región mediterránea; solamente el suelo basal de un complejo está a menudo senalado por una formación de cárisco (syn- ó parapedogenética) de los horizontes CCa (costras calizas), lo que indica una mayor humedad, ó mayor tiempo de formación (ó ambas cosas), asi que se vea en él una formación correlativa del interglacial centroeuropeo (ó partes de él). Los complejos de suelos pueden ser divididos — conforme a los resultados obtenidos en perfiles de loess por KUKLA en la ČSSR — en varios subciclos según diferentes tipos de sedimentos e intensidad de formación de los diferentes suelos. Las secciones superiores de los glaciales están formadas — según las circunstancias locales — especialmente de aluviones de clasticidad fina hasta gruessa ó de series de dunas de períodos regresionarios ó un alternar de ambas formaciones. En las series de los glaciales superiores disminuyen ostensiblemente, en contraposición a las series interglaciares hasta glaciares prematuras, fuera de sedimentos de suelos, también suelos autóctonos. La formación de suelos no falta absolutamente, pero es de una intensidad débil. Suelos muy débiles (en todas las secciones de perfiles), como también suelos muy pronunciados, pueden mostrar todos los horizontes CCa en forma de cementación, como también en formación de costras calcáreas laminadas. Esto quiere decir que en todos los períodos de formación de suelos, ya sea en periodos cálidos como frios de los ciclos interglaciales — glaciales del Cuaternario, reinaba un clima de caraćter mediterráneo con fuerte disecación de suelo en verano. La serie de sedimentos más reciente concluye con varios suelos, que pertenecen al holoceno y probablemente también al glacial tardío (Würm superior). Los perfiles de suelos y sedimentos alternados en la zona mediterránea poseen una estructura, asimismo rica sorprendentemente semjante a los perfiles más ricos en Centroeuropa. Según KUKLA se pueden dividir en series “inferiores” y “superiores”. Las series inferiores están caracterizadas ante todo por períodos de estabilidad, que estaban separadas sólo por fases de actividad relativamente cortas, en las cuales además predominaba — en su mayoría — sólo el estado de actividad parcial (vegetación moderadamente aclarada). Las series superiores consisten en períodos de actividad más largos, en los cuales, se ha dado mayor importancia al estado de actividad plena (destrucción fuerte de la vegetación) paralelamente a les fases de actividad parcial. Fases intercaladas de estabilidad eran aqui de muy corta duración. El holoceno es un período estratigráfico, que estaba caracterizado — bajo condiciones normales — por predominante estabilidad morfodinámica con intensiva formación de suelos, ya que la repartición de precipitación — aúnque más acentuada que en Centroeuropa — favorecía hasta en las regiones más áridas a tipos de vegetación rica en plantas ligneas con suficiente protección de suelos. Capas cerradas de suelos se encuentran en el campo abierto casi solamente en regiones con poca inclinación; en relieves empinados sólo en algunas extensiones mayores de bosques (ejemplo: bosques de encinas de corcho en Cataluña). En áreas muy grandes el solum del suelo holoceno está bastante ó totalmente erodado, así que las rocas ó el horizonte CCa están en grandes extensiones cerca de la superficie ó forman directamente la superficie. Para la casi total ó total desaparición de gran parte de las capas de suelos holocenos, hay que hacer altamente responsable la directa ó indirecta destrucción antropógena de la vegetación (p. ej. por enpastamiento). Esta erosión causada por la actividad antropógena, sea “casi natural”, es bastante menos intensiva que la erosión en los períodos pleistocenos de activided más fuerte. Hay preponderantemente actividad parcial, esto quiere decir lenta erosión de falda por desagüe superficial no concentrado, que no sigue al pie de la falda, sino que pasa aqui a una acumulación. Concentraciones de desagüe con revivificación de formas cavernosas preexistentes ó hasta formación nueva de formas de desagüe — o sea cortes de falda — es sólo más frequente en rocas arcillosas. Pero también en este caso predominan acumulación ó erosión linear en las areas al pie de faldas. Casos de erosión areal reciente en extensiones al pie de faldas, y con esto también extensión del areal de pedimentos de erosión o sea presencia de pedimentación reciente, son bién raros y limitados siempre a rocas con alto propicio de erosión. Igualmente signicativo es, que pedimentos recién formados tienen, en comparación a formas anteriores del pleistoceno, sólo una extensión sumamente pequeña. En varias localidades se ha observado, que la pedimentación ha tenido mayor importancia en períodos subrecientes que hoy. A esto corresponde que se pueden comprobar frequentemente formas de cortes de falda postpleistocenas, que se muestran inactivas hoy debido al cubrimiento con escombros de falda y de vegetación. Observaciones esporádicas demuestran, que esta ó estas fases subrecientes con erosión más intensa causadas probablemente por influencias climáticas y antropógenas superpuestas aún pueden tener una grande responsabilidad en la erosión ó desaparición de las capas de suelo postpleistocenas. O sea que hay que contar, que, especialmente en regiones áridas, el período holoceno de predominante estabilidad (como se ha comprobado en Africa occidental y Norteamérica) podría haber estado interrumpido por cortas fases de actividad, causadas climáticamente. Una comparación entre los resultados de la región mediterránea y las regiones desérticas de la Sahara central, da numerosas paralelas, pero también una marcante diferencia: en las regiones húmedas está marcada la morfodinámica por un alternar de estados de estabilidad y actividad (frecuentemente sólo estados de actividad parcial), mientras que en los desiertos el estado de estabilidad es sustituido en gran parte por el de actividad parcial. Suelos substanciosos no faltan absolutamente, pero están limitados a los períodos no muy frecuentes de mayor humedad pedoclimática. Aúnque todavía no sea posible una comparación directa entre la secuencia climática comprobada en la región mediterránea con los perfiles hasta ahora más completos de la Sahara (valle de Saoura), tiene que ser seguro, que fluctuaciones climáticas geomorfodinámicamente relevantes en la Sahara, no eran menos frecuentes que en el subtrópico más húmedo y que en Centroeuropa. La pregunta, si es que las fluctuaciones climáticas en la Sahara (y en Africa occidental) transcurrían con las de la región mediterránea y de la zona temperada en forma paralela ó contraria, no se puede determinar debido a la falta de una estratigrafía minuciosa. Varios ejemplos se pronuncian en favor de la hipótesis de paralelidad, mientras que no son justas las “teorias de pendulación”, sobre todo por que están basadas en parte sobre el concepto pluvial ambiguo.",1973,1326
Computed oscillator strengths and Landég values of Ce II,B.C. Fawcett,"Weighted oscillator strengths are tabulated for 6185 spectral lines in Ce II, some of which are found in the solar spectrum. They belong to transitions between the 4f5d2, 4f5d6s, 4f6s2, 4f26p, and 4f3 odd configurations and the 4f26s, 4f25d, 4f5d6p, 5d3, 4f6s6p, and 5d26s even configurations. The values were calculated by a method in which Slater parameters were first computed ab initio and then adjusted by means of a least-squares optimization routine so as to minimize discrepancies between measured and computed levels. Configuration interaction was included between all levels of the same parity. Landég values were also computed and compared with available measured data. Factors affecting the accurary of the data are discussed.",1990,1327
Nuclear data sheets for A = 56,R.L. Auble,"Nuclear structure data available through December 1976 are compiled, and adopted level properties are given. The bulk of the data is presented pictorially for easy comparison. Experimental details, references, and additional comments, where required, are given in the text. All drawings, tables, and comments are reproduced from the computerized Evaluated Nuclear Structure Data File (ENSDF). Any additions or corrections desired by the users should be addressed to the compilers for maintenance and updating of the computer file.",1977,1328
Author Index,,,2000,1329
"Reaction list for charged-particle-induced nuclear reactions: Part I: Z=1 to Z=98 (H to Cf), July 1970–June 1971 Part II: Coulomb Excitation, 1956–June 1971",F.K. McGowan and W.T. Milner,"PART I of this reaction list for charged-particle-induced nuclear reactions has been prepared from the journal literature for the period from July 1970 through June 1971. Each published experimental paper is listed under the target nucleus in the nuclear reaction with a brief statement of the type of data in the paper. The nuclear reaction is denoted by A(a,b)B, where Ma≥ (one-nucleon mass). There is no restriction on energy. Nuclear reactions involving mesons in the outgoing channel are not included. Beginning with this supplement, theoretical papers which treat directly with the analysis of nuclear reaction data and results are included in the reaction list. In PART II is presented a reaction list for the Coulomb excitation reaction which has been prepared from the journal literature for the period from 1956 through June 1971.",1971,1330
Injury to the sphincter of oddi in the course of gastric and duodenal surgery,Thomas T. White and Eric R. Sanderson and Alan Morgan,"The cause and management of injuries to the ampulla of Vater during operations on the stomach and duodenum are described. Nearly all of them occur during gastric resection for duodenal ulcer. In cases in which the injury was accidental, eight patients of a total of sixty-one died. Deliberate separation and reimplantation resulted in no deaths in ten patients. A method of duodenal advancement and anastomosis to the area of the ampulla together with drainage by transduodenal catheters of 2 to 4 F polyvinyl tubing in each duct is the most acceptable operative repair in these cases. Vagotomy with pyloroplasty is a preferred method of avoiding this injury.",1967,1331
The organic compounds of cobalt(III),D. Dodd and M.D. Johnson,,1973,1332
Range and stopping-power tables for 2.5–500 MeV/nucleon heavy ions in solids,F. Hubert and R. Bimbot and H. Gauvin,Stopping powers and ranges are tabulated for all ions of atomic number 2 ≤ Z ≤ 103 in the energy region 2.5 ≤ EA ≤ 500 MeV/u for 36 solid materials. The calculations use stopping powers for α particles and a new parameterization for the heavy-ion effective charge which is deduced from a set of about 600 experimental stopping-power values covering an energy range from 3 to 90 MeV/u for 15 incident heavy ions and 18 solid stopping materials.,1990,1333
General surgical complications associated with renal allotransplantation using related donors,Roy Cohn and Samuel Kountz and Robert Swenson and John Palmer,,1967,1334
Leptonic decays of hadrons,L.-M. Chounet and J.-M. Gaillard and M.K. Gaillard,"The theory of semi-leptonic weak interactions is reviewed and confronted with present experimental data. The theoretical points emphasized are the basic tenets of the Cabibbo theory, the algebra of currents, and the relevance of strong interaction symmetries to the weak semi-leptonic amplitudes. The experimental data is discussed in detail, and future lines of investigation, necessary for testing detailed theoretical predictions and for resolving outstanding questions, are indicated.",1972,1335
Hemodynamic and metabolic changes associated with bacterial peritonitis,Leonard Rosoff and Max Weil and Edward C. Bradley and Clarence J. Berne,"Hemodynamic and metabolic studies of ten patients critically ill with diffuse bacterial peritonitis have been presented. The cardiac output and velocity of blood flow were normal. Total peripheral arterial resistance was markedly reduced in patients in whom systolic blood pressure was reduced to 75 mm. Hg or less despite normal cardiac output. Blood lactate levels were greatly elevated in these patients, indicative of a critical reduction in blood flow necessary to sustain normal metabolism of vital tissues. Our data indicate that an abnormal variation in distribution of blood flow is an outstanding circulatory alteration in patients with diffuse bacterial peritonitis. This hemodynamic effect may be explained by arteriovenous shunting in the systemic vascular bed. The site of such shunting is as yet unidentified, but presumably occurs in the area of peritoneal inflammation. Decreases in alveolar oxygen exchange also indicate the coexistence of arteriovenous shunting in the pulmonary circulation. Ventilatory failure and profound alterations in pulmonary function appear to be significant factors in the lethality of diffuse bacterial peritonitis.",1967,1336
Recommandations en Onco-Urologie 2010 : Cancer de la prostate,L. Salomon and D. Azria and C. Bastide and P. Beuzeboc and L. Cormier and F. Cornud and D. Eiss and P. Eschwège and N. Gaschignard and C. Hennequin and V. Molinié and P. {Mongiat Artus} and J.-L. Moreau and Michel Péneau and M. Peyromaure and V. Ravery and X. Rebillard and P. Richaud and P. Rischmann and F. Rozet and F. Staerman and A. Villers and M. Soulié,,2010,1337
Special Interest Report Abstracts,,,2011,1338
Autoimmunity,,,1991,1339
Omphalocele and related defects,Allen H. Johnson,"Eighteen patients with severe congenital ventral abdominal wall defects were encountered in surgical practices in San Jose, California, from January 1, 1956, to December 31, 1965. Ten patients had defects which were not covered with a sac or membrane and appeared to have had a ruptured omphalocele existing many months before delivery. Distinctions are made between this group and the patients who have been categorized as having gastroschisis. Of the eight patients in whom a sac covered the congenital defect, there were six survivors. Of ten patients with no sac over the omphalocele defect, there were only two survivors. General considerations and details of surgical management are presented.",1967,1340
Chapter 1 - Intercalation compounds in layered Host lattices: Supramolecular Chemistry in nanodimensions,Anton Lerf,,2000,1341
"Computed oscillator strengths and energy levels for Fe III, Fe IV, Fe V, and Fe VI with calculated wavelengths and wavelengths derived from established data",B.C. Fawcett,"Calculated weighted oscillator strengths are tabulated for spectral lines of Fe III, Fe IV, Fe V, and Fe VI. The lines belong to transition arrays 3d6-3d54p and 3d54s-3d54p in Fe III, 3d5-3d44p and 3d44s-3d44p in Fe IV, 3d4-3d34p and 3d34s-3d34p in Fe V, and 3d3-3d24p and 3d24s-3d24p in Fe VI. For the calculations, Slater parameters are optimized on the basis of minimizing the discrepancies between observed and computed wavelengths. Configuration interaction was included among the 3dn, 3dn−14s, 3dn−24s2, 3dn−14d, and 3dn−15s even configurations and among the 3dn−14p, 3dn−24s4p, and 3dn−15p odd configurations, with 3p53dn+1 added for Fe VI. Calculated wavelengths are compared with observational data, and the compositions of energy levels are listed. This completes a series of similar computations for these complex configurations covering Fe I to Fe VI.",1989,1342
Surface spacings from the secondary electron yield,Robert L. Park,"Small inflections in plots of the total yield of secondary electrons versus primary electron energy were reported more than fifty years ago and attributed variously to thresholds for the excitation of core states, and to diffraction of the incident electrons. Although the existence of these inflections was discounted by later researchers, recent studies of the derivative of the yield leave no doubt as to their reality or their origin. The spectrum of core excitation thresholds provides information on the composition of the surface region. Near-threshold structure is related to the electronic state of the atoms. Extended fine structure above the threshold can be inverted to obtain interatomic spacings even from relatively disordered surfaces. The principal limitation in these measurements results from diffraction of the incident electron beam, which produces variations in electron reflectivity. The extreme simplicity of the equipment required makes this technique available to essentially every surface science laboratory.",1980,1343
Chapter 4 Producing oil wells — 2,,"Publisher Summary
Production by bottom-hole pumps is a mechanical technique. The fluid entering the well from the formation is lifted to the surface by a pump installed below the producing fluid level. The prime mover of the pump is installed either on the surface, or in the well; in the latter case, it is integral with the pump. The bottom-hole pump unit comprises all the mechanisms and equipment serving the purposes of production. The chapter discusses the sub-divisions of the bottom-hole pumps. The sucker-rod pump is a plunger pump performing a reciprocating motion. Its prime mover is installed on the surface. ways. If a crank and a flywheel are used, the installation is called a crank-type or walking-beam-type sucker-rod pump. long-stroke hydraulic pumps, a hydraulic means of transformation is adopted; the installation is called a hydraulic sucker-rod pump. If the transformation is by wireline and pulley, the installation is called a derrick-type sucker-rod pump. In rodless bottom-hole pump installations, the bottom-hole pump may be of plunger or centrifugal or some other type. Hydraulic pumps are driven by a hydraulic engine integral with them, driven in its turn by a power fluid to which pressure is imparted by a prime mover situated on the surface. This type is called a hydraulic (rodless) bottom-hole pump. Centrifugal pumps integral with an electric motor, and lowered to the well bottom, are called submersible pumps. Further rodless bottom-hole pumps include electric membmrbe pumps and sonic pumps.",1975,1344
Surgical treatment of the subclavian steal syndrome,R.Hewlett Lee and John H. Kieraldo and Robert W. Jamplis,Cerebrovascular insufficiency produced as a result of reversal of the flow in the vertebral artery secondary to occlusion in the proximal subclavian or innominate arteries is now termed the subclavian steal syndrome. Symptoms are primarily those of basilar artery insufficiency but forebrain ischemic symptoms also occur. The diagnosis is suspected when a difference in the blood pressure and pulse between the right and left arm is detected and it is confirmed by contrast arteriography demonstrating retrograde flow in the vertebral artery. Treatment is surgical relief of the arterial obstruction or stenosis in those patients with radiographically demonstrated subclavian steal syndrome. Two patients treated surgically are presented. One patient had occlusion of the right subclavian artery treated by thromboendarterectomy and patch graft through a transverse neck incision with resection of the clavicle. The other was treated in a similar fashion through a left posterolateral thoracotomy incision. Surgical therapy is effective.,1967,1345
Chapter 7 - The Carotenoid Group,J.B. Davis,"Publisher Summary
This chapter discusses the basic constitution, classification, and nomenclature; occurrence, function, and isolation; structure and synthesis; and properties of carotenoids. A carotenoid is still easily recognized by its methyl(or modified methyl)-substituted polyene chain. The C50 and C45 carotenoids, which barely fall within the classical definition of carotenoids, are considered as straightforward di-and monoalkylated derivatives of the more usual C40 skeleton. There are several methods of converting a carotenoid with all its double bonds trans into an equilibrium mixture of cis/trans isomers. The most commonly used method involves briefly exposing a solution of the compound containing a trace of iodine to light. Many of the mono- and bi-cyclic carotenoids contain one or more chiral centers. These are usually at C(3), C(6), C(5), or associated with the allene group. In all 3-substituted carotenoids, the configuration at the 3-position is R (the -OH substituent is of the “β” type using steroid nomenclature), and in all carotenoids containing α-type end-group that at the 6-position is also R (the-H is “α”).",1975,1346
Résumé des communications,,,2008,1347
Reticuloendothelial function in patients with cancer: Initial observations,Arthur J. Donovan,"Reticuloendothelial function has been studied in patients with cancer by determination of the rate of disappearance from the plasma of microaggregates of albumin. Comparison of the rate of disappearance in twenty-two patients with cancer as compared with fourteen patients without known cancer suggests that reticuloendothelial function was impaired in certain patients with cancer. Alterations in reticuloendothelial function were observed in nine patients undergoing palliative therapy, that is, administration of adrenocortical steroids, adrenalectomy, oophorectomy, administration of 5-fluorouracil, and radiation therapy. In these nine patients a favorable therapeutic response occurred in association with an increase in reticuloendothelial function, and an unfavorable response with decrease in reticuloendothelial function. These initial studies suggest that in man, important relationships exist between the reticuloendothelial system and the course of cancer.",1967,1348
A comprehensive bibliography of separations of organic substances by counter-current distribution,C.G. Casinovi,,1963,1349
Index of biochemical reviews 1985,,,1986,1350
Gastric secretory tests: Pro and con,Edward Passaro and H.Earl Gordon,,1967,1351
Significance of changes in the pulmonary lymph flow in acute and chronic experimental pulmonary edema,Sanford E. Leeds and Herman N. Uhley and John J. Sampson and Meyer Friedman,Several experimental technics for the production of acute and chronic pulmonary edema and the collection of pulmonary lymph are described. The response of the pulmonary lymph system in acute and chronic failure was studied. The studies suggest that the pulmonary lymphatics are capable of expanding and may play an important role in the removal of fluid from the lung.,1967,1352
The interaction of atoms with polarised light,F.K. Lamb and D. {Ter Haar},"We present a discussion of the interaction of atoms with a partially polarised radiation field. We take to some extent into account the differences in the populations of atomic states, quantum interference effects which occur when atomic states are overlapping, and collisional relaxation. The theory is presented in such a way that it is particularly suitable for an application to solar lines.",1971,1353
"The Diptera as a Model System in Cell and Molecular Biology11This paper is dedicated to the memory of Dr. Martin Hagopian, our colleague and friend, who originally recognized the value of a comprehensive review on dipteran systems to cell and molecular biologists.",Elena C. Zegarelli-Schmidt and Reba Goodman,"Publisher Summary
The cells of Diptera have been used as a model system in a wide variety of experiments designed to probe the organization of the eukaryotic genome. The cells of these organisms lend themselves readily to both cytological and biochemical investigations using specific techniques, such as light and electron microscopic autoradiography, immunofluorescence, salt or sucrose density gradient and gel electrophoretic analyses, hybridization in situ, and molecular cloning-recombinant DNA studies. This chapter reviews the research carried out in past years using dipteran model systems, specifically three families—the Drosophilidae, the Chironomidae, and the Sciaridae. Six separate areas of research are considered. They are chromatin structure, middle repeated DNA (MR DNA), highly repeated DNA (HR DNA), satellites, and heterochromatin, puffs, heat shock, and mechanisms of gene control: the nonhistone proteins.",1981,1354
[4] Genetic and microbiological research techniques for Neurospora crassa,Rowland H. Davis and Frederick J. {de Serres},"Publisher Summary
This chapter describes the microbiological and genetic handling of Neurospora crassa. It also reviews the biochemical methods of Neurospora. The simpler techniques followed by more complex modifications applicable to careful and sustained research are described. Neurospora crassa is a eukaryotic organism, a member of the fungal class Ascomycetes. As an ascomycete, it is related to yeasts, and as a fungus, it is more distantly related to mushrooms. The primary value of Neurospora in research is that the fungus is eukaryotic, can be handled as easily as bacteria, and thus provides a valuable basis of comparison between prokaryotes and eukaryotes in molecular biology. Several methods for the measurement of growth in Neurospora include measurement of the rate of mycelial elongation in race tubes, measurement of the amount of growth in stationary or shaken liquid culture after selected time intervals, and measurement of the doubling time in logarithmically growing culture. To study the physiological interaction of homologous genes in Neurospora crassa, different haploid nuclei must be associated in the same cell, where they function in a common cytoplasm.",1970,1355
Master index to volumes 601–610,,,2005,1356
"Chapter 9 Bioaffinity Chromatography in the Isolation, Determination or Removal of Biologically Active Substances",,"Publisher Summary
This chapter discusses the use of bioaffinity chromatography (BAC) in the isolation, determination, or removal of biologically active substances. The preparation of specific sorbents utilizing the exceptional properties of biologically active substances to form specific and reversible complexes has enormously facilitated the isolation of a number of antibodies, antigens and haptens, cells and cell organelles, cofactors and vitamins, and glycoproteins and saccharides. The different conditions applied during BAC depend on the nature of the substances to be isolated. In high-performance liquid bioaffinity chromatography (HPLBAC), the biospecificity of BAC is combined with a high-performance (pressure) technology based on the rigid particles of a uniform small size (high-performance liquid chromatography, HPLC). The separation times in HPLBAC are short (minutes) compared to hours for traditional, soft-gel BAC. Moreover, HPLBAC users have at their disposal a wide selection of HPLC equipment, including high-speed pumps, sophisticated injection units, detectors of various kinds, auto-sampling devices, and data-handling capabilities. This enables the users to fine-tune the separation process conveniently and promote higher productivity.",1993,1357
RIPL – Reference Input Parameter Library for Calculation of Nuclear Reactions and Nuclear Data Evaluations,R. Capote and M. Herman and P. Obložinský and P.G. Young and S. Goriely and T. Belgya and A.V. Ignatyuk and A.J. Koning and S. Hilaire and V.A. Plujko and M. Avrigeanu and O. Bersillon and M.B. Chadwick and T. Fukahori and Zhigang Ge and Yinlu Han and S. Kailas and J. Kopecky and V.M. Maslov and G. Reffo and M. Sin and E.Sh. Soukhovitskii and P. Talou,"We describe the physics and data included in the Reference Input Parameter Library, which is devoted to input parameters needed in calculations of nuclear reactions and nuclear data evaluations. Advanced modelling codes require substantial numerical input, therefore the International Atomic Energy Agency (IAEA) has worked extensively since 1993 on a library of validated nuclear-model input parameters, referred to as the Reference Input Parameter Library (RIPL). A final RIPL coordinated research project (RIPL-3) was brought to a successful conclusion in December 2008, after 15 years of challenging work carried out through three consecutive IAEA projects. The RIPL-3 library was released in January 2009, and is available on the Web through http://www-nds.iaea.org/RIPL-3/. This work and the resulting database are extremely important to theoreticians involved in the development and use of nuclear reaction modelling (ALICE, EMPIRE, GNASH, UNF, TALYS) both for theoretical research and nuclear data evaluations. The numerical data and computer codes included in RIPL-3 are arranged in seven segments: MASSES contains ground-state properties of nuclei for about 9000 nuclei, including three theoretical predictions of masses and the evaluated experimental masses of Audi et al. (2003). DISCRETE LEVELS contains 117 datasets (one for each element) with all known level schemes, electromagnetic and γ-ray decay probabilities available from ENSDF in October 2007. NEUTRON RESONANCES contains average resonance parameters prepared on the basis of the evaluations performed by Ignatyuk and Mughabghab. OPTICAL MODEL contains 495 sets of phenomenological optical model parameters defined in a wide energy range. When there are insufficient experimental data, the evaluator has to resort to either global parameterizations or microscopic approaches. Radial density distributions to be used as input for microscopic calculations are stored in the MASSES segment. LEVEL DENSITIES contains phenomenological parameterizations based on the modified Fermi gas and superfluid models and microscopic calculations which are based on a realistic microscopic single-particle level scheme. Partial level densities formulae are also recommended. All tabulated total level densities are consistent with both the recommended average neutron resonance parameters and discrete levels. GAMMA contains parameters that quantify giant resonances, experimental gamma-ray strength functions and methods for calculating gamma emission in statistical model codes. The experimental GDR parameters are represented by Lorentzian fits to the photo-absorption cross sections for 102 nuclides ranging from 51V to 239Pu. FISSION includes global prescriptions for fission barriers and nuclear level densities at fission saddle points based on microscopic HFB calculations constrained by experimental fission cross sections.",2009,1358
97–98 Author index,,,1998,1359
Reaction list for charged-particle-induced nuclear reactions,,,1972,1360
CHAPTER 9 - Sulfohydrolases,K.S. Dodgson and F.A. Rose,,1975,1361
"Crop Residues and Management Practices: Effects on Soil Quality, Soil Nitrogen Dynamics, Crop Yield, and Nitrogen Recovery",K. Kumar and K.M. Goh,"This review reveals that crop residues of common cultivated crops are an important resource not only as a source of significant quantities of nutrients for crop production but also affecting soil physical, chemical, and biological functions and properties and water and soil quality. When crop residues are returned to the soils, their decomposition can have both positive and negative effects on crop production and the environment. Our aim as agricultural scientists is to increase the positive effects. This can only be achieved with the better understanding of residue, soil, and management factors and their interactions, which affect the decomposition and nutrient release processes. Data on nitrogen benefits and nitrogen recoveries from residues show that a considerable potential exists from residues, especially leguminous residues, not only in meeting the N demands of the succeeding crops, but also in increasing the long-term fertility of the soils. In addition, crop residues and their proper management affects the soil quality either directly or indirectly. Intensive cropping systems are very diverse and complex, so no one residue management system is superior under all situations. Ideally, crop residue management practices should be selected to enhance crop yields with a minimum adverse effect on the environment. It is suggested that in each cropping system, the constraints to production and sustainability should be identified and conceptualized to guide toward the best option. Multidisciplinary and integrated efforts by soil scientists, agronomists, ecologists, environmentalists, and economists are needed to design a system approach for the best choice of crop residue management system to enhance both agricultural productivity and sustainability.",1999,1362
"Atropine, norepinephrine, and isoproterenol and the cardiac response to experimental lactic acidosis",Louis L. Smith and Martin Silberschmid and David B. Hinshaw,"Acute addition lactic acidosis causes marked cardiac slowing, decreased cardiac output, and a progressive rise in the central venous pressure in the dog, suggesting increased vagal activity as well as decreased myocardial function. Bilateral cervical vagotomy and atropine in large dosages were employed to evaluate the role of vagal innervation in producing bradycardia and rhythm changes during acute lactic acidosis. These experimental procedures increased cardiac rate but did not improve cardiac output after the induction of lactic acidosis. Isoproterenol was compared with norepinephrine as a therapeutic agent to improve myocardial function during acidosis. Norepinephrine administration increased arterial pressure by 16 per cent, but did not improve cardiac output or cardiac rate when infused during acidosis. By contrast, isoproterenol caused a 10 per cent fall in arterial pressure, a 93.5 per cent increase in the cardiac output, and a 32 per cent increase in cardiac rate. These experimental findings suggest that increased vagal activity is a cause for the bradycardia and rhythm changes observed during acute lactic acidosis. Isoproterenol was an effective therapeutic agent to improve cardiac output, decrease peripheral resistance, and increase cardiac rate during severe acidosis. The application of these findings to the management of low perfusion states has been discussed.",1967,1363
Technical result of carotid endarterectomy: Arteriographic assessment,F.William Blaisdell and Robert Lim and Albert D. Hall,"To evaluate the technical results of operative treatment of stenosis of the internal carotid artery, arteriographic assessment was used in one hundred consecutive procedures. Operative arteriograms were taken routinely at the completion of the endarterectomy and again two to eight weeks later. Late follow-up assessment was obtained by repeating the arteriograms whenever symptoms recurred or when the five-year follow-up period was reached. One fourth of the hundred arteriograms taken at the completion of operation revealed an unsuspected defect in the repair. In all but one of these patients immediate revision was carried out. The end point of the operation was a widely patent vessel as demonstrated by angiography. Follow-up arteriography at the time of discharge from the hospital revealed the only technical failure in the hundred operations, which was in the one patient in whom revision was not performed. Late follow-up examination revealed continued patency at the five year period in all but one instance, an asymptomatic occlusion which was found in a patient who had died of myocardial infarction. Since 25 per cent of the patients had unsuspected intraluminal defects or thrombosis, it is obvious that routine operative arteriograms will increase the technical success of carotid endarterectomy. When the operative arteriogram demonstrates a good result, long-term patency of the artery after endarterectomy is assured.",1967,1364
Abstracts of the 12th Annual Scientific Meeting of the Society of Cardiovascular Computed Tomography,,,2017,1365
The Pacific Coast: Its population and its medical manpower,Leon Goldman,,1967,1366
Wounds of the great vessels of the thorax: Diagnosis and surgical approach in twenty-four cases,Lyman A. Brewer and Richard Carter,,1967,1367
Complications of indwelling venous catheters: With particular reference to catheter embolus,Richard B. Doering and Edward A. Stemmer and John E. Connolly,The complications with the use of polyethylene catheters are discussed and the world literature is reviewed. The twenty-six previously reported cases of polyethylene catheter embolus are analyzed. Ten cases of embolus from our teaching hospitals are reviewed and an additional thirteen cases from the Los Angeles area are added. The mortality from catheter embolus is high unless vigorous surgical measures are instituted for removal. The factors leading to catheter breakage and the measures to be taken for prevention are discussed. Treatment for catheter embolus is outlined.,1967,1368
31 LINEAR ACETOGENINS,T.K. DEVON and A.I. SCOTT,,1975,1369
CHAPTER 6 - THE BIOGENESIS OF CARBOHYDRATES,PETER BERNFELD,,1963,1370
Reaction list for charged-particle-induced nuclear reactions: Part B: Z=28 to Z=99 (Ni to Es),F.K. McGowan and W.T. Milner and H.J. Kim and Wanda Hyatt,"This reaction list for charged-particle-induced nuclear reactions has been prepared from the journal literature for the period from 1948 through April 1969. Each published experimental paper is listed under the target nucleus in the nuclear reaction with a brief statement of the type of data in the paper. The nuclear reaction is denoted by A(a,b)B, where Ma≥ (one-nucleon mass). There is no restriction on energy. Nuclear reactions involving mesons in the outgoing channel are not included.",1969,1371
Effect of portacaval shunts on lymph flow in the thoracic duct: Experiments with normal dogs and dogs with cirrhosis and ascites,Marshall J. Orloff and Bernard Goodhead and Colin W.O. Windsor and Michael E. Musicant and David L. Annetts,"It has been shown that hepatic venous outflow obstruction is the primary hemodynamic lesion in cirrhosis. Furthermore, considerable evidence indicates that both the increased production of lymph and the formation of ascites in experimental and human cirrhosis are caused by the elevation of intrahepatic pressure which results from the outflow obstruction. Accordingly, the capacity of end to side and side to side portacaval shunts to reduce intrahepatic pressure was evaluated by measuring the effects of these procedures on pressure and lymph flow in the thoracic duct of fifty dogs. Portacaval shunts of both types were made in normal animals and in dogs with congestive cirrhosis, portal hypertension, and massive ascites produced by ligation of the hepatic veins. Normal dogs and dogs with cirrhosis but without shunts served as controls. Hepatic vein ligation resulted in massive ascites, which averaged 4.0 L., portal hypertension which averaged 208 mm. of saline solution, a mean thirteenfold increase in thoracic duct lymph flow, and a mean elevation of thoracic duct pressure which was almost three times the normal level. The end to side portacaval shunt reduced the high rate of lymph production and the elevated pressure significantly, but a fivefold increase in lymph flow and an approximately twofold elevation of thoracic duct pressure persisted. The side to side portacaval shunt lowered thoracic duct pressure to normal in every dog, and either abolished the excessive lymph formation or markedly reduced the rate of lymph production. The results of this study indicate that the side to side portacaval shunt is more effective than the end to side anastomosis in overcoming intrahepatic hypertension and decompressing the obstructed hepatic vascular bed. These findings represent an important consideration in the selection of surgical therapy for intractable cirrhotic ascites, and may have some bearing on the effects of portacaval shunts on nutrition of the hepatic parenchyma.",1967,1372
Surgical removal of cancer of the thyroid gland,Horace J. McCorkle,"The careful selection of patients with nodular goiter for probable malignancy increases the likelihood of finding thyroid cancer at operation. All other patients with thyroid nodules must be carefully followed up and evaluated frequently to determine the possible need for operation. The thyroid lobe (or isthmus) containing a suspected lesion should be removed for microscopic pathologic examination, which is the only certain way to diagnose cancer of the thyroid. In most operable cases of cancer of the thyroid gland, thyroidectomy is completed because cancer cells are frequently found in the contralateral lobes; however, unilateral lobectomy is sufficient treatment for localized malignant adenoma. Appropriate neck dissections are indicated if cervical nodes are hard and enlarged or otherwise strongly suspected of containing metastases. In selected cases in which thyroid cancer is known or believed to have extended substernally beyond the possibility of removal by cervical incision, mediastinal dissection may be performed.",1967,1373
Review of particle properties: Particle data group,C. Bricman and C. Dionisi and R.J. Hemingway and M. Mazzucato and L. Montanet and N. Barash-Schmidt and R.C. Crawford and M. Roos and A. Barbaro-Galtieri and C.P. Horne and R.L. Kelly and M.J. Losty and A. Rittenberg and T.G. Trippe and G.P. Yost and B. Armstrong,,1978,1374
"Abstracts of the XXV International Complement Workshop, 14–18 September 2014, Rio de Janeiro, Brazil",,,2014,1375
Analytical biotechnology,,,1992,1376
Limnology of Saratoga Lake,Donald B. Aulenbach and Nicholas L. Clesceri and James J. Ferris,,1980,1377
Author index,,,1993,1378
Energy levels of light nuclei A = 13–15,F. Ajzenberg-Selove,"Compilation of Energy Levels for A = 13, 14 and 15 Nuclei, With Emphasis on Material Leading to information about the Structure of the A = 13–15 Systems.",1970,1379
"Reaction list for charged-particle-induced nuclear reactions Z = 1 to Z = 98 (H to Cf), July 1972–June 1973",F.K. McGowan and W.T. Milner,"This Reaction List for charged-particle-induced nuclear reactions has been prepared from the journal literature for the period July 1972 through June 1973. Each published experimental paper is listed under the target nucleus in the nuclear reaction with a brief statement of the type of data in the paper. The nuclear reaction is denoted by A(a,b)B, where Ma≥ (one nucleon mass). There is no restriction on energy. Nuclear reactions involving mesons in the outgoing channel are not included. Theoretical papers which treat directly with the analysis of nuclear reaction data and results are included in the Reaction List.",1973,1380
CHAPTER 6 - Analysis of Finite Plates,,,1979,1381
Respiratory gas tensions and pH in healing wounds,Thomas K. Hunt and Patrick Twomey and Bengt Zederfeldt and J.Englebert Dunphy,"The technic of Schilling, Joel, and Shurley [2] has been adapted to the study of oxygen tension, carbon dioxide tensions, and hydrogen ion concentration in wound fluid. The gas tensions have been shown to be characteristic of those present at the advancing edge of the granulation tissue. Oxygen tensions were very low in the early phases of healing and rose as healing progressed. Carbon dioxide tensions were low five days after wounding but rose thereafter, probably because of increased production of carbon dioxide by the healing tissue. The low hydrogen ion concentration primarily reflects the high carbon dioxide tensions of wound fluid. The significance of these data is discussed.",1967,1382
4 - Disaccharides,GAD AVIGAD,,1990,1383
Expanded clinical and research uses of composite tissue transfers on isolated vascular pedicles,Robert A. Chase,,1967,1384
Cecostomy: An analysis of 102 cases,Paul P. Jackson and Robert M. Baird,,1967,1385
Volvulus of the ascending colon: A report of twenty-two cases,William W. Krippaehne and R.Mark Vetto and Charles C. Jenkins,Twenty-two cases of cecal volvulus in a geriatric hospital population are presented. The diagnosis is primarily derived from patterns determined by roentgenography. Most commonly colonic distention appeared to precipitate the volvulus in a geriatric patient with an anatomic arrangement conducive to volvulus. A search for the cause of the distention should be made. Early diagnosis and operation are imperative if vascular complications with their attendant morbidity and mortality are to be avoided. In one third of our patients the cecum was folded upward over bands across the ascending colon. This entity is not recognized by many surgeons. The operative mortality of eight of twenty patients was related to severe concomitant disease.,1967,1386
Oceanographic abstract Part II,,,1976,1387
Properties of synthetic membranes in extracorporeal circuits,Sherman W. Day and Dean K. Crystal and Clyde L. Wagner and Jay M. Kranz,"Membranes with capabilities which promise to produce a clinically effective artificial lung are now available. Silicone rubber, 18 mil Teflon, and cellophane all transfer carbon dioxide satisfactorily. These substances, as well as the thicker films of Teflon, appear to be adequate for oxygen transfer, although cellophane does it rather poorly. Turbulence is of considerable importance in the transfer of oxygen. A fruitful line of investigation might be that of evaluating the maximal degree of turbulence that one can introduce to an oxygenating system without producing blood damage. One could use the Reynolds∗∗Re = pVDu where p = density, V = velocity, D = inside diameter of conduit, and u = viscosity. number as a guide to quantitating the degree of turbulence produced and perhaps devise a scheme that produces even greater turbulent flow than that which we have described. Perhaps other membrane substances could be considered or those previously discarded could be re-evaluated in the light of improved turbulent flow. For example, cellophane, which is a wettable membrane and theoretically more desirable, might be made to transfer oxygen more satisfactorily, with improved flow characteristics. Other wettable membranes might also be devised and tested. One might suggest that the ideal gas exchange membrane would be one that is wettable, durable, easily fabricated into lung units, and sufficiently thin. It would need to provide adequate carbon dioxide and oxygen transfer in a highly turbulent flow system which also involves a tolerable amount of blood damage.",1967,1388
CHAPTER 16 - Agglutination and Flocculation,,"Publisher Summary
Agglutination is a sensitive method for the detection of antibody because much less antibody is required to agglutinate particles containing antigenic patches on their surfaces than is needed to aggregate antigens in free solution. This chapter provides an overview of agglutination and fiocculation. It discusses direct hemagglutination and indirect hemagglutination. The slide agglutination technique provides a simple and rapid means of determining blood groups. The method is most frequently employed in ABO and Rh grouping. The procedure may be carried out on microscope slides, large glass plates, or white porcelain or plastic tiles. The chapter reviews slide agglutination and tube agglutination. The cell-counting assay method of Wilkie and Becker makes possible the quantitative estimation of hemagglutinin activity and describes the course of the reaction as a curve that relates agglutination response to concentration of hemagglutinins. The chapter explains inhibition of hemagglutination by antibodies and antibody-like reagents in semi-quantitative tube tests. It further discusses bacterial agglutination, immobilization of motile bacteria by anti-flagellar antibody, agglutination of spermatozoa, agglutination with antigen on inert particles.",1977,1389
Thermodynamic properties of normal butane at refrigeration temperatures,Ronald T. Kurnik and Allen J. Barduhn,"Presented in both English and SI units are tables of the thermodynamic properties of normal butane over the temperature range -22 to +122°F (-30 to +50°C). Values are tabulated of the volume, enthalpy, and entropy for the subcooled liquid, the saturated liquid and vapor and the superheated vapor in both unit systems. The data used to calculate these properties have been carefully evaluated and include very recent values of the low temperature heat capacity of the vapor. Two enthalpy-entropy charts (Mollier diagrams), one in each of the unit systems are presented. These are suitable for the design of machinery used in thermo-mechanical energy cycles such as the freezing and eutectic processes for treating sea water and waste water. The calculated thermodynamic properties are the only ones available for n-butane which cover the above complete temperature range.",1978,1390
Bibliography of the current world literature,,,1989,1391
"Caspar: Towards decision making helpers agents for IoT, based on natural language and first order logic reasoning",Carmelo Fabio Longo and Francesco Longo and Corrado Santoro,"In the last decade, the market of Internet of Things has become quite disruptive, together with commercial clouds providing connection between every sort of devices and the global network, supported by vocal assistants. On the other hands, such commercial products are limited to work on limited domains, although easily scalable, without aspiring to higher level of reasoning in the field of Decisions Making. In this work, we show a way towards the design of an architecture for building cognitive agents leveraging Natural Language Processing. Such agents will be not based on clouds and do not require any semantic training, plus they will be able of deduction on facts and rules in First Order Logic inferred directly from Natural Language. After the description of the architecture and its underlying components, a case-study is provided to show the effectiveness in cases of direct commands and routines, subordinated also by a Meta-Reasoning in a conceptual space, parsing the utterances with promising real-time performances.",2021,1392
"Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy",Yogesh K. Dwivedi and Nir Kshetri and Laurie Hughes and Emma Louise Slade and Anand Jeyaraj and Arpan Kumar Kar and Abdullah M. Baabdullah and Alex Koohang and Vishnupriya Raghavan and Manju Ahuja and Hanaa Albanna and Mousa Ahmad Albashrawi and Adil S. Al-Busaidi and Janarthanan Balakrishnan and Yves Barlette and Sriparna Basu and Indranil Bose and Laurence Brooks and Dimitrios Buhalis and Lemuria Carter and Soumyadeb Chowdhury and Tom Crick and Scott W. Cunningham and Gareth H. Davies and Robert M. Davison and Rahul Dé and Denis Dennehy and Yanqing Duan and Rameshwar Dubey and Rohita Dwivedi and John S. Edwards and Carlos Flavián and Robin Gauld and Varun Grover and Mei-Chih Hu and Marijn Janssen and Paul Jones and Iris Junglas and Sangeeta Khorana and Sascha Kraus and Kai R. Larsen and Paul Latreille and Sven Laumer and F. Tegwen Malik and Abbas Mardani and Marcello Mariani and Sunil Mithas and Emmanuel Mogaji and Jeretta Horn Nord and Siobhan O’Connor and Fevzi Okumus and Margherita Pagani and Neeraj Pandey and Savvas Papagiannidis and Ilias O. Pappas and Nishith Pathak and Jan Pries-Heje and Ramakrishnan Raman and Nripendra P. Rana and Sven-Volker Rehm and Samuel Ribeiro-Navarrete and Alexander Richter and Frantz Rowe and Suprateek Sarker and Bernd Carsten Stahl and Manoj Kumar Tiwari and Wil {van der Aalst} and Viswanath Venkatesh and Giampaolo Viglia and Michael Wade and Paul Walton and Jochen Wirtz and Ryan Wright,"Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.",2023,1393
MalHyStack: A hybrid stacked ensemble learning framework with feature engineering schemes for obfuscated malware analysis,Kowshik Sankar Roy and Tanim Ahmed and Pritom Biswas Udas and Md. Ebtidaul Karim and Sourav Majumdar,"Since the advent of malware, it has reached a toll in this world that exchanges billions of data daily. Millions of people are victims of it, and the numbers are not decreasing as the year goes by. Malware is of various types in which obfuscation is a special kind. Obfuscated malware detection is necessary as it is not usually detectable and is prevalent in the real world. Although numerous works have already been done in this field so far, most of these works still need to catch up at some points, considering the scope of exploration through recent extensions. In addition to that, the application of a hybrid classification model is yet to be popularized in this field. Thus, in this paper, a novel hybrid classification model named, MalHyStack, has been proposed for detecting such obfuscated malware within the network. This proposed working model is built incorporating a stacked ensemble learning scheme, where conventional machine learning algorithms namely, Extremely Randomized Trees Classifier (ExtraTrees), Extreme Gradient Boosting (XgBoost) Classifier, and Random Forest are used in the first layer which is then followed by a deep learning layer in the second stage. Before utilizing the classification model for malware detection, an optimum subset of features has been selected using Pearson correlation analysis which improved the accuracy of the model by more than 2 % for multiclass classification. It also reduces time complexity by approximately two and three times for binary and multiclass classification, respectively. For evaluating the performance of the proposed model, a recently published balanced dataset named CIC-MalMem-2022 has been used. Utilizing this dataset, the overall experimental results of the proposed model represent a superior performance when compared to the existing classification models.",2023,1394
School-age children are more skeptical of inaccurate robots than adults,Teresa Flanagan and Nicholas C. Georgiou and Brian Scassellati and Tamar Kushnir,"We expect children to learn new words, skills, and ideas from various technologies. When learning from humans, children prefer people who are reliable and trustworthy, yet children also forgive people's occasional mistakes. Are the dynamics of children learning from technologies, which can also be unreliable, similar to learning from humans? We tackle this question by focusing on early childhood, an age at which children are expected to master foundational academic skills. In this project, 168 4–7-year-old children (Study 1) and 168 adults (Study 2) played a word-guessing game with either a human or robot. The partner first gave a sequence of correct answers, but then followed this with a sequence of wrong answers, with a reaction following each one. Reactions varied by condition, either expressing an accident, an accident marked with an apology, or an unhelpful intention. We found that older children were less trusting than both younger children and adults and were even more skeptical after errors. Trust decreased most rapidly when errors were intentional, but only children (and especially older children) outright rejected help from intentionally unhelpful partners. As an exception to this general trend, older children maintained their trust for longer when a robot (but not a human) apologized for its mistake. Our work suggests that educational technology design cannot be one size fits all but rather must account for developmental changes in children's learning goals.",2024,1395
Applying staged event-driven access control to combat ransomware,Timothy McIntosh and A.S.M. Kayes and Yi-Ping Phoebe Chen and Alex Ng and Paul Watters,"The advancement of modern Operating Systems (OSs), and the popularity of personal computing devices with Internet connectivity, have facilitated the proliferation of ransomware attacks. Ransomware has evolved from executable programs encrypting user files, to novel attack vectors including fileless command scripts, information exfiltration and human-operated ransomware. Many anti-ransomware studies have been published, but many of them assumed newer ransomware variants only performed file encryption, were similar to existing variants, and often did not consider those novel attack vectors. We have defined an updated ransomware threat model to include those novel attack vectors, and redefined false positives and false negatives in the context of ransomware mitigation. We proposed to apply both program-centric and user-centric access control to combat ransomware, but only delegate access control decisions that users are capable of making to users, while enforcing non-negotiable access control decisions by OS and software developers. We have designed a Staged Event-Driven Access Control (SEDAC) approach to incorporate both program-centric and user-centric access control measures, and demonstrated a prototype on Windows OS. Our prototype was able to intercept more types of ransomware attack vectors than existing proposals. We hope to convince OS and software architects to incorporate our design to better combat ransomware.",2023,1396
Code review guidelines for GUI-based testing artifacts,Andreas Bauer and Riccardo Coppola and Emil Alégroth and Tony Gorschek,"Context:
Review of software artifacts, such as source or test code, is a common practice in industrial practice. However, although review guidelines are available for source and low-level test code, for GUI-based testing artifacts, such guidelines are missing.
Objective:
The goal of this work is to define a set of guidelines from literature about production and test code, that can be mapped to GUI-based testing artifacts.
Method:
A systematic literature review is conducted, using white and gray literature to identify guidelines for source and test code. These synthesized guidelines are then mapped, through examples, to create actionable, and applicable, guidelines for GUI-based testing artifacts.
Results:
The results of the study are 33 guidelines, summarized in nine guideline categories, that are successfully mapped as applicable to GUI-based testing artifacts. Of the collected literature, only 10 sources contained test-specific code review guidelines. These guideline categories are: perform automated checks, use checklists, provide context information, utilize metrics, ensure readability, visualize changes, reduce complexity, check conformity with the requirements and follow design principles and patterns.
Conclusion:
This pivotal set of guidelines provides an industrial contribution in filling the gap of general guidelines for review of GUI-based testing artifacts. Additionally, this work highlights, from an academic perspective, the need for future research in this area to also develop guidelines for other specific aspects of GUI-based testing practice, and to take into account other facets of the review process not covered by this work, such as reviewer selection.",2023,1397
The application of Software Defined Networking on securing computer networks: A survey,Rishikesh Sahay and Weizhi Meng and Christian D. Jensen,"Software Defined Networking (SDN) has emerged as a new networking paradigm for managing different kinds of networks ranging from enterprise to home network through software enabled control. The logically centralized control plane and programmability offers a great opportunity to improve network security, like implementing new mechanisms to detect and mitigate various threats, as well as enables deploying security as a service on the SDN controller. Due to the increasing and fast development of SDN, this paper provides an extensive survey on the application of SDN on enhancing the security of computer networks. In particular, we survey recent research studies that focus on applying SDN for network security including attack detection and mitigation, traffic monitoring and engineering, configuration and policy management, service chaining, and middlebox deployment, in addition to smart grid security. We further identify some challenges and promising future directions on SDN security, compatibility and scalability issues that should be addressed in this field.",2019,1398
Traffic safety evaluation in Northwestern Federal District using sentiment analysis of Internet users’ reviews,Yaroslav Seliverstov and Svyatoslav Seliverstov and Igor Malygin and Oleg Korolev,"The paper addresses the task of analyzing traffic safety in the Northwestern Federal District according to the reviews published in the Web. To accomplish the task, the authors developed a system of automatic review classification based on a sentiment classifier. They analyzed open source libraries for data mining, developed a web crawler using Scrapy framework, written in Python 3, and collected reviews. They also considered the methods of text vectorization and lemmatization and their application in the Scikit-Learn library: Bag-of-Words, N-gram, CountVectorizer, and TF-IDF Vectorizer. For the purpose of classification, the authors used the naïve Bayes algorithm and a linear classifier model with stochastic gradient descent optimization. A base of tagged Twitter reviews was used as a training set. The classifier was trained using cross-validation and ShuffleSplit strategies. The authors also tested and compared the classification results for different classifiers. As a result of validation, the best model was determined. The developed system was applied to analyze the quality of roads in the Northwestern Federal District. Based on the outcome, the roads were marked-up in color to illustrate the results of the research.",2020,1399
Chapter 11 - Challenges and future work directions in artificial intelligence with human-computer interaction,Mahesh H. Panchal and Shaileshkumar D. Panchal,"Artificial intelligence–based systems are developed and successfully used for applications like home appliances, defense systems, virtual assistance, robotics, self-driving vehicles, and many more. Their success lies in accurate and timely decision-making ability. But the other side of these systems is a lack of transparency that can be described as black box. Due to the opaque nature of existing artificial intelligence systems, researchers are not able to interpret the decisions that have been derived from given input situations. The lack of openness not only causes the end users to resist trusting the system but also tends to make it difficult for machine learning engineers to detect and mitigate the fault in case of failure in deriving desired output. The solution is to open the black box working nature of the system and provide required explanations as well interpretations to making the whole processes humanly understandable and meaningful. This chapter focuses on the need for explainable artificially intelligent systems, present paradigms that exist to achieve it, along with various forms of explanations expected by different stakeholders and challenges in the field of making transparent systems in the direction of trustworthy human–computer interaction.",2023,1400
Do cryptocurrency rewards improve platform valuations?,Hemang Subramanian and Florent Rouxelin,"This study investigates the impact of cryptocurrency rewards and token prices on user-generated content (UGC) provision by content creators on a blockchain-based platform. Analyzing data from the Steemit platform, we find that although an increase in total reward value incentivizes UGC contributions, the rise in token prices alone does not lead to a surge in UGC. Instead, token prices have a mediating role in the relationship between total rewards earned by content creators and the volume of UGC they produce. Furthermore, we observe that an increase in UGC does not lead to a corresponding rise in the platform's market capitalization, as increased website traffic intensifies competition for rewards from a constant pool, suggesting that heightened user engagement does not translate to enhanced market capitalization. These findings imply that carefully designed reward mechanisms are crucial for sustaining user engagement and content creation amidst market fluctuations. Our study underscores the importance of a comprehensive approach to incentivizing user participation and ensuring platform growth, as a mere increase in token prices may not guarantee sustained engagement or an associated increase in market capitalization.",2024,1401
Glossary,,,2024,1402
Anomaly-based cyberattacks detection for smart homes: A systematic literature review,Juan Ignacio Iturbe Araya and Helena Rifà-Pous,"Smart homes, leveraging IoT technology to interconnect various devices and appliances to the internet, enable remote monitoring, automation, and control. However, collecting sensitive personal and business data assets renders smart homes a target for cyberattacks. Anomaly detection is a promising approach for identifying malicious behavior in smart homes. Yet, the current literature primarily discusses IoT-related cyberattacks and gives limited attention to detecting anomalies specific to the smart home context. Furthermore, there is a lack of datasets that accurately represent the complexity inherent in a smart home environment in terms of users with varying levels of expertise and diverse, evolving types of devices. Therefore, this paper presents a systematic literature review (SLR) that focuses on using anomaly detection to identify cyberattacks in smart home environments. The SLR includes an adapted taxonomy that classifies existing anomaly detection methods and a critical analysis of the current state of knowledge and future research challenges. Our findings show a growing interest in detecting cyberattacks with anomaly-based models in smart homes using centralized and network-based features. Ensemble and deep learning techniques are popular methods for detecting these anomalies. However, the limited diversity of cyberattacks in existing datasets and the absence of comprehensive datasets representing the complexity of smart home environments call for further research to improve the generalizability of detection models.",2023,1403
Discriminating flash crowds from DDoS attacks using efficient thresholding algorithm,Jisa David and Ciza Thomas,"Distributed Denial-of-Service attacks have been a challenge to cyberspace, as the attackers send a large number of attack packets similar to the normal traffic, to throttle legitimate flows. These attacks intentionally disrupt the services offered by the systems resulting in heavy cost. A flash crowd or flash event is an unexpected surge in the number of visitors to a particular website resulting in a sudden increase in server load. Flash crowds, which are legitimate flows, are difficult to be discriminated from Distributed Denial-of-Service attacks that are illicit flows. Effective and accurate detection of Distributed Denial of Service attacks still remains a challenge due to the difficulty in its detection and the false alerts generated in the case of flash crowds. There is a trade off between detection rate and false positive rate. This work deals with an efficient and early detection of distributed denial of service attacks and discriminates flash crowd by considering two network traffic parameters such as packet size and destination IP address. Using these traffic features two attributes are computed and its generalized entropies are calculated. The threshold is computed using the mean value of network attributes to detect the attacks. Threshold updater can automatically adjust the threshold values according to the changes in the channel conditions. The data sets used to evaluate the performance of the proposed approach are the MIT Lincoln Laboratory DARPA data set and a data set generated in a University network. Experimental results show this research approach achieves higher detection rate and lower false positives in a much reduced processing time as compared to the existing methods.",2021,1404
An effective convolutional neural network based on SMOTE and Gaussian mixture model for intrusion detection in imbalanced dataset,Hongpo Zhang and Lulu Huang and Chase Q. Wu and Zhanbo Li,"Network Intrusion Detection System (NIDS) is a key security device in modern networks to detect malicious activities. However, the problem of imbalanced class associated with intrusion detection dataset limits the classifier’s performance for minority classes. To improve the detection rate of minority classes while ensuring efficiency, we propose a novel class imbalance processing technology for large-scale dataset, referred to as SGM, which combines Synthetic Minority Over-Sampling Technique (SMOTE) and under-sampling for clustering based on Gaussian Mixture Model (GMM). We then design a flow-based intrusion detection model, SGM-CNN, which integrates imbalanced class processing with convolutional neural network, and investigate the impact of different numbers of convolution kernels and different learning rates on model performance. The advantages of the proposed model are verified using the UNSW-NB15 and CICIDS2017 datasets. The experimental results show that i) for binary classification and multiclass classification on the UNSW-NB15 dataset, SGM-CNN achieves a detection rate of 99.74% and 96.54%, respectively; ii) for 15-class classification on the CICIDS2017 dataset, it achieves a detection rate of 99.85%. We compare five imbalanced processing methods and two classification algorithms, and conclude that SGM-CNN provides an effective solution to imbalanced intrusion detection and outperforms the state-of-the-art intrusion detection methods.",2020,1405
Analysis of the sensitivity of the End-Of-Turn Detection task to errors generated by the Automatic Speech Recognition process,César Montenegro and Roberto Santana and Jose A. Lozano,"An End-Of-Turn Detection Module (EOTD-M) is an essential component of automatic Spoken Dialogue Systems. The capability of correctly detecting whether a user’s utterance has ended or not improves the accuracy in interpreting the meaning of the message and decreases the latency in the answer. Usually, in dialogue systems, an EOTD-M is coupled with an Automatic Speech Recognition Module (ASR-M) to transmit complete utterances to the Natural Language Understanding unit. Mistakes in the ASR-M transcription can have a strong effect on the performance of the EOTD-M. The actual extent of this effect depends on the particular combination of ASR-M transcription errors and the sentence featurization techniques implemented as part of the EOTD-M. In this paper we investigate this important relationship for an EOTD-M based on semantic information and particular characteristics of the speakers (speech profiles). We introduce an Automatic Speech Recognition Simulator (ASR-SIM) that models different types of semantic mistakes in the ASR-M transcription as well as different speech profiles. We use the simulator to evaluate the sensitivity to ASR-M mistakes of a Long Short-Term Memory network classifier trained in EOTD with different featurization techniques. Our experiments reveal the different ways in which the performance of the model is influenced by the ASR-M errors. We corroborate that not only is the ASR-SIM useful to estimate the performance of an EOTD-M in customized noisy scenarios, but it can also be used to generate training datasets with the expected error rates of real working conditions, which leads to better performance.",2021,1406
Malboard: A novel user keystroke impersonation attack and trusted detection framework based on side-channel analysis,Nitzan Farhi and Nir Nissim and Yuval Elovici,"Concealing malicious components within widely used USB peripherals has become a popular attack vector utilizing social engineering techniques and exploiting users’ trust in USB devices. This vector enables the attacker to easily penetrate an organization's computers even when the target is secured or in an air-gapped network. Such malicious concealment can be done as part of a supply chain attack or during the device manufacturing process. In cases where the device allows the user to update its firmware, a supply chain attack may involve changing just the device's firmware, thus compromising the device without the need for concealment. A compromised device can impersonate other devices like keyboards in order to send malicious keystrokes to the computer. However, the keystrokes generated maliciously do not match human keystroke characteristics, and therefore they can be easily detected by security tools that are designed to continuously verify the user's identity based on his/her keystroke dynamics. In this paper, we present Malboard, a sophisticated attack based on designated hardware concealment, which automatically generates keystrokes that have the attacked user's behavioral characteristics; in this attack these keystrokes are injected into the computer in the form of malicious commands and thus can evade existing detection mechanisms designed to continuously verify the user's identity based on keystroke dynamics. We implemented this novel attack and evaluated its performance on 30 subjects performing three different keystroke tasks; we evaluated the attack against three existing detection mechanisms, and the results show that our attack managed to evade detection in 83–100% of the cases, depending on the detection tools in place. Malboard was proven to be effective in two scenarios: either by a remote attacker using wireless communication to communicate with Malboard or by an inside attacker (malicious employee) that physically operates and uses Malboard. In addition, in order to address the evasion gap, we developed three different modules aimed at detecting keystroke injection attacks in general, and particularly, the more sophisticated Malboard attack. Our proposed detection modules are trusted and secured, because they are based on three side-channel resources which originate from the interaction between the keyboard, user, and attacked host. These side-channel resources include (1) the keyboard's power consumption, (2) the keystrokes’ sound, and (3) the user's behavior associated with his/her ability to respond to displayed textual typographical errors. Our results showed that each of the proposed detection modules is capable of detecting the Malboard attack in 100% of the cases, with no misses and no false positives; using them together as an ensemble detection framework will assure that an organization is immune to the Malboard attack in particular and other keystroke injection attacks in general.",2019,1407
A survey of public datasets for computer vision tasks in precision agriculture,Yuzhen Lu and Sierra Young,"Computer vision technologies have attracted significant interest in precision agriculture in recent years. At the core of robotics and artificial intelligence, computer vision enables various tasks from planting to harvesting in the crop production cycle to be performed automatically and efficiently. However, the scarcity of public image datasets remains a crucial bottleneck for fast prototyping and evaluation of computer vision and machine learning algorithms for the targeted tasks. Since 2015, a number of image datasets have been established and made publicly available to alleviate this bottleneck. Despite this progress, a dedicated survey on these datasets is still lacking. To fill this gap, this paper makes the first comprehensive but not exhaustive review of the public image datasets collected under field conditions for facilitating precision agriculture, which include 15 datasets on weed control, 10 datasets on fruit detection, and 9 datasets on miscellaneous applications. We survey the main characteristics and applications of these datasets, and discuss the key considerations for creating high-quality public image datasets. This survey paper will be valuable for the research community on the selection of suitable image datasets for algorithm development and identification of where creation of new image datasets is needed to support precision agriculture.",2020,1408
"Blockchain verification and validation: Techniques, challenges, and research directions",Dusica Marijan and Chhagan Lal,"As blockchain technology is gaining popularity in industry and society, solutions for Verification and Validation (V&V) of blockchain-based software applications (BC-Apps) have started gaining equal attention. To ensure that BC-Apps are properly developed before deployment, it is paramount to apply systematic V&V to verify their functional and non-functional requirements. While existing research aims at addressing the challenges of engineering BC-Apps by providing testing techniques and tools, blockchain-based software development is still an emerging research discipline, and therefore, best practices and tools for the V&V of BC-Apps are not yet sufficiently developed. In this paper, we provide a comprehensive survey on V&V solutions for BC-Apps. Specifically, using a layered approach, we synthesize V&V tools and techniques addressing different components at various layers of the BC-App stack, as well as across the whole stack. Next, we provide a discussion on the challenges associated with BC-App V&V, and summarize a set of future research directions based on the challenges and gaps identified in existing research work. Our study aims to highlight the importance of BC-App V&V and pave the way for a disciplined, testable, and verifiable BC development.",2022,1409
Using machine learning to identify common flaws in CAPTCHA design: FunCAPTCHA case analysis,Carlos Javier Hernández-Castro and María D. R-Moreno and David F. Barrero and Stuart Gibson,"Human Interactive Proofs (HIPs 11Human Interaction Proof, or also Human Interactive Proof. or CAPTCHAs 22Completely Automated Public Turing test to tell Computers and Humans Apart.) have become a first-level security measure on the Internet to avoid automatic attacks or minimize their effects. All the most widespread, successful or interesting CAPTCHA designs put to scrutiny have been successfully broken. Many of these attacks have been side-channel attacks. New designs are proposed to tackle these security problems while improving the human interface. FunCAPTCHA is the first commercial implementation of a gender classification CAPTCHA, with reported improvements in conversion rates. This article finds weaknesses in the security of FunCAPTCHA and uses simple machine learning (ML) analysis to test them. It shows a side-channel attack that leverages these flaws and successfully solves FunCAPTCHA on 90% of occasions without using meaningful image analysis. This simple yet effective security analysis can be applied with minor modifications to other HIPs proposals, allowing to check whether they leak enough information that would in turn allow for simple side-channel attacks.",2017,1410
Embedded Architecture Composed of Cognitive Agents and ROS for Programming Intelligent Robots,Gustavo R. Silva and Leandro B. Becker and Jomi F. Hübner,"This paper proposes and evaluates an embedded architecture aimed to promote the utilization of cognitive agents in cooperation with the Robotic Operating System (ROS), serving as an alternative for programming intelligent robots. It promotes the programming abstraction level in two directions. The first direction regards using cognitive agents facilities for programming the robots intelligence, consisting of its perceptions and related actions. The second direction exploits the facilities of using ROS layers for programming the robot interaction with its sensors and actuators. The paper reports experiments of using agents to command simulated UAVs while measuring performance metrics that allowed us to evaluate the benefits of the proposed architecture.",2020,1411
A Matter of (Joint) control? Virtual assistants and the general data protection regulation,Jurriaan {van Mil} and João Pedro Quintais,"This article provides an overview and critical examination of the rules for determining who qualifies as controller or joint controller under the General Data Protection Regulation. Using Google Assistant – an artificial intelligence-driven virtual assistant – as a case study, we argue that these rules are overreaching and difficult to apply in the present-day information society and Internet of Things environments. First, as a consequence of recent developments in case law and supervisory guidance, these rules lead to a complex and ambiguous test to determine (joint) control. Second, due to advances in technological applications and business models, it is increasingly challenging to apply such rules to contemporary processing operations. In particular, as illustrated by the Google Assistant, individuals will likely be qualified as joint controllers, together with Google and also third-party developers, for at least the collection and possible transmission of other individuals’ personal data via the virtual assistant. Third, we identify follow-on issues relating to the apportionment of responsibilities between joint controllers and the effective and complete protection of data subjects. We conclude by questioning whether the framework for determining who qualifies as controller or joint controller is future-proof and normatively desirable.",2022,1412
Characterization of halogenated organic compounds by the Fourier transform ion cyclotron resonance mass spectrometry: A critical review,Shixi Wu and Manabu Fujii and Xin Yang and Qing-Long Fu,"Halogenated organic compounds (HOCs), widely present in various environments, are generally formed by natural processes (e.g., photochemical halogenation) and anthropogenic activities (e.g., water disinfection and anthropogenic discharge of HOCs), posing health and environmental risks. Therefore, in-depth knowledge of the molecular composition, transformation, and fate of HOCs is crucial to regulate and reduce their formation. Because of the extremely complex nature of HOCs and their precursors, the molecular composition of HOCs remains largely unknown. The Fourier transform ion cyclotron resonance mass spectrometry (FT-ICR MS) offers the most powerful resolution and mass accuracy for the simultaneous molecular-level characterization of HOCs and their precursors. However, there is still a paucity of reviews regarding the comprehensive characterization of HOCs by FT-ICR MS. Based on the FT-ICR MS, the formation mechanism, sample pretreatment, and analysis methods were summarized for two typical HOCs classes, namely halogenated disinfection byproducts and per- and polyfluoroalkyl substances in this review. Moreover, we have highlighted data analysis methods and some typical applications of HOCs using FT-ICR MS and proposed suggestions for current issues. This review will deepen our understanding of the chemical characterization of HOCs and their formation mechanisms and transformation at the molecular level in aquatic systems, facilitating the application of the state-of-the-art FT-ICR MS in environmental and geochemical research.",2023,1413
Toward artificially intelligent cloud-based building information modelling for collaborative multidisciplinary design,Rafael Sacks and Zijian Wang and Boyuan Ouyang and Duygu Utkucu and Siyu Chen,"The technological tools people use for designing buildings have progressed from drawings to descriptive geometry, and from computer-aided drafting and design (CAD) to building information modelling (BIM). Yet despite their use of state-of-the-art BIM technology, the multidisciplinary teams that design modern buildings still face numerous challenges. Building models lack sufficient semantic content to properly express design intent, concurrent design is difficult due to the need for operators to maintain model consistency and integrity manually, managing design variations is cumbersome due to the packaging of information in files, and collaboration requires making-do with imperfect interoperability between application software. In response, we propose a ‘Cloud BIM’ (CBIM) approach to building modelling that seeks to automate maintenance of consistency across federated discipline-specific models by enriching models with semantic information that encapsulates design intent. The approach requires a new ontology to represent knowledge about the relationships between building model objects within and across disciplines. Discipline-specific building models are stored together with their data schema in knowledge graphs, and linked using objects and relationships from the CBIM ontology. The links are established using artificially intelligent semantic enrichment methods that recognize patterns of location, geometry, topology and more. Software methods that operate along CBIM relationship chains can detect inconsistencies that arise across disciplines and act to inform users, propose meaningful corrections, and apply them if approved. Future CBIM systems may provide designers with the functionality for collaborative multidisciplinary design by maintaining model consistency and managing versioning at the object level.",2022,1414
Robotic platform and path planning algorithm for in situ bioprinting,Gabriele Maria Fortunato and Gabriele Rossi and Amedeo Franco Bonatti and Aurora {De Acutis} and Christian Mendoza-Buenrostro and Giovanni Vozzi and Carmelo {De Maria},"The aim of this work is to design a robotic bioprinting platform able to fabricate a three-dimensional structure onto irregular surfaces. With respect to the limitations of current in vitro bioprinting approach, widely used in scaffold-based tissue engineering – handling difficulty, risk of contamination, shape not matching with the defect site – this robotic bioprinter can offer an innovative solution allowing in situ bioprinting, a direct dispensing of biological materials onto and into the damaged site. The robotic platform was developed starting from the 5 degrees-of-freedom open source MOVEO robot from BCN3D. The hardware and the software of the original project were re-engineered to control the robot using LinuxCNC, a path planning algorithm was developed in Matlab®, and the end-effector was equipped with a pneumatic extruder. The algorithm automatically projects any generic printing pattern on the surface on which the scaffold will be 3D bioprinted. For each point, the algorithm calculates the joint angles to keep the end effector always perpendicular to the surface. A g-code file is then exported to Linux CNC adding parameters to control the air pressure and the printing speed. The robotic platform was tested to evaluate its performances. Resolution (~200 ​μm) and repeatability were estimated and preliminary in situ bioprinting tests were performed onto different irregular surfaces, including a physiologically relevant bone model.",2021,1415
Progresses and challenges in link prediction,Tao Zhou,"Summary
Link prediction is a paradigmatic problem in network science, which aims at estimating the existence likelihoods of nonobserved links, based on known topology. After a brief introduction of the standard problem and evaluation metrics of link prediction, this review will summarize representative progresses about local similarity indices, link predictability, network embedding, matrix completion, ensemble learning, and some others, mainly extracted from related publications in the last decade. Finally, this review will outline some long-standing challenges for future studies.",2021,1416
A fog computing based approach to DDoS mitigation in IIoT systems,Luying Zhou and Huaqun Guo and Gelei Deng,"Distributed denial of service (DDoS) cyber-attack poses a severe threat to the industrial Internet of Things (IIoT) operation due to the security vulnerabilities resulted from increased connectivity and openness, and the large number of deployed low computation power devices. This paper applies Fog computing concept in DDoS mitigation by allocating traffic monitoring and analysis work close to local devices, and, on the other hand, coordinating and consolidating work to cloud central servers so as to achieve fast response while at low false alarm rate. The mitigation scheme consists of real-time traffic filtering via field firewall devices, which are able to reversely filter the signature botnet attack packets; offline specification based traffic analysis via virtualized network functions (VNFs) in the local servers; and centralized coordination via cloud server, which consolidates and correlates the information from the distributed local servers to make a more accurate decision. The proposed scheme is tested in an industrial control system testbed and the experiments evaluate the detection time and rate for two types of DDoS attacks and demonstrate the effectiveness of the scheme.",2019,1417
Challenges of the market for initial coin offerings,Pablo {de Andrés} and David Arroyo and Ricardo Correia and Alvaro Rezola,"This article analyzes the main problems and the solutions adopted in the market for Initial Coin Offerings (ICO), to anticipate the future of this market and determine implications for issuers, investors and regulators. ICOs represent an alternative and innovative financing solution that has experienced spectacular growth and notoriety in recent years. ICOs rely on Blockchain protocols and the ICO market is, therefore, characterized as decentralized, disintermediated and unregulated. Our results show that although the ICO market is innovative, it already displays many of the problems of traditional financial markets, and that these problems were at the genesis of the last financial crisis. Our analysis of the problems and solutions adopted shows a tension between what the Blockchain technology offers, and the problems associated with the financing of innovation. Considering the problems and solutions adopted, we no longer expect the ICO market to be characterized as disintermediated, unregulated or even decentralized in the near future. Furthermore, it is a real possibility that ICOs may end up being a progressor model eventually replaced by similar but more specialized financing models, some of which may already exist. With respect to the particular solutions of the ICO market, while some represent the realization of the potential of Blockchain, others such as forks have important Governance implications with the potential to create as many problems as the ones they address.",2022,1418
Dynamic path planning via Dueling Double Deep Q-Network (D3QN) with prioritized experience replay,Mehmet Gök,"Path planning is a key requirement for mobile robots employed for different tasks such as rescue or transport missions. Conventional methods such as A* or Dijkstra to tackle path planning problem need a premise map of the robot's environment. Nowadays, dynamic path planning is a popular research topic, which drives mobile robots without prior static requirements. Deep reinforcement learning (DRL), which is another popular research area, is being harnessed to solve dynamic path planning problem by the researchers. In this study, Deep Q-Networks, which is a subdomain of DRL are opted to solve dynamic path planning problem. We first employ well known techniques Double Deep Q-Networks (D2QN) and Dueling Double Deep Q-Networks (D3QN) to train a model which can drive a mobile robot in environments with static and dynamic obstacles within 3 different configurations. Then we propose D3QN with Prioritized Experience Replay (PER) extension in order to further optimize the DRL model. We created a test bed to measure the performance of the DRL models against 99 randomly generated goal locations. According to our experiments, D3QN-PER method performs better than D2QN and D3QN in terms of path length and travel time to the goal without any collisions. Robot Operating System and Gazebo simulation environment is utilized to realize the training and testing environments, thus, the trained DRL models can be deployed to any ROS compatible robot seamlessly.",2024,1419
"Chapter 28 - Understanding the Evolution of the Internet: Web 1.0 to Web3.0, Web3, and Web 3+∗∗The article is written with the help of ChatGPT to enhance the writing style. ChatGPT also assisted the author in verification of some information.",Zheng JinCheng and David Lee Kuo Chuen,"The Internet has undergone numerous changes since its emergence in 1969 and has now become an indispensable aspect of modern life. With the introduction of the World Wide Web by Tim Berners-Lee, the Internet has transformed into a tool for sharing and accessing vast amounts of information. As the Internet evolves toward its third iteration, Web3+ offers a decentralized solution that empowers users and returns control over the Internet to them. With the rise of cryptocurrency and blockchain, Web3+ focuses on data ownership and protection, making the Internet more secure and fair for everyone. In this article, we will explore the differences between Web 1.0, Web 2.0, Web 3.0, Web3, and Web3+ and how they shape the future of the Internet.",2024,1420
Model driven engineering for machine learning components: A systematic literature review,Hira Naveed and Chetan Arora and Hourieh Khalajzadeh and John Grundy and Omar Haggag,"Context:
Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components.
Objective:
The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations.
Method:
Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting.
Results:
We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research.
Conclusion:
This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners.",2024,1421
Reliance on scientists and experts during an epidemic: Evidence from the COVID-19 outbreak in Italy,Pietro Battiston and Ridhi Kashyap and Valentina Rotondi,"Research suggests trust in experts and authorities are important correlates of compliance with public health measures during infectious disease outbreaks. Empirical evidence on the dynamics of reliance on scientists and public health authorities during the early phases of an epidemic outbreak is limited. We examine these processes during the COVID-19 outbreak in Italy by leveraging data from Twitter and two online surveys, including a survey experiment. We find that reliance on experts followed a curvilinear path. Both Twitter and survey data showed initial increases in information-seeking from expert sources in the three weeks after the detection of the first case. Consistent with these increases, knowledge about health information linked to COVID-19 and support for containment measures was widespread, and better knowledge was associated with stronger support for containment policies. Both knowledge and containment support were positively associated with trust in science and public health authorities. However, in the third week after the outbreak, we detected a slowdown in responsiveness to experts. These processes were corroborated with a survey experiment, which showed that those holding incorrect beliefs about COVID-19 gave no greater – or even lower – importance to information when its source was stated as coming from experts than when the source was unstated. Our results suggest weakened trust in public health authorities with prolonged exposure to the epidemic as a potential mechanism for this effect. Weakened responsiveness to expert sources may increase susceptibility to misinformation and our results call for efforts to sustain trust in adapting public health response.",2021,1422
Environmental impact assessment of online advertising,M. Pärssinen and M. Kotila and R. Cuevas and A. Phansalkar and J. Manner,"There are no commonly agreed ways to assess the total energy consumption of the Internet. Estimating the Internet's energy footprint is challenging because of the interconnectedness associated with even seemingly simple aspects of energy consumption. The first contribution of this paper is a common modular and layered framework, which allows researchers to assess both energy consumption and CO2e emissions of any Internet service. The framework allows assessing the energy consumption depending on the research scope and specific system boundaries. Further, the proposed framework allows researchers without domain expertise to make such an assessment by using intermediate results as data sources, while analyzing the related uncertainties. The second contribution is an estimate of the energy consumption and CO2e emissions of online advertising by utilizing our proposed framework. The third contribution is an assessment of the energy consumption of invalid traffic associated with online advertising. The second and third contributions are used to validate the first. The online advertising ecosystem resides in the core of the Internet, and it is the sole source of funding for many online services. Therefore, it is an essential factor in the analysis of the Internet's energy footprint. As a result, in 2016, online advertising consumed 20–282 TWh of energy. In the same year, the total infrastructure consumption ranged from 791 to 1334 TWh. With extrapolated 2016 input factor values without uncertainties, online advertising consumed 106 TWh of energy and the infrastructure 1059 TWh. With the emission factor of 0.5656 kg CO2e/kWh, we calculated the carbon emissions of online advertising, and found it produces 60 Mt CO2e (between 12 and 159 Mt of CO2e when considering uncertainty). The share of fraudulent online advertising traffic was 13.87 Mt of CO2e emissions (between 2.65 and 36.78 Mt of CO2e when considering uncertainty). The global impact of online advertising is multidimensional. Online advertising affects the environment by consuming significant amounts of energy, leading to the production CO2e emissions. Hundreds of billions of ad dollars are exchanged yearly, placing online advertising in a significant role economically. It has become an important and acknowledged component of the online-bound society, largely due to its integration with the Internet and the amount of revenue generated through it.",2018,1423
GHGDroid: Global heterogeneous graph-based android malware detection,Lina Shen and Mengqi Fang and Jian Xu,"As the most popular mobile platform, Android has become the major attack target of malware, and thus there is an urgent need to effectively thwart them. Recently, the graph-based technique has been a promising solution for malware detection, which highly depends on graph structures to capture behaviors separating the malware from the benign apps. However, existing graph-based malware detection approaches still suffer from high computation cost in constructing or updating a graph for APK under detection, high false negative and false positive. To cope with these issues, we propose a novel global heterogeneous graph-based Android malware detection approach, named GHGDroid. A global heterogeneous graph (GHG) with a good updatability is first built on large-scale Android applications to characterize complex relationships among APKs and sensitive APIs. And then, using the GHG, a multi-layer graph convolutional network based embedding method is proposed to learn APK embeddings for well capturing behaviors that can separate malware from benign. Finally, using APK embeddings as well their labels, a malware classifier is trained. Experiments on real-world Android applications show that GHGDroid achieves 99.17 % F1-score, which outperforms the state-of-the-art approaches. Moreover, GHGDroid spends about 8 s on detecting an APK, which shows that it has a good potential as a practical tool for the Android malware detection task.",2024,1424
Adaptive online learning for IoT botnet detection,Zhou Shao and Sha Yuan and Yongli Wang,"With the number of Internet of Things (IoT) devices proliferating, the traffic volume of IoT-based attacks has shown a gradually increasing trend. The IoT botnet attack, which aims to commit real, efficient, and profitable cybercrimes, has become one of the most severe IoT threats. Applying traditional techniques to IoT is difficult due to its particular characteristics, such as resource-constrained devices, massive volumes of data, and real-time requirements. In this paper, we explore an adaptive online learning strategy for real-time IoT botnet attack detection. Furthermore, we operate the proposed adaptive strategy in conjunction with online ensemble learning. To evaluate the proposed strategy, we use real IoT traffic data, including benign traffic data and botnet traffic data infected by Mirai. In real-time IoT botnet attack detection, our experimental results demonstrate that the proposed adaptive online learning strategy achieves remarkable performance.",2021,1425
Intrusion detection and prevention in fog based IoT environments: A systematic literature review,Cristiano Antonio {de Souza} and Carlos Becker Westphall and Renato Bobsin Machado and Leandro Loffi and Carla Merkle Westphall and Guilherme Arthur Geronimo,"Currently, the Internet of Things is spreading in all areas that apply computing resources. An important ally of the IoT is fog computing. It extends cloud computing and services to the edge of the network. Smart environments are becoming real and possible through IoT and fog computing. However, they are not free from security threats and vulnerabilities. This makes special security techniques indispensable. Security is one of the biggest challenges to ensuring an optimal IoT and Fog environment. Combined with the significant damage generated by application attacks, this fact creates the need to focus efforts in this area. This need can be proven through existing reviews of the state-of-the-art that pointed out several open aspects that need greater research effort. In this way, this article presents a Systematic Literature Review (SLR) considering the context of intrusion detection and prevention in environments based on fog computing and IoT. This review addresses more than 100 studies that were included after undergoing an extensive inclusion/exclusion process with well-defined criteria. From these studies, information was extracted to build a view of the current state-of-the-art and answer the research questions of this study. In this way, we identify the state-of-the-art, open questions and possibilities for future research.",2022,1426
"What is “big data” and how should we use it? The role of large datasets, secondary data, and associated analysis techniques in outdoor recreation research",Dani T. Dagan and Emily J. Wilkins,"With researchers increasingly interested in big data research, this conceptual paper describes how large datasets, secondary data, and associated analysis techniques can be used to understand outdoor recreation. Some types of large, secondary datasets that have been increasingly used in outdoor recreation research include social media, mobile device data, and trip reports or online reviews. First, we give a brief overview of big data terms and outline the steps involved in conducting big data research. In doing so, we describe data sources and analysis techniques relevant for outdoor recreation, and review how they have been applied in previous published works. We then describe opportunities, limitations, and considerations of using big data. Finally, we outline several questions researchers may consider when designing, conducting, reporting, and reviewing outdoor recreation research using big data. Overall, big data approaches can expand our understanding of outdoor recreation and, by addressing key questions, may help researchers harness the strengths of big data while ensuring quality and integrity.",2023,1427
Conspiracy spillovers and geoengineering,Ramit Debnath and David M. Reiner and Benjamin K. Sovacool and Finn Müller-Hansen and Tim Repke and R. Michael Alvarez and Shaun D. Fitzgerald,"Summary
Geoengineering techniques such as solar radiation management (SRM) could be part of a future technology portfolio to limit global temperature change. However, there is public opposition to research and deployment of SRM technologies. We use 814,924 English-language tweets containing #geoengineering globally over 13 years (2009–2021) to explore public emotions, perceptions, and attitudes toward SRM using natural language processing, deep learning, and network analysis. We find that specific conspiracy theories influence public reactions toward geoengineering, especially regarding “chemtrails” (whereby airplanes allegedly spray poison or modify weather through contrails). Furthermore, conspiracies tend to spillover, shaping regional debates in the UK, USA, India, and Sweden and connecting with broader political considerations. We also find that positive emotions rise on both the global and country scales following events related to SRM governance, and negative and neutral emotions increase following SRM projects and announcements of experiments. Finally, we also find that online toxicity shapes the breadth of spillover effects, further influencing anti-SRM views.",2023,1428
A review of machine learning in building load prediction,Liang Zhang and Jin Wen and Yanfei Li and Jianli Chen and Yunyang Ye and Yangyang Fu and William Livingood,"The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.",2021,1429
Utilizing Twitter data for analysis of chemotherapy,Ling Zhang and Magie Hall and Dhundy Bastola,"Objective
Twitter has become one of the most popular social media platforms that offers real-world insights to healthy behaviors. The purpose of this study was to assess and compare perceptions about chemotherapy of patients and health-care providers through analysis of chemo-related tweets.
Materials and methods
Cancer-related Twitter accounts and their tweets were obtained through using Tweepy (Python library). Multiple text classification algorithms were tested to identify the models with best performance in classifying the accounts into individual and organization. Chemotherapy-specific tweets were extracted from historical tweetset, and the content of these tweets was analyzed using topic model, sentiment analysis and word co-occurrence network.
Results
Using the description in Twitter users’ profiles, the accounts related with cancer were collected and coded as individual or organization. We employed Long Short Term Memory (LSTM) network with GloVe word embeddings to identify the user into individuals and organizations with accuracy of 85.2%. 13, 273 and 14,051 publicly available chemotherapy-related tweets were retrieved from individuals and organizations, respectively. The content of the chemo-related tweets was analyzed by text mining approaches. The tweets from individual accounts pertained to personal chemotherapy experience and emotions. In contrast with the personal users, professional accounts had a higher proportion of neutral tweets about side effects. The information about the assessment of response to chemotherapy was deficient from organizations on Twitter.
Discussion
Examining chemotherapy discussions on Twitter provide new lens into content and behavioral patterns associated with treatments for cancer patients. The methodology described herein allowed us to collect relatively large number of health-related tweets over a greater time period and exploit the potential power of social media, which provide comprehensive view on patients’ perceptions of chemotherapy.
Conclusion
This study sheds light on using Twitter data as a valuable healthcare data source for helping oncologists (organizations) in understanding patients’ experiences while undergoing chemotherapy, in developing personalize therapy plans, and a supplement to the clinical electronic medical records (EMRs).",2018,1430
Environment detection system for localization and mapping purposes,P. Neduchal and L. Bureš and M. Železný,"The goal of this paper is to present a concept and implementation of an Environment Detection System. The system is supposed to collect data from sensors attached to the mobile robot and then enhance the map of the environment by this information. Moreover, the data can be processed to get valuable information about the environment or its change. The open-source implementation of the system is written for Robot Operating System. Thus, the system can handle data from different sensors using a unified way. It is possible by employing the messaging mechanism implemented in the Robot Operating System. Another contribution of this paper are records of our testing runs with a 6WD mobile robot equipped by multiple sensors, which can be used as a dataset for SLAM and Environment Detection System implementations.",2019,1431
Rise of the Metaverse’s Immersive Virtual Reality Malware and the Man-in-the-Room Attack & Defenses,Martin Vondráček and Ibrahim Baggili and Peter Casey and Mehdi Mekni,"The allure of the metaverse along with Virtual Reality (VR) technologies and speed at which they are deployed may shift focus away from security and privacy fundamentals. In this work we employ classic exploitation techniques against cutting edge devices to obtain equally novel results. The unique features of the Virtual Reality landscape set the stage for our primary account of a new attack, the Man-in-the-Room (MitR). This attack, realized from a vulnerable social networking application led to both worming and botnet capabilities being adapted for VR with potential critical impacts affecting millions of users. Our work improves the state-of-the-art in Virtual Reality (VR) security and socio-technical research in VR. It shares several analytical and attacking tools, example exploits, evaluation dataset, and vulnerability signatures with the scientific and professional communities to ensure secure VR software development. The presented results demonstrate the detection and prevention of VR vulnerabilities, and raise questions in the law and policy domains pertaining to VR security and privacy.",2023,1432
Discovering differential features: Adversarial learning for information credibility evaluation,Lianwei Wu and Yuan Rao and Ambreen Nazir and Haolin Jin,"A series of deep learning approaches extract a large number of credibility features to detect fake news on the Internet. However, these extracted features still suffer from many irrelevant and noisy features that restrict severely the performance of the approaches. In this paper, we propose a novel model based on Adversarial Networks and inspirited by the Shared-Private model (ANSP), which aims at reducing common, irrelevant features from the extracted features for information credibility evaluation. Specifically, ANSP involves two tasks: one is to prevent the binary classification of true and false information for capturing common features relying on adversarial networks guided by reinforcement learning. Another extracts credibility features (henceforth, private features) from multiple types of credibility information and compares with the common features through two strategies, i.e., orthogonality constraints and KL-divergence for making the private features more differential. Experiments first on two six-label LIAR and Weibo datasets demonstrate that ANSP achieves state-of-the-art performance, boosting the accuracy by 2.1%, 3.1%, respectively and then on four-label Twitter16 validate the robustness of the model with 1.8% performance improvements.",2020,1433
Twitter and social accountability: Reactions to the Panama Papers,Dean Neu and Greg Saxton and Abu Rahaman and Jeffery Everett,"The potential of social media to disseminate, aggregate, channel and democratize social accountability processes has encouraged a variety of organizations to actively promote and champion such initiatives. These initiatives typically envision a three step social accountability process where, for example, the publication of previously-private financial information about the inappropriate wealth accumulation activities of politicians and their business allies (step #1), combined with social media dissemination and discussion of these activities (step #2), can result in an accountability conversation that spills out of the medium and that sometimes results in positive social change (step #3). The current study examines Twitter reactions to the International Consortium of Investigative Journalist’s (ICIJ) publication of the Panama Papers. The analysis illustrates that there was a Twitter reaction: furthermore, that there were different styles of response and that certain styles were more likely to elicit an audience reaction, especially if the tweeter was a journalist or organization. While the provided analysis focuses on step #2 within the social accountability process, the results imply that publicly-interested accounting academics qua activists can facilitate social accountability by helping to make previously-private financial information public and by cultivating sympathetic individuals within the traditional media as well as within organizations that are active on social media.",2019,1434
Machine learning for metabolic engineering: A review,Christopher E. Lawson and Jose Manuel Martí and Tijana Radivojevic and Sai Vamshi R. Jonnalagadda and Reinhard Gentz and Nathan J. Hillson and Sean Peisert and Joonhoon Kim and Blake A. Simmons and Christopher J. Petzold and Steven W. Singer and Aindrila Mukhopadhyay and Deepti Tanjore and Joshua G. Dunn and Hector {Garcia Martin},"Machine learning provides researchers a unique opportunity to make metabolic engineering more predictable. In this review, we offer an introduction to this discipline in terms that are relatable to metabolic engineers, as well as providing in-depth illustrative examples leveraging omics data and improving production. We also include practical advice for the practitioner in terms of data management, algorithm libraries, computational resources, and important non-technical issues. A variety of applications ranging from pathway construction and optimization, to genetic editing optimization, cell factory testing, and production scale-up are discussed. Moreover, the promising relationship between machine learning and mechanistic models is thoroughly reviewed. Finally, the future perspectives and most promising directions for this combination of disciplines are examined.",2021,1435
Detecting Internet of Things attacks using distributed deep learning,Gonzalo {De La Torre Parra} and Paul Rad and Kim-Kwang Raymond Choo and Nicole Beebe,"The reliability of Internet of Things (IoT) connected devices is heavily dependent on the security model employed to protect user data and prevent devices from engaging in malicious activity. Existing approaches for detecting phishing, distributed denial of service (DDoS), and Botnet attacks often focus on either the device or the back-end. In this paper, we propose a cloud-based distributed deep learning framework for phishing and Botnet attack detection and mitigation. The model comprises two key security mechanisms working cooperatively, namely: (1) a Distributed Convolutional Neural Network (DCNN) model embedded as an IoT device micro-security add-on for detecting phishing and application layer DDoS attacks; and (2) a cloud-based temporal Long-Short Term Memory (LSTM) network model hosted on the back-end for detecting Botnet attacks, and ingest CNN embeddings to detect distributed phishing attacks across multiple IoT devices. The distributed CNN model, embedded into a ML engine in the client's IoT device, allows us to detect and defend the IoT device from phishing attacks at the point of origin. We create a dataset consisting of both phishing and non-phishing URLs to train the proposed CNN add-on security model, and select the N_BaIoT dataset for training the back-end LSTM model. The joint training method minimizes communication and resource requirements for attack detection, and maximizes the usefulness of extracted features. In addition, an aggregation of schemes allows the automatic fusion of multiple requests to improve the overall performance of the system. Our experiments show that the IoT micro-security add-on running the proposed CNN model is capable of detecting phishing attacks with an accuracy of 94.3% and a F-1 score of 93.58%. Using the back-end LSTM model, the model detects Botnet attacks with an accuracy of 94.80% using all malicious data points in the used dataset. Thus, the findings demonstrate that the proposed approach is capable of detecting attacks, both at device and at the back-end level, in a distributed fashion.",2020,1436
Stack is the New Black?: Evolution and Outcomes of the ‘India-Stackification’ Process,Smriti Parsheera,"India is going through a transformative phase in its digital journey. A large part of this is enfolding in the field of digital public infrastructures as the ‘India Stack’ branded suite of technological solutions permeates through areas like digital identity, instant payments, digital commerce, and consent management. The paper traces the socio-technical imaginaries that have fueled India's digital transformation strategy and how India Stack acquired its central place in that scheme. Drawing upon India's performance on global ICT-related indices and the OECD's Good Practice Principles for Public Service Design and Delivery, the paper also examines how the country is faring in translating its visions of digital transformation into outcomes. It identifies reliance on coercive digital adoption strategies, lack of participative decision-making, and insufficient accountability safeguards as some of the fault lines in India's path to fair and equitable digital transformation.",2024,1437
Designing scalability in required in-class introductory college courses,Gabriele Piccoli and Marcin Łukasz Bartosiak and Biagio Palese and Joaquin Rodriguez,"We posit that design science enables the creation of in-class introductory college courses that can scale to large numbers of students, under resource constraints. We build on the centrality of human interactions in learning environments and conceptualize a college course as a socio-technical (ST) artifact. Grounded in the intervention theory, we draw meta-requirements guiding the design of college courses that leverage IT to scale, while maintaining the centrality of the professor’s role. We use the design-build-evaluate cycle to instantiate the ST artifact and demonstrate its feasibility using evaluation episodes as prescribed by the Framework for Evaluation in Design Science Research.",2020,1438
11 - Assuring AI methods for economic policymaking,Anderson Monken and William Ampeh and Flora Haberkorn and Uma Krishnaswamy and Feras A. Batarseh,"AI methods are becoming more common in the field of economics, but these models must be bias-free, fair, and explainable. In other words, we need AI assurance. Economic forecasting has benefited from machine learning techniques, such as neural networks, to increase model performance, but these AI techniques must be audited, accountable, and interpretable to be useful for economic policymaking. The rise of natural language processing and large language models has created new challenges for economic policymaking institutions, which need to be aware of AI assurance and how to harness them safely.",2023,1439
THINK: A novel conversation model for generating grammatically correct and coherent responses,Bin Sun and Shaoxiong Feng and Yiwei Li and Jiamou Liu and Kan Li,"Many existing conversation models that are based on the encoder–decoder framework incorporate complex encoders. These powerful encoders serve to enrich the context vectors, so that the generated responses are more diverse and informative. However, these approaches face two potential challenges. First, the high complexity of the encoder means relative simplicity of the decoder. There is a danger that the decoder becomes too simple to effectively capture previously generated information. As a result, the decoder may produce duplicated and self-contradicting responses. Second, by having a complex encoder, the model may generate incoherent responses because the complex context vectors may deviate from the true semantics of context. In this work, we propose a conversation model named “THINK” (Teamwork generation Hover around Impressive Noticeable Keywords) that is equipped with a complex decoder to avoid generating duplicated and self-contradicting responses. The model also simplifies the context vectors and increases the coherence of generated responses in a reasonable way. For this model, we propose Teamwork generation framework and Semantics extractor. Compared with other baselines, both automatic and human evaluation showed the advantages of our model.",2022,1440
On distributed ledgers security and illegal uses,Joanna Moubarak and Maroun Chamoun and Eric Filiol,"Distributed ledgers stimulate innovative services and enabled new applications in several domains, creating new concepts for trust and regulation. However, this backbone that is enabling novelties and abridging businesses comes with drawbacks and security flaws. In this paper, we evaluate several Distributed Ledger Technologies (DLTs) features depicting the Bitcoin, Ripple, Ethereum, Hyperledger, Algorand and IOTA networks. We focus on their security challenges and expose numerous threats and vulnerabilities. For instance, we have simulated a few of their possible attacks proving them non-immune. In the other hand, we show a few of their malicious use cases. Meticulously presenting DLTs menaces and flaws, we are not involved in preferring any specific DLT network.",2020,1441
Tweets or nighttime lights: Comparison for preeminence in estimating socioeconomic factors,Naizhuo Zhao and Guofeng Cao and Wei Zhang and Eric L. Samson,"Nighttime lights (NTL) imagery is one of the most commonly used tools to quantitatively study socioeconomic systems over large areas. In this study we aim to use location-based social media big data to challenge the primacy of NTL imagery on estimating socioeconomic factors. Geo-tagged tweets posted in the contiguous United States in 2013 were retrieved to produce a tweet image with the same spatial resolution of the NTL imagery (i.e., 0.00833° × 0.00833°). Sum tweet (the total number of tweets) and sum light (summed DN value of the NTL image) of each state or county were obtained from the tweets and the NTL images, respectively, to estimate three important socioeconomic factors: personal income, electric power consumption, and fossil fuel carbon dioxide emissions. Results show that sum tweet is a better measure of personal income and electric power consumption while carbon dioxide emissions can be more accurately estimated by sum light. We further exploited that African-Americans adults are more likely than White seniors to post geotagged tweets in the US, yet did not find any significant correlations between proportions of the subpopulations and the estimation accuracy of the socioeconomic factors. Existence of saturated pixels and blooming effects and failure to remove gas flaring reduce quality of NTL imagery in estimating socioeconomic factors, however, such problems are nonexistent in the tweet images. This study reveals that the number of geo-tagged tweets has great potential to be deemed as a substitute of brightness of NTL to assess socioeconomic factors over large geographic areas.",2018,1442
Detecting violations of access control and information flow policies in data flow diagrams,Stephan Seifermann and Robert Heinrich and Dominik Werle and Ralf Reussner,"The security of software-intensive systems is frequently attacked. High fines or loss in reputation are potential consequences of not maintaining confidentiality, which is an important security objective. Detecting confidentiality issues in early software designs enables cost-efficient fixes. A Data Flow Diagram (DFD) is a modeling notation, which focuses on essential, functional aspects of such early software designs. Existing confidentiality analyses on DFDs support either information flow control or access control, which are the most common confidentiality mechanisms. Combining both mechanisms can be beneficial but existing DFD analyses do not support this. This lack of expressiveness requires designers to switch modeling languages to consider both mechanisms, which can lead to inconsistencies. In this article, we present an extended DFD syntax that supports modeling both, information flow and access control, in the same language. This improves expressiveness compared to related work and avoids inconsistencies. We define the semantics of extended DFDs by clauses in first-order logic. A logic program made of these clauses enables the automated detection of confidentiality violations by querying it. We evaluate the expressiveness of the syntax in a case study. We attempt to model nine information flow cases and six access control cases. We successfully modeled fourteen out of these fifteen cases, which indicates good expressiveness. We evaluate the reusability of models when switching confidentiality mechanisms by comparing the cases that share the same system design, which are three pairs of cases. We successfully show improved reusability compared to the state of the art. We evaluated the accuracy of confidentiality analyses by executing them for the fourteen cases that we could model. We experienced good accuracy.",2022,1443
M-RL: A mobility and impersonation-aware IDS for DDoS UDP flooding attacks in IoT-Fog networks,Saeed Javanmardi and Meysam Ghahramani and Mohammad Shojafar and Mamoun Alazab and Antonio M. Caruso,"The Internet of Things (IoT) has recently received a lot of attention from the information and communication technology community. It has turned out to be a crucial development for harnessing the incredible power of wireless media in the real world. The nature of IoT-Fog networks requires the use of defense techniques who are light and mobile-aware. The edge resources in such a distributed environment are open to various safety hazards. DDoS UDP flooding attacks are the most frequent threats to edge resources in IoT-Fog networks. It is crucial for sabotaging fog gateways and can overcome traditional data filtering techniques. This paper introduces M-RL, a lightweight intrusion detection system with mobility awareness that can detect DDoS UDP flooding attacks while taking into account adversarial IoT devices that engage in IP spoofing. To this end, this paper analyzes the malicious behaviors that result in anonymity against Rate Limiting and Received Signal Strength (RSS)-based approaches, combines their advantages, and addresses their vulnerabilities. We test our method in different contexts to achieve that goal, and we find that it may decrease the accuracy of the RL, RSS, and RSS-RL methods to 70%, 48.9%, and 64.3%, respectively. The outcomes demonstrate the proposed approach's resistance to software-based source address forgery, impersonation, and signal modification. It offers more than 99% accuracy and supports node mobility. In this case, the best possible accuracy of the previous methods is 77%.",2024,1444
Hooktracer: Automatic Detection and Analysis of Keystroke Loggers Using Memory Forensics,Andrew Case and Ryan D. Maggio and Md Firoz-Ul-Amin and Mohammad M. Jalalzai and Aisha Ali-Gombe and Mingxuan Sun and Golden G. Richard,"Advances in malware development have led to the widespread use of attacker toolkits that do not leave any trace in the local filesystem. This negatively impacts traditional investigative procedures that rely on filesystem analysis to reconstruct attacker activities. As a solution, memory forensics has replaced filesystem analysis in these scenarios. Unfortunately, existing memory forensics tools leave many capabilities inaccessible to all but the most experienced investigators, who are well versed in operating systems internals and reverse engineering. The goal of the research described in this paper is to make investigation of one of the greatest threats that organizations face, userland keyloggers, less error-prone and less dependent on manual reverse engineering. To accomplish this, we have added significant new capabilities to HookTracer, which is an engine capable of emulating code discovered in a physical memory captures and recording all actions taken by the emulated code. Based on this work, we present new memory forensics capabilities, embodied in a new Volatility plugin, hooktracer_messagehooks, that uses Hooktracer to automatically decide whether a hook in memory is associated with a malicious keylogger or benign software. We also include a detailed case study that illustrates our technique’s ability to successfully analyze very sophisticated keyloggers, such as Turla.",2020,1445
A survey of GPT-3 family large language models including ChatGPT and GPT-4,Katikapalli Subramanyam Kalyan,"Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI’s GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.",2024,1446
"Mixed Use of Analytical Derivatives and Algorithmic Differentiation for NMPC of Robot Manipulators⁎⁎The authors would like to thank Flanders Make SBO MULTIROB: “Rigorous approach for programming and optimal control of multi-robot systems”, FWO project G0A6917N of the Research Foundation - Flanders (FWO - Flanders), and KU Leuven-BOF PFV/10/002 Centre of Excellence: Optimization in Engineering (OPTEC) for supporting this research.",Alejandro Astudillo and Justin Carpentier and Joris Gillis and Goele Pipeleers and Jan Swevers,"In the context of nonlinear model predictive control (NMPC) for robot manipulators, we address the problem of enabling the mixed and transparent use of algorithmic differentiation (AD) and efficient analytical derivatives of rigid-body dynamics (RBD) to decrease the solution time of the subjacent optimal control problem (OCP). Efficient functions for RBD and their analytical derivatives are made available to the numerical optimization framework CasADi by overloading the operators in the implementations made by the RBD library Pinocchio and adding a derivative-overloading feature to CasADi. A comparison between analytical derivatives and AD is made based on their influence on the solution time of the OCP, showing the benefits of using analytical derivatives for RBD in optimal control of robot manipulators.",2021,1447
WhoReview: A multi-objective search-based approach for code reviewers recommendation in modern code review,Moataz Chouchen and Ali Ouni and Mohamed Wiem Mkaouer and Raula Gaikovina Kula and Katsuro Inoue,"Contemporary software development is distributed and characterized by high dynamics with continuous and frequent changes to fix defects, add new user requirements or adapt to other environmental changes. To manage such changes and ensure software quality, modern code review is broadly adopted as a common and effective practice. Yet several open-source as well as commercial software projects have adopted peer code review as a crucial practice to ensure the quality of their software products using modern tool-based code review. Nevertheless, the selection of peer reviewers is still merely a manual and hard task especially with the growing size of distributed development teams. Indeed, it has been proven that inappropriate peer reviewers selection can consume more time and effort from both developers and reviewers and increase the development costs and time to market. To address this problem, we introduce a multi-objective search-based approach, named WhoReview, to find the optimal set of peer reviewers for code changes. We use the Indicator-Based Evolutionary Algorithm (IBEA) to find the best set of code reviewers that are (1) most experienced with the code change to be reviewed, while (2) considering their current workload, i.e., the number of open code reviews they are working on. We conduct an empirical study on 4 long-lived open source software projects to evaluate our approach. The obtained results show that WhoReview outperforms state-of-the-art approach by an average precision of 68% and recall of 77%. Moreover, we deployed our approach in an industrial context and evaluated it qualitatively from developers perspective. Results show the effectiveness of our approach with a high acceptance ratio in identifying relevant reviewers.",2021,1448
Chapter 13 - Artificial intelligence and basic human needs: the shadow aspects of emerging technology,Tay Keong Tan,"While advancing artificial intelligence (AI) applications have brought ease and benefit to human life in meeting our physical needs, it is less obvious how they would impact psychological needs. This study analyzes three emerging technologies—autonomous vehicles; facial recognition systems; and AI writing or image generators—from the perspective of six fundamental human needs; certainty, variety, significance, connection, growth, and contribution. Our core human needs can greatly influence the acceptability, feasibility, and utility of these technologies. A prognosis of the human needs implications of AI can help algorithm designers, policymakers, regulators, and end users mitigate the risks and accentuate its benefits.",2024,1449
"Test flakiness’ causes, detection, impact and responses: A multivocal review",Amjed Tahir and Shawn Rasheed and Jens Dietrich and Negar Hashemi and Lu Zhang,"Flaky tests (tests with non-deterministic outcomes) pose a major challenge for software testing. They are known to cause significant issues, such as reducing the effectiveness and efficiency of testing and delaying software releases. In recent years, there has been an increased interest in flaky tests, with research focusing on different aspects of flakiness, such as identifying causes, detection methods and mitigation strategies. Test flakiness has also become a key discussion point for practitioners (in blog posts, technical magazines, etc.) as the impact of flaky tests is felt across the industry. This paper presents a multivocal review that investigates how flaky tests, as a topic, have been addressed in both research and practice. Out of 560 articles we reviewed, we identified and analysed a total of 200 articles that are focused on flaky tests (composed of 109 academic and 91 grey literature articles/posts) and structured the body of relevant research and knowledge using four different dimensions: causes, detection, impact and responses. For each of those dimensions, we provide categorization and classify existing research, discussions, methods and tools With this, we provide a comprehensive and current snapshot of existing thinking on test flakiness, covering both academic views and industrial practices, and identify limitations and opportunities for future research.",2023,1450
No more DoS? An empirical study on defense techniques for web server Denial of Service mitigation,Marta Catillo and Antonio Pecchia and Umberto Villano,"Denial-of-Service (DoS) attacks are becoming increasingly common and undermine the availability of widely used web servers. Even if DoS attacks cannot be rendered completely harmless, ready-to-use defense modules and solutions to mitigate their effect are highly beneficial for site administrators. Unfortunately, there is a lack of measurement studies that explore the pros and cons of common DoS web server defense modules in order to understand their limitations and to drive practitioners’ choices. This paper presents an empirical study of the ubiquitous Apache web server, with an assessment of two well-known pluggable defense modules and an enlargement technique that provides the server with additional resources. Measurements are based on a mixture of flooding and slow DoS attacks. The experimentation shows that, in spite of the large availability of pluggable security modules that can be usefully deployed in practice, there is not a bulletproof defense solution to mitigate the DoS attacks in hand. The findings of our analysis can be useful to support the deployment of proper defense mechanisms, as well as the development of robust and effective solutions for DoS protection.",2022,1451
Maintaining symmetry during body axis elongation,Celia M. Smits and Sayantan Dutta and Vishank Jain-Sharma and Sebastian J. Streichan and Stanislav Y. Shvartsman,"Summary
Bilateral symmetry defines much of the animal kingdom and is crucial for numerous functions of bilaterian organisms. Genetic approaches have discovered highly conserved patterning networks that establish bilateral symmetry in early embryos,1 but how this symmetry is maintained throughout subsequent morphogenetic events remains largely unknown.2 Here we show that the terminal patterning system—which relies on Ras/ERK signaling through activation of the Torso receptor by its ligand Trunk3—is critical for preserving bilateral symmetry during Drosophila body axis elongation, a process driven by cell rearrangements in the two identical lateral regions of the embryo and specified by the dorsal-ventral and anterior-posterior patterning systems.4 We demonstrate that fluctuating asymmetries in this rapid convergent-extension process are attenuated in normal embryos over time, possibly through noise-dissipating forces from the posterior midgut invagination and movement. However, when Torso signaling is attenuated via mutation of Trunk or RNAi directed against downstream Ras/ERK pathway components, body axis elongation results in a characteristic corkscrew phenotype,5 which reflects dramatic reorganization of global tissue flow and is incompatible with viability. Our results reveal a new function downstream of the Drosophila terminal patterning system in potentially active control of bilateral symmetry and should motivate systematic search for similar symmetry-preserving regulatory mechanisms in other bilaterians.",2023,1452
FaNDS: Fake News Detection System using energy flow,Jiawei Xu and Vladimir Zadorozhny and Danchen Zhang and John Grant,"Recently, the term “fake news” has been broadly and extensively utilized for disinformation, misinformation, hoaxes, propaganda, satire, rumors, click-bait, and junk news. It has become a serious problem around the world. We present a new system, FaNDS, that detects fake news efficiently. The system is based on several concepts used in some previous works but in a different context. There are two main concepts: an Inconsistency Graph and Energy Flow. The Inconsistency Graph contains news items as nodes and inconsistent opinions between them for edges. Energy Flow assigns each node an initial energy and then some energy is propagated along the edges until the energy distribution on all nodes converges. To illustrate FaNDS we use the original data from the Fake News Challenge (FNC-1). First, the data has to be reconstructed in order to generate the Inconsistency Graph. The graph contains various subgraphs with well-defined shapes that represent different types of connections between the news items. Then the Energy Flow method is applied. The nodes with high energy are the candidates for being fake news. In our experiments, all these were indeed fake news as we checked each using several reliable web sites. We compared FaNDS to several other fake news detection methods and found it to be more sensitive in discovering fake news items.",2022,1453
Identification of Chinese dark jargons in Telegram underground markets using context-oriented and linguistic features,Yiwei Hou and Hailin Wang and Haizhou Wang,"When cybercriminals communicate with their customers in underground markets, they tend to use secure and customizable instant messaging (IM) software, i.e. Telegram. It is a popular IM software with over 700 million monthly active users (MAU) up to June 2022. In recent years, more and more dark jargons (i.e. an innocent-looking replacement of sensitive terms) appear frequently on Telegram. Therefore, jargons identification is one of the most significant research perspectives to track online underground markets and cybercrimes. This paper proposes a novel Chinese Jargons Identification Framework (CJI-Framework) to identify dark jargons. Firstly, we collect chat history from Telegram groups that are related to the underground market and construct the corpus TUMCC (Telegram Underground Market Chinese Corpus), which is the first Chinese corpus in jargons identification research field. Secondly, we extract seven brand-new features which can be classified into three categories: Vectors-based Features (VF), Lexical analysis-based Features (LF), and Dictionary analysis-based Features (DF), to identify Chinese dark jargons from commonly-used words. Based on these features, we then run a statistical outlier detection to decide whether a word is a jargon. Furthermore, we employ a word vector projection method and a transfer learning method to improve the effect of the framework. Experimental results show that CJI-Framework achieves a remarkable performance with an F1-score of 89.66%. After adaptation for English, it performs better than state-of-the-art English jargons identification method as well. Our built corpus and code have been publicly released to facilitate the reproduction and extension of our work.",2022,1454
A model to detect domain names generated by DGA malware,T Divya and P.P Amritha and Sangeetha Viswanathan,"Command and control(C&C) servers are being more frequently used in cyberattacks in recent years. A malware-infected machine is controlled and directed by an attacker using a command-and-control server in order to steal data from the network. To hide their servers, attackers commonly employ a domain generation algorithm that generates domain names for them by concatenating words from word lists. Some of the algorithmically-generated domain names are used to connect to the C&C server. With the emergence of sophisticated domain generation algorithms, detecting such domains has become a challenge, which in turn poses a severe danger to computer networks. In this paper, we are proposing a concept called centrality, which is used as one of the features to analyze the words in the domain names generated by the domain generation algorithm malware. For classification, we are using Naïve Bayes, KNN, SVM, Decision Trees, Random Forest and logistic regression. Experimental results showed that Random Forest gave the highest classification accuracy rate of 88.64% and Naive Bayes gave the lowest accuracy of 44.32%.",2022,1455
AI-Based human audio processing for COVID-19: A comprehensive overview,Gauri Deshpande and Anton Batliner and Björn W. Schuller,"The Coronavirus (COVID-19) pandemic impelled several research efforts, from collecting COVID-19 patients’ data to screening them for virus detection. Some COVID-19 symptoms are related to the functioning of the respiratory system that influences speech production; this suggests research on identifying markers of COVID-19 in speech and other human generated audio signals. In this article, we give an overview of research on human audio signals using ‘Artificial Intelligence’ techniques to screen, diagnose, monitor, and spread the awareness about COVID-19. This overview will be useful for developing automated systems that can help in the context of COVID-19, using non-obtrusive and easy to use bio-signals conveyed in human non-speech and speech audio productions.",2022,1456
"FESTUNG 1.0: Overview, usage, and example applications of the MATLAB/GNU Octave toolbox for discontinuous Galerkin methods",Balthasar Reuter and Hennes Hajduk and Andreas Rupp and Florian Frank and Vadym Aizinger and Peter Knabner,"The present work documents the current state of development for our MATLAB/GNU Octave-based open source toolbox FESTUNG (Finite Element Simulation Toolbox for UNstructured Grids). The goal of this project is to design a user-friendly, research-oriented, yet computationally efficient software tool for solving partial differential equations (PDEs). Since the release of its first version, FESTUNG has been actively used for research and teaching purposes such as the design of novel algorithms and discretization schemes, benchmark studies, or just providing students with an easy-to-learn software package to study advanced numerical techniques and good programming practices. For spatial discretization, the package employs various discontinuous Galerkin (DG) methods, while different explicit, implicit, or semi-implicit Runge–Kutta schemes can be used for time stepping. The current publication discusses the most important aspects of our toolbox such as the code design concepts and various discretization procedures illustrated in some detail using a standard advection–diffusion–reaction equation. Moreover, we present selected applications already supported in FESTUNG including solvers for the two-dimensional shallow-water equations, the Cahn–Hilliard equation, and a coupled multi-physics model of free surface/subsurface flow.",2021,1457
An Ensemble Approach For Algorithmically Generated Domain Name Detection Using Statistical And Lexical Analysis,P. Mohan Anand and T. Gireesh Kumar and P.V. Sai Charan,"Domain Generation Algorithms are the new source of mediators which will provide the attackers an intelligent way of avoiding detection at the host level. Typically, before the existence of DGA, the malware was having a hardcoded command and control (C&C) IP address. That hardcoded mechanism is prone to detection and thus how DGA came into existence. Domain Generation Algorithms use the traditional cryptographic principles of Pseudo-random number generators (PRNGs) to generate a list of domain names to which malware communicates. In this paper, we constructed a list of 44 features (lexical+statistical) from domain names and used the ensemble approaches like C5.0, Random Forest, Gradient Boosting and CART to classify DGA domain names. C5.0 stands out as the best one with an accuracy value of 0.9704.",2020,1458
"Tweet, like, subscribe! Understanding leadership through social media use",Michael J. Matthews and Samuel H. Matthews and Dawei(David) Wang and Thomas K. Kelemen,"The proliferation of digital data has opened the door for a 21st-century social science that explores human relationships on an unprecedented scale. A particular area of interest is that of leader social media (SM) usage. As studies on leader SM usage have grown dramatically in the past several years, we take stock of the extant literature across various research disciplines. Within this manuscript, we contextualize leader SM usage and demonstrate how it compares to analogous concepts. We subsequently abridge relevant findings and reflect on methodological and theoretical components of the research studies identified in this review. Further, we outline the nature of SM data and provide practical recommendations for leadership scholars to capitalize on this rich data source in their investigations. We also offer a theoretical framework and summary of how scholars have studied leader SM usage. Specifically, this review article synthesizes the current literature while also elevating the academic rigor of leader SM research.",2022,1459
"A survey on deep learning for cybersecurity: Progress, challenges, and opportunities",Mayra Macas and Chunming Wu and Walter Fuertes,"As the number of Internet-connected systems rises, cyber analysts find it increasingly difficult to effectively monitor the produced volume of data, its velocity and diversity. Signature-based cybersecurity strategies are unlikely to achieve the required performance for detecting new attack vectors. Moreover, technological advances enable attackers to develop sophisticated attack strategies that can avoid detection by current security systems. As the cyber-threat landscape worsens, we need advanced tools and technologies to detect, investigate, and make quick decisions regarding emerging attacks and threats. Applications of artificial intelligence (AI) have the potential to analyze and automatically classify vast amounts of Internet traffic. AI-based solutions that automate the detection of attacks and tackle complex cybersecurity problems are gaining increasing attention. This paper comprehensively presents the promising applications of deep learning, a subfield of AI based on multiple layers of artificial neural networks, in a wide variety of security tasks. Before critically and comparatively surveying state-of-the-art solutions from the literature, we discuss the key characteristics of representative deep learning architectures employed in cybersecurity applications, we introduce the emerging trends in deep learning, and we provide an overview of necessary resources like a generic framework and suitable datasets. We identify the limitations of the reviewed works, and we bring forth a vision of the current challenges of the area, providing valuable insights and good practices for researchers and developers working on related problems. Finally, we uncover current pain points and outline directions for future research to address them.",2022,1460
Supporting unknown number of users in keystroke dynamics models,Itay Hazan and Oded Margalit and Lior Rokach,"In recent years, keystroke dynamics has gained popularity as a reliable means of verifying user identity in remote systems. Due to its high performance in verification and the fact that it does not require additional effort from the user, keystroke dynamics has become one of the most preferred second factor of authentication. Despite its prominence, it has one major limitation: keystroke dynamics algorithms are good at fitting a model to one user and one user only. When such algorithms try to fit a model to more than one user, the verification accuracy decreases dramatically. However, in real-world applications it is common practice for two or more users to use the same credentials, such as in shared bank accounts, shared social media profiles, and shared streaming licenses which allow multiple users in one account. In these cases, keystroke dynamics solutions become unreliable. To address this limitation, we propose a method that can leverage existing keystroke dynamics algorithms to automatically determine the number of users sharing the account and accurately support accounts that are shared with multiple users. We evaluate our method using eight state-of-the-art keystroke dynamics algorithms and three public datasets, with up to five different users in one model, achieving an average improvement in verification of 9.2% for the AUC and 8.6% for the EER in the multi-user cases, with just a negligible reduction of 0.2% for the AUC and 0.3% for the EER in the one-user cases.",2021,1461
"Sparse Bayesian ARX models with flexible noise distributions⁎⁎This work was supported by the Australian Research Council Discovery Project DP140104350. The EEG data was kindly provided by Eline Borch Petersen and Thomas Lunner at Eriksholm Research Centre, Oticon A/S, Denmark.",Johan Dahlin and Adrian Wills and Brett Ninness,"This paper considers the problem of estimating linear dynamic system models when the observations are corrupted by random disturbances with nonstandard distributions. The paper is particularly motivated by applications where sensor imperfections involve significant contribution of outliers or wrap-around issues resulting in multi-modal distributions such as commonly encountered in robotics applications. As will be illustrated, these nonstandard measurement errors can dramatically compromise the effectiveness of standard estimation methods, while a computational Bayesian approach developed here is demonstrated to be equally effective as standard methods in standard measurement noise scenarios, but dramatically more effective in nonstandard measurement noise distribution scenarios.",2018,1462
A Feature Similarity Machine Learning Model for DDoS Attack Detection in Modern Network Environments for Industry 4.0,Swathi Sambangi and Lakshmeeswari Gondi and Shadi Aljawarneh,"ABSTRACT
Recent advancements in artificial intelligence and machine learning technologies have laid the flagstone for the fourth industrial revolution, Industry 4.0. The industry 4.0 is at a very high momentum when compared to previous revolutions witnessed by humans in a way which was never anticipated. Cyber Physical Systems and Cloud computing are the basis for Industry 4.0. An ongoing research challenge in cloud computing is the immediate need to address security and data availability challenges coined in modern networking environments. For instance, DDoS attacks in cloud are continuously throwing new challenges to network community which makes detection of these attacks, an ongoing research challenge with respect to cloud security. At the outset, the research reported in this work has addressed three important contributions (i) A new gaussian based traffic attribute-pattern similarity function for evolutionary feature clustering to achieve feature transformation-based dimensionality reduction, (ii) A Gaussian based network traffic similarity function for similarity computation between network traffic instances and (iii) A machine learning model SWASTHIKA which uses feature transformation traffic for detection of low rate and high-rate network attacks. For experimental study, the most recent benchmark dataset namely IoT DoS and DDoS attack dataset available at IEEE Dataport is considered as this dataset has highly non-linear traffic instances which are like the real-world traffic. The performance evaluation of the proposed machine learning model SWASTHIKA is done by considering various classifier evaluation parameters such as accuracy, precision, detection rate, and F-Score. The experiment results proved that the attack detection rate of SWASTHIKA is significantly better compared to state of art machine learning classifiers.",2022,1463
Chapter 2 - Reconnaissance,Jeremy Faircloth,This chapter covers information gathering by focusing on reconnaissance and learning as much about a target as possible before you actually interact with it. This is typically a very stealthy part of penetration testing and is the first step in gathering the information that you need to move forward with testing.,2017,1464
Lightweight collaborative anomaly detection for the IoT using blockchain,Yisroel Mirsky and Tomer Golomb and Yuval Elovici,"Due to their rapid growth and deployment, the Internet of things (IoT) have become a central aspect of our daily lives. Unfortunately, IoT devices tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques, such as anomaly detection, can be used to secure these devices in a plug-and-protect manner. However, anomaly detection models must be trained for a long time in order to capture all benign behaviors. Furthermore, the anomaly detection model is vulnerable to adversarial attacks since, during the training phase, all observations are assumed to be benign. In this paper, we propose (1) a novel approach for anomaly detection and (2) a lightweight framework that utilizes the blockchain to ensemble an anomaly detection model in a distributed environment. Blockchain framework incrementally updates a trusted anomaly detection model via self-attestation and consensus among the IoT devices. We evaluate our method on a distributed IoT simulation platform, which consists of 48 Raspberry Pis. The simulation demonstrates how the approach can enhance the security of each device and the security of the network as a whole.",2020,1465
Simulating seasonal to multi-decadal variation in lake thermal response to meteorological forcing using the UCLAKE 1-dimensional model code,Luis A. Morales-Marín and Jon R. French and Helene Burningham and Chris Evans and Annette Burden,"Lake temperature responses to climate forcing are of interest on account of the important linkages between water temperature and ecosystem processes. This paper describes a new 1-dimensional (1D) numerical model code and its application to investigations of multi-scale linkages between the vertical temperature structure and meteorological forcing. UCLAKE is implemented as highly portable open-source software, based on computationally efficient algorithms, and able to resolve sub-daily (e.g., hourly) dynamics while retaining the efficiency to simulate multi-decadal time scales. A UCLAKE model is calibrated and validated against thermistor profile time series for a small upland lake in North Wales, UK. Some of the challenges in 1D model calibration are explored and a sensitivity analysis reveals a dependence of optimal parameter set values on water column depth and time. An exploratory 52-year hindcast simulation demonstrates the computational efficiency of UCLAKE for multi-decadal studies of trends in lake temperature that vary with depth. A supplementary application of UCLAKE to Windermere, in the English Lake District, demonstrates its performance for larger and deeper lakes.",2021,1466
DG Embeddings: The unsupervised definition embeddings learned from dictionary and glossary to gloss context words of Cloze task,Xiaodong Liu and Rafal Rzepka and Kenji Araki,"For both humans and machines to acquire vocabulary, it is effective to learn words from context while using dictionaries as an auxiliary tool. It has been shown in previous linguistic studies that for humans, glossing either target words to be learned or words comprising context is an effective approach. For machines, however, previous NLP studies are mainly focused on the former. In this paper, we investigate the potentiality of context words-glossed setting. During pre-training BERT, to infuse context words with semantic features of glosses, we propose DG embeddings — the unsupervised definition embeddings learned from dictionaries and glossaries. To employ unsupervised learning is inspired by a real-world scenario of dictionary use called headword search. This can also prevent a technical duplicate from happening, as learning words from context is already based on auto-encoding models with self-supervised learning. BERT-base is used for evaluation, and we refer to BERT-base with DG embeddings as DG-BERT. According to our experimental results, compared to the vanilla BERT, DG-BERT shows the following strengths: faster pre-training convergence, noticeable improvements on various downstream tasks, a better grasp of figurative semantics, more accurate self-attention for collocation of phrases, and higher sensitivity to context words for target-word predictions in psycholinguistic diagnostics.",2024,1467
Towards filtering undesired short text messages using an online learning approach with semantic indexing,Renato M. Silva and Tulio C. Alberto and Tiago A. Almeida and Akebo Yamakami,"The popularity and reach of short text messages commonly used in electronic communication have led spammers to use them to propagate undesired content. This is often composed by misleading information, advertisements, viruses, and malwares that can be harmful and annoying to users. The dynamic nature of spam messages demands for knowledge-based systems with online learning and, therefore, the most traditional text categorization techniques can not be used. In this study, we introduce the MDLText, a text classifier based on the minimum description length principle, to the context of filtering undesired short text messages. The proposed approach supports incremental learning and, therefore, its predictive model is scalable and can adapt to continuously evolving spamming techniques. It is also fast, with computational cost increasing linearly with the number of samples and features, which is very desirable for expert systems applied to real-time electronic communication. In addition to the dynamic nature of these messages, they are also short and usually poorly written, rife with slangs, symbols, and abbreviations that difficult text representation, learning, and filtering. In this scenario, we also investigated the benefits of using text normalization and semantic indexing techniques. We showed these techniques can improve the text content quality and, consequently, enhance the performance of the expert systems for spamming detection. Based on these findings, we propose a new hybrid ensemble approach that combines the predictions obtained by the classifiers using the original text samples along with their variations created by applying text normalization and semantic indexing techniques. It has the advantages of being independent of the classification method and the results indicated it is efficient to filter undesired short text messages.",2017,1468
Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web,Pouya Zangeneh and Brenda McCabe,"The fourth industrial revolution has affected most industries, including construction and those within the delivery chain of megaprojects. These major paradigm shifts, however, did not considerably improve the track record in predicting project outcomes and estimating required resources. One reason is the lack of unified data definitions and expandable knowledge representation across project lifecycle to represent megaprojects for analytics. This paper proposes and evaluates a unified ontology for project knowledge representation that facilitates data collection, processing, and utilization for industrial megaprojects through their lifecycle. The proposed Uniform Project Ontology, or UPonto, provides a data infrastructure for project analytics by enabling logical deductions and inferences, and flexible expansion and partitioning of the data utilizing linked data and the semantic web. The ontology facilitates cost normalization processes, temporal queries, and graph queries using SPARQL, while defining universal semantics for a wide range of project risk factors and characteristics based on comprehensive research of the empirical project risk and success literature augmented by practical considerations gained through expert consultations. UPonto forms the basis for a project knowledge graph to utilize unstructured data; it as well provides semantic definitions for smart IoT agents to consume project risk data and knowledge.",2020,1469
Intelligent approach to build a Deep Neural Network based IDS for cloud environment using combination of machine learning algorithms,Zouhair Chiba and Noreddine Abghour and Khalid Moussaid and Amina {El omri} and Mohamed Rida,"The appealing features of Cloud Computing continue to fuel its adoption and its integration in many sectors such industry, governments, education and entertainment. Nevertheless, uploading sensitive data to public cloud storage services poses security risks such as integrity, availability and confidentiality to organizations. Moreover, the open and distributed (decentralized) structure of the cloud has resulted this class of computing, prone to cyber attackers and intruders. Thereby, it is imperative to develop an anomaly network intrusion system to detect and prevent both inside and outside assaults in cloud environment with high detection precision and low false warnings. In this work, we propose an intelligent approach to build automatically an efficient and effective Deep Neural Network (DNN) based anomaly Network IDS using a hybrid optimization framework (IGASAA) based on Improved Genetic Algorithm (IGA) and Simulated Annealing Algorithm (SAA). The IDS resulted is called “MLIDS” (Machine Learning based Intrusion Detection System). Genetic Algorithm (GA) is improved through optimization strategies, namely Parallel Processing and Fitness Value Hashing, which reduce execution time, convergence time and save processing power. Moreover, SAA was incorporated to IGA with the aim to optimize its heuristic search. Our approach consists of using IGASAA in order to search the optimal or near-optimal combination of most relevant values of the parameters included in construction of DNN based IDS or impacting its performance, like feature selection, data normalization, architecture of DNN, activation function, learning rate and Momentum term, which ensure high detection rate, high accuracy and low false alarm rate. For simulation and validation of the proposed method, CloudSim 4.0 simulator platform and three benchmark IDS datasets were used, namely CICIDS2017, NSL-KDD version 2015 and CIDDS-001. The implementation results of our model demonstrate its ability to detect intrusions with high detection accuracy and low false alarm rate, and indicate its superiority in comparison with state-of-the-art methods.",2019,1470
On the effects of using word2vec representations in neural networks for dialogue act recognition,Christophe Cerisara and Pavel Král and Ladislav Lenc,"Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.",2018,1471
New optimization algorithms for neural network training using operator splitting techniques,Cristian Daniel Alecsa and Titus Pinţa and Imre Boros,"In the following paper we present a new type of optimization algorithms adapted for neural network training. These algorithms are based upon sequential operator splitting technique for some associated dynamical systems. Furthermore, we investigate through numerical simulations the empirical rate of convergence of these iterative schemes toward a local minimum of the loss function, with some suitable choices of the underlying hyper-parameters. We validate the convergence of these optimizers using the results of the accuracy and of the loss function on the MNIST, MNIST-Fashion and CIFAR 10 classification datasets.",2020,1472
Fake news virality: Relational niches and the diffusion of COVID-19 vaccine misinformation,Chen-Shuo Hong,"This study explores why some fake news publishers are able to propagate misinformation while others receive little attention on social media. Using COVID-19 vaccine tweets as a case study, this study combined the relational niche framework with pooled and multilevel models that address the unobserved heterogeneity. The results showed that, as expected, ties to accounts with more followers were associated with more fake news tweets, retweets, and likes. However, more surprisingly, embedding with fake news publishers had an inverted U-shaped association with diffusion, whereas social proximity to mainstream media was positively associated. Although the effect of influential users is in line with opinion leader theory, the newly-identified effects of social proximity to reliable sources and embeddedness suggest that the key to fake news virality is to earn greater organizational status and modest, not overly, echo chambers. This study highlights the potential of dynamic media networks to shape the misinformation market.",2024,1473
A toolkit for localisation queries,Gabriele Marini and Jorge Goncalves and Eduardo Velloso and Raja Jurdak and Vassilis Kostakos,"While UbiComp research has steadily improved the performance of localisation systems, the analysis of such datasets remains largely unaddressed. In this paper, we present a tool to facilitate querying and analysis of localisation time-series with a focus on semantic localisation. Drawing on well-established models to represent movement and mobility, we first develop a query language for localisation datasets. We then develop a software library in R that implements this querying. We use case studies to demonstrate how our programming tool can be used to query localisation datasets. Our work addresses an important gap in localisation research, by providing a flexible tool that can model and analyse localisation data programmatically and in real time.",2024,1474
Chapter 1 - Rigor and reproducibility in genetic research and the effects on scientific reporting and public discourse,Monika H.M. Schmidt and Douglas F. Dluzen,"The scientific method is the fundamental framework used to make observations, identify and address unanswered questions, and interpret outcomes against the context of existing knowledge and predictions. As scientific investigations become increasingly complex and are conducted with rapidly evolving technologies, a high degree of rigor is necessary to develop and conduct experiments while also ensuring the ability to reliably reproduce results. This introductory chapter provides a brief overview of the scientific method and highlights the challenges of rigor and reproducibility in present-day genetic and biomedical research. Furthermore, this chapter demonstrates how these challenges have impacted public discourse and trust in scientific—particularly biomedical—research. Finally, suggestions for addressing these challenges are presented, including the use of “open science” to redefine research parameters and encourage collaboration.",2024,1475
Malware Persistence Mechanisms,Zane Gittins and Michael Soltys,"In the public imagination Cybersecurity is very much about malware, even though malware constitutes only part of all the threats faced by Cybersecurity experts. However, malware is still one of the best methods to gain persistent access and control of a target system. Malware is often combined with a well socially-engineered phishing attack that deceives a user to gain a foothold on a system. Once the attakcer gains a beachhead in the victim’s network, it may be used to download additional payloads and exploit vulnerabilities, to gain more control and access within a network. Using malware as their foothold, attackers are able to to conduct reconnaissance, gather intelligence (e.g., exfiltration of intellectual property) or simply inflict damage or extortion (e.g., ransomware). All of this has to be done in a way that allows an attacker to retain access for as long as possible; the ability to do so is called persistence, and this paper examines the different techniques used by malware to accomplish persistence in an ever evolving landscape.",2020,1476
"Introducing the Co-oriented Scansis (CoS) model: A case of chatbot, Lee-Luda",Heesoo Jang and Suman Lee,"This study presents the Co-oriented Scansis (CoS) model, which provides a comprehensive understanding of scansis—a recently identified crisis type integrated into the Situational Crisis Communication Theory (SCCT). Using a crisis case of Scatter Lab, a South Korean AI company, as a model case, the study applies the CoS model to analyze the perceptions and meta-perceptions of both the organization and the public regarding the crisis. The data collection involved three official statements released by Scatter Lab and an analysis of 365 reviews from the Google Play users' reviews page of Science of Love—the app used by Scatter Lab to collect intimate conversations between romantic partners. The findings highlight the utility of the CoS model in explaining how Scatter Lab's AI crisis evolved into a scansis. Specifically, the organization's failure to accurately comprehend the public's perception of the crisis (second level co-orientation) and the resulting discrepancy between the organization and the public's perceptions (third level co-orientation) contributed to moral outrage, ultimately leading to a scansis. The study concludes by discussing the theoretical contributions of the CoS model and its practical implications for crisis management.",2023,1477
Web of Things Semantic Interoperability in Smart Buildings,Amir Laadhar and Junior Dongo and Søren Enevoldsen and Frédéric Revaz and Dominique Gabioud and Torben Bach Pedersen and Martin Meyer and Brian Nielsen and Christian Thomsen,"Buildings are the largest energy consumers in Europe and are responsible for approximately 40% of EU energy consumption and 36% of the greenhouse gas emissions in Europe. Two-thirds of the building consumption is for residential buildings. To achieve energy efficiency, buildings are being integrated with IoT devices through the use of smart IoT services. For instance, a smart space heating service reduces energy consumption by dynamically heating apartments based on indoor and outdoor temperatures. The W3C recommends the use of the Web of Things (WoT) standard to enable IoT interoperability on the Web. However, in the context of a smart building, the ability to search and discover building metadata and IoT devices available in the WoT ecosystems remains a challenge due to the limitation of the current WoT Discovery, which only includes a directory containing only IoT devices metadata without including building metadata. Integrating the IoT device's metadata with building metadata in the same directory can provide better discovery capabilities to the IoT services providers. In this paper, we integrate building metadata into the W3C WoT Discovery through the construction of a Building Description JSON-LD file. This Building Description is integrated into the W3C WoT Discovery and based on the domOS Common Ontology (dCO) to achieve semantic interoperability in smart residential buildings for the WoT IoT ecosystem within the Horizon 2020 domOS project. This integration results in a Thing and Building Description Directory. dCO integrates the SAREF core ontology with the Thing Description ontology, devices, and building metadata. We have implemented and validated the WoT discovery on top of a WoT Thing and Building Description Directory. The WoT Discovery implementation is also made available for the WoT community.",2022,1478
Intelligent IoT-BOTNET attack detection model with optimized hybrid classification model,Balaganesh Bojarajulu and Sarvesh Tanwar and Thipendra Pal Singh,"The botnet have developed into a severe risk to Internet of Things (IoT) systems as a result of manufacturers ‘insufficient security policies and end users’ lack of security awareness. By default, several ports are open and user credentials are left unmodified. ML and DL strategies have been suggested in numerous latest research for identifying and categorising botnet assaults in the IoT context, but still, it has a few issues like high error susceptibility, working only with a large amount of data, poor quality, and data acquisition. This research provided use of a brand-new IoT botnet detector built on an improved hybrid classifier. The proposed work's main components are ""pre-processing, feature extraction, feature selection, and attack detection."" Following that, the improved Information Gain (IIG) model is used to choose the most reliable characteristics from the received information. To detect an attack, a hybrid classifier is utilized which can be constructed by integrating the optimized Bi-GRU with the Recurrent Neural Network (RNN). To increase the detection accuracy of IoT-BOTNETS, a novel hybrid optimization approach called SMIE (Slime Mould with Immunity Evolution) is created by conceptually integrating two conventional optimization modes: Coronavirus herd immunity optimizer (CHIO) and the Slime mould algorithm. The final output of the hybrid classifier displays the presence or absence of IoT-BOTNET attacks. The projected model's accuracy is 97%, which is 22.6%, 18.5%, 27.8%, 22.6%, and 24.8% higher than the previous models like GWO+ HC, SSO+ HC, WOA+ HC, SMA+ HC, and CHIO+ HC, respectively.",2023,1479
Human evaluation of automatically generated text: Current trends and best practice guidelines,Chris {van der Lee} and Albert Gatt and Emiel {van Miltenburg} and Emiel Krahmer,"Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated, with a particularly high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how (mostly intrinsic) human evaluation is currently conducted and presents a set of best practices, grounded in the literature. These best practices are also linked to the stages that researchers go through when conducting an evaluation research (planning stage; execution and release stage), and the specific steps in these stages. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.",2021,1480
Multi-stage hybrid algorithm-enabled optimization of sequence-dependent assembly line configuration for automotive engine,Miao Yang and Congbo Li and Ying Tang and Wei Wu and Yan Lv,"Engines are the most expensive and technology-intensive components in automobiles, so an optimized configuration of the automotive engine assembly line (AEAL) is anticipated to improve efficiency and reduce cost. The traditional methods for assembly line configuration can mainly work out a proper machine number, but they generally ignore process sequences that could also influence the buffer cost derived from the assignment of divergence and confluence buffers. Simultaneously, how to reduce the number of variables in the algorithm iteration process to improve computational efficiency is rarely considered in the existing studies. To bridge the gaps, this study proposes a multi-stage hybrid algorithm based on a backtracking searching algorithm (BSA) to realize an effective configuration that can further improve production efficiency and reduce equipment cost for sequence-dependent AEALs. First, an AEAL configuration model is developed to involve machine number and process sequence as decision variables and aims to satisfy multiple objectives concerned with equipment cost and cycle time. Then, a multi-stage hybrid algorithm is proposed to efficiently acquire the optimal solutions to machine number and process sequence in multiple stages that can improve computational efficiency. Finally, the effectiveness and superiority of the proposed method are validated via a case study. The numerical results show that the proposed method can effectively improve production efficiency and reduce equipment cost for sequence-dependent AEALs with a better convergence and diversity performance.",2023,1481
Adversarial examples: A survey of attacks and defenses in deep learning-enabled cybersecurity systems,Mayra Macas and Chunming Wu and Walter Fuertes,"Over the last few years, the adoption of machine learning in a wide range of domains has been remarkable. Deep learning, in particular, has been extensively used to drive applications and services in specializations such as computer vision, natural language processing, machine translation, and cybersecurity, producing results that are comparable to or even surpass the performance of human experts. Nevertheless, machine learning systems are vulnerable to adversarial attacks, especially in nonstationary environments where actual adversaries exist, such as the cybersecurity domain. In this work, we comprehensively survey and present the latest research on attacks based on adversarial examples against deep learning-based cybersecurity systems, highlighting the risks they pose and promoting efficient countermeasures. To that end, adversarial attack methods are first categorized according to where they occur and the attacker’s goals and capabilities. Then, specific attacks based on adversarial examples and the respective defensive methods are reviewed in detail within the framework of eight principal cybersecurity application categories. Finally, the main trends in recent research are outlined, and the impact of recent advancements in adversarial machine learning is explored to provide guidelines and directions for future research in cybersecurity. In summary, this work is the first to systematically analyze adversarial example-based attacks in the cybersecurity field, discuss possible defenses, and highlight promising directions for future research.",2024,1482
Automatically classifying source code using tree-based approaches,Anh Viet Phan and Phuong Ngoc Chau and Minh Le Nguyen and Lam Thu Bui,"Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. To overcome the limitations, this paper aims to solve software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. We propose two combination models between a tree-based convolutional neural network (TBCNN) and k-Nearest Neighbors (kNN), support vector machines (SVMs) to exploit both structural and semantic ASTs' information. In addition, to deal with high-dimensional data of ASTs, we present several pruning tree techniques which not only reduce the complexity of data but also enhance the performance of classifiers in terms of computational time and accuracy. We survey many machine learning algorithms on different types of program representations including software metrics, sequences, and tree structures. The approaches are evaluated based on classifying 52000 programs written in C language into 104 target labels. The experiments show that the tree-based classifiers dramatically achieve high performance in comparison with those of metrics-based or sequences-based; and two proposed models TBCNN + SVM and TBCNN + kNN rank as the top and the second classifiers. Pruning redundant AST branches leads to not only a substantial reduction in execution time but also an increase in accuracy.",2018,1483
The life cycle of altmetric impact: A longitudinal study of six metrics from PlumX,José Luis Ortega,"The main objective of this study is to describe the life cycle of altmetric and bibliometric indicators in a sample of publications. Altmetrics (Downloads, Views, Readers, Tweets, and Blog mentions) and bibliometric counts (Citations) (in this study, the indicators will be capitalized to differentiate them from the general language) of 5185 publications (19,186 observations) were extracted from PlumX to observe their distribution according to the publication age. Correlations between these metrics were calculated from month to month to observe the evolution of these relationships. The results showed that mention metrics (Tweets and Blog mentions) are the earliest metrics that become available most quickly and have the shortest life cycle. Next, Readers are the metrics with the highest prevalence and with the second fastest growth. Views and Downloads show a continuous growth, being the indicators with the longest life cycles. Finally, Citations are the slowest indicators and have a low prevalence. Correlations show a strong relationship between mention metrics and Readers and Downloads, and between Readers and Citations. These results enable us to create a schematic diagram of the relationships between these metrics from a longitudinal view.",2018,1484
"“Our world is worth fighting for”: Gas mask agency, copypasta sit-ins, and the material-discursive practices of the Blitzchung controversy",Elizabeth F. Chamberlain,"In 2019, video game giant Blizzard banned a competitive e-sports player who made a pro-Hong Kong statement during a post-game interview. The international game community responded with outrage, organizing both on- and offline actions to provoke change within the organization. This article examines the #BoycottBlizzard gaming counterpublic via deceptively discrete mixed methods: a new materialist investigation of protest gear and a distant reading of a Reddit dataset of 3500 posts between October 7 and 10, 2019. The investigation concludes that gas masks demonstrate nonhuman aleatory agency in the #BoycottBlizzard protest movement, by inserting subversive subtext into costumes and gameplay. Online, protestors relied heavily on other resistance tactics, including using Twitch copypasta spam; this article suggests this form of resistance functions similarly to a sit-in. Finally, the article iconographically tracks the rise and dissemination of a particular meme image representing the movement's appointed mascot, a Chinese climatologist named Mei. Ultimately, the Blitzchung counterpublic achieved only modest success; the player's ban was reversed and his prize money reinstated, but many protestors considered Blizzard's response milquetoast. However, this analysis proposes that the Blitzchung counterpublic likely emboldened the 2021 #BoycottBlizzard movement and may in some measure be responsible for its success.",2022,1485
Discovering features for detecting malicious websites: An empirical study,John McGahagan and Darshan Bhansali and Ciro Pinto-Coelho and Michel Cukier,"Website features and characteristics have shown the ability to detect various web threats – phishing, drive-by downloads, and command and control (C2). Prior research has thoroughly explored the practice of choosing features ahead of time (a priori) and building detection models. However, there is an opportunity to investigate new techniques and features for detection. We perform a comprehensive evaluation of discovering features for malicious website detection versus selecting features a priori. We gather 46,580 features derived from a response to a web request and, through a series of feature selection techniques, discover features for detection and compare their performance to those used in prior research. We build several detection models using unsupervised and supervised learning algorithms over various sampling and feature transformation scenarios. Our approach is evaluated on a diverse dataset composed of common threats on the internet. Overall, we find that discovered features can achieve more efficient and comparable detection performance to a priori features with 66% fewer features and can achieve a Matthews Correlation Coefficient (MCC) of up to 0.9008.",2021,1486
,,,2023,1487
Chapter 3 - Agitation Methods – Tumbling Agglomeration,,,1980,1488
With Suggestions for a Correlation of Psychic and Hormonal Organizations: PART II PREPUBERTY DIFFUSION AND REINTEGRATION,Judith S. Kestenberg,,1967,1489
Differential cross-section measurements in π−p charge exchange scattering from 620 to 2730 MeV/c,R.M. Brown and A.G. Clark and P.J. Duke and W.M. Evans and R.J. Gray and E.S. Groves and R.J. Ott and H.R. Renshall and T.P. Shah and A.J. Shave and J.J. Thresher and M.W. Tyrrell,Results are presented of differential cross-section measurements for the reaction π−p→π0n;π0→γγ at 22 incident pion momenta between 618 and 2724 MeV/c. The results are in good agreement with those of other experiments. They represent the first comprehensive set of high statistics measurements of the π−p charge-exchange differential cross section at closely spaced momenta in the resonance region.,1976,1490
The seasonal variation of the thermal structure of the atmosphere of Uranus,L. Wallace,"A series of time-dependent radiative/convective models are presented for the atmosphere of Uranus. The effects of atmospheric dynamics have been omitted from the models. The inclination of the pole of rotation to the pole of the orbit, approximately 90°, produces large seasonal changes in the insolation. Because of the relatively small flow of heat from the interior, these seasonal changes cause the effective temperature, which is about 60°K, to vary through the 84-year orbital period by ∼5°K at the poles, ∼4°K at ±60° latitude, ∼2°K at ±30° latitude, and ∼0.5°K at the equator. For a particular latitude, the minimum effective temperature and the maximum convective flow of heat from the interior occur near the end of the period when the sun remains below the horizon during the Uranian day. If the methane mixing ratio is not limited by its saturated vapor pressure (SVP) in the convective region, the maximum convective flow would be a few times the orbital average convective flow and persist for an interval of several years. On the other hand, if the methane mixing ratio is limited by its SVP in the convective regions, the maximum convective flow could be orders of magnitude greater than the orbital average and could persist for less than an hour. If the orbital mean internal heat flow is negligible, the difference in effective temperatures between 30 and 60° latitude would be in the range 2 to 4°K. If the internal heat is taken to be about the maximum allowable and is assumed to be redistributed in the interior in a manner to compensate for the minimum in insolation at low latitudes, the corresponding temperature difference would be in the range 12 to 2°K. In either case, the existing theory of atmospheric dynamics for the outer planets indicates that such large temperature differences will drive large-scale motions which would in turn reduce these temperature differences.",1983,1491
Algorithmic type checking for a pi-calculus with name matching and session types,Marco Giunti,"We present a type checking algorithm for establishing a session-based discipline in a π-calculus with name matching. We account for analysing processes exhibiting different behaviours in the branches of the if-then-else by imposing an affine discipline for session types. This permits to obtain type-safety or absence of communication errors while accepting processes of the form if x=y then P else 0 that install a session protocol P whenever the test succeeds, and abort otherwise. To this aim we define a type system based on a notion of context split, and we prove that it satisfies subject reduction and type-safety. We implement the type system in a split-free type checking algorithm, and we prove that processes accepted by the algorithm are well-typed. We then show that processes that are typed and do not contain Wait for deadlocks – an input and its corresponding output (or vice versa) are in the same thread instead of in parallel ones – are accepted by the algorithm, thus providing a partial completeness result. We conclude by investigating the expressiveness of the typing system and show that our theory subsumes recent works on linear and session types.",2013,1492
LA BIOSYNTHÈSE INDUITE DES ENZYMES (ADAPTATION ENZYMATIQUE),JACQUES MONOD and MELVIN COHN,,1978,1493
Table of the earth satellites launched in 1965,D.G. King-Hele and Eileen Quinn,"All known artificial satellites launched during 1965 are listed chronologically. Lifetimes, weights, dimensions and orbital details are given for instrumented satellites and their final-stage rockets. Other fragments are listed without these details. The methods used in compiling the Table are outlined.
Реферат
Дaeтcя пepeчeнь, в чpoнoлoгичecкoм пopядкe, вceч иэвecтныч иcкyccт-вeнныч cпyтникoв Эeмли, эaпyщeнныч в тeчeниe 1965г. cooбщayтcя вpeмя пpeбыaaния cпyтникoa нa opбитe, aec, paзмepы и дeтaли opбит длн ocнaщeнныч пpибopaми cпyтникoa и ич пocлeднecтyпeнчaтыч paкeт. Пepeчиcляютcя и дpyгиe oтдeльныe фaктopы, иo бeи дeтaлeй. a oбщич чepтaч oпиcыaaeтcя пpимeнeнный для cocтaaлeния этoй тaблицы cнocoб.",1966,1494
Geologic evolution and hydrocarbon habitat of the ‘Arctic Alaska Microplate’,Richard J. Hubbard and Steven P. Edrich and R. {Peter Rattey},"Depositional sequence mapping has been used to analyse the late Devonian to Recent geologic evolution and hydrocarbon habitat of north Alaska and northwest Canada. Eight depositional megasequences have been identified, each of which records a discrete, major phase of basin evolution. The three oldest megasequences are named the Ellesmerian and reflect deposition on a subsiding fold belt terrane. We name the subsequent two megasequences of early Jurassic to Aptian age, the Beaufortian. They record a 100 m.y. period of extension during which a Jurassic failed rift episode was followed by onset of the successful rift episode in the Hauterivian. This extension led to the opening of the oceanic Canada Basin. The final three megasequences record geographically distinct pulses of Brookian orogenesis. The major proven hydrocarbon habitat occurs on the Barrow Arch of north Alaska. This is a volumetrically large, but greatly restricted, hydrocarbon province, which developed as a result of constructive interference between Beaufortian rift and Brookian orogenic tectonics. Two other, relatively minor, hydrocarbon provinces have also been discovered. They are the Mackenzie Delta and Kugmallit Trough provinces of northwest Canada, which developed in passively subsided basins, located just beyond the influence of Brookian orogenic uplift.",1987,1495
Mechanizing ω-order type theory through unification,D.C. Jensen and T. Pietrzykowski,This paper deals with the problems of applying a unification procedure in a mechanization of the full type theory. The early sections describe the unification problem and an algorithm which produces a complete set of unifiers. The special case of the 2nd order problem is differentiated. Then there is presented an extensive proof of completeness of the algorithm. The later sections are devoted to application of the unification in mechanical theorem proving. A generalization of the resolution principle as well as some specialized strategies are presented.,1976,1496
On the ternary complex formation of d-glyceraldehyde-3-phosphate dehydrogenase with nicotimide-adenine dinucleotide and Ag+,L. Bross,"d-Glyceraldehyde-3-phosphate dehydrogenase forms a ternary complex with Ag+ and nicotinamide-adenine dinucleotide accompanied by the loss of enzymic activity and by a change of light absorption in the 300–400 mμ region. The ternary complex shows maximum absorption at 335 mμ. The ternary complex can be crystallized. The formation of the complex is reversible, the native enzyme-coenzyme complex being restored by the addition of mercaptoethanol. Maximum yield of the complex is obtained when about 4 moleequiv of Ag+ are added; the addition of more Ag+ results in the disintegration of the ternary complex and nicotinamide-adenine dinucleotide is severed from the enzyme.",1965,1497
Chapter 11 Neural mechanisms underlying corticospinal and rubrospinal control of limb movements,Paul D. Cheney and Eberhard E. Fetz and Klaus Mewes,"Publisher Summary
This chapter reviews the present understanding of the organization and functional properties of descending systems and recent advances that have come from single unit recording in awake monkeys using new techniques that reveal the synaptic connections of single premotor neurons with motoneurons of agonist and antagonist muscles. The technique of spike-triggered averaging of electromyogram activity in awake monkeys is yielding new information at the level of individual premotor neurons concerning the sign, strength, and distribution of synaptic effects from descending systems to spinal motoneurons. The chapter emphasizes the role of descending systems in the control of limb movements, although it is recognized that locomotion and other motor behaviors involving axial, head, and/or facial muscles may involve similar principles. It also emphasizes motor control in primates, although relevant data from the cat and other species will be included where no primate data are available.",1991,1498
On two-particle correlations in high energy production experiments,A. Bassetto and M. Toller and L. Sertorio,"One-particle and two-particle inclusive distributions are considered in the light of future production experiments at CERN Intersecting Storage Rings machine. We discuss the general properties of those models which are based on factorized approximations to the production amplitudes and are in agreement with some general hypotheses like the Feynman scaling law. First the role of phase space arguments is clarified and the importance of detecting correlations in high energy production experiments as an hint for understanding the underlying dynamics is emphasized. Then two peculiar (and, in a sense, complementary) kinds of correlation in two-particle inclusive distributions are examined: the one coming from multiperipheral dynamics with Lorentz pole exchanges, the other due to “direct channel” resonances. Some general properties of inclusive distributions are expressed in terms of quantities which are suitable for direct measurement.",1971,1499
¶ List of French patents,,,1833,1500
7 - Managing Access to Your Server Resources,Alan Winston,"Publisher Summary
There are two sides to managing access to the server resources, namely outside and the inside. It is important to control which and how many entities from outside the system can get at what's inside and in what way. This, however, brings up the issues of user identification, either anonymously or identifiably. Resource mapping, access control, and authentication are often closely intertwined. The server maps the URL into a resource name recognizable by the operating system and determines any access restrictions on that resource. If the access is restricted to particular users, than the server authenticate the requester as one of the users. Several types of authentications are available to web server and structurally all of them work the same way. There are files that only certain people should be able to run. This can be achieved by restricting or allowing access to pages in a variety of ways such as using the IP address it comes from and using the “Referer” header. In this context, each of the servers has particular strengths and weaknesses as Ohio State University DECthreads HTTP Server (OSU) have different people authenticate in different ways, some against the UAF and some against a file-specific password. Compaq Secure Web Server (CSWS) uses arbitrarily complicated access-control logic.",2003,1501
"Report of the European meeting on the microbiology of irradiated foods: Paris, 20–23 April 1960",,,1963,1502
Egineering and design,,,1993,1503
"The development of the cultural landscape around Diss Mere, Norfolk, UK, during the past 7000 years",Sylvia M. Peglar,"Diss Mere is a small lake around which the town of Diss has developed. Pollen analysis of the lake sediments deposited during the last 7000 years yielded a rich pollen and spore flora. Numerical methods were used to sort the pollen taxa into recurrent groups, which are groups of taxa with similar occurrences through time. With the aid of the recurrent groups the pollen diagram was interpreted in terms of the vegetational history of the catchment. The calcareous sediments were unsuitable for radiocarbon dating, but a chronology was established by correlation with nearby sites and by comparison with historical records. People may have lived in the mid-Holocene forest, and created small clearances prior to the Ulmus decline. After the Ulmus decline at ca. 3000 BC, the forest became more open, but eventually human activity declined, and clearings were colonised by secondary scrub. Subsequently, Bronze Age people lived by the lake. They cleared substantial parts of the Quercus/Corylus-dominated forests on the slopes and the Tilia-dominated forest on the plateau above. Dereliction and scrub development, particularly by Taxus, at the end of the Bronze Age, was followed by Iron Age colonisation. Superior technology lead to almost complete forest clearance and farming of the catchment, and the origin of the town. The town developed through Medieval time, and cultivation became specialised with Cannabis (hemp) and Linum (flax) cultivation. After the collapse of the hemp industry, arable farming prevailed. The lake becamme highly eutrophic due to nutrient addition from farming and from the town. Eventually a proper sewage treatment was installed. The uppermost pollen record reflects the conversion of the non-urbanised part of the catchment to parkland.",1993,1504
Chapter 13 - Windows 2000 Bastion Hosts,Robert J. Shimonski and Will Schmied and Thomas W. Shinder and Victor Chang and Drew Simonis and Damiano Imperatore,"Publisher Summary
This chapter focuses on the Windows 2000 Server referred as a popular choice for deployment to Internet-facing networks. Despite Microsoft's poor reputation for security in such hostile environments, it is quite possible to secure Windows 2000. Windows 2000's built-in security mechanisms highlight the poor planning that all too often accompanies a new Windows 2000 DMZ server build. The chapter assists the systems engineer to avoid the most common pitfalls in deploying Windows 2000 to a hostile Internet-facing environment using mostly on onboard, built-in tools. Windows 2000 gives some planning, forethought, and creative nondefault configuration steps. Windows 2000 is too risky at the time of building DMZ hosts; too many older, less secure defaults are retained in the name of backward compatibility. Windows 2000 has the feature of “remembering” service pack installed files and so newly installed components install using the most current service pack version by referencing the source files listed. This somewhat under documented feature enhancement of Windows 2000, introduced in Service Pack 1, saves a significant amount of time and eliminates the human error that used to be associated with making minor system state changes to a running Windows server.",2003,1505
Keyword index to volumes 400–420,,,1997,1506
A Transition State Analog for Lysozyme,Isaac I. Secemski and Sherwin S. Lehrer and Gustav E. Lienhard,"The δ-lactone derived from tetra-N-acetylchitotetraose (TACL) has been prepared by oxidation of tetra-N-acetyl-chitotetraose with iodine. The binding of TACL to lysozyme has been investigated by its inhibition of the lysozymecatalyzed lysis of Micrococcus lysodeikticus cells and by its perturbation of the tryptophyl fluorescence spectrum of lysozyme. At pH 6.2 the concentration of TACL that is required for 50% inhibition of the rate of lysis is 0.7 µm, which is 1/110 of the concentration of the unmodified tetrasaccharide that is required for such inhibition. The association constants for the binding of TACL to lysozyme over the pH range from 2 to 8 were obtained by fluorescence measurements. Their pH dependence shows that TACL binds most strongly to the species of lysozyme in which the carboxyl group of glutamate 35 is dissociated. In agreement with this result, the fluorescence-pH profile of the TACL-lysozyme complex indicates that the pK of glutamate 35 is about 4.7 in the complex, whereas the pK of glutamate 35 in the enzyme alone is about 6.0. The value of the association constant for the binding of TACL at pH 5.0 and 25° is 3.3 x 106m-1, which is 32 times larger than that for the binding of the unmodified tetrasaccharide under the same conditions. On the basis of these results and of the similarity between the known conformation of the lactone ring and the proposed conformation of the transition state for lysozyme-catalyzed reactions (both half-chair ones), we conclude that TACL is a transition state analog for lysozyme. Furthermore, with these results we can estimate that the affinity of Subsite D of lysozyme for the half-chair conformation of the pyranose ring of N-acetylglucosamine is greater by a factor of 6 x 103 than its affinity for the chair conformation and thus contributes this factor to catalysis.",1972,1507
The permeability of the corneal epithelium and endothelium to water,Sahchi Mishima and Bengt O. Hedbys,"The anterior chamber of the enucleated rabbit eye was perfused with tissue culture medium (TC 199); the temperature of the medium (35°C), the intraocular pressure (20 mmHg), and the rate of perfusion (0·33 cc/min) were controlled. The corneal surface of the living rabbit was bathed with 0·9% sodium chloride solution after the aqueous humor had been replaced with silicone oil. These procedures did not affect the normal corneal thickness for a period of 4 hr. Various degrees of hypertonicity were produced in the perfusing solutions by the addition of various solutes, i.e. sodium chloride, urea, glucose, sucrose, raflinose, and albumin. When a cornea came into contact with a hypertonic solution, its thickness rapidly decreased, reaching a new steady state in 10–15 min. When TC 199 or 0·9% sodium chloride solution was then applied, the thickness rapidly increased to the original level. An analysis of the time sequence of the changes in corneal thickness, from the viewpoint of the thermodynamics of irreversible processes, yielded values for the hydraulic conductivity of the corneal epithelium and endothelium and for their reflection coefficients with the different solutes used. The hydraulic conductivity of the epithelium and endothelium, respectively, was approximately 1·0 × 10−4 mm/min msOm l−1 and 2·3 × 10−4 mm/min mOsm l−1. The reflection coefficient of the epithelium was 1 with all solutes used; that of the endothelium was 0·6 with urea and sodium chloride and 1 with solutes of larger molecular weights than glucose.",1967,1508
"Towards an improved serum-free, chemically defined medium for long-term culturing of cerebral cortex tissue",H.J. Romijn and F. {van Huizen} and P.S. Wolters,"The present study describes a series of experiments which have led to a substantially improved serum-free, chemically defined medium (CDM) for long-term culturing of reaggregated fetal rat cerebral cortex tissue. A reduction of the original medium concentrations of the hormones insuline, T3 and corticosterone, on the one hand, and an enrichment of the medium with the vitamins A, C and E, the unsaturated fatty acids linoleic and linolenic acid, and biotin, L-carnitine, D(+)-galactose, glutathione (reduced) and ethanolamine, on the other hand, formed the most important chemical adjustments of the medium. With the aid of this CDM (encoded R12), the light- and electron microscopic architecture of the tissue could be kept in a good condition (superior to that seen earlier in serum-supplemented medium) up to 23 days in vitro. From that time on, the neuronal network lying between the reaggregates degenerated for the largest part, while a portion of the large neurons (probably pyramidal cells) plus some of the neuronal network within the reaggregates degenerated too. This degeneration process continued during the following weeks, but the reaggregates nevertheless retained most of their mass, so that both small and large neuronal cell bodies (visible in transparent regions at the edge of the reaggregates) remained in good condition up to at least 103 DIV. Stout, thick nerve bundles interconnecting the reaggregates, also survived up to this point. Electron microscopic evaluation of such ‘aged’ reaggregates revealed degenerating as well as healthy regions. The latter had indeed retained healthy-looking pyramidal and non-pyramidal neurons, embedded within a dense neuropil which was often traversed by myelinated axons. The numerical synapse density in such selected, healthy tissue regions reached its maximum during the sixth week in vitro, followed by a rapid decrease and a stabilization at about half the peak values. The present culture system has opened the possibility for performing controlled quantitative studies on the relationship between structure and function of cerebral cortex tissues during development and aging, on its dependence on nutrients, hormones and drugs, and on special factors synthesized by the tissue and released into the nutrient medium.",1984,1509
Prokaryotes and lower eukaryotes,,,1992,1510
Immunoblotting and dot blotting,D.I. Stott,,1989,1511
African horse sickness virus structure,Polly Roy and Peter {P.C. Mertens} and Ignacio Casal,"African horse sickness virus (AHSV), of which there are nine serotypes (AHSV-1, -2, etc.), is a member of Orbivirus genus within the Reoviridae family. Both in morphology and molecular constituents AHSV particles are comparable to those of bluetongue virus (BTV), the prototype virus of the genus. The two viruses have seven structural proteins (VP1–7) organized in two layered capsid. The outer capsid is composed of VP2 and VP5. The inner capsid, or core, is composed of two major proteins, VP3 and VP7, and three minor proteins, VP1, VP4 and VP6. Within the core is the virus genome. This genome consists of 10 double-stranded (ds)RNA segments of different sizes, three large, designated L1–L3, three medium, M4–M6, and four small, S7–S10. In addition to the seven stuctural proteins that are coded by seven of the RNA species, four non-structural proteins, NS1, NS2, NS3 and NS3A, are coded by three RNA segments, M5, S8 and S10. The two smallest proteins (NS3 and NS3A) are synthesized by the S10 RNA segment, probably from different in-frame translation initiation codons. Nucleotide sequences of eight RNA segments (L2, L3, M4, M5, M6, S7, S8 and S10) and the predicted amino acid sequences of the encoded gene products are also available, mainly representing one serotype, AHSV-4. In this review the properties of the AHSV genes and gene products are discussed. The sequence and hybridization analyses of the different AHSV dsRNA segments indicate that the segments that code for the core proteins, as well as those that code for NS1 and NS2 proteins, are highly conserved between the different virus serotypes. However, the RNA encoding NS3 and NS3A, and the two segments encoding the outer capsid proteins, are more variable between the AHSV serotypes. A close phylogenetic relationship between AHSV, BTV and epizootic haemorrhagic disease virus (EHDV), three Culicoides-transmitted orbiviruses, has been revealed when the equivalent sequences of genes and gene products are compared. Recently, the four major AHSV capsid proteins have been expressed using recombinant baculoviruses. Biochemically and antigenically these proteins are similar to the authentic proteins. Since the AHSV VP7 protein is highly conserved among the different serotypes, it has been utilized as a diagnostic reagent. The expressed VP7 protein has also been purified to homogeneity and crystallized for three-dimensional X-ray analysis. The expressed outer capsid proteins, VP2 and VP5, have been purified and used to raise antisera in rabbits. The VP2 antisera neutralize virus infections in vitro indicating the importance of this protein for vaccine development.
Résumé
Le virus de la maladie du cheval Africain (AHSV), pour lequel il existe neuf sérotypes (AHSV-1, -2, etc.), est un membre du genre des Orbivirus dans la famille des Reoviridae. Les composants morphologiques et moléculaires des particules de l'AHSV sont comparables à celles du virus “bluetongue” (BTV), virus référence de ce genre. Ces deux virus possèdent sept protéines structurales différentes (VP1–7) réparties en deux capsides. La capside externe est composée de VP2 et de VP5. La capside interne, ou “core”, est composée de deux protéines majeures, VP3 et VP7, et de trois protéines mineures, VP1, VP4 et VP6. Le génome, localisé dans la capside interne, est composé de dix segments d'ARN double-brin (ds) de tailles différentes, trois grands, désignes L1–L3, trois moyens, M4–M6, et quatre petits, S7–S10. En plus de sept protéines structurales codées par sept fragments d'ARN, quatre protéines non structurales, NS1, NS2, NS3 et NS3A, sont codés par les segments M5, S8 et S10, respectivement. Les deux protéines de petite taille (NS3 et NS3A) sont synthetisées à partir du segment d'ARN S10, probablement suite a l'initiation de la traduction a partir de deux phases de lectures différentes. La séquence nucléotidique de huit segments d'ARN (L2, L3, M4, M5, M6, S7, S8 et S10) ainsi qui les séquences en acide aminés qui en sont déduites, sont désormais connues, représentant le sérotype AHSV-4. Les propriétés des genes et de leurs produits sont discutées. Les analyses de séquences ainsi que celles des hybridations moléculaires des différents segments d'ARN indiquent que les segments codant pour les proteines de la capside interne (core), ainsi que ceux codant pour les protéines NS1 et NS2, sont hautement conservés. Cependant, le segment codant pour NS3 et NS3A, ainsi que deux autres segments codant pour des protéines de la capside externe, sont plus variables entre les différents sérotypes AHSV. Des relations phylogénétiques entre l'AHSV, le BTV et le virus de la maladie hémorrhagique épizootique du cerf (EHDV), trois orbivirus transmis par des moucherons, ont été mis en évidence lors de la comparaison de séquences équivalentes de gènes et de leurs produits. Les quatre protéines de capside majeures de l'AHSV ont été recemment exprimées dans le système d'expression du baculovirus recombinant. Les protéines exprimées sont biochimiquement et antigéniquement identiques aux protéines naturelles. Puisque la protéine VP7 de l'AHSV et hautement conservée dans les différents sérotypes, elle a été utilisée comme outil de diagnostic sensible de groupespécifique. La protéine VP7 exprimée a ensuite été purifiée et cristallisée pour une analyse tridimentionnelle par rayons X. Les protéines exprimées de la capside externe, VP2 et VP5 ont aussi été purifiées puis ont été utilisées pour la production d'antiserum par des lapins. L'antiserum de VP2 obtenu neutralise in vitro des infections virales, ce qui met en évidence son importance pour le développement de vaccins.",1994,1512
"Constitution and stereochemistry of enmein, a diterpene from isodon trichocarpus kudo",T. Kubota and T. Matsuura and T. Tsutsui and S. Uyeo and H. Irie and A. Numata and T. Fujita and T. Suzuki,"The total structure of enmein, a diterpene, isolated from Isodon trichocarpus Kudo has been established as shown in the formula If.",1966,1513
"New Ordovician cornutes(Echinodermata, Stylophora) from Montagne Noire and Brittany (France) and a revision of the order Cornuta Jaekel 1901: To the memory of Jean Chauvel",Bertrand Lefebvre and Daniel Vizcaino,"Plate homologies are identified and discussed in cornute stylophoran echinoderms. The main resultsare: 1) the homology of the posterior zygal plate in all cornutes and, 2) the non-homology of the spinal process, which can be borne by two distinct plates from the marginal frame. A functional analysis of stylophoran “accessory orifices” as exchange systems is realised: they are interpreted as respiratory structures. Sutural pores of Phyllocystis blayaci and cothurnopores could represent exothecal pore-structures, and lamellipores endothecal pore-structures. Other possible means of respiration are also envisaged in cornutes. A systematic revision of the Order Cornuta is also presented. The new genus Arauricystis is proposed for two species of cornutes previously assigned to the genus Cothurnocystis.. Two new species of cornutes from the Lower Arenig (Lower Ordovician) of Montagne Noire (Southern France) are described, Ampelocarpus landeyranensis nov. gen. et nov. sp. and Thoralicystis ubaghsi nov. sp.. A new cornute from the Llandeilo (Middle Ordovician) of Brittany (Western France), Scotiaecystis guilloui nov. sp. is also described. Finally, a cladistic analysis of cornutes confirms the results obtained by the identification of plate homologies: 1) cornutes and mitrates are sister-groups, 2) Ceratocystis belongs to the stem-group of both cornutes and mitrates, 3) Amygdalothecida and Cothurnocystida are sister-groups and, 4) Protocystites belongs to the stem-group of both Amygdalothecida and Cothurnocystida.
Résumé
Les homologies de plaques entre les divers genres de cornutes (échinodermes stylophores) sont identifiées et discutées. Les principaux résultats sont: 1) l'homologie de la plaque zygale postérieure chez tous les cornutes et, 2) la non-homologie de la spinale, susceptible d'être développée par deux plaques marginales distinctes. L'analyse morpho-fonctionnelle des “orifices accessoires” des stylophores en tant que système d'échange permet de les proposer comme structures respiratoires. D'autres moyens de respiration sont également envisagés chez les cornutes. Une révision systématique de l'Ordre Cornuta est également réalisée. Le genre nouveau Arauricystis est proposé pour deux espèces de cornutes attribuées jusqu'alors au genre Cothurnocystis.. Trois nouvelles espèces de cornutes sont décrites dans l'Arenig inférieur (Ordovicien inférieur) de la Montagne Noire (France méridionale), Ampelocarpus landeyranensis nov.gen. et nov. sp. et Thoralicystis ubaghsi nov. sp., et dans le Llandeilo (Ordovicien moyen) de Bretagne (Ouest de la France), Scotiaecystis guilloui nov. sp.. Enfin, l'analyse cladistique confirme les résultats obtenus par la reconnaissance des homologies de plaques, à savoir que: 1) cornutes et mitrates sont groupes-frères, 2) Ceratocystis appartient au groupe-souche des cornutes et des mitrates, 3) Amygdalothecida et Cothurnocystida sont groupes-frères et, 4) Protocystites appartient au groupe-souche des Amygdalothecida et des Cothurnocystida.",1999,1514
A REGIONALIST'S VIEW OF PUBLIC SECTOR PLANNING IN A CAPITALIST SOCIETY,CHARLES L. LEVEN,,1966,1515
Engineering materials science,C.J. McHargue and J.R. Cost and C.A. Wert,,1978,1516
Слово и премудрость (“логосная структура”): “Проглас” Константина Философа,В.Н. Топоров,,1988,1517
Improvement of Forest Growth on Poorly Drained Peat Soils,LEO HEIKURAINEN,,1964,1518
A re-examination of the organ specificity of lens antigens,R.M. Clayton and J.C. Campbell and D.E.S. Truman,"The cross-reacting antigenic determinants between lens proteins and other tissues of Xenopus laevis and the chick have been studied by immunodiffusion, immunoelectrophoresis and the Osserman technique, using antisera detceting minor as well as major antigenic determinants of the lens proteins. All tissues tested contained some antigenic determinants similar to those found in lens proteins and it was demonstrated that all of the major classes of lens proteins contained such cross-reacting groups. The pattern of cross-reactivity varied both qualitatively and quantitatively from tissue to tissue and there were no lens antigens which were found in all of the other tissues tested. The cross-reacting material from extralenticular tissue is not always identical in molecular form to that obtained from the lens. The results of the Osserman tests have been interpreted as indicating heterogeneity in a population of protein molecules which may be based either on a heteropolymer structure or on the selective binding of cross-reacting antigenic moieties. The results described are taken to mean that much tissue specificity is the result of a unique selection of antigens genetically available, any one of which may be found in other tissues, rather than being due exclusively to the possession of some specific protein restricted to that tissue.",1968,1519
Presence and Significance of Lactulose in Milk Products: A Review1,Susumu Adachi and Stuart Patton,"Summary
Lactulose is produced from lactose during the heat processing and storage of certain dairy products. This sugar is somewhat sweeter and more soluble than lactose. It also seems to have unique growth-promoting properties for certain desirable types of Lactobacilli in the intestinal tract of the infant. The chemistry and possible nutritional roles of this sugar are reviewed. The relationship of lactulose to bifidus factor and other carbohydrate moieties of milk also are considered.",1961,1520
Stratigraphic evolution of Mesozoic continental margin and oceanic sequences: Northwest Australia and northern Himalayas,Felix M Gradstein and Ulrich von Rad,"During Mesozoic time the Northwest Australian continental margin and the Thakkhola region of North Central Nepal were “adjacent” passive margins, off Northeast Gondwana. The relatively sediment-starved basins provide excellent opportunities for studying the early structural and depositional evolution of the Indian Ocean, during the Triassic/Jurassic rifting stages (Neo-Tethys), and the Jurassic/Cretaceous transition from rifting to drifting. Nepalese and Australian paleomagnetic studies indicate subtropical paleolatitudes during the Late Triassic to Early Jurassic, but higher latitudes of 35–40°S during the Late Jurassic to Early cretaceous, prior to the sustained northward flight of Australia and Greater India in the Late Cretaceous to Cenozoic. The latitudinal position of the basins and type of sediment are related. Principal continental margin formation along the northern edge of eastern Gondwana started in late Permian to Triassic time. By late Triassic-early Jurassic time platform carbonates with thin, lagoonal shales were laid down in a subtropical climate. The Carnian to “Rhaetian” siliciclastics and carbonates show repeated shallowing-upward sequences. Subsequent southward drift of “Greater India” and Australia during mid-Jurassic time replaced carbonates with more siliciclastic sediment input. Widespread erosion was caused during local uplift of parts of the Northwest Australian continental margin as a result of Jurassic late-rift block faulting. A mid-Callovian-early Oxfordian hiatus in Nepal is a submarine condensed sequence and non-tectonic in origin. In Nepal, the overlying 250 m thick organic-rich dark shales, which are correlated to the Oxfordian/Kimmeridgian clays of circum-Atlantic hydrocarbon bearing basins, can be traced along the northern Himalayan range. These shales probably represent an extensive continental slope deposit formed in a “high-productivity of organics” belt. The diverse foraminiferal microfauna of the belt was previously only known from boreal Laurasia. The Callovian “break up” uncorformity off Northwest Australia is actually a post-rift unconformity and precedes the onset of seafloor spreading by about 5–10 m.y. Seafloor spreading, leading to formation of the present Indian Ocean, started in the Argo Abyssal Plain around 155 Ma ago, in late Jurassic time. Australia and Greater India, including Northern Nepal, separated as early as in the late Valanginian, about 130 Ma ago. Altered ash layers off Northwest Australia record an important volcanic phase in Berriasian-Valanginian time. Mafic volcaniclastics in Nepalese deltaic sediments also testify to continental margin volcanic activity, which may be a precursor to the slightly younger Rajmahal traps in eastern India. An important tectonic event off Northwest Australia took place in Aptian time, 120-115 Ma ago, when hemipelagic sedimentation changed gradually into more pelagic sedimentation, with sharply decreasing rates of deposition. In Nepal, this marks the transition from coarse to fine-grained, organic-rich terrigenous clastics, leading eventually to more carbonate-bearing strata during the mid-Cretaceous global sealevel highstand.",1991,1521
Subject index,,,1984,1522
Hydrothermal behaviour of simulated waste glass—And waste—Rock interactions under repository conditions,D. Savage and N.A. Chapman,"Hydrothermal experiments have been performed investigating granodiorite-water, waste glass-water and granodiorite-waste glass-water systems in the temperature range 100–350°C at pressures of 50 or 60 MPa. Experimental equipment consisted of closed-system large-volume gas-pressurised reaction vessels. Run durations lasted from less than one day to one hundred days at fluid/solid mass ratios between 0.4 and 6. Fluids generated in the granodiorite-water experiments were alkaline (pH = 7.4–8.9) with low total dissolved solids (< 500 ppm) and low chloride (< 40 ppm) and sulphate (< 30 ppm) contents. Silica concentrations in solution approximate to theoretical values in the quartz-water system at 100° and 150°C, although 200°C fluids show evidence of re-equilibration during the quenching process. From available thermodynamic data, the chemistries of the 100° and 150°C fluids appear to be governed by feldspar-water equilibria, whilst at 200°C, feldspar-montmorillonite reactions dominate the fluids. Montmorillonite is an identified secondary phase at 200°C. From these preliminary data, the “evolved groundwaters” produced through reaction of granodiorite and water at high temperatures seem suitable for canister preservation, actinide immobilisation and compatibility with bentonite as a backfill material. Two sodium borosilicate glasses containing simulated waste components have been used in a kinetic study of dissolution at 100° and 150°C, 60 MPa. The dissolution of both glasses is governed by parabolic kinetics with run-lengths up to 14 days, and this is interpreted as being controlled by diffusion through a surface layer. This surface layer develops through incongruent dissolution of the glass and is enhanced by saturation of major components in solution and precipitation of secondary-mineral forms. The crystallinity of this alteration layer is increased by increasing time and temperature of the reaction. SEM and XRD analysis have indicated that at 100° and 150°C the layer is largely amorphous with traces of a poorly crystalline dioctahedral smectite. At 200°C the layer consists of fairly well crystallised smectite and at 350°C degradation of the glass is rapid and the solid reaction products are composed of a complex mineral assemblage including smectite, a lithium-sodium borosilicate hydrate, aegirine, riebeckite, albite, stillwellite, zektzerite and two barium molybdates. Some of these mineral phases incorporate and concentrate waste components, e.g., the zektzerite may contain up to 20 wt.% zirconium oxide, 2.5 wt.% caesium oxide and 0.7 wt.% uranium oxide. The rates of release of certain radionuclides have been investigated in granodiorite-waste glass-water systems. This study has revealed that the rate of release of a particular nuclide is a function of temperature and time. Caesium leach rates obtained in this manner are at least an order of magnitude lower than leach rates derived from refluxing dynamic leach tests. This is because incongruent dissolution of the glass, saturation of major components in solution and precipitation of mineral phases incorporating waste components in these closed-system experiments all serve to lower rates of radionuclide release. The work was carried out by N.E.R.C.-I.G.S. under contract to the U.K. Department of the Environment and the Commission of the European Communities.",1982,1523
Growth and characterization of alkali halide mixed crystals,V.Hari Babu and U.V.Subba Rao,"The study of the physical properties of alkali halide mixed crystals has been a subject of wide interest in the recent past. The main aim of this review is to provide a survey of the current state of knowledge about the nature of imperfections present and their role in understanding various properties associated with alkali halide mixed crystals. An attempt has been made to distinguish different types of mixed crystals, the conditions for the formation of a mixed crystal, the local strains that arise in the lattice due to the difference in the size of the ions that constitute the mixed crystal. The results obtained from various studies such as ionic conductivity, dielectric loss, microhardness, radiation hardening, colour centres, thermoluminescence, optical absorption etc. on alkali halide mixed crystals are presented. The non-linear variation of electrical conductivity, microhardness, half widths of many colour centre bands with composition have been discussed in terms of the concentration of various defects present in them. The aspects of the problem described include: 1.a) Dislocation density and distribution studies.2.b) Influence of dislocations and grain boundaries on transport properties of these crystals.3.c) Role of ionic size on microhardness.4.d) Lesser rate of hardening in mixed crystals due to irradiation.5.e) Stability of colour centres in mixed crystals in comparison with the end products, finally the present position and out look are summarized.",1984,1524
Chapter 21 - Preservation of Phytopathogenic Prokaryotes,JOHN P. SLEESMAN,,1982,1525
Tables of contents,,,1994,1526
Developmental biochemistry of cotton seed embryogenesis and germination: VII. Characterization of the cotton genome,Virginia Walbot and L.S. Dure,"The DNA of cotton, Gossypium hirsutum, has been characterized as to spectral characteristics, buoyant density in CsCl, base composition, and genetic complexity. The haploid genome size is found to bo 0.795 pg DNA/cell. However, the amount of DNA per cell in the cotyledons increases during embryogenesis to an average ploidy level of 12N in the mature seed cotyledons. Reassociation kinetics indicate that this increase is due to endoreduplication of the entire genome. Non-repetitive deoxynucleotide sequences account for approximately 60.5% of the cotton genome (C0t12pure¶¶Abbreviations used: C0t, product of DNA concentration (mol nucleotido l−1) × time(s); C0t12, that C0t value at which one-half of a kinetic component of DNA will have reassociated; C0t12 pure, that C0t value at which one-half of a kinetic component of DNA would have reassociated if only that component were present, calculated as the product of C0t12 × the fraction of the genome in that kinetic component; 5 MeC, 5-methylcytosine; rDNA, ribosomal RNA cistrons. = 437); highly repetitive sequences (> 10,000 repetition frequency) constitute about 7.7% of the genome. (C0t12 pure = 4.6 × 10−4) and intermediately repetitive sequences constitute the remaining 27% of the genome (C0t12 pure = 1.46). Hybridization of 125I-labeled cytoplasmic ribosomal RNA to whole-cell DNA on filters and in solution indicate approximately 300 to 350 copies of the rRNA cistrons per haploid genome. The interspersion of repetitive sequences that reassociate between C0t values of 0.1 and 50 with non-repetitive sequences of the cotton genome has been examined by determining the reassociation kinetics of DNA of varying fragment lengths and by the electron microscopy of reassociated molecules. About 60% of the genome consists of non-repetitive regions that average 1800 base-pairs interpersed with repetitive sequences that average 1250 base-pairs. Approximately 20% of the genome may be involved in a longer period interspersion pattern containing non-repetitive sequences of approximately 4000 base-pairs between repetitive sequences. Most of the individual sequences of the interspersed repetitive component are much smaller than the mass average size, containing between 200 and 800 base-pairs. Sequence divergence is evident among the members of this component. Highly repetitive sequence elements that are reassociated by a C0t value of 0.1 average 2500 base-pairs in length, appear to have highly divergent regions and do not appear to be highly clustered. A portion of this highly repetitive component reassociates by C0t = 10−4, zero-time binding DNA, and accounts for less than 3% of the genome. At least a third of these sequences appear by electron microscopy to be intramolecular duplexes (palindromes) of 150 to 200 base-pairs and to occur in clusters.",1976,1527
Stochastic Differential Game Techniquesa aThe material presented in this chapter was sponsored in part under an AFSOR grant.,B. MONS,,1981,1528
Index of biochemical reviews,,,1990,1529
"Chapter 4 - Model of Inexact Reasoning in Medicine††Much of the material in this chapter has appeared in an article in Mathematical Biosciences [Shortliffe, 1975a]. That paper was co-authored with Dr. Bruce Buchanan who contributed substantially to the development of the model.",Edward Hance Shortliffe,,1976,1530
Book notices and abstracts,,,1925,1531
Oceanographic bibliography: Part II,,,1976,1532
"Nature and classification of waterlain glaciogenic sediments, exemplified by Pleistocene, Late Paleozoic and Late Precambrian deposits",C.P. Gravenor and V. {von Brunn} and A. Dreimanis,"This study of waterlain glaciogenic sediments is designed to present both a review and new information on glaciogenic subaquatic deposits of differing age in a number of localities in North and South America and South Africa. The Late Paleozoic glaciogenic deposits of the Parana´Basin in Brazil and the Karoo Basin of South Africa are singled out for special attention as they show a reasonably complete lateral sequence of terrestrial to off-shore glaciogenic sedimentation. Although the environment of subaquatic glaciogenic sedimentation varies from one area to the next, certain common elements are found which can be used to develop a generalized model for both glaciomarine and glaciolacustrine sedimentation. For descriptive purposes, the model is divided into two broad categories: a shelf facies and a basinal facies. The shelf facies is marked by massive diamicton(ite) which may be 200 m or more in thickness and which is frequently overlain by a complex of clastic sediments consisting primarily of gravity and fluid flows. The basinal facies is marked by products of subaquatic slumps and more distal turbidites and glaciomarine sediments. New terminology is introduced. The massive diamicton(ite), which is diagnostic of the shelf facies, probably represents deposition from the base of active ice in a subaquatic environment and is termed undermelt diamicton(ite). The gravity and fluid flows which are usually found overlying undermelt diamicton(ite) and in the basinal facies are subdivided into six categories: glaciogenic subaquatic outwash, glaciogenic suspension flow, glaciogenic chaotic debris flow, glaciogenic subaquatic debris flow, glaciogenic slurry flow and glaciogenic turbidity flow. The relative abundance of undermelt diamicton(ite) and the various types of gravity and fluid flows can be used to define inner shelf, outer shelf, inner basin and outer basin facies of glaciomarine sedimentation.",1984,1533
Extracorporeal shock wave lithotripsy for large renal calculi: The role of ureteral stents. A Randomized Trial,Alexander F. Bierkens and AD J.M. Hendrikx and Wim A.J.G. Lemmens and Frans M.J. Debruyne,"Ureteral stents reduce complications after extracorporeal shock wave lithotripsy (ESWL**Dornier Medical Systems, Inc., Marietta, Georgia.) and contribute to successful stone passage. However, some reports note complications that are attributed to indwelling ureteral stents. We randomized 64 patients with large renal calculi (stone burden more than 200mm.2) for in situ treatment or treatment with a prophylactically inserted stent. We used a 6Ch round stent with single-coiled ends or a triangular shaped stent with double-coiled ends. Patients were treated with a Siemens Lithostar lithotriptor.† After 3 months we evaluated the results of treatment and post-ESWL morbidity. Of the in situ group (23 patients) treatment complications consisted of fever in 3, pyelonephritis in 1 and steinstrasse in 3. After 3 months 8 patients (35%) were free of stones. Of the stented population (41 patients) treatment complications consisted of fever in 7, pyelonephritis in 1, steinstrasse in 6 and bladder discomfort in almost half of the patients. Stent calcification and stent migration were also seen in 7 and 10 patients, respectively. Calcified stents had been in situ longer than noncalcified stents. The round stents migrated and calcified more often than the more rigid triangular stents. After 3 months 18 of the stented patients were stone-free (44%). We conclude that ureteral stents do not reduce post-ESWL complications. They are clearly associated with morbidity and do not improve stone passage markedly. Therefore, patients with a stone burden of more than 200mm.2 should be treated in situ without auxiliary stenting.",1991,1534
Compactness for omitting of types,Miroslav Benda,,1978,1535
An ecological discussion of the environmental impact of the highway construction program in the Amazon basin,R.J.A. Goodland and Howard S. Irwin,"Goodland, R.J.A. and Irwin, H.S., 1974. An ecological discussion of the environmental impact of the highway construction program in the Amazon Basin. Landscape Plann., 1: 123–254. Started in 1970, the 6 368 km Transamazônica, the 2 465 km Perimetral Norte and the 1 747 km Cuiabá—Santarém highways lacerating the Amazon jungle are now nearing completion. Large-scale cattle ranching and peasant agriculture by thousands of government-assisted settlers are being officially hastened along these highways. The likely environmental consequences of these activities is discussed. Human ecology and nosogeography. Diseases likely to increase due to increase in vectors; vectors encouraged by construction methods; route planning and known disease foci; effect of settlements, deforestation and agriculture on disease. Malaria, onchocerciasis, schistosomiasis, trypanosomiasis and verminoses: relation between indigenous and alloch-thonous human populations; remediation. Amerindians. Lack of information on location and numbers even of already contacted tribes. Tribes to be disturbed by highways; governmental policy and the constitution. Effect of relocation on survival, national parks, disease, sedentary vs nomadic habits, money economy and education. Ecological perceptions and abilities of the Amerindian; their potential role in Amazonia. Deforestation and agriculture. Direct and closed nutrient cycles in tropical wet forest, implications for vegetational management. Methods and degree of extirpation: erosion, laterization, decreased fertility. Pests and weeds competing with crops; adverse effects of fertilizer; lack of dry season or winter. Climatic change. Sustainable agroecosystems; fish, riparian communities, mixed tree plantations and refining methods. Ecological considerations of cattle raising. Fauna and faunation. Unknown, rare and endangered taxa; occurrence, value and preservation, state of knowledge, population equilibria of pests, disease vectors, rodents. Role of faunation in nutrient dynamics. Use of faunation in forest ecosystem harvest and raising of indigenous fauna. Flora and vegetation. Physiognomic types of vegetation; undiscovered species and their characteristics; species extinction by exploitation, regeneration of forest, conservation areas.",1974,1536
Adhesion and detachment of biological cellsin vitro,Martin A. Hubbe,"Adhesion between biological cells and various surfaces is explained in terms of various models, including coagulation at primary or secondary minima of free energy, macromolecular bridges or matrices, and specialized structures at the surfaces of some cells. These models are used to predict the magnitudes of force necessary to detach a cell in the limiting cases of peeling and simultaneous separation over finite areas of contact. Diverse experimental assays of cellular adhesiveness are reviewed and the forces applied to individual cells are estimated. A very wide range of the forces applied to cells in different assays suggests that different mechanisms of bonding are dominant for different types of cells and surfaces under various conditions of growth and chemical environment. The peeling mode of separation is most consistent with the magnitudes of applied force used experimentally in the detachment of cells.",1981,1537
Recent Progress in Pharmacogenetics,Elliot S. Vesell,"Publisher Summary
This chapter discusses the hereditary factors causing clinically significant variations in human responsiveness to drugs and the substantial advances made in pharmacogenetics. Along with past much recent work that has been reviewed in the chapter is devoted to these genetically transmitted conditions, but only a few new examples have been described. In the chapter, the term pharmacogenetics is applied to clinically significant consequences of hereditary variations in the handling of drugs. The chapter deals mainly with an increasing body of data on the defects in human and only tangentially with the very large literature, concerning animal experiments. Search for hereditary variations, affecting the way the body handles drugs, has until recently turned up almost exclusively traits inherited as single factors— that is, traits produced by point mutations at a single genetic locus and transmitted, either as Mendelian dominants or recessives. Investigation of the responsiveness of the general population to a drug in terms of the amount of a drug required to produce a given effect may take the form of a continuous unimodal distribution curve or of a discontinuous polymodal curve. Until recently, studies of drug responses that yield a normal or continuous distribution curve have been almost entirely ignored in pharmacogenetic investigations. To construct unimodal, Gaussian distribution curves large populations are required. Furthermore, genotypes are hard to deduce from such curves. In contrast, discontinuous, bimodal, or trimodal curves of response obtained from disorders transmitted as Mendelian dominants or recessives are more easily analyzed because each discrete curve generally corresponds to a different genotype.",1970,1538
"Chapter 6 - ANDROGENS IN FISHES, AMPHIBIANS, REPTILES, AND BIRDS",R. OZON,,1972,1539
Abstracts DGP Wuppertal 2005,,,2005,1540
Chapter 1 - FUNDAMENTAL RELATIONSHIPS OF CHROMATOGRAPHY,Colin F. Poole and Sheila A. Schuette,,1984,1541
Schumpeterian competition in alternative technological regimes,Sidney G. Winter,"This paper presents an extension of the Nelson-Winter model of Schumpeterian competition that focuses on certain features of the ‘historical’ shape of industry evolution, particularly on the relative importance of entrants and established firms as sources of innovation.",1984,1542
Hormonal control of rat liver regeneration,H.L. Leffert and K.S. Koch and T. Moran and B. Rubalcava,,1979,1543
Expansion of C4 grasses in the Late Miocene of Northern Pakistan: evidence from stable isotopes in paleosols,Jay Quade and Thure E. Cerling,"Stable-isotopic, clay-mineralogic, and bulk-chemical analyses were conducted on paleosols of the Neogene Siwalik sections in northern Pakistan in order to reconstruct floodplain environments over the past ∼ 17 Ma. The stable carbon isotopic composition of soil carbonate (mean δ13C (PDB) = -10.2%) and associated organic matter (mean δ13C (PDB) = −24.1%) in paleosols representing 17− ∼ 7.3 Ma reveal that floodplain vegetation was dominated by C3 plants. At 7.3 Ma, a shift toward more positive carbon isotopic values began, signaling the gradual expansion of C4 grasses onto the floodplain. From 6 Ma to present, carbon isotopic values for paleosol carbonate (mean δ13C (PDB) = +0.6%) and organic matter (mean δ13C (PDB) = −14.4%) are uniformly enriched in 13C, indicating the presence of nearly pure C4 grassland. The scarcity of kaolinite and abundance of smectite and pedogenic carbonate in most paleosols suggest that rainfall in the region remained 1.0–1.25 m/yr or less for the entire 17 Ma of record. Paleosols in the lower portion of the section lack organic A horizons but have reddish B horizons often containing secondary iron-oxide nodules. Leaching depths of soil carbonate in these older paleosols are typically greater than those in the Plio-Pleistocene part of the section, where organic A horizons are common, and B horizons are markedly more yellow. The combined evidence suggests that the mature paleosols in the pre-7.3 Ma part of the record are dominantly calcareous Alfisols or Mollisols that once underlay nearly pure C3 vegetation, perhaps trees and shrubs, while calcareous Mollisols underlying C4 grassland dominate the upper part of the record. The carbon- and oxygen-isotopic trends in the paleosol record in Pakistan are also evident in the diet of fossil mammals, and in paleosols from Nepal, thus demonstrating that these paleoenvironmental changes in floodplain vegetation may be continent-wide. Local effects, such as the development or intensification of the Asian Monsoon driven by uplift of the Tibetan Plateau, may have led to the expansion of C4 grasses. If, however, the expansion of C4 grasses proves globally synchronous, then a larger scale cause, such as a marked decrease in ϱCO2, may be the driving mechanism.",1995,1544
"Monthly bibliography on Neuropeptides prepared by the University of Sheffield Biomedical Information Service Sheffield S10 2TN, England",,,1983,1545
An interactive fortran program for crosscorrelation of signals on a PC with CGA graphics: an application in marine geoacoustics,Roddy V. Amenta,"Cross-correlation is a method used for comparing two similar signals. CRSCOR is a general purpose program which integrates three tasks: the management, cross-correlation, and graphical display of signals and cross-correlation functions on a PC workstation. Program variables relating to these tasks may be changed through menus thereby precluding the need for recompiling and relinking. Cross-correlation is performed by Fast Fourier Transform with optional use of an 8087 math coprocessor for increasing throughput. The program has a modular architecture with 46 subroutines which simplifies modification of source code if necessary. CRSCOR was developed as part of an overall study of acoustical signals which have interacted with the ocean bottom. From data collected over the continental rise east of Cape Hatteras, tests were conducted on a series of signals from adjacent bottom paths with the grazing angle of each path differing by about 0.3°. Each member of the series was cross-correlated with the first member. Peak values of the cross-correlation function obtained from unfiltered signals were significantly lower than were the peak values from the band-pass filtered (90–200 Hz) versions of these signals indicating that the high-frequency spectrum is more sensitive to path grazing angle than the lower frequency spectrum.",1990,1546
Chapter 2 - Electrophoretic Methods,David E. Garfin,"Publisher Summary
There are various techniques for separating proteins and nucleic acids based on their chemical and physical properties. In particular, the intrinsic charges of proteins and nucleic acids are much exploited in biochemistry. Electrophoresis, isoelectric focusing, ion-exchange chromatography, and mass spectrometry use the characteristic charges of molecules to separate them. Electrophoresis is the motion of charged particles in externally applied electric fields. Electrophoresis is the highest resolution method available for the separation of proteins and nucleic acids. Gel electrophoresis is the most used electrophoretic technique. Electrophoresis evolved from Tiselius' fundamentally simple moving boundary technique in a free solution to one- and two-dimensional gel systems is capable of exquisite resolution of highly complex mixtures. Sophisticated capillary instruments with online detection bring automation to the analytical processes. Most kinds of electrophoresis are relatively inexpensive and easy to perform, and the results are easy to interpret. It is not possible to obtain quantitative structural data from electrophoresis, but valuable qualitative information about the relative charges and sizes of macromolecules can be acquired from it.",1995,1547
К определению стиля модерн в русской и чешской поэзии,Моймир Григар,,1980,1548
Chapter 14 - Breeding,Andrew G. Hendrickx and W. Richard Dukelow,"Publisher Summary
Effective maintenance of breeding colonies of nonhuman primates is based on a number of factors. The first of these is the basic knowledge of the reproductive physiology of the species involved. The status of nonhuman primate research is such that there are often large gaps in breeding knowledge, particularly in the species that are not normally held in captivity or are heavily involved in research programs. For effective breeding of these animals, it must rely on reports from zoological gardens and private or commercial colonies. Field studies of the animal's activities in the wild are very useful in establishing breeding colonies. The final factor is the common sense husbandry that individuals acquire through work with a large variety of animals in captive situations. This chapter discusses the methods that have been used to breed a variety of species in domestic and international facilities. It also discusses the detection and monitoring of pregnancy, and prenatal growth and development.",1995,1549
GEOCHEMISTRY ARTICLES – April 2021,,,2021,1550
Eucalyptus,GAVIN F. MORAN and J. CHARLES BELL,,1983,1551
Analytical problems associated with core support structure of PWR,George J. Böhm,"The design of core support structures of a pressurized water reactor requires the solution of structural problems with a degree of sophistication that is not usual to other types of power generating plants. This situation arises not only from the complexity of both the structures and the external loads, but also from safety specifications requiring a precise knowledge of component behavior. The analytical problems derived from normal and abnormal operation covers a wide range of static and dynamic problems. Extensive use of modern, high-speed computing tools is required to solve these problems. Linear and nonlinear analyses for static and dynamic problems use computer programs of varied complexity, leading to the use of newly developed techniques in the area of finite elements and numerical stability of integration methods. The present paper reviews the existing type of analytical problems faced in design, and the methods used to solve them.",1972,1552
Professor Roy L. Whistler,James N. BeMiller,,1979,1553
"86th Annual Conference of the German Society of Mammalogy (Deutsche Gesellschaft für Säugetierkunde e.V.) Frankfurt a.M., 4th–8th September 2012",,,2012,1554
Terpenoids and alkaloids of the leaves of Tabernaemontana coronaria,Bani Talapatra and Amarendra Patra and Talapatra {Sunil K},,1975,1555
Subject Index,,,2004,1556
"Effects of norepinephrine, isoproterenol and sympathetic stimulation on aqueous humour dynamics in vervet monkeys",Anders Bill,"An attempt was made to analyze what types of adrenergic receptors are involved in the mechanisms for aqueous humour production and drainage and to determine the maximum effects of these receptors. The parameters determined were: mean arterial blood pressure, intraocular pressure, rate of aqueous flow via Schlemm's canal, rate of flow through uvcoscleral routes and gross facility of outflow. The recipient venous pressure was calculated. Isoproterenol, 0·1 and 1·0 μg/ml, perfused through the anterior chamber increased the rate of aqueous humour production and the gross facility of outflow and tended to increase the rate of uveoscleral drainage of aqueous humour. A dose of 10 μg/ml produced more variable results than the lower doses. Norepinephrine perfused through the anterior chamber at a dose of 1 or 10 μg/ml had no significant effect on the gross facility of outflow or the rate of aqueous formation. Neither isoproterenol nor norepinephrine had statistically significant effects on the intraocular pressure and the recipient venous pressure. Stimulation of the cervical sympathetic nerves to the eye gave a probably significant increase in the rate of aqueous humour formation. After beta-receptor blockade with propranolol, 5 mg/kg, neither the effects on the rate of aqueous formation nor those on the facility of outflow could be elicited. The results suggest that there are adreuergic beta-receptors in the ciliary processes and in the chamber angle of the monkey eye which stimulate the secretion of aqueous humour and increase the facility of outflow, respectively. The average effect of maximum adrenergic beta-receptor stimulation was to increase the rate of aqueous formation by about 30% and to raise the facility of outflow by about 55%.",1970,1557
"Subject and author index vol. 19, 1989",,,1989,1558
Chapter 6 Fishes and Fisheries,Evald Ojaveer and Arne Lindroth and Ole Bagge and Hannu Lehtonen and Jorma Toivonen,"Publisher Summary
The evolution of the contemporary fish fauna of the Baltic Sea has been mostly influenced by (1) changeability of environmental conditions during the short history of this sea, (2) horizontal and vertical configuration of the Baltic Sea, determining its brackish-water character, with dominating two-layered water mass, as well as the character of currents, and (3) climate. The main components of the contemporary fish fauna of the Baltic Sea, the arctic, marine-boreal, anadromous, katadromous, and fresh-water fishes, have immigrated into this sea at different times by various ways. Marine species in the Baltic Sea could have adapted mainly eurihaline species being capable to endure a relatively low temperature. The species composition of its fish fauna was probably different. It is probable that, especially during the Litorina time with the warmest climate and the highest salinity in the history of the Baltic Sea, a number of marine fishes immigrated into the Baltic basin. Because of environmental conditions in the Baltic Sea, a characteristic distribution of fishes has become established. Because of a low content or absence of oxygen in the bottom layers of the deep areas, the fishes are periodically or permanently absent. In the northern part of the Baltic Sea, mainly plankton eating herring and sprat, are constantly abundant and to a smaller degree also the flounder. In the southern and southwestern Baltic Sea, more marine fishes including benthos-eaters and predatory fishes are relatively abundant.",1981,1559
Дешифровка,Jerzy Faryno,,1989,1560
Replication of tobacco mosaic virus: VI. Replicative intermediate and TMV-RNA-related RNAs associated with polyribosomes,Roger N. Beachy and Milton Zaitlin,"Polyribosomes prepared from both the membrane (bound polyribosomes) and cytoplasmic (free polyribosomes) fractions of tobacco mosaic virus (TMV)-infected tobacco leaves were found to contain small RNAs of several sizes which, by molecular hybridization with denatured double-stranded TMV-RNA, were shown to consist of portions of the TMV-RNA genome. In addition, full-length (30 S) TMV-RNA was found on the free but not the bound polyribosomes. These RNAs were associated with all sizes of polyribosomes as analyzed on sucrose gradients, with the larger species of RNA predominating the regions containing the larger polyribosomes. A heterogeneous population of replicative intermediate molecules was associated with the bound, but not the free polyribosomes. Here, too, the molecules were distributed throughout the polyribosome gradient. The possible functions of the ribosome-associated RNAs as messenger RNAs for viral-coded proteins are discussed.",1975,1561
Viral diseases causing the greatest economic losses to the tomato crop. II. The Tomato yellow leaf curl virus — a review,Belén Picó and María José Díez and Fernando Nuez,"Tomato yellow leaf curl geminivirus (TYLCV), transmitted by the whitefly Bemisia tabaci (Gennadius) is one of the most devastating diseases of cultivated tomato (Lycopersicon esculentum Mill.). TYLCV causes economic losses up to 100% in tomato crop in many tropical and subtropical regions, and is spreading towards new areas. The increasing economic importance of TYLCV has resulted in the need for accurate detection and identification procedures, stimulating intensive research efforts focused on virus biology, diversity, and epidemiology to develop successful control strategies. Breeding for resistance appears to be the best approach to control this disease, but to date only partially resistant hybrids are commercially available. Search for new sources of disease resistance needs to be intensified. The purpose of this paper is to collect and summarize all these efforts, offering an updated review of these new approaches on virus transmission, molecular biology, variability, diagnostic methods, and potential ways to control. This will lead to a better understanding of the virus-vector-host relationships, key factors in the adoption of disease control measures.",1996,1562
Report on the symposium on the biochemistry of the retina,S.L. Bonting and F.J.M. Daemen,"The Symposium was devoted primarily to the visual mechanism. In the selection of the 34 participants an effort was made to represent the various disciplines, which nowadays are brought to bear on the elucidation of the processes taking place in the photoreceptor cell upon illumination. In this report the abstracts of each of the individual papers are presented together with highlights of the discussion. These are followed by transcribed and edited reports of four Round Table Discussions on the topics:o1.Current insights in visual pigment structure. (Chairman: C. D. Bridges)2.Which step in the photolytic cycle is responsible for the visual excitation mechanism? (Chairman: E. W. Abrahamson)3.Present status of the three hypotheses proposed for the visual excitation process. (Chairman: B. Rosenberg)4.Relation between photolytic and electrophysiologic events. (Chairman: S. L. Bonting)",1969,1563
"Joint Annual Meeting of the German and Dutch Societies for Immunology (JAMI) (20-23 October 2004, Maastricht, the Netherlands)",,,2004,1564
Prions and related neurological diseases,Maurizio Pocchiari,,1994,1565
New or interesting records of British Heterobasidiomycetes,Derek A. Reid,"Accounts are given of the following members of the Auriculariales: Eocronartium muscicola Herpobasidium filicinum and Helicogloea lagerheimii; additional localities for Hirneola auricula-judae var. laciea are also listed; and members of the Tremellales: Tremella steidleri which is new to Britain Stypella versiformis, Protodontia subgelatinosa and P. ellipsospora n.sp. Additional localities for Heterochaetella dubia are also cited.",1990,1566
Die historia francorum senonensis und der Aufstieg des Hauses Capet,Joachim Ehlers,"In this inquiry the author confronts the historiographical view of the rise of the Capetians to power, as represented by the Historia Francorum Senonensis, with historical reality. He comes to the conclusion that the medieval historian, writing in the thirties of the eleventh century, sought by the selection, combination, interpretation, and presentation of his passages to propagate a view which had originated at the archbishop's court at Sens. The actual political motive was the dispute of Sens with Reins over coronation rights; it was this that explains the anti-Capetian tendency of the author's account of the dynastic change in 987. Moreover, it is possible to discern a political consciousness which was able to consider the West-Frankish/French monarchy as independent from dynastic considerations. We are thus dealing not with a historiographical statement of the Carolingian point of view, but with the reaction to a particular situation in ecclesiastical politics combined with a non-personal theory of the state.",1978,1567
"International Society for Advancement of Respiratory Psychophysiology. Proceedings of the annual meeting September 19–21, 2014",,,2016,1568
Chapter 8 - Microbial Biomass from Renewables: A Second Review of Alternatives,Carlos Rolz,"Publisher Summary
This chapter discusses microbial biomass (MB), which, as a source of nutrients, is receiving worldwide attention. Microbes can be grown in renewable and synthetic substrates, and the resulting biomass can be employed as a nutrient source as produced or after processing through animal feed rations or in processed food products. The imminent world food crisis has encouraged research and development activities related to the production and processing of MB. The standard process for MB production as developed for large-scale capital intensive operations consists of a single-species operation carried out in synthetic substrates, where biomass is reproduced aerobically and continuously in a highly diluted aqueous media under mesophilic temperatures and in large-scale plants. This operation is followed by cell recovery, washing, and drying where a nonviable, easily stored biomass powder is obtained. There is repetitive research work that does not produce new insights to process development; however, there have been outstanding new ideas brought to the general attention, which have increased the number of process alternatives available.",1984,1569
English and foreign mining glossary,,,1848,1570
Immobilized Living Cells and Their Applications,John F. Kennedy and Joaquim M.S. Cabral,"Publisher Summary
This chapter discusses immobilized living cells, their advantages, and their applications in industry. Over the past two decades, there have been rapid developments in the use of enzymes as catalysts for industrial, analytical, and medical purposes and a new field of research called enzyme technology. Current and industrial applications of continuous single-enzyme reactions are carried out using immobilized microbial cells. Immobilized living cells should be preferred to immobilized enzymes for degradative and synthetic reactions in industry; another major advantage of immobilized cells is that the operational stability of the immobilized living cells may often be greatly enhanced by the regeneration of the enzyme activities of the immobilized cells. The use of immobilized cells enables greater control throughout the reaction. The applications of immobilized living cells are not limited to the production of chemical fermentation products but have been extended to the production of viral particles or synchronous cells, the chromatographic separation of special cells, the culturing of animal tissue, and, more recently, the immobilization and use of plant cells in the production of alkaloids.",1983,1571
Correlation and geochronology of middle Eocene strata from the Western United States,John Joseph Flynn,"In this study I integrate marine and continental biostratigraphy, magnetic polarity stratigraphy, and radioisotopic chronology in a synthetic correlation of middle Eocene strata from the western United States. More than 2000 m of section were sampled from volcaniclastic deposits, Aycross and Tepee Trail Formations (northwestern Wyoming); lacustrine and fluviatile deposits, Washakie Formation (southwestern Wyoming); and intertonguing marine and continental strata, La Jolla and Poway Groups (San Diego area, California). Detailed demagnetization studies on numerous pilot samples from all three field areas revealed moderately complex magnetizations. Most sample NRM's are dominated by a strong normal polarity magnetic component; alternating field demagnetization does not consistently isolate the primary magnetization. High blocking temperature hematite is frequently a significant carrier of remanence. Therefore, most samples were subjected to detailed, stepwise alternating field and thermal demagnetization to 600–650°C. The East Fork Basin area (northwestern Wyoming) magnetic polarity sequence consists of five major polarity intervals, A- to E-; the thick Washakie Formation sequence contains four, A+ to D-; and the San Diego area sequence has four polarity intervals, A- to D+. A new biochronologic interval, the Shoshonian Land Mammal Subage (Earliest Uintan), is defined and characterized. In all three field areas Shoshonian (Earliest Uintan) faunas and the Bridgerian/Uintan boundary occur within a single long reversed polarity interval. Correlation of marine biostratigraphy between the San Diego area section and deep sea sections allows precise identification of San Diego polarity interval B+ as Chron C21N. Therefore, the Bridgerian/Uintan boundary and earliest Uintan faunas occur within the reversed interval of Chron C20R. High-temperature, KAr dates bracketing this horizon in northwestern Wyoming provide an age estimate of approximately 49.5 Ma for the top of Chron C21N and 49 Ma for the Bridgerian/Uintan boundary. Berggren et al. (1985) use the age estimate of 49.5 Ma for the top of anomaly 21 (younger boundary of Chron C21N) as one calibration point for the generation of a Paleogene geochronology. The methodology and conclusions of this geochronology are compared to those of other recent geochronologies. Data from independent studies integrating high temperature radioisotopic dates, biochronology, and magnetochronology are used to test the validity of the Berggren et al. (1985) geochronology.",1986,1572
Lactose in Animal and Human Feeding: A Review,R.L. Atkinson and F.H. Kratzer and G.F. Stewart,"Summary
An attempt has been made to review the available information on the uses of lactose in animal and human feeding. From the information reviewed, the following conclusions can be drawn:1.Lactose is tolerated by the rat up to about 25% of the diet. Above this level poor growth, diarrhea, alopecia, and cataract formation result.2.Young pigs and dairy calves are able to tolerate extremely high levels of lactose. This tolerance for lactose decreases with the age of the animal.3.Poultry, either young or old, are unable to utilize even moderate levels of lactose.4.Lactose has a definite effect on the intestinal tract of all animals. This effect is characterized by a lowering of the acidity and a change of the intestinal flora to an acidophilic type.5.Lactose stimulates the synthesis of B-vitamins by the intestinal bacteria in both mammals and poultry.6.Lactose feeding causes a change in the phospholipids of the tissues of animals fed diets containing large amounts of lactose.7.Lactose favorably influences the absorption, retention, and utilization of calcium, phosphorus, and magnesium. This may or may not be related to the change in pH of the intestinal tract following lactose feeding.8.Lactose is well-tolerated by the human adult and it may be used in therapy against constipation and diarrhea.9.Lactose finds wide use in infant feeding and should be considered the carbohydrate of choice for the modification of cow's milk and for the formulation of infant foods. It is especially useful as an additive to human milk for the feeding of premature infants.10.Lactose has been shown to improve the problem-solving ability of rats, which suggests a nutritive value not found in other carbohydrates.11.Lactose has been shown to protect the rat against alloxan poisoning. Sucrose and starch did not give this protecting action against alloxan poisoning.12.Lactose has been shown to be a lipotropic agent and a sparer of choline.13.Lactose allows normal reproduction in the rat. This was not true for sucrose.",1957,1573
Magnetic properties of the rare earth pnictides,F. Hulliger,"The magnetic properties of the binary rare-earth compounds with the nitrogen-group elements are briefly discussed, emphasizing the NaCl-type phases, for which the main magnetic data are collected.",1978,1574
FULL ISSUE PDF,,,2024,1575
FERTILITY IN MEN WITH CHRONIC RENAL FAILURE BEFORE AND AFTER KIDNEY TRANSPLANTATION,J. Nohra and A. Zairi and G. Ghazal and N. Kamar and L. Rostaing and P. Plante and E. Huyghe,,2008,1576
Chapter 6 - Intrusion Detection in Contemporary Environments,Tarfa Hamed and Rozita Dara and Stefan C. Kremer,"This chapter discusses intrusion detection applications for two contemporary environments: mobile devices and cloud computing. The chapter starts by introducing the most well-known mobile device operating systems and cloud computing models. Next, the chapter discusses the risks to which these environments are exposed as a result of intrusions, and the sources and origins of attacks in both environments. Furthermore, classes of malware and types of attacks are explained. In addition, the chapter explores techniques employed by mobile malware as well as techniques employed by intrusions that infect cloud computing systems. The chapter also gives a variety of new examples of malware that infect mobile phones and intrusions into cloud computing systems. Moreover, the chapter discusses types of intrusion detection systems and explains performance metrics for evaluating intrusion detection systems in both environments.",2017,1577
Towards resilience in Industry 5.0: A decentralized autonomous manufacturing paradigm,Jiewu Leng and Yuanwei Zhong and Zisheng Lin and Kailin Xu and Dimitris Mourtzis and Xueliang Zhou and Pai Zheng and Qiang Liu and J. Leon Zhao and Weiming Shen,"Manufacturers are increasingly aware of the importance of system resilience against unexpected disruptive occurrences, such as the recent global Covid-19 pandemic and geopolitical wars in Europe. Meanwhile, Decentralized Autonomous Organization (DAO) is recently envisioned as the Blockchain 3.0 stage, in which enabling DAO in the manufacturing domain could be a promising attempt and will lead to decentralized autonomous manufacturing. Blockchain-enabled smart contracts and decentralized applications have the characteristics of verifiability, decentralization, transparency, autonomy, and tamper-proofing, which can enable a mass individualization paradigm to realize the promising Industry 5.0 vision of resilience. DAO provides a probable manner to regulate cross-prosumer activities under the Industry 5.0 context. Inspired by this vision, our paper reviews the literature on Decentralized Manufacturing (DM) and Autonomous Manufacturing (AM). Then, these two streams of efforts are unified, and a manufacturing paradigm, named Decentralized Autonomous Manufacturing (DAM), is defined towards resilience in Industry 5.0. In this paper, a reference architecture of the DAM is given. Followed by a comprehensive investigation of the key enablers, challenges, and barriers in the implementation of DAM, based on the insights from this analysis, future research directions of DAM are highlighted. We believe that our effort can lay a foundation for positioning DAM in futuristic Industry 5.0 research and engineering practice.",2023,1578
Towards a taxonomy of code review smells,Emre Doğan and Eray Tüzün,"Context:
Code review is a crucial step of the software development life cycle in order to detect possible problems in source code before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the code review process, many companies and open source software (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective code review process. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are code review (CR) smells.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight OSS projects.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the OSS projects are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.",2022,1579
A proactive malicious software identification approach for digital forensic examiners,Muhammad Ali and Stavros Shiaeles and Nathan Clarke and Dimitrios Kontogeorgis,"Digital investigators often get involved with cases, which seemingly point the responsibility to the person to which the computer belongs, but after a thorough examination malware is proven to be the cause, causing loss of precious time. Whilst Anti-Virus (AV) software can assist the investigator in identifying the presence of malware, with the increase in zero-day attacks and errors that exist in AV tools, this is something that cannot be relied upon. The aim of this paper is to investigate the behaviour of malware upon various Windows operating system versions in order to determine and correlate the relationship between malicious software and OS artifacts. This will enable an investigator to be more efficient in identifying the presence of new malware and provide a starting point for further investigation. The study analysed several versions of the Windows operating systems (Windows 7, 8.1 and 10) and monitored the interaction of 90 samples of malware (across three categories of the most prevalent (Trojan, Worm, and Bot) and 90 benign samples through the Windows Registry. Analysis of the interactions has provided a rich source of knowledge about how various forms of malware interact with key areas of the Registry. Using this knowledge, the study sought to develop an approach to predict the presence and type of malware present through an analysis of the Registry. To this end, different classifiers such as Neural Network, Random forest, Decision tree, Boosted tree and Logistic regression were tested. It was observed that Boosted tree was resulting in a correct classification of over 72% – providing the investigator with a simple approach to determining which type of malware might be present independent and faster than an Antivirus. The modelling of these findings and their integration in an application or forensic analysis within an existing tool would be useful for digital forensic investigators.",2019,1580
A Clinical Approach to Periodontal Regeneration,Myron Nevins and Marc L. Nevins and Marcelo Camelo and James T. Mellonig,,2001,1581
Chapter 12 - The Botnet Problem,Daniel Ramsbrock and Xinyuan Wang,"This chapter addresses the issue of Internet service provider (ISP) network protection, with a focus on addressing bots and botnets, which are a serious and growing problem for end users and ISP networks. Botnets are formed by maliciously infecting end-user computers and other devices with bot (from the word robot) software through a variety of means, and surreptitiously controlling the devices remotely to transmit onto the Internet spam and other attacks (targeting both end users and the network itself). This chapter examines potentially relevant existing best practices (BPs) and identifies additional best practices to address this growing problem. The chapter also identifies best practices that address protection for end users as well as the network. The best practices are organized into the logical steps required to address botnets. The first step is prevention, followed by detection, notification, and then mitigation. In addition, the best practices on privacy considerations are also identified to address the handling of customer information in botnet response. The best practices identified are primarily for use by ISPs that provide service to consumer end users on residential broadband networks, but may apply to other end users and networks as well. It is critical to note that best practices in general are not applicable in every situation because of multiple factors. Therefore, the best practices are intended to be voluntary in nature for ISPs and may not apply in all contexts (and thus for a host of reasons should not be made mandatory). With this understanding, this chapter recommends that the best practices can be implemented by ISPs, where applicable, in order to address the growing botnet problem in consumer end-user devices and ISP networks.",2013,1582
Morphology of the decrementing expiratory neurons in the brainstem of the rat,Yoshiaki Saito and Ikuko Tanaka and Kazuhisa Ezure,"In anesthetized and artificially-ventilated rats, the morphological properties of decrementing expiratory (E-DEC) neurons were studied using intracellular recording and labeling with Neurobiotin. Sixteen E-DEC neurons were successfully labeled; ten of which were cranial motoneurons located in the facial (FN) and ambiguus (NA) nuclei. Two interneurons were labeled in the Bötzinger complex (BOT) and the ventral respiratory group (VRG) rostral to the obex, and the remaining four in the VRG caudal to the obex. All the interneurons had extensive intramedullary collaterals within the ventrolateral medulla. Terminal-like boutons were distributed ventral to the NA at the level of the BOT, both ventral to and within the NA at the level rostral to the obex and largely within the cell column tentatively designed as the ambiguous-retroambiguus complex (NA/NRA) caudal to the obex. The four interneurons in the NA/NRA had axons projecting to the spinal cord as well. The extensive intramedullary projections suggest that these E-DEC interneurons of the BOT and the VRG play a significant role in respiration. The simultaneous projections from the caudal E-DEC neurons to both the spinal cord and the NA suggest that these neurons also play integrative roles in non-respiratory behaviors including vocalization, swallowing and defecation.",2002,1583
Congenital neuroblastoma in a bot born to a woman with bipolar disorder treated with carbamazepine during pregnancy,Trino Baptista and Hilarion Araujo and Pedro Rada and Luis Hernández,,1998,1584
"A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem",Fabio Calefato and Filippo Lanubile and Bogdan Vasilescu,"Context
Large-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.
Objective
We aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global software engineering — has been found to influence positively the result of code reviews in distributed projects.
Method
In this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.
Results
We find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.
Conclusion
Overall, our findings reinforce the need for future studies on human factors in software engineering to use psychometric tools to control for differences in developers’ personalities.",2019,1585
An empirical study of sentiments in code reviews,Ikram El Asri and Noureddine Kerzazi and Gias Uddin and Foutse Khomh and M.A. {Janati Idrissi},"Context
Modern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.
Objective
In this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the code review process.
Method
Based on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.
Results
We found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (e.g., core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.
Conclusion
Through this work, we provide evidences that text-based sentiments have an impact on the duration of the code review process as well as the acceptance or rejection of the suggested changes.",2019,1586
Efficacy of in vitro fertilization after chemotherapy,Marie-Madeleine Dolmans and Dominique Demylle and Belen Martinez-Madrid and Jacques Donnez,"Objective
To evaluate if in vitro fertilization (IVF) with embryo cryopreservation can be proposed to patients immediately after one or two regimens of chemotherapy.
Design
Retrospective study.
Setting
Academic research center and IVF unit.
Patient(s)
Eleven young patients diagnosed with cancer between September 1999 and April 2003 who wanted to preserve their fertility via IVF.
Intervention(s)
Stimulation and IVF before or soon after chemotherapy treatment.
Main outcome measure(s)
The number and quality of embryos obtained after stimulation in cancer patients undergoing IVF before or soon after chemotherapeutic treatment.
Result(s)
Four patients underwent IVF in the interval between two regimens of chemotherapy. Two of them had no follicular development; one underwent follicular puncture but no oocytes were retrieved; and, in one, six oocytes were harvested but only one good quality embryo was obtained. In the seven patients who underwent IVF before starting chemotherapy, between 4 and 11 embryos were obtained per patient, the majority being good quality embryos.
Conclusion(s)
Because the efficacy of IVF is dramatically reduced after even one round of chemotherapy, IVF should be performed before chemotherapy. For those who require immediate chemotherapy, ovarian tissue cryopreservation and/or oocyte cryopreservation could be used before treatment.",2005,1587
P3.01-027 TET2 Mutation as a Novel Mechanism of Acquired Resistance to EGFR TKIs Identified by a Mutational Profiling Using NGS,Y. Jin and X. Hu and M. Chen and X. Yu,,2017,1588
AGE AS ONLY PREDICTIVE FACTOR FOR SUCCESSFUL SPERM RECOVERY IN PATIENTS WITH KLINEFELTER'S SYNDROME,K. Ferhi and R. Avakian and J.F. Griveau and D. Lelannou and J.J. Patard,,2008,1589
How are issue reports discussed in Gitter chat rooms?,Hareem Sahar and Abram Hindle and Cor-Paul Bezemer,"Informal communication channels like mailing lists, IRC and instant messaging play a vital role in open source software development by facilitating communication within geographically diverse project teams e.g., to discuss issue reports to facilitate the bug-fixing process. More recently, chat systems like Slack and Gitter have gained a lot of popularity and developers are rapidly adopting them. Gitter is a chat system that is specifically designed to address the needs of GitHub users. Gitter hosts project-based asynchronous chats which foster frequent project discussions among participants. Developer discussions contain a wealth of information such as the rationale behind decisions made during the evolution of a project. In this study, we explore 24 open source project chat rooms that are hosted on Gitter, containing a total of 3,133,106 messages and 14,096 issue references. We manually analyze the contents of chat room discussions around 457 issue reports. The results of our study show the prevalence of issue discussions on Gitter, and that the discussed issue reports have a longer resolution time than the issue reports that are never brought on Gitter.",2021,1590
Rate of oxygen effect reactions in irradiated barley seeds,E. Donaldson and R.A. Nilan and C.F. Konzak,"The influence of concentrations of oxygen and radiation-induced oxygen sensitive sites on their rates of reactions in barley seeds was investigated. Himalaya (C.I. 620) barley seeds were adjusted to 9.9% water content irradiated with 60Co gamma rays and soaked at 0°C in distilled water bubbled with oxygen and nitrogen gas mixtures containing 0.0, 12.5, 25, 50 and 100% oxygen. Treatment effects were measured as M1 seedling injury. In one experiment, irradiated seeds were initially soaked in oxygen-saturate water, then transferred to O2-free water (nitrogen soaking) at selected time intervals. An increase in oxygen enhancement (OE) as measured by seedling injury is obtained with increased O2-soaking duration. A measure of the reaction rate between O2 and O2-sensitive sites (OSS) is thus obtained. This reaction is radiation exposure dependent. A reverse experiment (initial nitrogen soaking with transfer to O2 soaking at selected intervals) gives a measure of the lifetime of the OSS in water (quenching reaction). The same experimental plan is followed in two other experiments where the oxygen concentration in the gas phase of the soaking solution (OC) is variable. These experiments provide a measure of the influence of OC on the OE and OSS quenching reactions. A single radiation exposure was used. These results are demonstrated in four figures and numerical results and “reaction rates” are in two tables. The quenching reactions were independent of radiation exposure and OC. At low to intermediate OC the quenching reactions terminated the O2-OSS reaction. At 100% OC the O2-OSS reaction was two to three times faster and started three times sooner than the quenching reaction.",1982,1591
Controlled volatile release of structured emulsions based on phytosterols crystallization,Xiao-Wei Chen and Jian Guo and Jin-Mei Wang and Shou-Wei Yin and Xiao-Quan Yang,"Flavor is one of the most important criteria for consumer acceptance of food products, especially for low-fat food emulsions. In this study, we prepared structured flavoring oil-in-water (O/W) emulsions based on the crystallization behavior of β-sitosterol (Sito), a functional phytosterol, in the presence of emulsifier (sodium caseinate, SC and octenylsuccinate starch, OSS), These structured emulsions improved colloidal stability during long-term storage and delayed volatiles release under real time dynamic condition. However, the equilibrium static headspace analysis did not show significant differences in the affinities of hydrophobic volatile compounds with pure constitutes of unstructured and structured emulsions. This highlighted the importance of structural properties of the O/W interface in volatile release modulation. A modified gel trapping technology (GTT) combined with polarized light microscopy (PLM) and confocal laser scanning microscope (CLSM) were applied to characterize the microstructure at the oil-water interface, and it clearly showed the formation of a novel Sito crystal/OSA starch complex interface for OSS stabilized structured emulsion. This unique interfacial microstructure might contribute to the strong retention of volatile compounds due to steric barrier and enhanced affinity to those lipophilic volatiles. The formulated flavor emulsion with controlled volatile release profile was successfully prepared by simply blending the unstructured and structured flavoring emulsions. This work provides indications for potential applications of the formulation design in flavor emulsions and phytosterols structured emulsion as novel aroma delivery systems to improve flavor perception.",2016,1592
13 - External Servers Protection,John R. Vacca and Scott R. Ellis,"Publisher Summary
This chapter focuses on traffic problem and data security. Being “aware” of the precautions that need to be taken does not serve anyone; security is less about understanding and all about taking action. Access lists should be treated with the same level of security as government threat codes. They should change frequently (daily), should be token driven, and the storage of critical information (including and especially e-mail addresses) should be encrypted at the data level. Merely protecting these items from access is not enough. Special encryption algorithms are required for even simple examination of the data. Only the full gamut of security measures can ensure the protection of critical data―data that, if compromised and released to the public, may cause the annihilation of public trust when the data fall into the wrong hands. Such flagrant heists as the AOL breach give spammers and hackers a veritable gold mine of information. AOL users are, typically, novice users and are the most likely to fall prey to spam scams, or “scam mail.” By carefully and thoroughly monitoring traffic and engaging the full capabilities of access control lists (ACLs), application firewalls, and inherent database security, much of the problematic traffic present on the Internet today can have been avoided.",2005,1593
The impacts of financial linkage on sustainability of less-formal financial institutions: Experience of savings and credit co-operative societies in Tanzania,Benson Otieno Ndiege and Xuezhi Qin and Isaac Kazungu and John Moshi,"The developing economies are experiencing a growing trend of financial Linkage between formal and less-formal financial institutions. Normally, less-formal financial institutions receive loanable funds from formal financial institutions as an approach to meet their financing deficit, while formal financial institutions engage in linkage as a mean to expand business. The main concern of stakeholders regarding this practice is how such linkage can affect the performance of the less-formal financial institutions. In Tanzania, the Savings and Credit Co-operative Societies (SACCOS) are the most used less-formal financial institutions which are also highly involved in financial linkage. In this study therefore, we used Tanzania SACCOS’ financial statement data, for the period of 2004–2011, and panel data regression model to examine the relationship between financial linkage (measured as financial dependency ratio) and sustainability (measured as Operational Self Sufficiency) of less-formal financial institutions. The findings suggest that the higher the level of financial linkage the more the SACCOS become unsustainable. Implying that, to be sustainable institutions, the SACCOS should try keep away from the use of external funds in their loan portfolio.",2014,1594
EFFICACY AND SAFETY OF ETONOGESTREL AND TESTOSTERONE UNDECANOATE FOR MALE HORMONAL CONTRACEPTION,E. Mommers and W.M. Kersemaekers and J. Elliesen and E.J.H. Meuleman and M. Kepers and D. Apter and H.M. Behre and J. Beynon and P.M. Bouloux and A. Costantino and H.P. Gerbershagen and L. Grønlund and D. Heger-Mahn and I. Huhtaniemi and E. Koldewijn and C. Lange and S. Lindenberg and C. Meriggiola and P. Mulders and E. Nieschlag and A. Perheentupa and A. Solomon and L. Väisälä and F. Wu and M. Zitzmann,,2008,1595
Future mode of operations for 5G – The SELFNET approach enabled by SDN/NFV,Pedro Neves and Rui Calé and Mário Costa and Gonçalo Gaspar and Jose Alcaraz-Calero and Qi Wang and James Nightingale and Giacomo Bernini and Gino Carrozzo and Ángel Valdivieso and Luis Javier {García Villalba} and Maria Barros and Anastasius Gravas and José Santos and Ricardo Maia and Ricardo Preto,"The 5G infrastructure initiative in Europe115G Infrastructure Public Private Partnership, [Online]. Available here: https://5g-ppp.eu/ has agreed a number of challenging key performance indicators (KPIs) to significantly enhance the user experience and support a number of use cases with very demanding requirements on the network infrastructure. At the same time there is high pressure on the reduction of the operational expenditure (OPEX). A contribution to meeting the KPIs and to reduce OPEX is to evolve the management of the network into a fully autonomic and intelligent framework. Based on advanced technologies, such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV), the EU H2020 project SELFNET (https://selfnet-5g.eu/) is proposing an advanced network management framework to achieve these objectives.",2017,1596
Constructing a meta-model for assembly tolerance types with a description logic based approach,Yanru Zhong and Yuchu Qin and Meifa Huang and Wenlong Lu and Liang Chang,"There is a critical requirement for semantic interoperability among heterogeneous computer-aided tolerancing (CAT) systems with the sustainable growing demand of collaborative product design. But current data exchange standard for exchanging tolerance information among these systems can only exchange syntaxes and cannot exchange semantics. Semantic interoperability among heterogeneous CAT systems is difficult to be implemented only with this standard. To address this problem, some meta-models of tolerance information supporting semantic interoperability and an interoperability platform based on these meta-models should be constructed and developed, respectively. This paper mainly focuses on the construction of a meta-model for assembly tolerance types with a description logic ALC(D) based approach. Description logics, a family of knowledge representation languages for authoring ontologies, are well-known for having rigorous logic-based semantics which supports semantic interoperability. ALC(D) can provide a formal method to describe the research objects and the relations among them. In this formal method, constraint relations among parts, assembly feature surfaces and geometrical features are defined with some ALC(D) assertional axioms, and the meta-model of assembly tolerance types is constructed through describing the spatial relations between geometrical features with some ALC(D) terminological axioms. Besides, ALC(D) can also provide a highly efficient reasoning algorithm to automatically detect the inconsistency of the knowledge base, a finite set of assertional and terminological axioms. With this reasoning algorithm, assembly tolerance types for each pair of geometrical features are generated automatically through detecting the inconsistencies of the knowledge base. An application example is provided to illustrate the process of generating assembly tolerance types.",2014,1597
"On data-driven curation, learning, and analysis for inferring evolving internet-of-Things (IoT) botnets in the wild",Morteza {Safaei Pour} and Antonio Mangino and Kurt Friday and Matthias Rathbun and Elias Bou-Harb and Farkhund Iqbal and Sagar Samtani and Jorge Crichigno and Nasir Ghani,"The insecurity of the Internet-of-Things (IoT) paradigm continues to wreak havoc in consumer and critical infrastructures. The highly heterogeneous nature of IoT devices and their widespread deployments has led to the rise of several key security and measurement-based challenges, significantly crippling the process of collecting, analyzing and correlating IoT-centric data. To this end, this paper explores macroscopic, passive empirical data to shed light on this evolving threat phenomena. The proposed work aims to classify and infer Internet-scale compromised IoT devices by solely observing one-way network traffic, while also uncovering, reporting and thoroughly analyzing “in the wild” IoT botnets. To prepare a relevant dataset, a novel probabilistic model is developed to cleanse unrelated traffic by removing noise samples (i.e., misconfigured network traffic). Subsequently, several shallow and deep learning models are evaluated in an effort to train an effective multi-window convolutional neural network. By leveraging active and passing measurements when generating the training dataset, the neural network aims to accurately identify compromised IoT devices. Consequently, to infer orchestrated and unsolicited activities that have been generated by well-coordinated IoT botnets, hierarchical agglomerative clustering is employed by scrutinizing a set of innovative and efficient network feature sets. Analyzing 3.6 TB of recently captured darknet traffic revealed a momentous 440,000 compromised IoT devices and generated evidence-based artifacts related to 350 IoT botnets. Moreover, by conducting thorough analysis of such inferred campaigns, we reveal their scanning behaviors, packet inter-arrival times, employed rates and geo-distributions. Although several campaigns exhibit significant differences in these aspects, some are more distinguishable; by being limited to specific geo-locations or by executing scans on random ports besides their core targets. While many of the inferred botnets belong to previously documented campaigns such as Hide and Seek, Hajime and Fbot, newly discovered events portray the evolving nature of such IoT threat phenomena by demonstrating growing cryptojacking capabilities or by targeting industrial control services. To motivate empirical (and operational) IoT cyber security initiatives as well as aid in reproducibility of the obtained results, we make the source codes of all the developed methods and techniques available to the research community at large.",2020,1598
Center Prediction Loss for Re-identification,Lu Yang and Yunlong Wang and Lingqiao Liu and Peng Wang and Yanning Zhang,"The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e., the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.",2022,1599
Remarks on the analysis method for determining diffusion coefficient in ternary mixtures,Miren Larrañaga and M. Mounir Bou-Ali and Daniel Soler and Manex Martinez-Agirre and Aliaksandr Mialdun and Valentina Shevtsova,"The objective of this work is the determination of diagonal and cross-diagonal molecular diffusion coefficients in a ternary mixture, using the ‘Sliding Symmetric Tubes’ (SST) technique. The analyzed mixture consists of two aromatics and one normal alkane (tetrahydronaphthalene–dodecane–isobutylbenzene) with an equal mass fraction for all components (1:1:1) at 25 °C. The analytical solution corresponding to the SST technique has been successfully derived. The different fitting procedures were utilized by two scientific teams to subtract diffusion coefficients from the experimentally measured time-dependent concentration field. None of the attempts provided reliable results for the data from a single experiment. The “simplex”-based methods display reasonable results assuming that cross-diagonal coefficients are close to zero, i.e. quasi-binary and diluted mixtures. The results obtained by “trust region method” are satisfactory if the initial guess is good. To achieve better results, it is necessary to increase the number of experimental data.",2013,1600
iNIDS: SWOT Analysis and TOWS Inferences of State-of-the-Art NIDS solutions for the development of Intelligent Network Intrusion Detection System,Jyoti Verma and Abhinav Bhandari and Gurpreet Singh,"Introduction:
The growth of ubiquitous networked devices and the proliferation of geographically dispersed ‘Internet of Thing’ devices have exponentially increased network traffic. The socio-economical society is highly dependent on modern devices, and unavailability may lead to catastrophic results for even a short time. The less secure and heterogeneous devices in the public domain have shaped a cyber-attack surface in the cloud environment. Traditional approaches for Network Intrusion Detection Systems have proven ineffective and insufficient in defending against zero-day attacks.
Methods:
This article visited the advancements in the intrusion detection realm in the last five years and conducted a comprehensive retrospection of modern network intrusion detection systems. The authors have performed a comprehensive SWOT (Strength, Weakness, Opportunities, Threats) analysis of contemporary Network Intrusion Detection Systems in multiple technology dimensions, including big-data processing of high volume network traffic, machine learning, deep learning for self-learning machines, readiness for zero-day attacks, distributed processing, cost-effective solution, and ability to perform autonomous operations.
Results:
The paper turns SWOT analysis into TOWS inferences from the retrospective study for strategy formulation and features the attributes of a futuristic NIDS solution.
Discussion:
The article concludes with the discussion and future scope as the pinnacle of security solution development against zero-day attacks.",2022,1601
Brainstem and spinal projections of augmenting expiratory neurons in the rat,Kazuhisa Ezure and Ikuko Tanaka and Yoshiaki Saito,"There are two types of expiratory neurons with augmenting firing patterns (E-AUG neurons), those in the Bötzinger complex (BOT) and those in the caudal ventral respiratory group (cVRG). We studied their axonal projections morphologically using intracellular labeling of single E-AUG neurons with Neurobiotin, in anesthetized, paralyzed and artificially-ventilated rats. BOT E-AUG neurons (n=11) had extensive axonal projections to the brainstem, but E-AUG neurons (n=5) of the cVRG sent axons that descended the contralateral spinal cord without medullary collaterals. In addition to these somewhat expected characteristics, the present study revealed a number of new projection patterns of the BOT E-AUG neurons. First, as compared with the dense projections to the ipsilateral brainstem, those to the contralateral side were sparse. Second, several BOT E-AUG neurons sent long ascending collaterals to the pons, which included an axon that reached the ipsilateral parabrachial and Kölliker–Fuse nuclei and distributed boutons. Third, conspicuous projections from branches of these ascending collaterals to the area dorsolateral to the facial nucleus were found. Thus, the present study has shown an anatomical substrate for the extensive inhibitory projections of single BOT E-AUG neurons to the areas spanning the bilateral medulla and the pons.",2003,1602
"A survey on DoS/DDoS attacks mathematical modelling for traditional, SDN and virtual networks",Juan Fernando Balarezo and Song Wang and Karina Gomez Chavez and Akram Al-Hourani and Sithamparanathan Kandeepan,"Denial of Service and Distributed Denial of Service (DoS/DDoS) attacks have been one of the biggest threats against communication networks and applications throughout the years. Modelling DoS/DDoS attacks is necessary to get a better understanding of their behaviour at each step of the attack process, from the Botnet recruitment up to the dynamics of the attack. A deeper understanding of DoS/DDoS attacks would lead to the development of more efficient solutions and countermeasures to mitigate their impact. In this survey, we present a classification approach for existing DoS/DDoS models in different kinds of networks; traditional networks, Software Defined Networks (SDN) and virtual networks. In addition, this article provides a thorough review and comparison of the existing attack models, in particular we explain, analyze and simulate different aspects of three prominent models; congestion window, queuing, and epidemic models (same model used for corona virus spread analysis). Furthermore, we quantify the damage of DoS/DDoS attacks at three different levels; protocol (Transmission Control Protocol-TCP), device’s resources (bandwidth, CPU, memory), and network (infection and recovery speed).",2022,1603
"Ethical hacking for IoT: Security issues, challenges, solutions and recommendations",Jean-Paul A. Yaacoub and Hassan N. Noura and Ola Salman and Ali Chehab,"In recent years, attacks against various Internet-of-Things systems, networks, servers, devices, and applications witnessed a sharp increase, especially with the presence of 35.82 billion IoT devices since 2021; a number that could reach up to 75.44 billion by 2025. As a result, security-related attacks against the IoT domain are expected to increase further and their impact risks to seriously affect the underlying IoT systems, networks, devices, and applications. The adoption of standard security (counter) measures is not always effective, especially with the presence of resource-constrained IoT devices. Hence, there is a need to conduct penetration testing at the level of IoT systems. However, the main issue is the fact that IoT consists of a large variety of IoT devices, firmware, hardware, software, application/web-servers, networks, and communication protocols. Therefore, to reduce the effect of these attacks on IoT systems, periodic penetration testing and ethical hacking simulations are highly recommended at different levels (end-devices, infrastructure, and users) for IoT, and can be considered as a suitable solution. Therefore, the focus of this paper is to explain, analyze and assess both technical and non-technical aspects of security vulnerabilities within IoT systems via ethical hacking methods and tools. This would offer practical security solutions that can be adopted based on the assessed risks. This process can be considered as a simulated attack(s) with the goal of identifying any exploitable vulnerability or/and a security gap in any IoT entity (end devices, gateway, or servers) or firmware.",2023,1604
Operator Support System for Fertilizer Plant,A. Mjaavatten and S. Saelid,"Norsk Hydro a.s. is designing an operator support system to be implemented in a compound fertilizer plant by the summer of 1992. The goal of the system is to help operators keep process upsets to a minimum, thus reducing the total pollution from the plant as well as ensuring more stable product quality. This will be done by giving early warnings of process conditions that may lead to undesirable effects at a later time and by assisting the operators in tracing process upsets to their root cause. A combination of qualitative (rule-based) and quantitative (model-based) methods are used. The diagnosis is performed by a topological search from the detection point to the probable cause. The search is directed by automatic checking of process streams for unacceptable deviations. With few exceptions, rules are independent of the specific configuration, thus minimising the work needed after process modifications.",1992,1605
Chapter 15 - Production Operations,James Farmer and Brian Lane and Kevin Bourg and Weyl Wang,"This chapter provides information for operating the production network in steady-state production with a focus on the lifecycle of the network and the role of the Operations team in maintaining and optimizing the network. Several models for building the network are detailed, and key processes used to operate the network are enumerated and discussed.",2017,1606
RSTrace+: Reviewer suggestion using software artifact traceability graphs,Emre Sülün and Eray Tüzün and Uğur Doğrusöz,"Context:
Various types of artifacts (requirements, source code, test cases, documents, etc.) are produced throughout the lifecycle of a software. These artifacts are connected with each other via traceability links that are stored in modern application lifecycle management repositories. Throughout the lifecycle of a software, various types of changes can arise in any one of these artifacts. It is important to review such changes to minimize their potential negative impacts. To make sure the review is conducted properly, the reviewer(s) should be chosen appropriately.
Objective:
We previously introduced a novel approach, named RSTrace, to automatically recommend reviewers that are best suited based on their familiarity with a given artifact. In this study, we introduce an advanced version of RSTrace, named RSTrace+ that accounts for recency information of traceability links including practical tool support for GitHub.
Methods:
In this study, we conducted a series of experiments on finding the appropriate code reviewer(s) using RSTrace+ and provided a comparison with the other code reviewer recommendation approaches.
Results:
We had initially tested RSTrace+ on an open source project (Qt 3D Studio) and achieved a top-3 accuracy of 0.89 with an MRR (mean reciprocal ranking) of 0.81. In a further empirical evaluation of 40 open source projects, we compared RSTrace+ with Naive-Bayes, RevFinder and Profile based approach, and observed higher accuracies on the average.
Conclusion:
We confirmed that the proposed reviewer recommendation approach yields promising top-k and MRR scores on the average compared to the existing reviewer recommendation approaches. Unlike other code reviewer recommendation approaches, RSTrace+ is not limited to recommending reviewers for source code artifacts and can potentially be used for recommending reviewers for other types of artifacts. Our approach can also visualize the affected artifacts and help the developer to make assessments of the potential impacts of change to the reviewed artifact.",2021,1607
Anthelmintics for horses,Richard B. Wescott,"Modern equine anthelmintics can be divided into at least seven principal groups based on mode of action, i.e., benzimidazoles, pro-benzimidazoles, imidothiazoles, tetrahydropyrimidines, organophosphates, piperazines, and avermectins. The spectrum of activity of these drugs varies and resistance of cyathostomes to benzimidazole and pro-benzimidazole drugs has been observed in many areas. Cross resistance with other groups has not been reported in equines, however. Control is dependent upon understanding the capabilities of anthelmintics and the epizootiology of the important parasites, e.g., large strongyles, cyathostomes, ascarids and bots. Development of effective programs in a given region should be based on routine fecal analysis to ensure that treatment schedule and products selected are adequate for local climatic conditions and management methods.",1987,1608
Analysis on the acceptance of Global Trust Management for unwanted traffic control based on game theory,Yue Shen and Zheng Yan and Raimo Kantola,"The Internet has witnessed an incredible growth in its pervasive use and brought unprecedented convenience to its users. However, an increasing amount of unwanted traffic, such as spam and malware, severely burdens both users and Internet service providers (ISPs), which arouses wide public concern. A Global Trust Management (GTM) system was proposed and demonstrated to be accurate, robust and effective on unwanted traffic control in our previous work (Yan et al., 2011, Yan et al., 2013). But its acceptance by network entities (ISPs and hosts) is crucial to its practical deployment and final success. In this paper, we investigate the acceptance conditions of the GTM system using game theory. Considering the selfish nature of network entities, we address our problem as a social dilemma. To enhance cooperation among network entities, a public-goods-based GTM game is formulated with a trust-based punishment mechanism that can provide the incentives of behaving cooperatively for network entities. Meanwhile, the conditions of the adoption of GTM system are figured out. We also carry out a number of simulations to illustrate the acceptance conditions of the GTM system in practical deployment, and show the effectiveness of the trust-based punishment mechanism. Furthermore, suggestions for ISPs cooperating with antivirus vendors are put forward.",2014,1609
"The contribution of tipping fees to the operation, maintenance, and management of fecal sludge treatment plants: The case of Ghana",Rebecca Tanoh and Josiane Nikiema and Zipporah Asiedu and Nilanthi Jayathilake and Olufunke Cofie,"Globally, collection of tipping fees is being promoted as a solution to sustain the operation of fecal sludge treatment plants (FSTPs). Currently, there are six large-scale FSTPs in Ghana, of which five were in operation in June 2017. In Kumasi, Sekondi-Takoradi and Tamale, fecal sludge (FS) is co-treated with landfill leachate using waste stabilization ponds (WSPs). In Tema and Accra, FS is treated using WSPs and a mechanical dewatering system coupled with an upflow anaerobic sludge blanket (UASB). The focus of this study is FSTPs and to assess how, and if, the tipping fees set by the municipalities could enable cost recovery to sustain their long-term operation. Using a questionnaire survey to interview plant managers from the public and private sectors, and directors of waste management departments, we found that the overall average operation, maintenance and management (OM&M) costs per 1000 m3 of treated waste (FS or FS + leachate) in 2017 were USD89 in Kumasi, USD150 in Tamale, USD179 in Tema, USD244 in Sekondi-Takoradi and USD1,743 in Accra. There were important disparities between FSTPs due to their scale, age, and level of treatment and monitoring. Currently, most FSTPs charge tipping fees that range between USD310 and USD530/1000 m3 of FS, averaging USD421 ± 98/1000 m3 of FS discharged at FSTPs. Our study also showed that the OM&M costs of large-scale intensive FSTPs cannot be sustained by relying solely on tipping fees. However, there could be potential to cover the routine expenditures associated with operating smaller FSTPs that relying on WSP technologies.",2022,1610
On the impact of Continuous Integration on refactoring practice: An exploratory study on TravisTorrent,Islem Saidani and Ali Ouni and Mohamed Wiem Mkaouer and Fabio Palomba,"Context:
The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption.
Objective:
We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied.
Method:
We collect a corpus of 99,545 commits and 89,926 refactoring operations extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA).
Results:
Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context.
Conclusion:
Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems.",2021,1611
The State and Profile of Open Source Software Projects in health and medical informatics,Balaji Janamanchi and Evangelos Katsamakas and Wullianallur Raghupathi and Wei Gao,"Purpose
Little has been published about the application profiles and development patterns of open source software (OSS) in health and medical informatics. This study explores these issues with an analysis of health and medical informatics related OSS projects on SourceForge, a large repository of open source projects.
Methodology
A search was conducted on the SourceForge website during the period from May 1 to 15, 2007, to identify health and medical informatics OSS projects. This search resulted in a sample of 174 projects. A Java-based parser was written to extract data for several of the key variables of each project. Several visually descriptive statistics were generated to analyze the profiles of the OSS projects.
Results
Many of the projects have sponsors, implying a growing interest in OSS among organizations. Sponsorship, we discovered, has a significant impact on project success metrics. Nearly two-thirds of the projects have a restrictive license type. Restrictive licensing may indicate tighter control over the development process. Our sample includes a wide range of projects that are at various stages of development (status). Projects targeted towards the advanced end user are primarily focused on bio-informatics, data formats, database and medical science applications.
Conclusion
We conclude that there exists an active and thriving OSS development community that is focusing on health and medical informatics. A wide range of OSS applications are in development, from bio-informatics to hospital information systems. A profile of OSS in health and medical informatics emerges that is distinct and unique to the health care field. Future research can focus on OSS acceptance and diffusion and impact on cost, efficiency and quality of health care.",2009,1612
Participatory Framework for Urban Pluvial Flood Modeling in the Digital Twin Era,Samuel Park and Jaekyoung Kim and Yejin Kim and Junsuk Kang,"The recent advancement in digital twin technology, which creates virtual replicas of real-world processes, offers an interactive testbed for understanding and predicting environmental changes. As pluvial flood damage escalates globally in urban areas, there remains a gap in understanding the most effective collaboration between governments and local residents for sustainable flood risk management. To address this gap, we develop a participatory framework for urban pluvial flood modeling, incorporating open source software, virtual reality, minimum viable product, and gamification components. This framework engages citizens in every phase of the participatory modeling process, from input data preparation, through hydrological model construction, to model verification, and experiments. We present a case study on the recurring pluvial flood damages in South Korea's Gangnam region, demonstrating the practical implications of an interactive, web-based crowdsourcing platform to leverage community engagement and local knowledge. The results underscore the evidence of a proactive role for citizens, not merely as recipients of disaster information but as key contributors collaborating with a range of stakeholders in stormwater management and modeling. Combining digital twin technology with citizen participation can empower informed decision-making and collective actions in the evolving digital era, leading to a disaster-resilient community.",2024,1613
Surgical teams on GitHub: Modeling performance of GitHub project development processes,Oskar Jarczyk and Szymon Jaroszewicz and Adam Wierzbicki and Kamil Pawlak and Michal Jankowski-Lorek,"Context: Better methods of evaluating process performance of OSS projects can benefit decision makers who consider adoption of OSS software in a company. This article studies the closure of issues (bugs and features) in GitHub projects, which is an important measure of OSS development process performance and quality of support that project users receive from the developer team. Objective: The goal of this article is a better understanding of the factors that affect issue closure rates in OSS projects. Methodology: The GHTorrent repository is used to select a large sample of mature, active OSS projects. Using survival analysis, we calculate short-term, and long-term issue closure rates. We formulate several hypotheses regarding the impact of OSS project and team characteristics, such as measures of work centralization, measures that reflect internal project workflows, and developer social networks measures on issue closure rates. Based on the proposed features and several control features, a model is built that can predict issue closure rate. The model allows to test our hypotheses. Results: We find that large teams that have many project members have lower issue closure rates than smaller teams. Similarly, increased work centralization increases issue closure rates. While desirable social network characteristics have a positive impact on the amount of commits in a project, they do not have significant influence on issue closure. Conclusion: Overall, findings from empirical analysis support the classic notion of Brook’s – the “surgical team” – in the context of OSS project development process performance on GitHub. The model of issue closure rates proposed in this article is a first step towards an improved understanding and prediction of this important measure of OSS development process performance.",2018,1614
DeepLib: Machine translation techniques to recommend upgrades for third-party libraries,Phuong T. Nguyen and Juri {Di Rocco} and Riccardo Rubei and Claudio {Di Sipio} and Davide {Di Ruscio},"To keep their code up-to-date with the newest functionalities as well as bug fixes offered by third-party libraries, developers often need to replace an old version of third-party libraries (TPLs) with a newer one. However, choosing a suitable version for a library to be upgraded is complex and susceptible to error. So far, Dependabot is the only tool that supports library upgrades; however, it targets only security fixes and singularly analyzes libraries without considering the whole set of related libraries. In this work, we propose DeepLib as a practical approach to learn upgrades for third-party libraries that have been performed by similar clients. Such upgrades are considered safe, i.e., they do not trigger any conflict, since, in the training clients, the libraries already co-exist without causing any compatibility or dependency issues. In this way, the upgrades provided by DeepLib allow developers to maintain a harmonious relationship with other libraries. By mining the development history of projects, we build migration matrices to train deep neural networks. Once being trained, the networks are then used to forecast the subsequent versions of the related libraries, exploiting the well-founded background related to the machine translation domain. As input, DeepLib accepts a set of library versions and returns a set of future versions to which developers should upgrade the libraries. The framework has been evaluated on two real-world datasets curated from the Maven Central Repository. The results show promising outcomes: DeepLib can recommend the next version for a library as well as a set of libraries under investigation. At its best performance, DeepLib gains a perfect match for several libraries, earning an accuracy of 1.0.",2022,1615
Nanoscale soil-water retention mechanism of unsaturated clay via MD and machine learning,Zhe Zhang and Xiaoyu Song,"In this article, we investigate the nanoscale soil-water retention mechanism of unsaturated clay through molecular dynamics and machine learning. Pyrophyllite was chosen due to its stable structure and as the precursor of other 2:1 clay minerals. A series of molecular dynamics simulations of clay at low degrees of saturation were conducted. Soil water was represented by a point cloud through the center-of-mass method. Water-air interface area was measured numerically by the alpha-shape method. The soil-water retention mechanism at the nanoscale was analyzed by distinguishing adsorptive pressure and capillary pressure at different mass water contents and considering the apparent capillary interface area (i.e., water-air interface area per unit water volume). The water number density profile was used to quantify the adsorption effect. A neural-network based machine learning technique was utilized to construct functional relationships among matric suction, the mass water content, and the apparent water-air interface area. Our numerical results have demonstrated from a nanoscale perspective that the adsorption effect is dominated by the van der Waals force and hydroxyl hydration between the clay surface and water. As the mass water content increases, the adsorption pressure decreases, and capillarity plays a prominent role in the soil-water retention mechanism at the nanoscale.",2023,1616
Measuring shoulder injury function: Common scales and checklists,G.P. Slobogean and B.L. Slobogean,"The increasing shift towards patient-centred healthcare has lead to an emergence of patient-reported outcome instruments to quantify functional outcomes in orthopaedic patients. Unfortunately, selecting an instrument for use in a shoulder trauma population is often problematic because most shoulder instruments were initially designed for use with chronic shoulder pathology patients. To ensure an instrument is valid, reliable, and sensitive to clinical changes, it is important to obtain psychometric evidence of its use in the target population. Four commonly used shoulder outcome instruments are reviewed in this paper: American Shoulder and Elbow Surgeons Standardized Shoulder Assessment Form (ASES); Constant–Murley shoulder score (CMS); Disabilities of Arm, Shoulder, and Hand (DASH); Oxford Shoulder Score (OSS). Each instrument was reviewed for floor or ceiling effects, validity, reliability, responsiveness, and interpretability. Additionally, evidence of each instrument's psychometric properties was sought in shoulder fracture populations. Based on the current literature, each instrument has limited amounts of evidence to support their use in shoulder trauma populations. Overall, psychometric evaluations in isolated shoulder fracture populations remain scarce, and clinicians must remember that an instrument's properties are defined for the population tested and not the instrument. Therefore, caution must always be exercised when using an instrument that has not been fully evaluated in trauma populations.",2011,1617
BERT-Based Medical Chatbot: Enhancing Healthcare Communication through Natural Language Understanding,Arun Babu and Sekhar Babu Boddu,"The advent of modern technologies like Artificial Intelligence(AI), Internet of Things(IoT) and Deep Learning(DL) has ushered in a transformative era in healthcare, offering innovative solutions towards personalized healthcare by enhancing the quality of various medical services. Our proposed methodology involves the development of a BERT-based medical chatbot, leveraging cutting-edge deep learning technology to significantly enhance healthcare communication and accessibility. The traditional challenges faced by medical chatbots, such as imprecise understanding of medical conversations, inaccurate responses to jargon, and the inability to offer personalized feedback, are addressed through the utilization of Bidirectional Encoder Representations from Transformers (BERT). The performance metrics of our chatbot underscore its effectiveness. With an accuracy of 98%, the chatbot ensures a high level of precision in handling medical queries. The precision score of 97% attests to the accuracy and reliability of its responses. The AUC-ROC score of 97% indicates the chatbot's exceptional ability to predict specific diseases based on user queries and symptoms, showcasing its robust predictive power. Furthermore, a recall of 96% demonstrates the chatbot's capability to avoid missing cases in medical diagnoses, ensuring comprehensive coverage of potential conditions. The F1 score of 98% showcases the chatbot's proficiency in delivering accurate and personalized healthcare information, striking a harmonious balance between precision and recall. Our BERT-based medical chatbot not only addresses the limitations of traditional approaches but also achieves a remarkable performance with high accuracy, precision, predictive power, and comprehensive coverage, making it a valuable tool for advancing the quality of healthcare services.",2024,1618
"Eco-friendly modification of bitumen: The effects of rubber wastes and castor oil on the microstructure, processability and properties",Maciej Sienkiewicz and Przemysław Gnatowski and Mateusz Malus and Anna Grzegórska and Hossein Ipakchi and Maryam Jouyandeh and Justyna Kucińska-Lipka and Francisco Javier Navarro and Mohammad Reza Saeb,"The bitumen industry in the European Union is facing several difficulties, including rising demand, unstable oil supply, rising prices for synthetic polymer modifiers, and a focus on lowering carbon footprint. Bitumen modification with crumb rubber (CR) is one of the most promising solution to these challenges. However, CR-modified bitumen have poor processability and low storage stability. To overcome these flaws we are introducing a sustainable approach for ecological modification of bitumen taking advantage of renewable resources. For this reason, unmodified castor oil was selected as a green modifier of reclaimed rubber dust. The ecologically modified bitumen underwent visco-elastic behavior analysis based on rheological tests varying the temperature. The modification with rubber-oil improved the longevity of typical pavement, featured by an exceptional deformation resistance at elevated temperatures (well above 70 °C, the maximum pavement temperature reported in the region). The Cole-Cole graphs and black space diagrams unraveled the enhanced elasticity of bitumen. Technically, in comparison to plain bitumen, the compatibility ratio of modified bitumen to aggregates showed an uplift by 258%. The environmentally friendly bitumen modified ecologically herein revealed potential for performance window enlargement. Nevertheless, future investigations should focus on optimization of the bitumen formulation, along with examination of other sustainable moieties for the sake of commercialization of the developed binders in pavement construction.",2024,1619
Outcome measures in the management of proximal humeral fractures: a systematic review of their use and psychometric properties,Alexander T.M. {van de Water} and Nora Shields and Nicholas F. Taylor,,2011,1620
D-Score: An expert-based method for assessing the detectability of IoT-related cyber-attacks,Yair Meidan and Daniel Benatar and Ron Bitton and Dan Avraham and Asaf Shabtai,"IoT devices are known to be vulnerable to various cyber-attacks, such as data exfiltration and the execution of flooding attacks as part of a DDoS attack. When it comes to detecting such attacks using network traffic analysis, it has been shown that some attack scenarios are not always equally easy to detect if they involve different IoT models. That is, when targeted at some IoT models, a given attack can be detected rather accurately, while when targeted at others the same attack may result in too many false alarms. In this research, we attempt to explain this variability of IoT attack detectability and devise a risk assessment method capable of addressing a key question: how easy is it for an anomaly-based network intrusion detection system to detect a given cyber-attack involving a specific IoT model? In the process of addressing this question we (a) investigate the predictability of IoT network traffic, (b) present a novel taxonomy for IoT attack detection which also encapsulates traffic predictability aspects, (c) propose an expert-based attack detectability estimation method which uses this taxonomy to derive a detectability score (termed ‘D-Score’) for a given combination of IoT model and attack scenario, and (d) empirically evaluate our method while comparing it with a data-driven method.",2023,1621
An effective technique for detecting minority attacks in NIDS using deep learning and sampling approach,R. Harini and N. Maheswari and Sannasi Ganapathy and M. Sivagami,"Anomaly-based intrusion detection system have been consistently used in business organizations and military to detect a breach in network by identifying any activity that deviates from the baseline pattern. In this paper, we propose an effective intrusion detection technique to identify and predict the minority attacks with three layers. Here, the first layer utilizes a Weighted Deep Neural Network (WDNN) for identifying the suspicious traffic samples in network and it is passed to the second layer. Layer 2 classifies the traffic samples as normal or majority and minority attacks using Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM). Any traffic sample classified as minority attack is sent to Layer 3 that utilizes XGBoost algorithm. Layer 3 classifies the samples into their respective minority attack classes. To boost the detection rate of minority attacks, system employs a One-Sided Selection under-sampling algorithm to remove noisy samples from the majority attack classes. An Adaptive Synthetic (ADASYN) oversampling algorithm generates synthetic samples of minority attack classes. To evaluate the system, the datasets namely NSL KDD, CICIDS-2017 and CIDDS 001 dataset are used. The system attained an overall accuracy of 97.94% on NSL KDD dataset, 98.3% on CICIDS-2017 dataset and 97.9% on CIDDS 001 dataset.",2023,1622
Ex-vivo oocyte retrieval for fertility preservation,Human M. Fatemi and Dimitra Kyrou and Majedah Al-Azemi and Dominique Stoop and Philippe {De Sutter} and Claire Bourgain and Paul Devroey,"Objective
To report a novel fertility preservation strategy in a woman with recurrent serous borderline ovarian tumor in the conserved ovary involving ex-vivo retrieval of in vivo matured oocytes and subsequent embryo cryopreservation.
Design
Case report.
Setting
Tertiary infertility care unit.
Patient(s)
A 27-year-old woman presented for follow-up visit with a history of borderline serous adenocarcinoma treated conservatively with left oophorectomy and fertility-sparing laparoscopic staging. Ultrasound scan revealed a recurrent disease in the right ovary.
Intervention(s)
Ex-vivo retrieval of mature oocytes after ovarian stimulation.
Main Outcome Measure(s)
Fertility preservation.
Result(s)
The patient underwent ovarian stimulation followed by a laparotomy and oophorectomy on the day of oocyte retrieval. A puncture of the follicles was performed in the operating theatre with a maximum ischemia time of 14 minutes. Eleven mature oocytes were aspirating, resulting in seven zygotes for cryopreservation.
Conclusion(s)
Mature oocytes can be successfully retrieved ex-vivo from the oophorectomy specimen after a controlled ovarian hyperstimulation (COH) protocol. This method provides a possible strategy for fertility preservation in patients with recurrent ovarian cancer without the risk of cancer cells spillage associated with the standard transvaginal oocyte retrieval.",2011,1623
A survey of security issues for cloud computing,Minhaj Ahmad Khan,"High quality computing services with reduced cost and improved performance have made cloud computing a popular paradigm. Due to its flexible infrastructure, net centric approach and ease of access, the cloud computing has become prevalent. Its widespread usage is however being diminished by the fact that the cloud computing paradigm is yet unable to address security issues which may in turn aggravate the quality of service as well as the privacy of customers' data. In this paper, we present a survey of security issues in terms of security threats and their remediations. The contribution aims at the analysis and categorization of working mechanisms of the main security issues and the possible solutions that exist in the literature. We perform a parametric comparison of the threats being faced by cloud platforms. Moreover, we compare various intrusion detection and prevention frameworks being used to address security issues. The trusted cloud computing and mechanisms for regulating security compliance among cloud service providers are also analyzed. Since the security mechanisms continue to evolve, we also present the future orientation of cloud security issues and their possible countermeasures.",2016,1624
A hybrid meta-heuristic algorithm for multi-objective IoT service placement in fog computing environments,Hemant Kumar Apat and Bibhudutta Sahoo and Veena Goswami and Rabindra K. Barik,"The fog computing paradigm is promising for deploying various delay-sensitive Internet of Things (IoT) applications. The resource-constrained fog devices restrict the number of application deployments due to a lack of efficient resource estimation and discovery mechanisms for various emergent heterogeneous IoT applications. An efficient resource allocation strategy is one of the best choices to meet these application’s Quality of Service (QoS) requirements and improve system performance. However, finding the best allocation strategy for IoT applications with more than one QoS parameter is a challenge, and it has been proved as a non-deterministic polynomial time (NP)-complete problem. This article formulates a classical weighted multi-objective IoT service placement to optimize three parameters, i.e., makespan, cost, and energy. The non-convexity nature of the solution space motivates us to focus on the population-based meta-heuristic algorithm, i.e. Genetic Algorithm (GA), Simulated Annealing (SA) and Particle Swarm Optimization (PSO), along with their combination GA-SA, and GA-PSO. It implements the algorithm and compares it with the greedy-based random placement approach, varying the number of IoT applications with different parameters. The final results reveal that the hybrid method GA-SA outperforms other state-of-the-art algorithms.",2024,1625
Chapter 14 - The Botnet Problem,Nailah Mims,"Automated programs that covertly infect vulnerable computers and turn them into hosts for unauthorized networks are called bots. These infected hosts are configured to report back to a central system(s) run by an attacker, or bot-herder, and collectively form a botnet. Botnets may contain thousands of hosts and can be used to execute a variety of cyber-based attacks, in particular flooding target's networks and devices with too much traffic and stealing data from hosts infected with the bots. Accordingly, the aim of attackers who deploy botnets is usually to interrupt a target's operations or achieve financial gain. This problem is compounded by the spread of new technologies that connect a variety of devices to the Internet, making them susceptible to being an unwitting bot host or else a target of the botnet's massive disruption campaign. Given the global implications of so many potential hosts, preventing the spread of botnets and tracing the malicious cyber activity are key challenges to overcome when addressing the botnet problem.",2017,1626
12 - Application of Social Network Analysis to the Study of Open Source Software,Jin Xu and Scott Christley and Gregory Madey,"Publisher Summary
This chapter constructs four social networks for the Open Source Software (OSS) development community at Source Forge. Social network analysis has been used in many research areas to discover the intrinsic mechanisms of social communities by examining the topological properties of the social network formed by relationships between the actors and the groups in those communities. For each social network, number of people are expanded in the network by including the next set of peripheral users as defined by their role in the community, moving from the core project leaders, to the core developers, to the co-developers, and finally out to active users. All the social networks have scale-free properties, and the inclusion of the co-developers and active users triggers the emergence of the small world phenomenon for the social network. The chapter examines how these topological network properties potentially explain the success and efficiency of OSS development practices.",2006,1627
Who should comment on this pull request? Analyzing attributes for more accurate commenter recommendation in pull-based development,Jing Jiang and Yun Yang and Jiahuan He and Xavier Blanc and Li Zhang,"Context: The pull-based software development helps developers make contributions flexibly and efficiently. Commenters freely discuss code changes and provide suggestions. Core members make decision of pull requests. Both commenters and core members are reviewers in the evaluation of pull requests. Since some popular projects receive many pull requests, commenters may not notice new pull requests in time, and even ignore appropriate pull requests. Objective: Our objective in this paper is to analyze attributes that affect the precision and recall of commenter prediction, and choose appropriate attributes to build commenter recommendation approach. Method: We collect 19,543 pull requests, 206,664 comments and 4817 commenters from 8 popular projects in GitHub. We build approaches based on different attributes, including activeness, text similarity, file similarity and social relation. We also build composite approaches, including time-based text similarity, time-based file similarity and time-based social relation. The time-based social relation approach is the state-of-the-art approach proposed by Yu et al. Then we compare precision and recall of different approaches. Results: We find that for 8 projects, the activeness based approach achieves the top-3 precision of 0.276, 0.386, 0.389, 0.516, 0.322, 0.572, 0.428, 0.402, and achieves the top-3 recall of 0.475, 0.593, 0.613, 0.66, 0.644, 0.791, 0.714, 0.65, which outperforms approaches based on text similarity, file similarity or social relation by a substantial margin. Moreover, the activeness based approach achieves better precision and recall than composite approaches. In comparison with the state-of-the-art approach, the activeness based approach improves the top-3 precision by 178.788%, 30.41%, 25.08%, 41.76%, 49.07%, 32.71%, 25.15%, 78.67%, and improves the top-3 recall by 196.875%, 36.32%, 29.05%, 46.02%, 43.43%, 27.79%, 25.483%, 79.06% for 8 projects. Conclusion: The activeness is the most important attribute in the commenter prediction. The activeness based approach can be used to improve the commenter recommendation in code review.",2017,1628
Quality of life outcomes comparing primary Transoral Robotic Surgery (TORS) with primary radiotherapy for early-stage oropharyngeal squamous cell carcinoma: A systematic review and meta-analysis,Keshav Kumar Gupta and Mriganka De and Thanos Athanasiou and Christos Georgalas and George Garas,"Background
Transoral Robotic Surgery (TORS) and radiotherapy are considered oncologically equivalent primary treatment options for early-stage HPV-positive oropharyngeal squamous cell carcinoma (OPSCC). Quality of Life (QoL) and Patient Reported Outcome Measures (PROMs) are therefore imperative in supporting clinical decision-making and optimising patient-centred care. The aim of this article is to evaluate how these primary treatment modalities compare in terms of QoL.
Materials and methods
Systematic review and meta-analysis of studies comparing primary TORS and primary radiotherapy for OPSCC using validated QoL tools. Swallowing and global QoL were the primary endpoints with secondary endpoints including all other QoL domains. An inverse variance random-effects model was employed to calculate the weighted estimate of the treatment effects across trials.
Results
A total of six studies collectively reporting on 555 patients were included (n = 236 TORS and n = 319 radiotherapy). Meta-analysis showed no significant difference for swallowing (mean difference = −0.24, p = 0.89) and global QoL (mean difference = 4.55, p = 0.14). For the remaining QoL domains (neck/shoulder impairment, neurotoxicity, voice, xerostomia, speech, and distress), the scarcity of data did not permit meta-analysis. However, the existing data showed no significant difference for any except for xerostomia where TORS appears favourable in the sole study reporting on this.
Conclusions
TORS and radiotherapy appear to be comparable primary treatment options for early stage OPSCC when it comes to QoL. However, a substantial proportion of patients in the TORS group received adjuvant (chemo)radiotherapy rendering it difficult to establish the ‘true’ QoL outcomes following surgery alone. There are also minimal studies reporting QoL outcomes beyond swallowing and global QoL. Further research is therefore needed, including more randomised trials adequately powered to detect differences in QoL outcomes.",2024,1629
Impact of infertility drugs after treatment of borderline ovarian tumors: results of a retrospective multicenter study,Anne Fortin and Philippe Morice and Anne Thoury and Sophie Camatte and Caroline Dhainaut and Patrick Madelenat,"Objective
To evaluate safety and fertility outcome after the use of infertility drugs in patients who were treated conservatively for a borderline ovarian tumor (BOT).
Design
A retrospective multicenter study.
Setting
Centers participating in the French National Register on In Vitro Fertilization registry.
Patient(s)
Thirty patients who were treated for BOT who underwent ovarian induction (OI).
Intervention(s)
Ovarian induction was performed in 25 patients for infertility after conservative surgery and before surgery for recurrent disease in 5 patients with a single ovary (emergency cases).
Main Outcomes Measure(s)
Fertility and recurrences rates.
Result(s)
The mean number of cycles of OI per patient was 2.6 (range, 1–10 cycles). The median follow-up time after treatment of the BOT was 93 months (range, 26–276 months). After a median follow-up time of 42 months after OI, 4 recurrences were observed (initial management was simple cystectomy in 3 of them). All recurrences were borderline tumors on a remaining ovary that had been treated by surgery alone. All patients are currently disease-free. Thirteen pregnancies were observed (10 pregnancies (40%) in the group of 25 patients who were treated for infertility).
Conclusion(s)
These results suggest that infertility drugs could be used safely in patients who experience infertility after conservative management of an early-stage BOT.",2007,1630
Common vulnerability scoring system prediction based on open source intelligence information sources,Philipp Kühn and David N. Relke and Christian Reuter,"The number of newly published vulnerabilities is constantly increasing. Until now, the information available when a new vulnerability is published is manually assessed by experts using a Common Vulnerability Scoring System (CVSS) vector and score. This assessment is time consuming and requires expertise. Various works already try to predict CVSS vectors or scores using machine learning based on the textual descriptions of the vulnerability to enable faster assessment. However, for this purpose, previous works only use the texts available in databases such as National Vulnerability Database. With this work, the publicly available web pages referenced in the National Vulnerability Database are analyzed and made available as sources of texts through web scraping. A Deep Learning based method for predicting the CVSS vector is implemented and evaluated. The present work provides a classification of the National Vulnerability Database’s reference texts based on the suitability and crawlability of their texts. While we identified the overall influence of the additional texts is negligible, we outperformed the state-of-the-art with our Deep Learning prediction models.",2023,1631
Technical debt management automation: State of the art and future perspectives,João Paulo Biazotto and Daniel Feitosa and Paris Avgeriou and Elisa Yumi Nakagawa,"Context:
Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system’s maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most.
Objectives:
The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM.
Methods:
We conducted a systematic mapping study (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation.
Results:
We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges.
Conclusion:
The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts.",2024,1632
Bug priority change: An empirical study on Apache projects,Zengyang Li and Guangzong Cai and Qinyi Yu and Peng Liang and Ran Mo and Hui Liu,"In issue tracking systems, each bug is assigned a priority level (e.g., Blocker, Critical, Major, Minor, or Trivial in JIRA from highest to lowest), which indicates the urgency level of the bug. In this sense, understanding bug priority changes helps to arrange the work schedule of participants reasonably, and facilitates a better analysis and resolution of bugs. According to the data extracted from JIRA deployed by Apache, a proportion of bugs in each project underwent priority changes after such bugs were reported, which brings uncertainty to the bug fixing process. However, there is a lack of in-depth investigation on the phenomenon of bug priority changes, which may negatively impact the bug fixing process. Thus, we conducted a quantitative empirical study on bugs with priority changes through analyzing 32 non-trivial Apache open source software projects. The results show that: (1) 8.3% of the bugs in the selected projects underwent priority changes; (2) the median priority change time interval is merely a few days for most (28 out of 32) projects, and half (50. 7%) of bug priority changes occurred before bugs were handled; (3) for all selected projects, 87.9% of the bugs with priority changes underwent only one priority change, most priority changes tend to shift the priority to its adjacent priority, and a higher priority has a greater probability to undergo priority change; (4) bugs that require bug-fixing changes of higher complexity or that have more comments are likely to undergo priority changes; and (5) priorities of bugs reported or allocated by a few specific participants are more likely to be modified, and maximally only one participant in each project tends to modify priorities.",2024,1633
"Streaming software development: Accountability, community, and learning",Ella Kokinda and Paige Rodeghero,"People use the Internet to learn new skills, stay connected with friends, and find new communities to engage with. Live streaming platforms like Twitch.tv, YouTube Live, and Facebook Gaming provide a place where all three of these activities intersect and enable users to live-stream themselves playing a video game or live-coding software and game development, as well as the ability to participate in chat while watching someone else engage in an activity. Through fifteen interviews with software and game development streamers, we investigate why people choose to stream themselves programming and if they perceive themselves improving their programming skills by live streaming. We found that the motivations to stream included accountability, self-education, community, and visibility of the streamers’ work, and streamers perceived a positive influence on their ability to write source code. Our findings implicate that alternative learning methods like live streaming programming are a beneficial tool in the age of the virtual classroom. This work also contributes to and extends research efforts surrounding educational live streaming and collaboration in developer communities.",2023,1634
F.70. Melanoma Inhibitory Activity (MIA) Reflects Chondrocyte Anabolism in Chronic Inflammatory Arthritis: Suppression By Proinflammatory Cytokines Is Reversed By Targeted Therapy,Bernard Vandooren and Marie-Jose {van Lierop} and Tineke Cantaert and Leen {De Rycke} and Elli Kruithof and Eric Veys and Ebo Bos and Annemieke Boots and Dominique Baeten,,2006,1635
Analyzing the Tower of Babel with Kaiaulu,Carlos Paradis and Rick Kazman and Damian Tamburri,"Context:
An extensive body of work has examined socio-technical activities in software development; however, the availability of tools to enable these studies is limited.
Aim:
We extend Kaiaulu, a software package for Mining Software Repositories to enable a broad spectrum analysis of Social Smells and Motifs.
Methods:
We perform a literature review to identify what tools are available which implement graph construction methods and social smell metrics, contextualizing the contributions of our tool.
Results:
The few tools identified in the literature either leverage fewer parts of the software ecosystem, have been archived, or depend on components no longer maintained.
Conclusion:
The socio-technical features in Kaiaulu complement existing tools and related literature, while providing a simple architecture to facilitate ease or use, and ease of learning, benefitting reproducibility.
Tool Repository:
github.com/sailuh/kaiaulu",2024,1636
Aptamers as the powerhouse of dot blot assays,Marimuthu Citartan,"Dot blot assays have always been associated with antibodies as the main molecular recognition element, which are widely employed in a myriad of diagnostic applications. With the rising of aptamers as the equivalent molecular recognition elements of antibodies, dot blot assays are also one of the diagnostic avenues that should be scrutinized for their amenability with aptamers as the potential surrogates of antibodies. In this review, the stepwise procedures of an aptamer-based dot blot assays are underscored before reviewing the existing aptamer-based dot blot assays developed so far. Most of the applications center on monitoring the progress of SELEX and as the validatory assays to assess the potency of aptamer candidates. For the purpose of diagnostics, the current effort is still languid and as such possible suggestions to galvanize the move to spur the aptamer-based dot blot assays to a point-of-care arena are discussed.",2021,1637
Floatability of polymer materials modulated by frothers,Hui Wang and Chong-qing Wang and Jian-gang Fu,"Flotation tests of 35 polymer materials were carried out to investigate their floatability modulated by frothers. Results of flotation tests demonstrated that polymer resins and soft PVC showed high floatability, floatability of hard PVC plastics was relatively low and was related to the frothers, and there exists significant difference in the floatability of different post-consumer plastics. Flotation rate of post-consumer plastics varies from 0% to 100%. Furthermore, three-category low-energy surface (LES) was defined based on the hydrophile index of the materials involved in this paper, and an adsorption model was proposed to explain the results of flotation and to discuss the floatability of polymer materials modulated by frothers. Frother molecules are prone to adsorb on the surface of bubble rather than LES at relatively low concentration, bubble adsorbed by frother molecules is prone to approach first-category LES rather than third-category LES, and the structure of liquid film is formed on the first-category LES at large concentration. Floatability of polymer materials modulated by frothers is further discussed: frothers increase the floatability of the first-category LES but decrease the floatability of the third-category LES, while the floatability of the second-category LES is related to the type of frothers.",2013,1638
Did the global warming confirm in central northern Sahara (case of the Ouargla region )?,Fatiha Hadjaidji-Benseghier and Talbi Nadjib and et Derridj Arrezki,"Climate change shows itself in various scales in the Mediterranean and Sahara region. The study aims at characterizing climate of the Sahara in a bigger scale otherwise-said, precision as for the current climate reigning (evolution), at the level of the region of Ouargla. For that purpose, we adopted a complementary, dynamic and static approach. The dynamic approach was approached by compilations of the previous works. Followed by a static analysis, leaning on climatological data, spread over a period going from 1978 till 2015. It emerges from it that the dynamic character is characterized by the frequency of the regime NAO + with regard to that of the NAO-. This regime expresses himself daily, in terms, of temperature and haste, explaining the importing lived reheating these last decades. So, the results show that the region is characterized by a ""hot"" thermoclimate, expressed by all the energy parameters (rise of the fraction of sunstroke and the temperature) and a ""dry""ombroclimate.",2017,1639
Our referees – An appreciation,Michael Blackman and David Newton,,2014,1640
Surface Modification of Chlorella Vulgaris Cells Using Magnetite Particles,G. Procházková and I. Šafařík and T. Brányik,"Expensive cell concentration procedures represent one of the bottlenecks of large-scale microalgal biotechnological processes as many industrially attractive species have a small cell size and sustain in suspension. An economically effective solution is to alter the process conditions for the cells to form aggregates, which sediment faster. The use of magnetic agents binding to the cell surface and forming larger complexes, that sediment very fast upon application of an external magnetic field, is a rarely explored possibility in this area. We used commercially available, finely pulverized magnetite (Sigma Aldrich) as a potential harvesting agent and studied its surface interactions with an industrially important microalgal strain (Chlorella vulgaris). Firstly, we characterized the interacting surfaces in model environments by zeta potential and contact angle measurements, which were followed by particle size determination. Secondly, we applied the XDLVO theory to predict favorable experimental conditions for a successful magnetic cell modification, which would lead to an effective biomass separation. The hypotheses were then tested by using various ratios of magnetic agent and microalgal biomass under different environmental conditions. Obtained results were in good accordance with the predictions and we achieved an excellent separation efficiency of over 90% within a few minutes at a ratio of microalgae to magnetite 1:26 (w/w). We can conclude that magnetite successfully modifies the microalgal surface under certain conditions and is a promising agent for harvesting C. vulgaris, enabling high separation efficiencies in a very short period of time, but further research is necessary to optimize the process.",2012,1641
"Oral testosterone supplementation and chronic low-grade inflammation in elderly men: A 26-week randomized, placebo-controlled trial",Hamid Reza Nakhai-Pour and Diederick E. Grobbee and Marielle H. Emmelot-Vonk and Michiel L. Bots and Harald J.J. Verhaar and Yvonne T. {van der Schouw},"Background
To determine the effect of oral testosterone supplementation on systemic low-grade inflammation measured by high-sensitive C-reactive protein (hs-CRP) in aging men with low testosterone levels.
Methods
Two hundred thirty-seven men aged 60 to 80 years with a testosterone level of <13.7 nmol/L (below the 50th percentile of the population distribution) were recruited into a double-blind randomized placebo-controlled trial. Participants were randomized to either 4 capsules of 40 mg testosterone undecanoate (Andriol Testocaps, NV Organon, Oss, The Netherlands) or placebo daily for 26 weeks. Serum levels of hs-CRP were measured at baseline and at 26 weeks using a near-infrared particle immunoassay of the Synchron LX System (Beckman Coulter, Fullteron, CA).
Results
The median baseline hs-CRP level was 1.95 mg/L (0.30-6.43) in the testosterone group compared with 1.90 mg/L (0.40-5.91) in the placebo group. After 26 weeks of testosterone supplementation therapy, the 2 intervention groups were not statistically significantly different (median hs-CRP 2.20 vs 2.00 mg/L, interquartile range 0.40-6.54 vs 0.50-5.70, P = .36). In subgroup analysis, neither baseline testosterone level, nor age, nor baseline CRP-level modified the effect of testosterone supplementation on CRP levels.
Conclusion
Oral testosterone undecanoate supplementation, in dosage of 160 mg daily for 26 weeks, does not increase hs-CRP levels in elderly men.",2007,1642
Comprehensive Mechanism for Four-Level Energy Cost Control,Vladimir V. Tsyganov,"The paper examines the model of energy cost management in a four-level control system of company, includes its boss with the advisor on top level, administrator on the middle level, and managing director of the plant at the bottom level. Neither the boss nor the advisor knows minimal administrator and plant stochastic energy costs, and need to learn to control them. The administrator knows minimal stochastic costs better than the boss and the advisor. So the administrator can manipulate costs in order to influence the results of learning the advisor and the boss in own favor. But the administrator itself does not know the minimum stochastic costs in plant. This can be used by its director to achieve own goal. So the administrator also needs to learn to control the managing director. All of the interests of such active elements are reflected in the model by introducing goal functions. On the basis of this model, sufficient conditions have been found for the synthesis of a hierarchy of mechanisms for managing energy costs, ensuring the use of stochastic possibilities of reducing these costs. In these conditions, the boss needs training with the help of a self-learning consultant. Through this training, the boss can rate the administrator's costs. This mechanism encourages the administrator, firstly, to minimize overhead energy costs and, secondly, to implement an adaptive cost savings mechanism on the plant. Such an adaptive mechanism includes standardization and stimulation procedures that encourage the managing director to minimize the energy costs of the plant. The application of this approach is illustrated by the example of managing energy costs in a wagon repair company.",2022,1643
Ovarian-sparing surgery for ovarian tumors in children: A systematic review and meta-analysis,Luca Pio and Ahmed Abu-Zaid and Tarek Zaghloul and Huma F. Halepota and Andrew M. Davidoff and Paul D. Losty and Hafeez H. Abdelhafeez,"Introduction
An increased number of children and adolescents with ovarian tumors have been managed with ovarian-sparing surgery in the last few years. However, comprehensive data on fertility outcomes and local relapse are scarce. In this study, we systematically describe the contemporary outcomes of ovarian-sparing surgery, as reported in the literature.
Materials and methods
Using PRISMA guidelines, we analyzed studies reporting ovarian-sparing techniques for ovarian tumors in children and adolescents. from 1980 to 2022. Reports with fewer than three patients, narrative reviews, and opinion articles were excluded. Statistical analysis was performed for dichotomous and continuous variables.
Results
Of 283 articles screened, 16 papers (3057 patients) met inclusion criteria (15 retrospective/1 prospective) and were analyzed. The vast majority of studies had no long-term fertility follow-up data and direct comparison between ovarian-sparing surgery vs oophorectomy was reported in only a few studies. Ovarian sparing surgery was not associated with worse oncologic outcomes in terms of (i) tumour spillage or (ii) recurrence rates, and of key importance allowed a higher ovarian reserve at long term follow-up.
Conclusions
Ovarian-sparing surgery is a safe and feasible technique for benign tumors. Long-term outcome studies are needed to show efficacy and fertility preservation.",2023,1644
A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets,Nour Moustafa,"While there has been a significant interest in understanding the cyber threat landscape of Internet of Things (IoT) networks, and the design of Artificial Intelligence (AI)-based security approaches, there is a lack of distributed architecture led to generating heterogeneous datasets that contain the actual behaviors of real-world IoT networks and complex cyber threat scenarios to evaluate the credibility of the new systems. This paper presents a novel testbed architecture of IoT network which can be used to evaluate Artificial Intelligence (AI)-based security applications. The platform NSX vCloud NFV was employed to facilitate the execution of Software-Defined Network (SDN), Network Function Virtualization (NFV) and Service Orchestration (SO) to offer dynamic testbed networks, which allow the interaction of edge, fog and cloud tiers. While deploying the architecture, real-world normal and attack scenarios are executed to collect labeled datasets. The generated datasets are named ‘TON_IoT’, as they comprise heterogeneous data sources collected from telemetry datasets of IoT services, Windows and Linux-based datasets, and datasets of network traffic. The TON_IoT network dataset is validated using four machine learning-based intrusion detection algorithms of Gradient Boosting Machine, Random Forest, Naive Bayes, and Deep Neural Networks, revealing a high performance of detection accuracy using the set of training and testing. A comparative summary of the TON_IoT network dataset and other competing network datasets demonstrates its diverse legitimate and anomalous patterns that can be used to better validate new AI-based security solutions. The architecture and datasets can be publicly accessed from TON_IOT Datasets (2020).",2021,1645
Enforcing situation-aware access control to build malware-resilient file systems,Timothy McIntosh and Paul Watters and A.S.M. Kayes and Alex Ng and Yi-Ping Phoebe Chen,"Traditional non-semantic file systems are not sufficient in protecting file systems against attacks, either caused by ransomware attacks or software-related defects. Furthermore, outbreaks of new malware often cannot provide a large quantity of training samples for machine-learning-based approaches to counter malware campaigns. The malware defense system should aim to achieve the best balance between early detection and detection accuracy. In this paper, we present a situation-aware access control framework to work with existing file systems as a stackable add-on. Our framework enables the access control decision making to be deferred when required, to observe the consequence of such an access request to the file system and to roll back changes if required. As an application against ransomware attacks, it can be applied to preserve file content integrity, by enforcing that all binary files written to the file system have consistent internal file structures with the declared file types, and rolling back changes that violate such constraints. We envision our access control framework to complement existing operating system access control frameworks, to significantly reduce the dimension of data required for machine learning, and to build extra resilience into the operating systems against damages caused by either malware or software defects. We demonstrate the practicality of our framework through a prototype testing, capturing relevant ransomware situations. The experimental results along with a large ransomware dataset show that our framework can be effectively applied in practice.",2021,1646
"A survey of contemporary open-source honeypots, frameworks, and tools",Niclas Ilg and Paul Duplys and Dominik Sisejkovic and Michael Menth,"Automated attacks allow adversaries to exploit vulnerabilities in enterprise IT systems at short notice. To identify such attacks as well as new cybersecurity threats, defenders use honeypot systems; these monitored decoy resources mimic legitimate devices to entice adversaries. The domain of enterprise IT honeypots has been an active area of development and research, especially in the open-source community. In this work, we survey open-source honeypots, honeypot frameworks, and tools that help to develop or discover honeypot deployments. In contrast to existing surveys, our work provides a detailed discussion of the honeypots’ system architecture, software architecture, and cloud-native deployment options. In addition, we cover the most recent academic research in honeypot detection and evasion techniques, and discuss how these advances impact current open-source honeypots. This work helps the reader to make an educated choice when selecting a honeypot for deployment or further development.",2023,1647
Large-scale intent analysis for identifying large-review-effort code changes,Song Wang and Chetan Bansal and Nachiappan Nagappan,"Context: Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. Change intents have been studied for years to help developers understand the rationale behind code commits. However, in most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis. Objective: In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes—changes with large review effort. Method: Specifically, we first propose a feedback-driven and heuristics-based approach to identify change intents of code changes. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on four large-scale projects, one from Microsoft and three are open source projects, i.e., Qt, Android, and OpenStack. Results: Our results show that, (i) code changes with some intents (i.e., Feature and Refactor) are more likely to be LRE changes, (ii) machine learning based prediction models are applicable for identifying LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. Conclusion: The change intent analysis and its application on LRE identification proposed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process. To show how to deploy our approaches in real-world practice, we report a case study of developing and deploying the intent analysis system in Microsoft. Moreover, we also evaluate the usefulness of our approaches by using a questionnaire survey. The feedback from developers demonstrate its practical value.",2021,1648
All of the same breed? A networking perspective of private-collective innovation,George Kuk and Mario Schaarschmidt and Dirk Homscheid,"Much research on open source software development has highlighted the best-of-both-worlds benefits generated by private and collective contributions from a broad base of developers. However, these studies have tended to overlook the heterogeneous nature of various developer groups including firm-sponsored developers. To unveil the behavioural differences between developer groups, we expand upon the private-collective innovation model using a networking approach, linking network closure and positional embeddedness with technical contribution. We tested our predictions using a lagged analysis based on communication and networking behaviours on the Linux kernel mailing-list in the production of the Linux operation system over a three-year period. Our findings support our predictions, showing that network closure has an adverse impact on technical contribution. Additionally, the relationship between positional embeddedness and technical contribution follows an inverted U-shape. In addition, high positional embeddedness counteracts the negative influence of extensive network closure on technical contribution. These effects are partly moderated by respective developer groups. Our model and results offer important theoretical and practical implications for community management within the framework of private-collective innovation.",2024,1649
Web-based experimental economics software: How do they compare to desirable features?,Shu Wing Chan and Steven Schilizzi and Md Sayed Iftekhar and Raymond {Da Silva Rosa},"Web-based experiments that cut across the lab vs. field distinction are increasingly popular with economists. However, non-standardized software features and services hinder comparability and replication. This study reviews a wide selection of experimental economics software packages and evaluates them against criteria based on the logistics and operational requirements of economic experiments. We find that oTree and SoPHIE rank highest across criteria, but Veconlab and classEx might be suitable for those with a dominant need for a large library of ready-made experiments. We find a portability gap: no presently available software allows portability of experiments across platforms because of technical complexity and the challenging coordination needs of experimental economists. As a result, experiments may be replicated only on the same platform or with the same software, but general replicability is slow and costly. This constrains the development of experimental economics as a replicable science.",2019,1650
Hans Visser 1936–2001,Carel Jan {van Oss},,2002,1651
"GitHub repositories with links to academic papers: Public access, traceability, and evolution",Supatsara Wattanakriengkrai and Bodin Chinthanet and Hideaki Hata and Raula Gaikovina Kula and Christoph Treude and Jin Guo and Kenichi Matsumoto,"Traceability between published scientific breakthroughs and their implementation is essential, especially in the case of open-source scientific software which implements bleeding-edge science in its code. However, aligning the link between GitHub repositories and academic papers can prove difficult, and the current practice of establishing and maintaining such links remains unknown. This paper investigates the role of academic paper references contained in these repositories. We conduct a large-scale study of 20 thousand GitHub repositories that make references to academic papers. We use a mixed-methods approach to identify public access, traceability and evolutionary aspects of the links. Although referencing a paper is not typical, we find that a vast majority of referenced academic papers are public access. These repositories tend to be affiliated with academic communities. More than half of the papers do not link back to any repository. We find that academic papers from top-tier SE venues are not likely to reference a repository, but when they do, they usually link to a GitHub software repository. In a network of arXiv papers and referenced repositories, we find that the most referenced papers are (i) highly-cited in academia and (ii) are referenced by repositories written in different programming languages.",2022,1652
Our Referees – An Appreciation,Michael Blackman and David Newton,,2012,1653
Deep brain stimulation electrode modeling in rats,Andrea Andree and Ningfei Li and Konstantin Butenko and Maria Kober and Jia Zhi Chen and Takahiro Higuchi and Mareike Fauser and Alexander Storch and Chi Wang Ip and Andrea A. Kühn and Andreas Horn and Ursula {van Rienen},"Deep Brain Stimulation (DBS) is an efficacious treatment option for an increasing range of brain disorders. To enhance our knowledge about the mechanisms of action of DBS and to probe novel targets, basic research in animal models with DBS is an essential research base. Beyond nonhuman primate, pig, and mouse models, the rat is a widely used animal model for probing DBS effects in basic research. Reconstructing DBS electrode placement after surgery is crucial to associate observed effects with modulating a specific target structure. Post-mortem histology is a commonly used method for reconstructing the electrode location. In humans, however, neuroimaging-based electrode localizations have become established. For this reason, we adapt the open-source software pipeline Lead-DBS for DBS electrode localizations from humans to the rat model. We validate our localization results by inter-rater concordance and a comparison with the conventional histological method. Finally, using the open-source software pipeline OSS-DBS, we demonstrate the subject-specific simulation of the VTA and the activation of axon models aligned to pathways representing neuronal fibers, also known as the pathway activation model. Both activation models yield a characterization of the impact of DBS on the target area. Our results suggest that the proposed neuroimaging-based method can precisely localize DBS electrode placements that are essentially rater-independent and yield results comparable to the histological gold standard. The advantages of neuroimaging-based electrode localizations are the possibility of acquiring them in vivo and combining electrode reconstructions with advanced imaging metrics, such as those obtained from diffusion or functional magnetic resonance imaging (MRI). This paper introduces a freely available open-source pipeline for DBS electrode reconstructions in rats. The presented initial validation results are promising.",2022,1654
A socio-cognitive analysis of online design discussions in an Open Source Software community,Flore Barcellini and Françoise Détienne and Jean-Marie Burkhardt and Warren Sack,"This paper is an analysis of online discussions in an Open Source Software (OSS) design community, the Python project. Developers of Python are geographically distributed and work online asynchronously. The objective of our study is to understand and to model the dynamics of the OSS design process that takes place in mailing list exchanges. We develop a method to study distant and asynchronous collaborative design activity based on an analysis of quoting practices. We analyze and visualize three aspects of the online dynamics: social, thematic temporal, and design. We show that roles emerge during discussions according to the involvement and the position of the participants in the discussions and how they influence participation in the design discussions. In our analysis of the thematic temporal dynamics of discussion, we examine how themes of discussion emerge, diverge, and are refined over time. To understand the design dynamics, we perform a content analysis of messages exchanged between developers to reveal how the online discussions reflect the “work flow” of the project: it provides us with a picture of the collaborative design process in the OSS community. These combined results clarify how knowledge and artefacts are elaborated in this epistemic, exploration-oriented, OSS community. Finally, we outline the need to automate of our method to extend our results. The proposed automation could have implications for both researchers and participants in OSS communities.",2008,1655
A systematic literature review and taxonomy of modern code review,Nicole Davila and Ingrid Nunes,"Context:
Modern Code Review (MCR) is a widely known practice of software quality assurance. However, the existing body of knowledge of MCR is currently not understood as a whole.
Objective:
Our goal is to identify the state of the art on MCR, providing a structured overview and an in-depth analysis of the research done in this field.
Methods:
We performed a systematic literature review, selecting publications from four digital libraries.
Results:
A total of 139 papers were selected and analyzed in three main categories. Foundational studies are those that analyze existing or collected data from the adoption of MCR. Proposals consist of techniques and tools to support MCR, while evaluations are studies to assess an approach or compare a set of them.
Conclusion:
The most represented category is foundational studies, mainly aiming to understand the motivations for adopting MCR, its challenges and benefits, and which influence factors lead to which MCR outcomes. The most common types of proposals are code reviewer recommender and support to code checking. Evaluations of MCR-supporting approaches have been done mostly offline, without involving human subjects. Five main research gaps have been identified, which point out directions for future work in the area.",2021,1656
Mining Recency–Frequency–Monetary enriched insights into resources’ collaboration behavior from event data,Leen Jooken and Benoît Depaire and Mieke Jans,"Organizations increasingly rely on teamwork to achieve their goals. Therefore they continuously strive to improve their teams as their performance is interwoven with that of the organization. To implement beneficial changes, accurate insights into the working of the team are necessary. However, team leaders tend to have an understanding of the team’s collaboration that is subjective and seldom completely accurate. Recently there has been an increase in the adoption of digital support systems for collaborative work that capture objective data on how the work took place in reality. This creates the opportunity for data-driven extraction of insights into the collaboration behavior of a team. This data however, does not explicitly record the collaboration relationships, which many existing techniques expect as input. Therefore, these relationships first have to be discovered. Existing techniques that apply discovery are not generally applicable because their notion of collaboration is tailored to the application domain. Moreover, the information that these techniques extract from the data about the nature of the relationships is often limited to the network level. Therefore, this research proposes a generic algorithm that can discover collaboration relationships between resources from event data on any collaborative project. The algorithm adopts an established framework to provide insights into collaboration on a fine-grained level. To this end, three properties are calculated for both the resources and their collaboration relationships: a recency, frequency, and monetary value. The technique’s ability to provide valuable insights into the team structure and characteristics is empirically validated on two use cases.",2023,1657
Using rootkits hiding techniques to conceal honeypot functionality,Maryam Mohammadzad and Jaber Karimpour,"Honeypot is one of the existing technologies in the area of computer network security. The goal of Honeypot is to create a tempting target for the attacker. The system that is considered as a Honeypot in the network includes the services and functions of a real system that the attacker sees as a normal system and enters for exploitation. In this way, Honeypot can monitor the behaviors, patterns, and tools used in various attacks. Certainly, if the intelligent attacker realizes the existence of this trap in the target network, he can design ways to bypass it. In this case, Honeypot practically loses its effectiveness. Therefore, the issue of hiding Honeypot is one of the primary challenges in this field. In this paper, a solution to this problem is presented. In the present paper, Honeypot is concealed using the concealment techniques used in Rootkits. We use application examples and theoretical analysis results to show that the proposed Honeypots concealment approach is strong against existing kernel-based Honeypots detection methods. Sebek, VMScope, and Qebek are three Honeypots that we choose for comparison purposes. The proposed Honeypot has been compared with them in virtualization, memory usage, and kernel modification. The experimental results show that the proposed hidden Honeypot in addition to low kernel modification has no track in the memory. Also, the proposed Honeypot does not use virtualization. It can successfully be concealed in the kernel of the target system without any effect on the target system. We also implemented the proposed algorithm on the example network and test all Sebek’s detection methods on it. The experimental results show that the proposed kernel-based approach can bypass these detection methods.",2023,1658
A fast infrared radiative transfer model based on the adding–doubling method for hyperspectral remote-sensing applications,Zhibo Zhang and Ping Yang and George Kattawar and Hung-Lung {(Allen) Huang} and Thomas Greenwald and Jun Li and Bryan A. Baum and Daniel K. Zhou and Yongxiang Hu,"A fast infrared radiative transfer (RT) model is developed on the basis of the adding–doubling principle, hereafter referred to as FIRTM-AD, to facilitate the forward RT simulations involved in hyperspectral remote-sensing applications under cloudy-sky conditions. A pre-computed look-up table (LUT) of the bidirectional reflection and transmission functions and emissivities of ice clouds in conjunction with efficient interpolation schemes is used in FIRTM-AD to alleviate the computational burden of the doubling process. FIRTM-AD is applicable to a variety of cloud conditions, including vertically inhomogeneous or multilayered clouds. In particular, this RT model is suitable for the computation of high-spectral-resolution radiance and brightness temperature (BT) spectra at both the top-of-atmosphere and surface, and thus is useful for satellite and ground-based hyperspectral sensors. In terms of computer CPU time, FIRTM-AD is approximately 100–250 times faster than the well-known discrete-ordinate (DISORT) RT model for the same conditions. The errors of FIRTM-AD, specified as root-mean-square (RMS) BT differences with respect to their DISORT counterparts, are generally smaller than 0.1K.",2007,1659
Warnings: Violation symptoms indicating architecture erosion,Ruiyin Li and Peng Liang and Paris Avgeriou,"Context:
As a software system evolves, its architecture tends to degrade, and gradually impedes software maintenance and evolution activities and negatively impacts the quality attributes of the system. The main root cause behind architecture erosion phenomenon derives from violation symptoms (i.e., various architecturally-relevant violations, such as violations of architecture pattern). Previous studies focus on detecting violations in software systems using architecture conformance checking approaches. However, code review comments are also rich sources that may contain extensive discussions regarding architecture violations, while there is a limited understanding of violation symptoms from the viewpoint of developers.
Objective:
In this work, we investigated the characteristics of architecture violation symptoms in code review comments from the developers’ perspective.
Methods:
We employed a set of keywords Related to violation symptoms to collect 606 (out of 21,583) code review comments from four popular OSS projects in the openStack and qt communities. We manually analyzed the collected 606 review comments to provide the categories and linguistic patterns of violation symptoms, as well as the reactions how developers addressed them.
Results:
Our findings show that: (1) three main categories of violation symptoms are discussed by developers during the code review process; (2) The frequently-used terms of expressing violation symptoms are “inconsistent” and “violate”, and the most common linguistic pattern is Problem Discovery; (3) Refactoring and removing code are the major measures (90%) to tackle violation symptoms, while a few violation symptoms were ignored by developers.
Conclusions:
Our findings suggest that the investigation of violation symptoms can help researchers better understand the characteristics of architecture erosion and facilitate the development and maintenance activities, and developers should explicitly manage violation symptoms, not only for addressing the existing architecture violations but also preventing future violations.",2023,1660
ULTRASONOGRAPHIC EVALUATION OF THE EPIDIDYMIS IN 139 SUB FERTILE MALES AND COMPARISON WITH CLINICAL FINDINGS,J.H. {Van Roijen} and R.S.G.M. Bots and M.C. Schoemaker,,2008,1661
Simulation and evaluation study of atmospheric aerosol nonsphericity as a function of particle size,Qianjun Mao and Xin Nie,"Aerosol nonsphericity causes great uncertainty in radiative forcing assessments and climate simulations. Although considerable studies have attempted to quantify this uncertainty, the relationship between aerosol nonsphericity and particle size is usually not considered, thus reducing the accuracy of the results. In this study, a coupled inversion algorithm combining an improved stochastic particle swarm optimization algorithm and angular light scattering is used for the nonparametric estimation of aerosol nonsphericity variation with particle size, and the optimal sample selection method is employed to screen the data. Based on the verification of inversion accuracy, the variation of aerosol aspect ratio with particle size based on the ellipsoidal model in global regions has been obtained from Aerosol Robotic Network (AERONET) data, and the effect of nonsphericity on radiative forcing and dry deposition has been studied. The results show that the aspect ratio increases with particle size in all regions, with the maximum ranging from 1.4 to 1.8 in the desert, reflecting the differences in aerosol composition at different particle sizes. In radiation calculations, considering aerosol nonsphericity makes the aerosol cooling effect weaker and surface radiative fluxes increase, but hardly changes the aerosol absorption, with maximum differences of 9.22% and 22.12% at the bottom and top of the atmosphere, respectively. Meanwhile, the differences in radiative forcing between aspect ratios as a function of particle size and not varying with particle size are not significant, averaging less than 2%. Besides, the aspect ratio not varying with particle size underestimates the deposition velocity of small particles and overestimates that of large particles compared to that as a function of particle size, with maximum differences of 7% and 4%, respectively.",2024,1662
Comparison of three methods for the determination of oxysterols in spray-dried egg,Francesc Guardiola and Rafael Codony and Magda Rafecas and Josep Boatella,"Three methods for the GC determination of oxysterols (OSs) in spray-dried egg, which combine different steps of purification, are compared. In addition, the efficiency of silica cartridges in the purification of OSs using four different systems of elution with increasing polarities is studied. The absence of cholesterol oxidation during the application of the analytical procedures is checked, and the linearity of the response and the chromatographic limits of detection and quantification are established. The methods are characterized by the calculation of precision and recovery for the different OSs. The method based on saponification alone is rejected, since it shows much lower precision. The method that includes saponification and silica cartridge purification offers higher reliability than the method based on cartridge purification alone, because it shows a higher precision and larger samples can be processed, which improves the limits of detection and quantification.",1995,1663
GAP: Forecasting commit activity in git projects,Alexandre Decan and Eleni Constantinou and Tom Mens and Henrique Rocha,"Abandonment of active developers poses a significant risk for many open source software projects. This risk can be reduced by forecasting the future activity of contributors involved in such projects. Focusing on the commit activity of individuals involved in git repositories, this paper proposes a practicable probabilistic forecasting model based on the statistical technique of survival analysis. The model is empirically validated on a wide variety of projects accounting for 7528 git repositories and 5947 active contributors. We found that a model based on the last 20 observed days of commit activity per contributor provides the best concordance. We also found that the predictions provided by the model are generally close to actual observations, with slight underestimations for low probability predictions and slight overestimations for higher probability predictions. This model is implemented as part of an open source tool, called GAP, that predicts future commit activity.",2020,1664
Triage Software Update Impact via Release Notes Classification,Solomon Berhe and Vanessa Kan and Omhier Khan and Nathan Pader and Ali Zain Farooqui and Marc Maynard and Foutse Khomh,"In the rapidly evolving domain of Industry 4.0, effective management of software updates is crucial for maintaining system continuity and security. This paper presents a novel machine learning-based approach for a prompt and effective triage of software updates, leveraging an evaluation of six release note classifiers to categorize updates by component type, release type, and security risk. Our methodology, tested on a dataset of 1,000 release notes commonly encountered in Industry 4.0 ecosystems, demonstrates Logistic Regression as the most accurate classifier. The findings not only highlight the practical applicability of our approach in real-world data but also set the foundation for future enhancements to streamline the machine learning triage process further.",2024,1665
"Fungal Portraits: No. 47: Volvariella aethiops, the first British collection",Geoffrey Kibby,,2011,1666
Moving Target Defense for the cloud/edge Telco environments,Pedro Escaleira and Vitor A. Cunha and Diogo Gomes and João P. Barraca and Rui L. Aguiar,"The Internet of Things (IoT) paradigm has been one of the main contributors, in recent years, to the growth in the number of connected equipment. This fact has predominantly contributed to IoT being constrained by the 5th Generation Mobile Network (5G) progress and the promises this technology brings. However, this can be a double-edged sword. On the one hand, it will benefit from those progresses, but on the other, it will also be impacted by any security risk associated with 5G. One of the more serious security problems associated with it is the new wave of virtualization and softwarization of networks and analogous appliances, brought to light by paradigms such as Network Functions Virtualization (NFV) and Multi-access Edge Computing (MEC). Considering these predicaments, we propose a state-of-the-art Moving Target Defense (MTD) approach that defends Cloud-based Network Functions (CNFs) launched within MEC and NFV environments. Furthermore, our mechanism follows the famous Everything as a Service (XaaS) ideology, allowing any CNF provider to use this protection system, working agonistically. In the end, we created a Proof of Concept (PoC) of our proposed methodology, which we then used to conduct an extensive practical security analysis against the multiple phases of the Intrusion Kill Chain. Our final results have proven that our MTD as a Service (MTDaaS) approach can effectively delay and, in some cases, stop an attacker from achieving its objectives when trying to attack a CNF, even if the related vulnerability is a zero-day.",2023,1667
Software metrics fluctuation: a property for assisting the metric selection process,Elvira-Maria Arvanitou and Apostolos Ampatzoglou and Alexander Chatzigeorgiou and Paris Avgeriou,"Context
Software quality attributes are assessed by employing appropriate metrics. However, the choice of such metrics is not always obvious and is further complicated by the multitude of available metrics. To assist metrics selection, several properties have been proposed. However, although metrics are often used to assess successive software versions, there is no property that assesses their ability to capture structural changes along evolution.
Objective
We introduce a property, Software Metric Fluctuation (SMF), which quantifies the degree to which a metric score varies, due to changes occurring between successive system's versions. Regarding SMF, metrics can be characterized as sensitive (changes induce high variation on the metric score) or stable (changes induce low variation on the metric score).
Method
SMF property has been evaluated by: (a) a case study on 20 OSS projects to assess the ability of SMF to differently characterize different metrics, and (b) a case study on 10 software engineers to assess SMF's usefulness in the metric selection process.
Results
The results of the first case study suggest that different metrics that quantify the same quality attributes present differences in their fluctuation. We also provide evidence that an additional factor that is related to metrics’ fluctuation is the function that is used for aggregating metric from the micro to the macro level. In addition, the outcome of the second case study suggested that SMF is capable of helping practitioners in metric selection, since: (a) different practitioners have different perception of metric fluctuation, and (b) this perception is less accurate than the systematic approach that SMF offers.
Conclusions
SMF is a useful metric property that can improve the accuracy of metrics selection. Based on SMF, we can differentiate metrics, based on their degree of fluctuation. Such results can provide input to researchers and practitioners in their metric selection processes.",2016,1668
Learning lessons from data breaches,Steve Roberts,"Bricks-and-mortar retail sales are rapidly being replaced by online shopping. And where money is spent, bad actors will follow. Data breaches involving e-commerce have now surpassed breaches at the point of sale and they're becoming very expensive. In spite of this, there remains an alarming lack of corporate self-awareness when it comes to cyber security. Where money is spent, bad actors will follow. Data breaches involving e-commerce have now surpassed breaches at the point of sale and they're becoming very expensive. In spite of this, there remains an alarming lack of corporate self-awareness when it comes to cyber security. This has led to major incidents that could have been avoided. Steve Roberts of OSS Technology analyses some major breaches of the past few years and asks what went wrong, how could they have been prevented and what lessons can we learn?",2018,1669
Characteristics and Evolution of Sedimentary Microfacies of Chang 6–4+5 Layer in the Northern Area of Western Mahuang Mountain,Yumei LIU and Xinghe YU and Shengli LI and Weiwei DU and Mei LI and Shunli LI,"Although there are widespread indications of oil and gas in the Chang 6–4+5 layer of western Mahuang Mountian, well test proves that there is more water and less oil with the distribution and continuity of sand being not yet clear. To understand the distribution and evolution of sedimentary microfacies in the area, six kinds of sedimentary microfacies and ten kinds of sedimentary structures were identified, and ten patterns of contact between microfacies and five kinds of sedimentary sequence model were summarized through detailed observation and description of the core of rock and logging data. Each model represents different sedimentary facies. Reversed cycle is dominant and normal short-term cycle also exists in the vertical direction; Mouth bar and subaqueous distributary channel developed alternately in the lateral direction. The lower channel sand is “large in quantity but small in size with the layer narrow and thin”, whereas the upper sand is “few in quantity but large in size with a wide and thick layer”. The “single-factor analysis and multifactor comprehensive” research method was used to divide this area into three phase belts—delta inner front, delta outer front, and predelta. Subaqueous distributary channel, distributary mouth bar, and interdistributary deposits are prevailing in the delta inner front. Distal bar and sand sheet are dominant in the delta outer front, and predelta mud is dominant in the predelta. According to modern sedimentary facies model, the area is a constructive braided river-lake delta. Three stages appeared in its evolution: the initial development period (Chang 63), in which the predelta and the delta outer front are the main facies belts; the early-middle development period (Chang 62–Chang 61), in which the inner delta front and the delta outer front dominates and each micro-facies is a single phase and is deposited alternately; the middle-late development period (Chang 4 +52–Chang 4 +51), in which the delta inner front dominates and subaqueous distributary channel shows multiphase stacking pattern.",2009,1670
A review of code reviewer recommendation studies: Challenges and future directions,H. Alperen Çetin and Emre Doğan and Eray Tüzün,"Code review is the process of inspecting code changes by a developer who is not involved in the development of the changeset. One of the initial and important steps of code review process is selecting code reviewer(s) for a given code change. To maximize the benefits of the code review process, the appropriate selection of the reviewer is essential. Code reviewer recommendation has been an active research area over the last few years, and many recommendation models have been proposed in the literature. In this study, we conduct a systematic literature review by inspecting 29 primary studies published from 2009 to 2020. Based on the outcomes of our review: (1) most preferred approaches are heuristic approaches closely followed by machine learning approaches, (2) the majority of the studies use open source projects to evaluate their models, (3) the majority of the studies prefer incremental training set validation techniques, (4) most studies suffer from reproducibility problems, (5) model generalizability and dataset integrity are the most common validity threats for the models and (6) refining models and conducting additional experiments are the most common future work discussions in the studies.",2021,1671
3D Sonography,Berthold Hell,"A method of 3-dimensional (3D) sonography (US) is described in this paper. Special emphasis is laid upon the basic problem of generating well-orientated 3D visualizations on the basis of different evaluation techniques. Furthermore, some problems of data acquisition and data processing using US are presented. Alternative solutions thereto are briefly discussed.
Es wird eine Methode zur Erzeugung 3-dimensionaler (3D) Sonogramme (US) dargelegt. Neben der prinzipiellen Problematik der Erzeugung einer räumlichen Orientierung und der Generierung von 3D Bildern auf der Basis unterschiedlicher Untersuchungsmethoden werden einige spezielle Schwierigkeiten der US Datengewinnung und-weiterverarbeitung aufgezeigt. Alternative Lösungsansätze zur 3D US werden diskutiert.",1995,1672
Will they stay or will they go? How network properties of WebICs predict dropout rates of valuable Wikipedians,Jürgen Lerner and Patrick Kenis and Denise van Raaij and Ulrik Brandes,"Summary
This paper contributes to our understanding of an increasingly prevalent work system, web-based internet communities (WebICs). We are particularly interested in how WebICs are governed given the fact how different they are compared to more classical forms of organization. We study the governance of a WebIC by studying the structure and dynamics of their edit network. Given the fact that the edit network is a relational structure, social network analysis is key to understanding these work systems. We demonstrate that characteristics of the edit network contribute to predicting the dropout hazard of valuable WebIC members. Since WebICs exist only thanks to the activity of their contributors, predicting drop-outs becomes crucial. The results show that reputation and controversy have different effects for different types of Wikipedians; i.e., an actor’s reputation decreases the dropout hazard of active Wikipedians, while participation on controversial pages decreases the dropout hazard of highly active Wikipedians.",2011,1673
NetSentry: A deep learning approach to detecting incipient large-scale network attacks,Haoyu Liu and Paul Patras,"Machine Learning (ML) techniques are increasingly adopted to tackle ever-evolving high-profile network attacks, including Distributed Denial of Service (DDoS), botnet, and ransomware, due to their unique ability to extract complex patterns hidden in data streams. These approaches are however routinely validated with data collected in the same environment, and their performance degrades when deployed in different network topologies and/or applied on previously unseen traffic, as we uncover. This suggests malicious/benign behaviors are largely learned superficially and ML-based Network Intrusion Detection Systems (NIDS) need revisiting, to be effective in practice. In this paper we dive into the mechanics of large-scale network attacks, with a view to understanding how to use ML for Network Intrusion Detection (NID) in a principled way. We reveal that, although cyberattacks vary significantly in terms of payloads, vectors and targets, their early stages, which are critical to successful attack outcomes, share many similarities and exhibit important temporal correlations. Therefore, we treat NID as a time-sensitive task and propose NetSentry, perhaps the first of its kind NIDS that builds on Bidirectional Asymmetric LSTM (Bi-ALSTM), an original ensemble of sequential neural models, to detect network threats before they spread. We cross-evaluate NetSentry using two practical datasets, training on one and testing on the other, and demonstrate F1 score gains above 33% over the state-of-the-art, as well as up to 3× higher rates of detecting attacks such as Cross-Site Scripting (XSS) and web bruteforce. Further, we put forward a novel data augmentation technique that boosts the generalization abilities of a broad range of supervised deep learning algorithms, leading to average F1 score gains above 35%. Lastly, we shed light on the feasibility of deploying NetSentry in operational networks, demonstrating affordable computational overhead and robustness to evasion attacks.",2022,1674
Fixation of inorganic carbon from different sources and its translocation in Spartina alterniflora Loisel,Yuan-Hsun Hwang and James T. Morris,"The extent to which Spartina alterniflora Loisel. fixes lacunar CO2 and utilizes dissolved inorganic carbon in the interstitial water of sediment was investigated, and the contribution of these two processes to total primary productivity was estimated and compared with photosynthesis in the leaf blades. Atmospheric CO2 is primarily fixed in the leaf blades, and the resultant assimilates are rapidly translocated through the leaf sheaths to the below ground tissues and growing points above ground. Lacunar CO2 is primarily fixed in the leaf sheaths and stem. The fact that these assimilates remain in the tissues around the lacunar spaces indicates that lacunar CO2 does not diffuse freely into the leaf blades but is recycled in the lacunar spaces. Dissolved inorganic carbon taken up by the plant from the sediment is primarily fixed by a dark reaction in the root tissues. Some dissolved inorganic carbon in the interstitial water may collect as gaseous CO2 in the lacunae and be fixed in the leaf sheaths and stem. In addition, dissolved inorganic carbon may be taken up in the transpiration stream and be fixed in the leaf blades by photosynthesis. However, internal CO2 fixation and dissolved inorganic carbon utilization together are only about 10% of the rate of fixation of atmospheric CO2 by leaves.",1992,1675
Multicast dataset synchronization and agent negotiation in distributed manufacturing control systems,Octavian Morariu and Cristina Morariu and Theodor Borangiu and Silviu Raileanu,"Multi agent systems represent an elegant approach for the control architecture of manufacturing systems. Distributed control architectures have the potential to achieve greater flexibility by being capable of local decision making based on real time reasoning. One of the main challenges of these distributed architectures is represented by the capability to synchronize the production data across all execution points in a reliable and consistent fashion. In this context, this paper aims to resolve the problems associated with real time production data synchronization in distributed multi-agent control systems by proposing a common dataset synchronized across all agent entities using multicast network communication. On top of this common dataset approach, an agent negotiation mechanism is proposed that addresses the operation sequencing and resource allocation in decentralized operation model. The pilot implementation is using JADE multi agent platform and JGroups for real time data synchronization and NetLogo for abstract representation of the simulation system. Experimental results gathered from the pilot implementation are discussed.",2015,1676
"Infrastructural justice for responsible software engineering,",Sarah Robinson and Jim Buckley and Luigina Ciolfi and Conor Linehan and Clare McInerney and Bashar Nuseibeh and John Twomey and Irum Rauf and John McCarthy,"In recent years, we have seen many examples of software products unintentionally causing demonstrable harm. Many guidelines for ethical and responsible computing have been developed in response. Dominant approaches typically attribute liability and blame to individual companies or actors, rather than understanding how the working practices, norms, and cultural understandings in the software industry contribute to such outcomes. In this paper, we propose an understanding of responsibility that is infrastructural, relational, and cultural; thus, providing a foundation to better enable responsible software engineering into the future. Our approach draws on Young's (2006) social connection model of responsibility and Star and Ruhleder's (1994) concept of infrastructure. By bringing these theories together we introduce a concept called infrastructural injustice, which offers a new way for software engineers to consider their opportunities for responsible action with respect to society and the planet. We illustrate the utility of this approach by applying it to an Open-Source software communities’ development of Deepfake technology, to find key leverage points of responsibility that are relevant to both Deepfake technology and software engineering more broadly.",2024,1677
PPP in Public Schools as Means for Value Creation for User and Owner,Ole Andreas Aarseth and Vegar Mong Urdal and Svein Bjørberg and Marit Støre-Valen and Jardar Lohne,"The purpose of this paper is to assess if and to what extent PPPs contribute to value creation for user and owner, by highlighting how PPP contribute to value creation in public schools in a Norwegian context. Little research has been found concerning PPPs contribution to value creation for user and owner. The analysis and document studies in this paper show that PPP compels to consideration of the life cycle, incentivises project owners to focus on output-based specifications and indicates commitment for the contractors to deliver. In sum, this indicates that PPP is suited for public schools in Norway.",2016,1678
Use and Development of the Wetland Macrophyte Index to Detect Water Quality Impairment in Fish Habitat of Great Lakes Coastal Marshes,Melanie V. Croft and Patricia Chow-Fraser,"Indices have been developed with invertebrates, fish, and water quality parameters to detect the impact of human disturbance on coastal wetlands, but a macrophyte index of fish habitat for the Great Lakes does not currently exist. Because wetland macrophytes are directly influenced by water quality, any impairment in wetland quality should be reflected by taxonomic composition of the aquatic plant community. We developed a wetland macrophyte index (WMI) with plant presence/absence data for 127 coastal wetlands (154 wetland-years) from all five Great Lakes, using results of a canonical correspondence analysis (CCA) to ordinate plant species along a water quality gradient (CCA axis 1). We validated the WMI with data collected before and after the implementation of remedial actions plans (RAPs) in Sturgeon Bay (Severn Sound) and Cootes Paradise Marsh. Consistent with predictions, WMI scores for Sturgeon Bay were significantly higher after the implementation of the RAP. Historical data from Cootes Paradise Marsh were used to track the declining condition of the plant community from the 1940s to 1990s. Subsequently, when remedial actions had been implemented in 1997, the calculated WMI scores showed improvement, but when the presence of exotic species (WMIadj) was accounted for, improvements in ecological integrity of the aquatic-plant community were no longer evident. We show how WMI scores can be used by environmental agencies to assess the historic, current, and future ecological status of wetland ecosystems in two Canadian national parks, Point Pelee National Park (PPNP) and Fathom Five National Marine Park (FFNMP).",2007,1679
Chapter 3 - Guarding Against Network Intrusions,Thomas M. Chen and Patrick J. Walsh,"Guarding against network intrusions requires the monitoring of network traffic for particular network segments or devices and analysis of network, transport, and application protocols to identify suspicious activity. This chapter provides a detailed discussion of network-based intrusion protection technologies. It contains a brief overview of the major components of network-based intrusion protection systems and explains the architectures typically used for deploying the components. It also examines the security capabilities of the technologies in depth, including the methodologies they use to identify suspicious activity. The rest of the chapter discusses the management capabilities of the technologies and provides recommendations for implementation and operation.",2014,1680
Our reviewers – An appreciation,Susanne Hantos and Jane List,,2015,1681
MARIOLA: a model for calculating the response of mediterranean bush ecosystem to climatic variations,J.L. Usó-Domenech and Y. Villacampa-Esteve and G. Stübing-Martinez and T. Karjalainen and M.P. Ramo,"The paper summarizes the bush ecosystem model developed for assessing the effects of climatic change on the behaviour of mediterranean bushes assuming that temperature, humidity and rain-fall are the basic dimensions of the niche occupied by shrub species. In this context, changes in the monthly weather pattern serve only to outline the growth conditions due to the nonlinearity of response of shrubs to climatic factors. The plant-soil-atmosphere system is described by means of ordinary non-linear differential equations for the state variables: green biomass, woody biomass, the residues of green and woody biomasses, faecal detritus of mammals on the soil, and the total organic matter of the soil. The behaviour of the flow variables is described by means of equations obtained from non-linear multiple regressions from the state variables and the input variables. The model has been applied with success to the behaviour of Cistus albidus in two zones of the Province of Alicante (Spain). The data base for the parametrical locations (zone 1) and validation (zone 2) is based upon measurements taken weekly over a 2-year period. The model is used to simulate the response of this shrub to a decreasing tendency in precipitation combined with a simultaneous rise in temperature. A period of 10 years is simulated and it is observed that plants with woody biomass smaller than 85 g die between the first and the third month and other plants' biomass decreases during this period, and strongly thereafter.",1995,1682
Hydrolysable tannins as chemotaxonomic markers in the rosaceae,Takuo Okuda and Takashi Yoshida and Tsutomu Hatano and Mayumi Iwasaki and Makiko Kubo and Teruyo Orime and Masao Yoshizaki and Naohiro Naruhashi,"A HPLC survey of leaves of 80 plants (62 species, 15 hybrids, one variety and two cytotypes) from 18 genera of four subfamilies of Rosaceae, using five oligomeric hydrolysable tannins, five monomeric hydrolysable tannins, and chlorogenic acid as reference compounds, showed that the oligomers can be used as chemotaxonomic markers for the family, viz., sanguiin H-6 and H-11 in the genera Sanguisorba and Rubus, gemin A in Geum, agrimoniin in Agrimonia, Fragaria and Potentilla, and rugosin D in Filipendula. The hydrolysable tannin monomers were widely distributed in the herbaceous and frutescent Rosoideae species, but not in the arborous species of the other subfamilies. Chlorogenic acid was found in almost all of the plants examined.",1992,1683
Thymol derivatives from Doronicum hungaricum,Ferdinand Bohlmann and Autar Krishen Dhar and Maniruddin Ahmed,,1980,1684
"The Descent of Pluto: Interactive dynamics, specialisation and reciprocity of roles in a Wikipedia debate",Françoise Détienne and Michael Baker and Dominique Fréard and Flore Barcellini and Alexandre Denis and Matthieu Quignard,"This research focuses on analysing collective activity in Wikipedia, conceptualised as an Online Epistemic Community (“OEC”). Previous research on Wikipedia has shown that widespread participation, coupled with the principle of neutrality of viewpoint, has led to ‘editing wars’ and associated high coordination costs. The question that we address is therefore that of how to analyse the interactive dynamics of conflictual OEC discussions. To address this issue, we performed a longitudinal analysis of a specific case-study within the French-speaking “astronomy” Wikipedia OEC, revolving around the renaming of the article on the celestial body “Pluto”, given the ‘descent’ of its scientific status from that of a planet to an asteroid. Our choice was to focus on the analysis of dialogic and epistemic roles, as an appropriate meso-level unit of analysis. We present a qualitative-quantitative method for analysis of roles, based on filtering major participants and analysing the dialogic functions and epistemic contents of their communicative acts. Our analyses showed that online epistemic communities can be communities in the true sense of their involving cooperation, in that roles become gradually specialised and reciprocal over sequences of the discussion: when one participant changes role from one sequence to another, other participants ‘fill in’ for the vacant role. Secondly, we show that OECs, in the case of Wikipedia, do not function purely on a knowledge-level, but also involve, crucially, negotiation of images of participants’ competences with respect to the knowledge domain. In that sense, OECs can be seen as socio-cognitive communities. The originality of our research resides in the qualitative-quantitative method for analysing interactive roles, and the results of its application to an extended longitudinal case study.",2016,1685
Devonian spore ultrastructure: Rhabdosporites,Thomas N. Taylor and Stephen E. Scheckler,"The fine structure of in situ Rhabdosporites langii miospores is described. The trilete spores are characterized by an irregular pseudosaccus that is conspicuous on the distal surface of the grain; ornamentation consists of narrow coni. The central body of the spore is constructed of superimposed lamellae, each up to 50 nm thick. Pseudosaccus formation occurs as a result of a separation of the outer one third of the sporoderm of the central body. As development continues small perforations are formed in the pseudosaccus wall where lamellae separate. The development of the saccus, protosaccus and pseudosaccus is compared, and the biological significance of camerate grains discussed.",1996,1686
Shaped up materials: A new design concept of environmental conscious heat resistant steels,K. Kimura and H. Kushima and K. Yagi and C. Tanaka,"Addition of alloying elements and optimization of chemical composition are common and important means of material development to satisfy a demanded performance. On the other hand, a large amounts and many types of alloying elements and, therefore, complicated chemical composition is detrimental for recycling of materials and saving of scarce material resources. Modern design concept of heat resistant materials to minimize the contents of alloying elements and to simplify the chemical composition without loss of long-term creep strength has been devised from a viewpoint of “Inherent Creep Strength”. The materials designed from the new concept, which minimize environmental load, have been named as “Shaped Up Materials”, and proposed as one of the candidate for “ECOMATERIALS”.",1994,1687
Chapter 4 - Broadcast operation centers in transition,Philip J. Cianci,,2009,1688
I know what you streamed last night: On the security and privacy of streaming,Alexios Nikas and Efthimios Alepis and Constantinos Patsakis,"Streaming media are currently conquering traditional multimedia by means of services like Netflix, Amazon Prime and Hulu which provide to millions of users worldwide with paid subscriptions in order to watch the desired content on-demand. Simultaneously, numerous applications and services infringing this content by sharing it for free have emerged. The latter has given ground to a new market based on illegal downloads which monetizes from ads and custom hardware, often aggregating peers to maximize multimedia content sharing. Regardless of the ethical and legal issues involved, the users of such streaming services are millions and they are severely exposed to various threats, mainly due to poor hardware and software configurations. Recent attacks have also shown that they may, in turn, endanger others as well. This work details these threats and presents new attacks on these systems as well as forensic evidence that can be collected in specific cases.",2018,1689
"A new root-nodulating symbiont of the tropical legume Sesbania, Rhizobium sp SIN-1, is closely related to R. galegae, a species that nodulates temperate legumes",Debashis Rana and Hari B. Krishnan,"Rhizobium sp. SIN-1, isolated in India from root nodules on the tropical legume Sesbania aculeata, also induces nitrogen-fixing nodules on roots of S. macrocarpa, S. speciosa, S. procumbens, S. punicea, S. rostrata, and Vigna unguiculata. Unlike Azorhizobium caulinodans, SIN-1 does not induce stem nodules on S. rostrata. The nodules induced by SIN-1 develop exclusively at the bases of secondary roots. Electron microscopic studies of mature nodule sections revealed rhizobia within intercellular spaces, indicating a ‘crack entry’ mechanism of root infection. SIN-1 is a fast-growing, acid-producing, salt-tolerant Rhizobium that utilizes a wide variety of carbon sources. The nodulation (nod) genes of this strain are located on a 300-MDa symbiosis (sym) plasmid. Fatty acid profile and sequence comparison of a 260-bp conserved region of the 16S rRNA gene demonstrated that SIN-1 is phylogenetically closely related to R. galegae, a species that nodulates temperate legumes.",1995,1690
Early prediction for merged vs abandoned code changes in modern code reviews,Khairul Islam and Toufique Ahmed and Rifat Shahriyar and Anindya Iqbal and Gias Uddin,"Context:
The modern code review process is an integral part of the current software development practice. Considerable effort is given here to inspect code changes, find defects, suggest an improvement, and address the suggestions of the reviewers. In a code review process, several iterations usually take place where an author submits code changes and a reviewer gives feedback until is happy to accept the change. In around 12% cases, the changes are abandoned, eventually wasting all the efforts.
Objective:
In this research, our objective is to design a tool that can predict whether a code change would be merged or abandoned at an early stage to reduce the waste of efforts of all stakeholders (e.g., program author, reviewer, project management, etc.) involved. The real-world demand for such a tool was formally identified by a study by Fan et al. (2018).
Method:
We have mined 146,612 code changes from the code reviews of three large and popular open-source software and trained and tested a suite of supervised machine learning classifiers, both shallow and deep learning-based. We consider a total of 25 features in each code change during the training and testing of the models. The features are divided into five dimensions: reviewer, author, project, text, and code.
Results:
The best performing model named PredCR (Predicting Code Review), a LightGBM-based classifier achieves around 85% AUC score on average and relatively improves the state-of-the-art (Fan et al., 2018) by 14%–23%. In our extensive empirical study involving PredCR on the 146,612 code changes from the three software projects, we find that (1) The new features like reviewer dimensions that are introduced in PredCR are the most informative. (2) Compared to the baseline, PredCR is more effective towards reducing bias against new developers. (3) PredCR uses historical data in the code review repository and as such the performance of PredCR improves as a software system evolves with new and more data.
Conclusion:
PredCR can help save time and effort by helping developers/code reviewers to prioritize the code changes that they are asked to review. Project management can use PredCR to determine how code changes can be assigned to the code reviewers (e.g., select code changes that are more likely to be merged for review before the changes that might be abandoned).",2022,1691
"Photostress, photoprotection, and water soluble antioxidants in the canopies of five Canarian laurel forest tree species during a diurnal course in the field",Michael Tausz and Águeda María González-Rodríguez and Astrid Wonisch and Juliane Peters and Dieter Grill and Domingo Morales and María Soledad Jiménez,"Summary
The Canarian laurel forest ecosystem is composed of several co-dominant evergreen tree species including Ilex perado, I. canariensis, Myrica faya, Laurus azorica, and Persea indica. With leaves of these trees the diurnal course of stress parameters (chlorophyll fluorescence Fv/Fm, pigments, ascorbate, glutathione, gas exchange, water relations) was investigated during a mildly stressful summer day. Sun leaves generally had lower photochemical efficiencies (morning Fv/Fm in sun leaves were below 0.80 and above 0.80 in shade leaves), less chlorophyll, a larger xanthophyll cycle pool per unit chlorophyll, and more glutathione and ascorbate. Minimal relative water contents of more than 85% indicated that dehydration was not a stress factor. Stomatal conductances decreased from 150 to 200 mmol H2O m−2 s−1 in the morning to about 50 mmol H2O m−2 s−1 during the day in all species, but this did not limit CO2 uptake. De-epoxidation of xanthophylls only occurred in sun leaves of I. canariensis (to more than 50%) and M. faya (more than 60%). Decrea Fv/Fm were only found in sun leaves of P. indica (from ca. 0.80 in the morning to a minimum of 0.70) and, as a trend, also in L. azorica (from ca. 0.75 to ca. 0.65). I. perado showed neither of those responses. P. indica and L. azorica exhibited the highest photosynthesis rates of about 10 μmol CO2 m−2 s−1 compared to 8 in the other species. The photoprotection strategy of P. indica and L. azorica admitted slow recovery from photoinhibition, did not activate protective energy dissipation through xanthophylls, and allowed highest production under these typical conditions.",2004,1692
"The interaction of oxygen, radiation exposure and seed water content on γ-irradiated barley seeds",E. Donaldson and R.A. Nilan and C.F. Konzak,"The roles of oxygen concentration, seed water content, and their interaction in the γ-ray-induced damage to dry barley seeds were investigated. Himalaya (C.I. 620) barley seeds were adjusted to water contents ranging from 2 to 10%, irradiated with 60Co γ-rays, and soaked at 0°C in distilled water bubbled with oxygen-nitrogen gas mixtures containing 0.0, 3.1, 6.25, 12.5, 25, 50 and 100% oxygen. Biological effects of the treatments were recorded as M1 seedling injury. Essentially no oxygen enhancement of biological damage was obtained with an oxygen concentration of 3.1% in the gas phase of the soaking solution. The minimum OCHG needed to cause an oxygen enhancement of biological damage (3.1% OCHG) increased with increasing seed water content between 1.8 and 10.0%, and decreased as the radiation dose increased, suggesting a triple factor interaction. For moderate levels of injury (between 20 and 60%), a nearly linear increase in seedling injury was obtained when the OCHG was increased in an exponential fashion. For greater seedling injury, the response tended toward a sigmoid shaped curve, probably due to limitations of the biological parameter. The same types of response were obtained when the results were analyzed in terms of oxygen enhancement ratios (OER) obtained from seedling injury data. Decreases in oxygen-independent radiosensitivity, determined by the increase in radiation exposure required to induce 40% injury with anaerobic soakings, were obtained as the seed water content was increased from 2.0 to 9.9%. The same pattern of radiosensitivity was observed with aerobic soakings and was more pronounced at intermediate levels of OCHG. The change in radiosensitivity of seeds between seed water contents of 2.0 and 7.7% was consistent with published EPR data. The increase in response at intermediate OCHG was in part a reflection of the influence of seed water content on the level of OCHG needed to produce an oxygen enhancement of damage. Cooling seeds of 6.1% water content to dry ice temperatures immediately after irradiation at 0°C decreased both oxygen-dependent and oxygen-independent damage. The decreases in damage were greater at 20 krad than at 10 krad and tended to be greater at the higher OCHG.",1979,1693
Alkaloids of erythroxylum macrocarpum and E. sideroxyloides,Mansour S. Al-said and William C. Evans and Raymond J. Grout,"Erythroxylum macrocarpum and E. sideroxyloides, two closely related species indigenous to Mauritius, contain a similar range of alkaloids consisting mainly of benzoyl esters of tropan-3α-ol, tropan-3β-ol, and tropan-3α,6β-diol together with their nor-derivatives. 3α-Benzoyloxytropan-6β-ol (E. sideroxyloides) and 3α-benzoyloxynortropane and 3β-benzoyloxynortropan-6β-ol (both species) are reported for the first time.",1986,1694
Atherosclerosis: The evolving role of vascular image analysis,,,2013,1695
Automated Preforming of Braided Hoses Made of Thermoplast-glass Fiber Hybrid Yarns,A. Liebsch and R. Kupfer and A. Defranceski and B. Rösler and J. Janik and M. Gude,"Due to their outstanding mechanical properties, fiber reinforced composites are particularly suitable for lightweight applications. Especially hollow structures made of braided hybrid yarn hoses offer a high application potential as the unconsolidated hoses provide a good drapability and thus allow the fabrication of complex shaped torsion- and bend-resistant structures. At present, the shaping and preforming of braided hoses is usually performed manually. To enable a high volume production of lightweight hollow structures, automated preforming technologies are required. This article discusses a new technical approach which allows a fully automated preforming of braided hybrid yarn hoses. Starting with pre-fabricated and rolled up quasi-endless braided hoses, the conceived automation system has to cut several segments from the hose and subsequently pull them above each other to produce a multi-ply preform. For an automated process, a reproducible and damage-free manipulation of the hoses has to be ensured during preforming. Therefore, the process is analyzed and concepts for unrolling and cutting of the endless hoses as well as stacking and fixation of the obtained segments are developed and evaluated experimentally. As a result of the investigation, a fully automated preforming station is built up.",2017,1696
A survey of intrusion detection in Internet of Things,Bruno Bogaz Zarpelão and Rodrigo Sanches Miani and Cláudio Toshio Kawakani and Sean Carlisto {de Alvarenga},"Internet of Things (IoT) is a new paradigm that integrates the Internet and physical objects belonging to different domains such as home automation, industrial process, human health and environmental monitoring. It deepens the presence of Internet-connected devices in our daily activities, bringing, in addition to many benefits, challenges related to security issues. For more than two decades, Intrusion Detection Systems (IDS) have been an important tool for the protection of networks and information systems. However, applying traditional IDS techniques to IoT is difficult due to its particular characteristics such as constrained-resource devices, specific protocol stacks, and standards. In this paper, we present a survey of IDS research efforts for IoT. Our objective is to identify leading trends, open issues, and future research possibilities. We classified the IDSs proposed in the literature according to the following attributes: detection method, IDS placement strategy, security threat and validation strategy. We also discussed the different possibilities for each attribute, detailing aspects of works that either propose specific IDS schemes for IoT or develop attack detection strategies for IoT threats that might be embedded in IDSs.",2017,1697
Leaf hairs of Olea europeae protect underlying tissues against ultraviolet-B radiation damage,George Karabourniotis and Aris Kyparissis and Yiannis Manetas,"The photochemical efficiency of photosystem II, as measured by chlorophyll fluorescence induction, was not affected in de-haired olive leaves kept in the dark or intact leaves irradiated with a moderate (3.75 W m−2) ultraviolet-B (UV-B) intensity. In de-haired, UV-B-irradiated leaves, however, the ratio of variable to maximum (Fv/Fm) chlorophyll fluorescence declined significantly and irreversibly. Reduction in Fv/Fm was associated with an increase in instantaneous (F0) and a decrease in maximum (Fm) fluorescence, indicating perturbation by the UV-B exposure of more than one photosynthetic site. Extensive epidermal browning in dehaired, UV-B irradiated leaves was also observed, indicating possible damage to cell membranes. The results strengthen the hypothesis that leaf hairs protect the underlying tissues against UV-B radiation damage.",1993,1698
The effect of ethephon seed treatment on leaf development and head initiation of wheat,Gary M. Banowetz,"The ability to control the transition from vegetative to reproductive growth could potentially increase wheat yields, particularly in regions subject to midseason drought stress. Little is known about the factors that regulate or affect this transition in wheat and other graminae. Because ethephon is known to accelerate this transition in other plants, the effect of seed treatment on spring wheat growth and development was investigated. Seedlings derived from seeds imbibed in ethephon underwent the transition from vegetative to reproductive growth earlier than seeds imbibed in water. The percentage of 17-day-old plants with four fully-emerged leaves (i.e., plants in which meristem development had proceeded from vegetative apex to double-ridge formation) showed a positive correlation with ethephon concentration. Subsequent yield components were unaffected with the exception of a small increase in the number of kernels/spike in plants derived from seeds imbibed in ethephon solutions for 24 h. Neither root nor shoot dry weights of 21-day-old plants were affected by the treatments. Germination percentages were unaffected by the treatments but 8-day-old plants derived from seeds imbibed for 24 h had reduced height with increasing ethephon concentrations. These results will be useful in further studies of the biochemical basis of the transition to reproductive growth.",1993,1699
Influence of oxygen at high pressure on the induction of damage in barley seeds by gamma radiation,E. Donaldson and R.A. Nilan and C.F. Konzak,"The influence of high pressure oxygen (HOP) before, during and after irradiation on seedling injury (percent reduction in seedling height relative to the non-irradiated controls) was investigated using Himalaya (C.I. 620) barley seeds. Seeds were adjusted to water contents of 2–14% by storage in vacuum desiccators over calcium oxide or mixtures of glycerol and water and then irradiated in vacuo or under various oxygen tensions with 60Co gamma rays. After irradiation, the seeds were soaked at approximately 0°C in oxygen or nitrogen bubbled water. Treatment effects were recorded as M1 seedling injury. Seeds exposed to HPO before, during or after irradiation followed by soaking in oxygen or nitrogen expressed two or three times more damage than irradiation in vacuo followed by soaking in oxygenated water. That a reaction between oxygen and radiation-induced sites probably occurs before the seeds are soaked was demonstrated by the failure to remove completely the effect of HPO by vacuum between HPO treatment and irradiation. The results indicate that placing the seeds under HPO may increase the rate and extent of reactions which occur during post-radiation storage of seeds in the presence of oxygen. The increase in damage associated with oxygen soaking (oxygen-dependent damage) is partially lost during aerobic storage and is largely pre-empted when seeds are placed under HPO. This decrease in oxygen-dependent damage is accompanied by an increase in damage occurring with nitrogen soaking, suggesting that the reaction which leads to damage was initiated before soaking and to the same oxygen sensitive sites.",1980,1700
"Circulating chemoattractants RANTES, negatively related to endogenous androgens, and MCP-1 are differentially suppressed by hormone therapy and raloxifene",George E. Christodoulakos and Irene V. Lambrinoudaki and Emmanuel V. Economou and Constantinos Papadias and Nikolaos Vitoratos and Constantinos P. Panoulis and Evangelia E. Kouskouni and Sofia A. Vlachou and George C. Creatsas,"Background
The cardinal role of chronic inflammation in the development of atherosclerosis is increasingly being recognized. Estrogens may prevent the evolution of atherosclerosis by suppressing immune response. Furthermore, the conflicting reports on the cardiovascular effects of hormone therapy between observational and clinical trials have triggered interest on the effect of alternative therapies on the cardiovascular system.
Objective
The aim of this study was to assess the effect of estrogen, estrogen–progestin, tibolone and raloxifene therapy on circulating markers of chemotaxis in healthy postmenopausal women.
Methods
Eighty-eight postmenopausal women aged 44–62 years were randomly allocated to daily: (1) conjugated equine estrogens 0.625mg (CEE), (2) 17β-estradiol 1mg plus norethisterone acetate 0.5mg (E2/NETA), (3) tibolone 2.5mg, (4) raloxifene HCl 60mg or (5) no treatment. Serum monocyte chemoattractant protein-1 (MCP-1) and regulated upon activation, normal T-cell expressed and secreted (RANTES) were measured at baseline and at 3 months.
Results
Endogenous testosterone and free androgen index (FAI) correlated negatively, while SHBG correlated positively with serum RANTES (testosterone: r=−0.27, p=0.033; FAI: r=−0.43, p=0.004: SHBG: r=0.34, p=0.026). Serum MCP-1 decreased significantly in the CEE group (baseline 125.3±51pg/ml, 3 months 84.5±36.1pg/ml, p=0.043), while no difference was detected between baseline and post-treatment levels in the other groups. Furthermore, a significant decrease in serum RANTES was observed at the end of 3 months only in the E2/NETA and the raloxifene group (E2/NETA baseline 8690.6±3880.0pg/ml, 3 months 6894.0±1720.0pg/ml, p=0.007; raloxifene baseline 9042.4±3765.6pg/ml, 3 months 6718.1±2366.2pg/ml, p=0.011).
Conclusion
Endogenous androgens may suppress chemotactic response. Postmenopausal hormone therapy and raloxifene may inhibit the expression of chemoattractant molecules and thus attenuate inflammation. The relevance of these findings in terms of clinically established caridoprotection remains to be clarified.",2007,1701
Cellular automaton modeling of pattern formation in interacting cell systems,Andreas Deutsch and Uwe Börner and M. Bär,"Publisher Summary
A cellular automaton is viewed as simple models of spatially extended decentralized systems made up of a number of individual components (e.g. biological cells). The communication between constituent cells is limited to local interaction. Each individual cell is in a specific state, which changes over time depending on the states of its local neighbors. In particular, cellular automaton models have been proposed for biological applications, including ecological, epidemiological, ethological (game theoretical), evolutionary, immunobiological and morphogenetic aspects. The chapter discusses an overview of cellular automaton models of spatio-temporal pattern formation in interacting cell systems. Various automaton rules mimicking general pattern forming principles have been suggested, and may lead to models of (intracellular) cytoskeleton and membrane dynamics, tissue formation, tumor growth, life cycles of micro-organisms or animal coat markings. Automaton models of cellular pattern formation can be roughly classified according to the prevalent type of interaction. Cell-medium interactions dominate (nutrient-dependent) growth models, while one can further distinguish direct cell-cell and indirect cell-medium-cell interactions. Finally, the chapter focuses on a specific example—tippling pattern formation in myxobacteria and introduce a cellular automaton model for this phenomenon, which is able to lead to testable biological hypotheses.",2004,1702
Moving experimental psychology online: How to obtain high quality data when we can’t see our participants,Jennifer M. Rodd,"The past 10 years have seen rapid growth of online (web-based) data collection across the behavioural sciences. Despite the many important contributions of such studies, some researchers have concerns about the reduction in experimental control when research moves outside of laboratory conditions. This paper provides an accessible overview of the issues that can adversely affect data quality in online experiments, with particular focus on cognitive studies of memory and language. I provide checklists for researchers setting up such experiments to help improve data quality. These recommendations focus on three key aspects of experimental design: the technology choices made by researchers and participants, participant recruitment methods, and the performance of participants during experiments. I argue that ensuring high data quality for online experiments requires significant effort prior to data collection to maintain the credibility of our rapidly expanding evidence base. With such safeguards in place, online experiments will continue to provide important, paradigm-changing opportunities across the behavioural sciences.",2024,1703
Purification of cereal prolamins by means of preparative PAGE at acid pH,Kathryn A. Caldwell,"Summary
Rye secalins have been separated using preparative Polyacrylamide gel electrophoresis (PAGE) in gradient gels. Fractionated secalin was recovered by solvent extraction. Based on the recovery of unfractionated secalin from the entire gel, nearly quantitative recovery of cereal prolamins is possible. However, the yield of purified secalin constituents was typically about 75 %. The procedure has been used to prepare milligram quantities of individual secalin bands or fractions. The method and equipment described are simple and can be applied for the fractionation of prolamins from other cereal grains. To demonstrate the general usefulness of the method, wheat gliadins and barley hordeins have been fractionated. Experimental conditions can readily be modified to enhance prolamin resolution. The procedure can probably be adapted to provide purified components equivalent to most of the prolamin bands visible in one- or two-dimensional PAGE patterns.",1984,1704
Removal of cervical mucus prior to embryo transfer improves pregnancy rates in women undergoing assisted reproduction,Mamdoh A Eskandar and Ahmed M Abou-Setta and Mohamed El-Amin and Mona A Almushait and Adekunle A Sobande,"The removal of cervical mucus during embryo transfer has been postulated to increase the pregnancy and implantation rates by not interfering with embryo implantation. Even so, this is a time-consuming procedure that may increase the incidence of difficult transfers by removing the naturally lubricant mucus. In addition, any cervical manipulations at the time of embryo transfer may cause unwarranted uterine contractions. In this prospective, controlled study, 286 women undergoing embryo transfer between January and May 2006 were divided into two groups according to whether the cervical mucus was scheduled to be aspirated (group A) or not (group B). The two groups were similar with regards to the demographics, cause of infertility, characteristics of ovarian stimulation and embryos transferred. Even so, the clinical pregnancy rate was significantly higher in group (A) than group (B) (OR = 2.18, 95% CI = 1.32–3.58), although there were easier transfers in group (B) than group (A) (OR = 3.00, 95% CI = 1.05–8.55). This demonstrates that even though embryo transfers were easier to perform when the cervical mucus was left in place, aspiration resulted in an increased chance of clinical pregnancy.",2007,1705
An evaluation of a rapid visual technique for estimating seagrass biomass,Jane E. Mellors,"A visual census technique for estimating seagrass biomass has been adapted from a comparative pasture yield method. The above-ground biomass of seagrass within sampling quadrats was ranked with respect to a set of reference quadrats which were preselected to provide a scale of standing crop dry weights. At the end of each sampling period, sufficient quadrats were harvested to calibrate the scale. Using the method, monthly mean standing crops were estimated from May 1987 to April 1988 for a multispecific seagrass bed on Green Island, North Queensland. Values obtained ranged between 61.52 and 113.08 g dry weight m−2. The precision (SE/x) of each monthly estimate ranged from 0.05 to 0.13, a satisfactory level for field programs. This method is more precise and time efficient, and is less destructive than some traditional harvesting methods.",1991,1706
Long clinostation influence on the localization of free and weakly bound calcium in cell walls of Funaria hygrometrica moss protonema cells,E.M. Nedukha,"The pyroantimonate method was used to study the localization of free and weakly bound calcium in cells of moss protonema of Funaria hygrometrica Hedw. cultivated on a clinostat (2 rev/min). Electroncytochemical study of control cells cultivated at 1 g revealed that granular precipitate marked chloroplasts, mitochondria, Golgi apparatus, lipid drops, nucleoplasma, nucleolus, nucleus membranes, cell walls and endoplasmic reticulum. In mitochondria the precipitate was revealed in stroma, in chloroplast it was found on thylakoids and envelope membranes. The cultivation of protonema on clinostat led to the intensification in cytochemical reaction product deposit. A considerable intensification of the reaction was noted in endomembranes, vacuoles, periplasmic space and cell walls. At the same time analysis of pectinase localization was made using the electroncytochemical method. A high reaction intensity in walls in comparison to that in control was found out to be a distinctive pecularity of the cells cultivated on clinostat. It testifies to the fact that increasing of freee calcium concentrations under conditions of clinostation is connected with pectinic substances hydrolysis and breaking of methoxy groups of pectins. Data obtained are discussed in relation to problems of possible mechanisms of disturbance in calcium balance of plant cells and the role of cell walls in gomeostasis of cell grown under conditions of simulated weighlessness.",1989,1707
Agarics from xerophytic grasslands in Central Spain,G. Moreno and F. Esteve-Raventós,"Agrocybe pusilla, A. vervacti, Crinipellis tomentosa, Marasmius anomalus and Psilocybe calongei sp.nov. are reported from xerophytic grasslands in central Spain.",1988,1708
Second order virial coefficients from phase diagrams,Belinda P.C. Dewi and Erik {van der Linden} and Arjen Bot and Paul Venema,"The prediction of phase separation is essential to understand and control the properties of food systems. In this work, an existing theoretical model for describing phase separation between binary mixtures of hydrocolloids, using a virial approach up to second order, is extended with several new analytical expressions. These new expressions allow one to determine the three virial coefficients directly from three characteristics of the phase diagram, where the critical point plays a pivotal role and allows one to predict the complete phase diagram. The advantage of this approach is that experimental techniques, like membrane osmometry or static light scattering, to directly measure virial coefficients can be, in principle, avoided. It was found that just the location of the critical point is sufficient to determine two of the three virial coefficients, when one of the virial coefficients is known. When, in addition to the critical point, one other characteristic of the phase diagram is known with sufficient accuracy, like the slope of the tie-lines near or far away from the critical point, all three virial coefficients can be determined from the phase diagram. Using this approach, three virial coefficients for aqueous mixtures of dextran and polyethylene oxide were determined and compared to the ones obtained from membrane osmometry.",2020,1709
Lipoprotein (a) is associated with endothelial function in healthy postmenopausal women,Hanneke W. Wilmink and Miriam J.J. {de Kleijn} and Michiel L. Bots and Annette A.A. Bak and Yvonne T. {van der Schouw} and Sylvia Engelen and Jose Planellas and Jan-Dirk Banga and Diederick E. Grobbee,"Background. Lipoprotein (a) (Lp(a)) is an independent risk factor for atherosclerotic cardiovascular disease. The atherogenic potential of Lp(a) may be by impairment of endothelial function. Objectives. We investigated the relation of Lp(a) plasma levels to endothelium dependent and independent dilatation of the brachial artery in healthy postmenopausal women. Methods. One hundred and five healthy postmenopausal women aged 52–67 years were included in the study. Endothelial function was assessed non-invasively by measuring percent lumen diameter change in the brachial artery after reactive hyperemia and sublingual nitroglycerine spray. Results. Flow mediated dilatation was inversely related to the plasma logLp(a) level. Mean change per unit logLp(a) increase:−2.83% (95% CI: −5.22–−0.43). Elevated Lp(a) (>239 mg/l) (upper quartile) was associated with an impaired flow mediated vasodilatation (2.4%±1.2) compared to Lp(a) ≤239 mg/l (5.2%±0.7). Adjustment for other cardiovascular risk factors did not change the magnitude of the association. Nitroglycerine-induced vasodilatation was not significantly lower in the high Lp(a) level group, compared to the group with normal levels of Lp(a) (≤239 mg/l) (8.0±1.2 vs 11.4%±0.8). Conclusion. Elevated lipoprotein (a) levels are associated with an impaired endothelial function in healthy postmenopausal women, independent of conventional risk factors for cardiovascular disease. Since Lp(a) may be pathogenetically important for early vascular damage, elevated Lp(a) levels might contribute to the increased cardiovascular risk seen in postmenopausal women.",2000,1710
Chapter 36 - Network Forensics,Yong Guan,"Today’s cyber criminal investigator faces a formidable challenge: tracing network-based cyber criminals. The possibility of becoming a victim of cyber crime is the number-one fear of billions of people. This concern is well founded. The findings in the annual CSI/FBI Computer Crime and Security Surveys confirm that cyber crime is real and continues to be a significant threat. Traceback and attribution are performed during or after cyber violations and attacks, to identify where an attack originated, how it propagated, and what computer(s) and person(s) are responsible and should be held accountable. The goal of network forensics capabilities is to determine the path from a victimized network or system through any intermediate systems and communication pathways, back to the point of attack origination or the person who is accountable. In some cases, the computers launching an attack may themselves be compromised hosts or be controlled remotely. Attribution is the process of determining the identity of the source of a cyber attack. Types of attribution can include both digital identity (computer, user account, IP address, or enabling software) and physical identity (the actual person using the computer from which an attack originated). Cyber crime has become a painful side effect of the innovations of computer and Internet technologies. With the growth of the Internet, cyber attacks and crimes are happening every day and everywhere. It is very important to build the capability to trace and attribute attacks to the real cyber criminals and terrorists, especially in this large-scale human-built networked environment. In this chapter, we discuss the current network forensic techniques in cyber attack traceback. We focus on the current schemes in IP spoofing traceback and stepping-stone attack attribution. Furthermore, we introduce the traceback issues in Voice over IP, Botmaster, and online fraudsters.",2013,1711
"Hydrophobicity of biosurfaces — Origin, quantitative determination and interaction energies",C.J. {van Oss},"It is shown that the “hydrophobic” attraction energy between two apolar moieties (as well as between one polar and one apolar moiety) immersed in water is the sole consequence of the hydrogen-bonding energy of cohesion of the water molecules surrounding these moieties. It is also shown that “hydrophobic” surfaces do not repel, but on the contrary attract water. The theory is given of hydrophobic interactions at a macroscopic level, as well as various methods for their quantitative measurement. The properties of hydrophobic, partly hydrophobic and hydrophilic compounds and surfaces are described, including those of amino acids, proteins (incorporating protein solubility), proteins at the air-water interface, carbohydrates, phospholipids, phospholipid layers, and nucleic acids. Finally, some effects and applications of hydrophobic interactions are discussed, including protein adsorption, protein precipitation, cell adhesion, cell fusion, and liquid chromatography approaches such as reversed-phase and hydrophobic interaction chromatography. Finally, the influence of hydrophobic forces is treated in antigen-antibody and other ligand-receptor interactions.",1995,1712
Reduction of adhesion of a Lactobacillus sp. to collagen sausage casing by proteins,M.I. Barriga and J.P.G. Piette,"The adhesion of a meat Lactobacillus sp. to sausage collagen casing was substantially reduced (by 1–2 log CPU cm−2) in the presence of bovine serum albumin, casein, and a yeast cell wall mannoprotein. These proteins affected the [bacterium-medium-casing]system in several ways. They interacted with the cell wall surface, making it more positively charged, they lowered the surface tension of the adhesion medium, and they decreased the hydrophobicity of the casing. Reduction of bacterial adhesion appeared to be mostly related to the decrease in casing hydrophobicity and, indeed, the proteins which did not reduce adhesion (gelatin or casamino acids) had no effect on casing hydrophobicity. The fact that collagen reduced adhesion without affecting the casing surface free energy is consistent with the previous report that some lactobacilli can specifically bind to collagen through a receptor-adhesion site interaction.",1996,1713
A machine learning approach for detecting fast flux phishing hostnames,Thomas Nagunwa and Paul Kearney and Shereen Fouad,"Attackers are increasingly using Fast Flux Service Networks (FFSNs), networks of compromised machines, to host phishing websites. In FFSNs, the machines rapidly change such that blacklisting them does not entirely stop the networks from operating the websites. This increases the longevity of the websites thus becoming more harmful. Existing solutions for detecting the websites are limited with relatively low or moderate prediction performances, high prediction time and use of less diversified features which increases their susceptibility to detection evasions. This paper proposes a Machine Learning (ML) based approach for detecting phishing websites hosted in FFSNs using a novel set of 56 features. Compared with previous works, the approach achieves high accuracy, a low detection time and uses highly diversified features to enhance resilience to detection evasion. The effectiveness of the features for prediction was evaluated in the context of binary and multi-class classification tasks using multiple traditional and deep learning ML algorithms. The proposed approach achieves an accuracy of 98.42% and 97.81% for binary and multi-class classification tasks respectively. Our results showed that temporal and DNS based features are the strongest predictors while network and host related features are the weakest. Our approach is a significant step towards tracking of core components of FFSNs with an aim of shutting down the entire phishing ecosystem.",2022,1714
Chapter 12 - Mobile health (mHealth),Sriram Iyengar,"Mobile health (mHealth), the use of mobile devices such as cell phones to support health, is poised to greatly impact healthcare in many ways. Smartphones are ubiquitous in developed countries and are rapidly being adopted in developing countries. Since they are highly portable and very powerful computing and communication devices with large storage and excellent graphic capabilities, they have the capability to provide clinical decision support, deliver media-rich medical advice, and support documentation of care even when there is no connectivity. The emerging discipline of persuasive technology provides a conceptual framework for developing effective smartphone systems for healthcare. In developing countries, smartphones can serve as powerful support tools for healthcare professionals including community health workers. Google, Apple, and other third parties provide systems for the development of smartphone systems. There are many new mobile technologies for the health including activity sensors and smartwatches.",2020,1715
Are the Communication and Professionalism Competencies the New Critical Values in a Resident’s Global Evaluation Process?,Mounir J. Haurani and I. Rubinfeld and S. Rao and J. Beaubien and J.L. Musial and A. Parker and C. Reickert and A. Raafat and A. Shepard,"Background
The ACGME requires the assessment of resident competency in 6 domains. Global evaluations covering all 6 competencies are routinely used. Evaluators may be overly influenced by resident affability and availability, thereby resulting in a halo effect. We hypothesized that the Interpersonal Skills and Communications (ICS) and Professionalism (PR) competencies would unduly influence other competency scores.
Methods
General surgery resident evaluations are performed by staff and peers on a rotational basis using competency-based questions. Each question is scored using a 5-point Likert scale. Mean individual composite scores for each competency were calculated and then correlated with other mean composite competency scores. Data from patient evaluations were similarly analyzed. A final correlation of competency scores to ABSITE scores, as an objective, standardized measure of a specific competency, Medical knowledge (MK) was also performed.
Results
Results were available for 37 residents (PGY 1-5). There was a significant association between ICS scores and higher scores in MK (r = 0.52, p = 0.004), PR (r = 0.826, p < 0.0001) and patient care (PC) (r = 0.619, p < 0.0001). No correlation, however, was found between patient evaluations of residents and their faculty/peer-based ICS scores. We found no association between ICS scores and improved patient evaluations. Lastly, we found no association between ICS or MK scores and ABSITE scores.
Conclusions
It was difficult to ascertain whether residents with better ICS scores had higher PR, PC, and MK scores because of the halo effect, improper completion of evaluations, or whether those residents were truly performing better clinically. External measures of resident performance did not correlate with faculty/peer evaluations of ICS and PR. Residency programs should consider adopting a more standardized way to objectively evaluate residents.",2007,1716
Removal of cervical mucus: effect on pregnancy rates in IVF/ICSI,BAJT Visschers and RSGM Bots and MF Peeters and BWJ Mol and HJHM {van Dessel},"Cervical mucus may cover the embryo transfer catheter during passage of the cervical canal, interfering with the correct placement of the embryo(s) into the uterine cavity. The effect of removal of cervical mucus prior to embryo transfer in IVF/intracytoplasmic sperm injection (ICSI) on live birth rate was studied. The study was set up as a single blind randomized controlled trial. Couples undergoing IVF/ICSI were randomly allocated to either removal of cervical mucus prior to embryo transfer, or a mock procedure. Randomization was done with stratification for age, cycle number and method of treatment. Primary outcome was live birth rate. A total of 317 couples were included and underwent 428 cycles, of which the outcome of 3 cycles was unknown. Baseline characteristics of both groups were comparable. Live birth occurred in 52 of 220 (24%) cycles in the treatment group and 42 of 205 (21%) cycles in the control group (risk difference 3%, 95% confidence interval −5–11%). It is unlikely that removal of cervical mucus prior to embryo transfer has a significant effect on live birth rate. A small effect, however, cannot be excluded.",2007,1717
Inorganic carbon uptake kinetics of the stream macrophyte Callitriche cophocarpa Sendt.,Tom Vindbæk Madsen,"Photosynthetic carbon uptake of Callitriche cophocarpa Sendt. was examined in plants collected from six Danish streams and in plants grown under variable inorganic carbon conditions in the laboratory. Both field and laboratory plants showed a low affinity for inorganic carbon (CO2 compensation points ranging from 0.7 to 22 μM, and K0.5(CO2) from 51 to 121 μM), consistent with C-3 photosynthesis and use of CO2 alone. Variation in inorganic carbon uptake characteristics was low in both groups of plants. Only in laboratory-grown plants was a coupling found between carbon uptake and the inorganic carbon regime of the medium. The carbon extraction capacity, expressed as a percentage of the initial amount of dissolved inorganic carbon (DIC) assimilated in PH-drift experiments, increased from −1.4 to 11.8% with declining external carbon availability, and the initial slope of the CO2 response curve increased from 6.4 to 15.3 g−1 h−1 dm3. The plasticity of the inorganic carbon uptake system of C. cophocarpa was very low compared to the plasticity observed for submerged macrophytes with accessory carbon uptake systems (i.e. HCO3− use or C-4 photosynthesis), suggesting that the plasticity of the C-3 photosynthetic apparatus as such is restricted. The low carbon affinity of C. cophocarpa indicates that this species depends on CO2 oversaturation for a sufficient supply of CO2 for photosynthesis and growth.",1991,1718
Mass balance of trichloroacetic acid in the soil top layer,Eddo J. Hoekstra and Ed W.B. {de Leer} and Udo A.Th. Brinkman,"Since the ban on the use of trichloroacetic acid (TCAA) as a herbicide in several countries, TCAA is still found ubiquitously in the environment. The presence of TCAA nowadays is suggested to originate mainly from the atmospheric degradation of tetrachloroethene. Our mass balance calculations indicate that this may be true for the presence of TCAA in the atmosphere. However, our mass balance calculations also provide tentative evidence for the formation of TCAA in soil. If our calculated production fluxes are realistic estimates, a very large source of TCAA in soil has been identified.",1999,1719
Chapter 5 - Guarding Against Network Intrusions,Thomas M. Chen and Patrick J. Walsh,"Guarding against network intrusions requires the monitoring of network traffic for particular network segments or devices and analysis of network, transport, and application protocols to identify suspicious activity. This chapter provides a detailed discussion of network-based intrusion protection technologies. It contains a brief overview of the major components of network-based intrusion protection systems and explains the architectures typically used for deploying the components. It also examines the security capabilities of the technologies in depth, including the methodologies they use to identify suspicious activity. The rest of the chapter discusses the management capabilities of the technologies and provides recommendations for implementation and operation.",2013,1720
Hormone replacement therapy and endothelial function: Results of a randomized controlled trial in healthy postmenopausal women,Miriam J.J {de Kleijn} and Hanneke W Wilmink and Michiel L Bots and Annette A.A Bak and Yvonne T {van der Schouw} and Juan Planellas and Sylvia Engelen and Jan-Dirk Banga and Diederick E Grobbee,"Objective: To compare the effects of 3 months treatment with tibolone (a single entity synthetic steroid hormone with estrogenic, progestanic and androgenic activities), or continuous combined conjugated equine estrogens (CEE) plus medroxyprogesterone acetate (MPA), with placebo, on endothelial function. Design: A single center, randomized, double-blind, placebo-controlled study. Setting: Research center as part of the University Medical Center Utrecht. Subjects: One hundred and five healthy postmenopausal women, sampled from the general population. Interventions: Three months treatment with tibolone or CEE+MPA or placebo. Main outcome measure: At baseline and after 3 months, endothelial function was assessed non-invasively by measuring percent lumen diameter change in the brachial artery after reactive hyperemia and sublingual nitroglycerine spray. Results: Results are presented as mean differences between treatment groups of endothelium dependent flow mediated dilatation (fmd) and endothelium independent nitroglycerine induced dilatation with 95% confidence intervals (95% CI). After treatment, there was a significant difference in mean fmd between the CEE+MPA group and the placebo group of 2.5% (95% CI: 0.3–4.6) while the tibolone group and the placebo group did not differ significantly (0.6%; 95% CI: 1.6–2.8). Nitroglycerine induced dilatation did not differ significantly between the groups. Conclusions: Hormone replacement therapy with CEE+MPA for 3 months increases endothelium dependent fmd of the brachial artery in healthy postmenopausal women. Tibolone did not alter fmd. The clinical significance of this improvement in fmd for cardiovascular disease risk needs to be established.",2001,1721
An approach to the taxonomy of Vallisneria L. (Hydrocharitaceae),Richard M. Lowden,"Taxonomic decisions presented in this study of Vallisneria are founded on the consistency of comparable staminate and pistillate floral structures considering the geography and dioecious nature of the genus. Field studies, realized in southern Europe, eastern North America, Central America, northern South America and the Greater Antilles, formed the basis for the development of the present knowledge of characteris. Umbel and spike-like inflorescences were discovered from three localities in the Americas. Staminodia were encountered in the female flowers of Vallisneria spiralis. Two species including four varieties are recognized, mapped and illustrated. Infraspecific taxa are V. spiralis L. var. spiralis, V. spiralis var. denseserrulata Makino, V. americana Michaux var. americana and V. americana var. biwaensis (Miki) Lowden, comb. nov. These taxa are delineated according to the degree of connation of fertile filaments in the staminate flower and adnation of staminodia to stigma—style surfaces in the pistillate flower. Both species converge along what appears to be a continuous gradient in floral variation.",1982,1722
"Identification of the lipids and the ant attractant 1,2-dioleoylglycerol in the arils of Commiphora guillaumini Perr. (Burseraceae) by supercritical fluid chromatography-atmospheric pressure chemical ionisation mass spectrometry",Karl Schmeer and Graeme Nicholson and Shigang Zhang and Ernst Bayer and Katrin Bohning-Gaese,"On-line coupling of supercritical fluid chromatography to atmospheric pressure chemical ionisation mass spectrometry was used to analyse a complex mixture of tri- and di-acylglycerols extracted from the tree Commiphora guillaumini Perr. (Burseraceae). The single components, including the ant attractant 1,2-dioleoylglycerol, were identified by mass spectrometry using skimmer-fragmentation in both the positive and the negative mode.",1996,1723
Conjugated linoleic acid production and probiotic assessment of Lactobacillus plantarum isolated from Pico cheese,Susana C. Ribeiro and Catherine Stanton and Bo Yang and R. Paul Ross and Célia C.G. Silva,"Lactic acid bacteria isolated from a traditional Azorean cheese were screened for their ability to convert free linoleic acid to conjugated linoleic acid (CLA). Two strains of Lactobacillus plantarum were recognized as potential CLA producers. GC analysis identified cis-9, trans-11 C18:2 as the predominant isomer (10–14 μg/mL), followed by trans-9, trans-11 C18:2 (4–6 μg/mL). The CLA producing strains demonstrated strong biofilm capacity, high cell surface hydrophobicity and good auto-aggregation ability. These strains were capable of surviving in the presence of bile salts (0.3%) and pancreatin (0.1%), but only the highest CLA producer (L3C1E8) was able to resist low pH (2.5). Moreover, the CLA-producers showed good adhesion capacity to intestinal human cells (Caco-2 and HT-29) and were able to prevent colonization of Escherichia coli. Of the two strains, Lactobacillus plantarum L3C1E8 revealed superior probiotic properties and great potential for producing food products enriched in the two CLA isomers, cis-9, trans-11 C18:2 (60%) and trans-9, trans-11 C18:2 (25%).",2018,1724
Chapter e43 - Network Forensics,Yong Guan,"Today's cyber-criminal investigator faces a formidable challenge: tracing network-based cyber criminals. The possibility of becoming a victim of cyber-crime is the number-one fear of billions of people. This concern is well founded. The findings in the annual Computer Security Institute/Federal Bureau of Investigation Computer Crime and Security Surveys confirm that cyber-crime is real and continues to be a significant threat. Trace-back and attribution are performed during or after cyber violations and attacks to identify where an attack originated, how it propagated, and what computer(s) and person(s) are responsible and should be held accountable. The goal of network forensics capabilities is to determine the path from a victimized network or system through any intermediate systems and communication pathways, back to the point of attack origination or the person who is accountable. In some cases, the computers launching an attack may themselves be compromised hosts or be controlled remotely. Attribution is the process of determining the identity of the source of a cyber-attack. Types of attribution can include both digital identity [computer, user account, Internet Protocol (IP) address, or enabling software] and physical identity (the actual person using the computer from which an attack originated). Cyber-crime has become a painful side effect of the innovations of computer and Internet technologies. With the growth of the Internet, cyber-attacks and crimes are happening every day and everywhere. It is very important to build the capability to trace and attribute attacks to the real cyber criminals and terrorists, especially in this large-scale human-built networked environment. In this chapter, we discuss the current network forensic techniques in cyber-attack trace-back. We focus on the current schemes in IP spoofing trace-back and stepping-stone attack attribution. Furthermore, we introduce the trace-back issues in Voice over Internet Protocol, botmaster, and online fraudsters.",2013,1725
Digital forensic tools: Recent advances and enhancing the status quo,Tina Wu and Frank Breitinger and Stephen O'Shaughnessy,"Publications in the digital forensics domain frequently come with tools – a small piece of functional software. These tools are often released to the public for others to reproduce results or use them for their own purposes. However, there has been no study on the tools to understand better what is available and what is missing. For this paper we analyzed almost 800 articles from pertinent venues from 2014 to 2019 to answer the following three questions (1) what tools (i.e., in which domains of digital forensics): have been released; (2) are they still available, maintained, and documented; and (3) are there possibilities to enhance the status quo? We found 62 different tools which we categorized according to digital forensics subfields. Only 33 of these tools were found to be publicly available, the majority of these were not maintained after development. In order to enhance the status quo, one recommendation is a centralized repository specifically for tested tools. This will require tool researchers (developers) to spend more time on code documentation and preferably develop plugins instead of stand-alone tools.",2020,1726
Does the beneficial effect of HRT on endothelial function depend on lipid changes,Marlies E. Ossewaarde and Michiel L. Bots and Yvonne T. {van der Schouw} and Miriam J.J. {de Kleijn} and Hanneke W. Wilmink and Annette A.A. Bak and Juan Planellas and Jan-Dirk Banga and Diederick E. Grobbee,"Objectives: To determine whether improvement in endothelial function of the brachial artery observed in women treated with hormone replacement therapy (HRT) may be explained by changes in lipid profile or blood pressure, information was used obtained in a single-centre, randomised, double blind, placebo-controlled trial. Methods: Hundred-and-five healthy postmenopausal women, aged 50–65 years, were treated with 0.625 mg conjugated equine estrogens (CEE) combined with 2.5 mg medroxyprogesterone acetate (MPA) (CEE+MPA), 2.5 mg tibolone or placebo for 3 months. At baseline and after 3 months, endothelial function was assessed using flow-mediated dilatation (FMD) and nitro glycerine-mediated dilatation (NMD). Furthermore, lipids were measured. Multivariate linear regression analysis was applied to address the research question. Results: Treatment with CEE+MPA resulted in an improvement in FMD of 2.0% (95% CI: −0.1; 4.1). CEE/MPA reduced total cholesterol with 13% (95% CI: −18%; −7%), LDL-cholesterol with 23% (95% CI: −30%; −15%) and lipoprotein(a) (Lp(a)) with 14% (95% CI: −26%; −2%). The magnitude of the relation of CEE/MPA with endothelial function was attenuated to from 2.0 to 1.6% when change in Lp(a) was taken into account. Adjustments for other lipids or blood pressure did not attenuate the association. Conclusions: The improvement in endothelial function in postmenopausal women treated with CEE+MPA appears to be partially mediated by change in Lp(a), and apparently not by changes in other lipids.",2003,1727
Dynamics of Pinus radiata foliage in relation to water and nitrogen stress: I. Needle production and properties,R.J. Raison and B.J. Myers and M.L. Benson,"Measurements of the following parameters were made over a 4 year period in 10- to 14-year-old stands of Pinus radiata subjected to markedly different degrees of water and nitrogen (N) stress: needle length, weight and specific leaf area (every 2 weeks), foliage biomass production (annually), pre-dawn needle water potential (every 2 weeks) and needle litter N concentration (monthly). Increments in needle length were a useful estimate of increments in needle weight for any given forest treatment and year because there was no consistent variation in weight per unit length of needles as they developed during the growing season. However, for well-illuminated needles, the ratio of weight per unit of needle length showed a large (approximately two-fold) variation attributable to treatment and year of foliage elongation. The ratio was loosely positively correlated with needle length (or the favourability of growing conditions) on non-irrigated plots, and appeared to result largely from increases in needle thickness rather than density. Final needle length, ranging between 40 and 160 mm, depended mainly on the amount of water and N stress experienced by trees during the growing season. The majority (greater than 90%) of needle extension occurred during a 4 month period (October–January) in spring and summer and the pattern of needle growth was affected only by water availability. Needle extension rates were negatively linearly correlated with the water stress integral (Sψ, a temporal integration of the effects of both water and N availability on needle water potentials) for monthly periods during the growing season. Needle extension, was most sensitive to the Sψ in mid-spring (October/November) when needles were elongating rapidly and still less than one-half grown. About 80% of the variation in annual foliage production (3.2–8.5 t ha−1) could be explained in terms of both (a) Sψ during the previous summer (when primordia were initiated), and (b) the water and N status of trees concurrent with needle extension. Final needle length and total foliage biomass production in the same year were poorly correlated. The specific leaf area (SLA, all sides) ranged from 10 to 17 m2 kg−1 and was greater for needles formed under low light. Irrigation or fertilisation had only an indirect effect on SLA by hastening canopy closure.",1992,1728
Evaluation of KL1 and the inference machine,Henri E Bal,"The paper first briefly describes the KL1 language and its implementation on the Parallel Inference Machine (PIM). KL1 is a successor of Flat GHC and adds features intended for writing efficient, production-quality programs that exploit physical parallelism. KL1 has been used for writing the PIMOS operating system. An implementation of KL1 on the PIM must solve several problems, such as: the representation of KL1's flat global name space on PIM's hierarchical memory; memory management; distributed unification; goals scheduling; and meta-control. The language and its implementation are evaluated and compared with related systems.",1993,1729
A Multiobjective Optimization Approach to a Thermal Designing Problem of a Continuous-Type Electric Furnace,K. Ito and T. Mukai and H. Yokohata and Y. Sato,"A thermal designing problem was considered for an electric resistance vacuum furnace of continuous-type used for high temperature sintering processes of new ceramic materials. The objective furnace has 12 small cells, and the static heat balance relations were formulated mathematically at each cells by investigating heat transfer relationships among heater, insulator, dividing board and heated material. Then, the thermal designing problem of the furnace was formulated as a multiobjective nonlinear optimization problem by investigating two objective functions; that is, (a) minimization of the! sum of the insulator's thickness, and (b) minimization of the total heat loss from the furnace. By adopting both the weighting method and the generalized reduced gradient algorithm, the set of Pare to optimal solutions was derived by determining the thickness of the heat insulator at each cell. The numerical results obtained suggest us the benefit to increase the thickness of the insulator at each cell according to the temperature rise of the corresponding heater.",1987,1730
Development of EMPRO: A Tool for the Standardized Assessment of Patient-Reported Outcome Measures,Jose M. Valderas and Montse Ferrer and Joan Mendívil and Olatz Garin and Luis Rajmil and Michael Herdman and Jordi Alonso,"Objective
This study was aimed to develop a tool for the standardized assessment of patient-reported outcomes (PROs) to assist the choice of instruments.
Methods
An expert panel adapted the eight attributes proposed by the Medical Outcomes Trust as evaluation review criteria, created items to evaluate them, and included a response scale for each item. A pilot test was designed to test the new tool's feasibility and to obtain preliminary information concerning its psychometric properties. The Spanish versions of five measures were selected for assessment: the SF-36 Health Survey, the Nottingham Health Profile, the COOP-WONCA charts, the EuroQol-5D, and the Quality of Life Questionnaire EORTC-QLQ-C30. We assessed the new tool's reliability (Cronbach's alpha and intraclass correlation coefficient [ICC]) and construct validity.
Results
The new EMPRO (Evaluating the Measurement of Patient-Reported Outcomes) tool has 39 items covering eight key attributes: conceptual and measurement model, reliability, validity, responsiveness, interpretability, burden, alternative modes of administration, and cross-cultural and linguistic adaptations. Internal consistency was high (α = 0.95) as was interrater concordance (ICC: 0.87–0.94). Positive associations consistent with a priori hypotheses were observed between EMPRO attribute scores and the number of articles identified for the measures, the years elapsed since the publication of the first article, and the number of citations.
Conclusion
A new tool for the standardized assessment of PRO measures is available. It has shown good preliminary reliability and validity and should be a useful aid to investigators who need to choose between alternative measures. Further assessment of the tool is necessary.",2008,1731
"A comprehensive survey on IoT attacks: Taxonomy, detection mechanisms and challenges",Tinshu Sasi and Arash Habibi Lashkari and Rongxing Lu and Pulei Xiong and Shahrear Iqbal,"The Internet of Things (IoT) has set the way for the continuing digitalization of society in various manners during the past decade. The IoT is a vast network of intelligent devices exchanging data online. The security component of IoT is crucial given its rapid expansion as a new technology paradigm since it may entail safety-critical procedures and the online storage of sensitive data. Unfortunately, security is the primary challenge when adopting Internet of Things (IoT) technologies. As a result, manufacturers’ and academics’ top priority now is improving the security of IoT devices. A substantial body of literature on the subject encompasses several issues and potential remedies. However, most existing research fails to offer a comprehensive perspective on attacks inside the IoT. Hence, this survey aims to establish a structure to guide researchers by categorizing attacks in the taxonomy according to various factors such as attack domains, attack threat type, attack executions, software surfaces, IoT protocols, attacks based on device property, attacks based on adversary location and attacks based on information damage level. This is followed by a comprehensive analysis of the countermeasures offered in academic literature. In this discourse, the countermeasures proposed for the most significant security attacks in the IoT are investigated. Following this, a comprehensive classification system for the various domains of security research in the IoT and Industrial Internet of Things (IIoT) is developed, accompanied by their respective remedies. In conclusion, the study has revealed several open research areas pertinent to the subject matter.",2023,1732
"Demersatheca Li et Edwards, gen. nov., a new genus of early land plants from the Lower Devonian, Yunnan Province, China",Cheng-Sen Li and Dianne Edwards,"Specimens from the Lower Devonian (possibly Pragian) Posongchong Formation of Wenshan, southeastern Yunnan Province, China, originally described as Zosterophyllum contiguum Li et Cai are reinterpreted and placed in the new combination Demersatheca contigua Li et Edwards, comb. nov. They consist of strobili in which sporangia, inserted decussately, form four vertical rows. The sporangia themselves have two valves, splitting around the convex margin, and are sunk into the strobilar axis such that only the abaxial valves, circular to elliptical in outline, are visible. Their contours are continuous with the surface of the strobilus producing a cylindrical structure. The strobili morphologically most closely resemble those of Zosterophyllum, but more detailed comparison is impossible because of the absence of anatomical information on the Chinese specimens.",1996,1733
Factors Affecting Formation of Spores (Akinetes) in Cyanobacterium Anabaena doliolum (AdS strain),K.D. Pandey and A.K. Kashyap,"Summary
The effect of various physical and chemical factors on spore formation of Anabaena doliolum [AdS strain*)] was investigated under laboratory conditions. Among the three inorganic nitrogen sources, nitrate and nitrite (0.5 mM) were slightly stimulatory to sporulation, however, the specific growth rate was reduced in comparison to N-free grown culture. Ammonium (all concentrations) was inhibitory to growth as well as to sporulation. Dilution of phosphate in the medium was stimulatory whereas dilution of the medium was inhibitory to sporulation. Temporal relationship between spore differentiation, depletion of phosphate from the medium, and development of alkaline phosphatase activity suggest that induction of this enzyme may be considered as one of the important events preceeding sporulation. Metabolic inhibitors like sodium azide, sodium fluoride, and sodium arsenate were stimulatory either in terms of the time required for sporulation or the spore frequency. Growth rate as well as sporulation of A. doliolum was more enhanced at 32 ±2 °C than at 25 ±1 °C. Increasing light intensities (from 500lx to 3,000lx) increased the sporulation frequency with a gradual decrease in the time required for spore initiation. pH 8.0 of the medium was most suitable for sporulation.",1987,1734
Chapter 9 - Application Service Providers,,"Publisher Summary
Application Service Providers (ASPs) have an increasing impact on how small- and medium-sized companies handle their messaging infrastructure. An ASP can help companies by outsourcing the implementation, configuration, and maintenance of a variety of services, including messaging. Although companies may derive financial benefits from reduced cost of ownership, they will also reap the benefits of allowing their overutilized staff to concentrate on more important projects than commodity messaging. This chapter explores the basic architecture of hosting Exchange 2000 and then discusses configuring Active Directory and Exchange 2000. ASPs provide access to applications (including the entire infrastructure for supporting that application) to customers who pay for access to the application in the form of a subscription. Customers access the application over the Internet or via a private leased line to the ASP's data center. There are many possibilities beyond messaging to provide services to subscribers. These include real-time conferencing, instant messaging, custom applications, and third-party add-ons. Security considerations for an ASP hosting Exchange 2000 are similar to a corporate implementation that offers messaging capabilities to users from the Internet.",2001,1735
MITTEILUNGEN DER DEUTSCHEN GESELLSCHAFT FÜR MEDIZINISCHE PHYSIK E.V.,,,2002,1736
Chapter 8 - Guarding Against Network Intrusions,Thomas M. Chen,"The Internet exposes computer users to risks from a wide variety of possible threats, including direct intrusions by exploits or social engineering, malware, and web-based attacks. Perfect network security is generally believed to be infeasible. Instead, a defense-in-depth strategy is to hinder the attacker as much as possible with multiple layers of defense, even though each layer might be surmountable. The combination of multiple layers increases the cost for the attacker to be successful, and the cost is proportional to the value of the protected assets. The cost for the attacker could be in terms of additional time, effort, or equipment. A variety of technological measures are used for layers of protection. Preventive measures aim to reduce the likelihood or potential impact of intrusions by eliminating vulnerabilities or exposure to threats. Prevention includes vulnerability assessments, software patching, system hardening, antivirus software, firewalls, and access controls. However, it is practically impossible to deter or prevent all attacks. Reactive measures are necessary to detect malicious activities and take defensive actions by blocking attacks, isolating valuable resources, or tracing the intruder. Host-based and network-based intrusion detection is an essential reactive measure that operates by a combination of misuse (signature-based) detection and anomaly (behavior-based) detection. The central issue is detection accuracy in terms of false positives and false negatives in the face of evolving, intelligent threats.",2013,1737
Adsorption behavior of weak hydrophilic substances on low-energy surface in aqueous medium,Wang Hui and Guo Chao and Fu Jiangang and He Zhangxing and Liang Wei and Chen Xiaolei and Zhuang Caihong,"Through the methods such as measurements of contact angle and surface tension, calculations of surface energy and interfacial interaction free energy, and four weak hydrophilic substances (WHS) were taken as research objects, some interesting conclusions were obtained as follow. In aqueous medium, the WHS give priority to adsorb on low-energy surface that is low polar or particularly non-polar. There is a clear corresponding relationship between the free energy and Lewis base component γ− or the hydrophile index of low-energy surface, and the specific relationship is obtained. Finally, we find hydrophobic attractive force of the Lewis acid–base interaction is mainly responsible for the absorption of WHS on low-energy surface. In short, an initial insight into adsorption behavior of WHS on low-energy surface is demonstrated in this paper.",2011,1738
Timing in the evolution of derived floral characters: upper cretaceous (turonian) taxa with tricolpate and tricolpate-derived pollen,William L. Crepet,"Various hypotheses that seek to explain the rich species diversity of angiosperms relative to other seed plants are briefly mentioned or reviewed. Of these, the subset that relates angiosperm diversity in some way to the relationship between angiosperms and insects, particularly anthophilous insects, is here the object of attention. Specifically, I address and reject the possibility that the relationship between angiosperm diversification and insects, particularly those demonstrating a preference for flowers with derived floral characteristics associated with insect pollination, may be ruled out because of asynchronous patterns of diversification in the fossil record. New data on floral structure from the Turonian of the Atlantic Coastal Plain reveal a surprising diversity of floral characters in taxa bearing tricolpate and tricolporate-derived pollen. The characters and taxa that appear in these Turonian sediments suggest that rather specific modes of insect pollination, perhaps involving highly derived insect pollinators, already existed at 90 Ma. Given the observed rate of diversification of angiosperms during that time and the pattern of evolution in insects, including what can be inferred about the history of the Apidae, these new floral data suggest that hypotheses relating angiosperm diversity to highly specific pollinators are still valid in the context of fossil evidence. Even so, consistency with fossil evidence is not necessarily proof of these relationships. In any case, there may well be multiple causes of relatively high angiosperm species diversity and understanding the relative importance of each of these requires neontological as well as paleontological investigations. One promising approach is to work within the context of phylogenetic patterns with more fossil data.",1996,1739
Editors' acknowledgement,,,1996,1740
Chapter 2 - Science and Cyber Security,Thomas W. Edgar and David O. Manz,"This chapter aims to introduce and define cyber space and cyber security, describing what constitutes cyber space and what it means to secure that space. It describes the foundational concepts of cyber security and discusses the philosophy of cyber security science. The chapter provides an overview and discussion on the fundamental concepts of cyber security, introducing the concepts of attackers, such as vulnerability, exploit, threat, malware etc., and then exploring security by design and the principles that it is based upon. Host security, network security, and risk are all considered, along with the many challenges involved in achieving security in cyber space. The chapter also provides an introductory overview of the cyber security research field and some of the many subfields.",2017,1741
Soy–tibolone combination—Effect on lipids in postmenopausal monkeys and women,Susan E. Appt and Riina Törmälä and Adrian A. Franke and Tomi S. Mikkola and Matti J. Tikkanen and Olavi Ylikorkala and Thomas B. Clarkson,"Objectives
To determine whether co-administration of soy during tibolone treatment would prevent tibolone-induced dyslipoproteinemia in postmenopausal monkeys and women.
Methods
Surgically postmenopausal cynomolgus monkeys (n=18) were assigned randomly to one of four dietary regimens in a Latin Square crossover design, such that all animals received all diets for 14 weeks with a 4-week washout period: (1) casein/lactalbumin (CL); (2) tibolone (Tib, 1.25mg/day women's equivalent); (3) soy (138mg isoflavones/day women's equivalent); (4) Soy+Tib. Postmenopausal women on tibolone treatment were randomized to receive soy powder (52g of soy protein containing 112mg isoflavones) or placebo (containing 52g of milk protein) daily in a crossover trial for 8 weeks with a 4-week washout period.
Results
Monkeys given Tib alone had ∼14% increase in plasma LDL+VLDL-C; whereas those given soy combined with tibolone had significant (∼22%) reductions. Tib treated monkeys had reductions in plasma HDL-C of about 48% vs. no reductions in Soy+Tib. In postmenopausal women using tibolone, soy reduced plasma LDL-C concentrations by ∼10% from baseline without a change in HDL-C.
Conclusions
Co-administration of soy during tibolone treatment improved the lipoprotein profile in both monkeys and women; however, the effects were more robust in monkeys.",2008,1742
"Myasthenia gravis: Major problems in neurology, vol. 11γ by R.P. Lisak and R.L. Barchi (Eds.), 244 pages, W.B. Saunders Comp., Philadelphia, PA, London, 1982. US$ 23.00",J.A. Aarli,,1982,1743
Fracture healing in osteoporotic fractures: Is it really different?: A basic science perspective,Peter Giannoudis and Christopher Tzioupis and Talal Almalki and Richard Buckley,"Summary
Osteoporosis is a major health problem characterized by compromised bone strength that predisposes patients to an increased risk of fracture. Osteoporotic patients differ from normal subjects in bone mineral composition, bone mineral content, and crystallinity. Poor bone quality in patients with osteoporosis presents the surgeon with difficult treatment decisions. Much effort has been expended on improving therapies that are expected to preserve bone mass and thus decrease fracture risk. Manipulation of both the local fracture environment in terms of application of growth factors, scaffolds and mesenchymal cells, and systemic administration of agents promoting bone formation and bone strength has been considered as a treatment option from which promising results have recently been reported. Surprisingly, less importance has been given to investigating fracture healing in osteoporosis. Fracture healing is a complex process of bone regeneration, involving a well-orchestrated series of biological events that follow a definable temporal and spatial sequence that may be affected by both biological factors, such as age and osteoporosis, and mechanical factors such as stability of the osteosynthesis. Current studies mainly focus on preventing osteoporotic fractures. In recent years, the literature has provided evidence of altered fracture healing in osteoporotic bone, which may have important implications in evaluating the effects of new osteoporosis treatments on fracture healing. However, the mechanics of this influence of osteoporosis on fracture healing have not yet been clarified and clinical evidence is still lacking.",2007,1744
Fusion-fission of 238U and 209Bi in different SSNTDs,Jolly Raju and K.K. Dwivedi,Fusion-Fission of 238U and 209Bi in different SSNTDs has been studied using 4 π-geometry technique. Results indicate that the projectile ions on interaction with the atoms of the detector media form compound nuclei and subsequently scission of the compound nuclei takes place in the forward hemisphere. Intricacies involved in the analysis of such kind of events have been discussed.,1993,1745
The Specification of a Computer Aided System for the Design and Manufacture of Lighting Columns,P.F. McGoldrick and C. O'Brien and N.A. Rusby and J.B. Lightbody,"This paper considers the problem of specifying special purpose CAD/CAM systems by presenting a case study which discusses the requirements for software that will be used to aid the design and manufacture of liqhting colmns. whilst many of the details of the programs will be unique to this particular set of problems, much of what is described may have wider applicability particularly, perhaps, for the smaller company or for an organisation with limited knowledge and experience of this technology.",1985,1746
Accumulation of fluoride by Xanthoria parietina growing in the vicinity of the bedfordshire brickfields,Frances B.M. Davies,"Accumulation of fluoride was determined in the thalli of Xanthoria parietina growing within a 30 km radius of the bedfordshire brickfields. Internal fluoride concentrations ranged from 158 ppm within 3 km of the brickfields to 1 ppm, 22 km distant from the source of emissions. The fluoride content was linked to the prevailing winds in the area. Visible damage to the lichen thalli was observed when internal fluoride concentrations exceeded 68 ppm and internal damage when fluoride concentrations exceeded 90 ppm. This damage was attributed to high fluoride concentrations rather than any other pollutant.",1982,1747
Application of In-Memory Computing to Online Power Grid Analysis,Mike Zhou and Donghao Feng,"To address the hard disk and network data I/O bottleneck issue in the large-scale online power grid analysis, an in-memory computing based power grid analysis approach is proposed in this paper. Typical in-memory computing application scenarios in the online power grid analysis are discussed. Using a large-scale online analysis network model as a sample case, in-memory computing simulations corresponding to the scenarios were performed, and the simulation results and analysis of the performance of the simulation are presented.",2018,1748
An Approach to Integrate Parameters and Indicators of Sustainability Management into Value Stream Mapping,Thomas Edtmayr and Alexander Sunk and Wilfried Sihn,"In production research, sustainability is discussed in various forms and often combined with Value Stream Mapping (VSM), a highly accepted method in practice for improving production systems using lean principles. In scientific literature, most authors present frameworks for scoring production processes (e.g. ratios, benchmarks). These approaches aim to reduce (material) input for producing a specific amount of goods. Hence, improved target-conditions of value streams can be designed to increase ecological efficiency and therefore decrease costs. However, the main aim of this contribution is to present an approach to combine generally accepted parameters and indicators of sustainability and VSM. This approach is based on process-oriented accounting of resource consumption along buffers, transports and processes along value streams. This model of integrating sustainability into VSM goes conform with international accepted guidelines to prevent disposals of input resources by reuse, recycle and recovery. On the one hand, following international guidelines and frameworks, this approach can be used for sustainability reporting; e.g. calculating emitted solvents per produced part, kilogram carbon dioxide equivalents per produced part (with units [kgCDE] or [kgCO2eq]), kilogram disposals per produced part, etc. On the other hand, companies will be able to calculate costs and revenues of sustainable value streams; i.e. to quantify their efforts and benefits monetary. Hence, it is necessary to immerse into material flows in value stream, material consumptions at processes, energy consumption of transports, buffers and processes in value stream, linkage of processes with scrap rates, creation of waste, etc. New data lines in VSM need to be created to represent the parameters and indicators of sustainability. The research findings will be presented by an use case from automotive industry.",2016,1749
On the spectra shape of seismic noise and earthquakes recorded in the ocean,A.A. Ostrovsky,"The shape of the spectra of ocean bottom noise and earthquakes recorded in the course of seismological observations in the ocean is analysed. The generalized spectra of bottom seismic noise are suggested as reference curves for comparison with new data. The comparison showed a similarity in the shape and level of the generalized spectra to the noise spectra obtained recently both on the bottom and under the bottom by seismographs placed in holes drilled by the Deep-sea Drilling Project. The spectral analysis of the earthquakes recorded on the bottom of the North-west Pacific basin has shown clear maxima at frequencies of 7–17 Hz formed both by site effects and by absorption of seismic energy in the lithosphere. The frequency of spectral maxima decrease with increasing epicentral distance. Comparison of the earthquake spectra with generalized spectra of bottom noise revealed (probably) incidental coincidence in the frequencies of the minimum for noise spectra (10 Hz) and the average maximum of earthquake record spectra. The sharp mechanical impacts on a bottom seismograph, usually induced by bottom displacements under the instrument, are suggested as rough analogues of the impulses for the bottom instrument system transient calibration. Test measurements have shown that in many cases this method makes it possible to distinguish spectral peaks characterising earthquakes and seismic noise wave trains from those which are caused by coupling resonances of the OBS-sediment interface.",1990,1750
Chapter 11 - Network Forensics,Yong Guan,"Today’s cyber criminal investigator faces a formidable challenge: tracing network-based cyber criminals. The possibility of becoming a victim of cyber crime is the number-one fear of billions of people. This concern is well founded. The findings in the annual CSI/FBI Computer Crime and Security Surveys confirm that cyber crime is real and continues to be a significant threat. Traceback and attribution are performed during or after cyber violations and attacks, to identify where an attack originated, how it propagated, and what computer(s) and person(s) are responsible and should be held accountable. The goal of network forensics capabilities is to determine the path from a victimized network or system through any intermediate systems and communication pathways, back to the point of attack origination or the person who is accountable. In some cases, the computers launching an attack may themselves be compromised hosts or be controlled remotely. Attribution is the process of determining the identity of the source of a cyber attack. Types of attribution can include both digital identity (computer, user account, IP address, or enabling software) and physical identity (the actual person using the computer from which an attack originated). Cyber crime has become a painful side effect of the innovations of computer and Internet technologies. With the growth of the Internet, cyber attacks and crimes are happening every day and everywhere. It is very important to build the capability to trace and attribute attacks to the real cyber criminals and terrorists, especially in this large-scale human-built networked environment. In this chapter, we discuss the current network forensic techniques in cyber attack traceback. We focus on the current schemes in IP spoofing traceback and stepping-stone attack attribution. Furthermore, we introduce the traceback issues in Voice over IP, Botmaster, and online fraudsters.",2014,1751
"Principal coordinate analysis of the distribution of radium-226 between water, sediment and the waterlily, Nymphaea violacea (Lehm), in the vicinity of a uranium mine in the Northern Territory, Australia",John R. Twining,"Radium-226 concentrations in water, sediment and Nymphaea violacea (Lehm) root and rhizome samples were strongly correlated over 2 years between three sample sites from Magela Creek, Northern Territory, Australia. The uptake by roots and rhizomes was due primarily to surface accumulation. Radium-226 concentrations in foliage were not correlated with media concentrations. However, foliar tissue senescence was shown to increase radium accumulation across a range of aquatic plant species including N. violacea (P < 0·05). Principal coordinate analysis showed that the distribution of radium and calcium concentrations in the foliar organs of N. violacea were strongly correlated (r = 0·522; P < 0·001). This result supported the hypothesis that radium was accumulated and/or distributed by the mechanisms involved in uptake of the nutrient divalent cation. However, subsequent analyses comparing the ratio of extractable radium and calcium in the supporting media to their ratio in the plant showed no correlation, which suggested that different uptake mechanisms were involved.",1989,1752
Hormone replacement therapy in perimenopausal women and 2-year change of carotid intima-media thickness,Miriam J.J. {de Kleijn} and Michiel L. Bots and Annette A.A. Bak and Iris C.D. Westendorp and Juan Planellas and Herjan J.T. {Coelingh Bennink} and Jacqueline C.M. Witteman and Diederick E. Grobbee,"Objectives: To assess the 2-year effects of a combined regimen of oral 17β-estradiol and desogestrel (17βE-D) and a sequential combination of conjugated equine estrogens and norgestrel (CEE-N) on common carotid intima-media thickness and end-diastolic lumen diameter in comparison to placebo in perimenopausal women. Methods: The study was a single center, randomized, group-comparative, double-blind study with respect to the 17βE-D and placebo groups and open with respect to CEE-N. After cycle 6, the blind was broken and the trial was continued as an open trial for another 18 months for the active study arms. The study included 121 perimenopausal women recruited from the general population. Common carotid intima-media thickness and end-diastolic lumen diameter were measured at baseline and cycle 24 with B-mode ultrasonography. Results: At cycle 24 small changes in intima-media thickness and lumen diameter were observed. Relative to placebo, changes in intima-media thickness were −0.009 mm [95% CI −0.045; 0.027] for 17βE-D and −0.016 mm [95% CI −0.055; 0.024] for CEE-N. For end-diastolic lumen diameter the changes were −0.091 mm [95% CI −0.236; 0.055] and −0.125 mm [95% CI −0.820; 0.032] for 17βE-D and CEE-N, respectively. Conclusions: In this study among perimenopausal women a significant effect of 17βE-D and CEE-N on common carotid intima-media thickness and lumen diameter could not be demonstrated. Although the sample size of the present trial is too limited to provide definite conclusions, the direction of the effect is in agreement with evidence from earlier studies on the effects of hormone replacement therapy in postmenopausal women.",1999,1753
Chapter 3 - Defining a Firewall,,,2006,1754
"Computing at DESY — current setup, trends and strategic directions",Michael Ernst,"Since the HERA experiments H1 and ZEUS started data taking in '92, the computing environment at DESY has changed dramatically. Running a mainframe centred computing for more than 20 years, DESY switched to a heterogeneous, fully distributed computing environment within only about two years in almost every corner where computing has its applications. The computing strategy was highly influenced by the needs of the user community. The collaborations are usually limited by current technology and their ever increasing demands is the driving force for central computing to always move close to the technology edge. While DESY's central computing has a multidecade experience in running Central Data Recording/Central Data Processing for HEP experiments, the most challenging task today is to provide for clear and homogeneous concepts in the desktop area. Given that lowest level commodity hardware draws more and more attention, combined with the financial constraints we are facing already today, we quickly need concepts for integrated support of a versatile device which has the potential to move into basically any computing area in HEP. Though commercial solutions, especially addressing the PC management/support issues, are expected to come to market in the next 2–3 years, we need to provide for suitable solutions now. Buying PC's at DESY currently at a rate of about 30/month will otherwise absorb any available manpower in central computing and still will leave hundreds of unhappy people alone. Though certainly not the only region, the desktop issue is one of the most important one where we need HEP-wide collaboration to a large extent, and right now. Taking into account that there is traditionally no room for R&D at DESY, collaboration, meaning sharing experience and development resources within the HEP community, is a predominant factor for us.",1998,1755
Potential effects of recombinant DNA organisms on ecosystems and their components,Mark Williamson,"There are many examples of organisms which, by new genetic construction or by finding themselves in new habitats, have established large populations. Some have changed ecosystems drastically, some slightly, some apparently scarcely at all as a result. In the light of these natural and unintentional experiments, protocols for the examination of proposals for genetic release can be derived. The risk of damage will be small, but the damage that could be caused is large. Because of the variety and subtlety of ecological interactions, some dangerous organisms will be passed as safe. Methods of control need to be considered at the time of release.",1988,1756
Addressing the challenges of modern DNS a comprehensive tutorial,Olivier {van der Toorn} and Moritz Müller and Sara Dickinson and Cristian Hesselman and Anna Sperotto and Roland {van Rijswijk-Deij},"The Domain Name System (DNS) plays a crucial role in connecting services and users on the Internet. Since its first specification, DNS has been extended in numerous documents to keep it fit for today’s challenges and demands. And these challenges are many. Revelations of snooping on DNS traffic led to changes to guarantee confidentiality of DNS queries. Attacks to forge DNS traffic led to changes to shore up the integrity of the DNS. Finally, denial-of-service attack on DNS operations have led to new DNS operations architectures. All of these developments make DNS a highly interesting, but also highly challenging research topic. This tutorial – aimed at graduate students and early-career researchers – provides a overview of the modern DNS, its ongoing development and its open challenges. This tutorial has four major contributions. We first provide a comprehensive overview of the DNS protocol. Then, we explain how DNS is deployed in practice. This lays the foundation for the third contribution: a review of the biggest challenges the modern DNS faces today and how they can be addressed. These challenges are (i) protecting the confidentiality and (ii) guaranteeing the integrity of the information provided in the DNS, (iii) ensuring the availability of the DNS infrastructure, and (iv) detecting and preventing attacks that make use of the DNS. Last, we discuss which challenges remain open, pointing the reader towards new research areas.",2022,1757
Acknowledgement to reviewers,,,1995,1758
Electrogenic active proton pump in Hevea brasiliensis laticiferous cells: Its role in activating sucrose/H+ and glucose/H+ symports at the plasma membrane,F. Bouteau and R. Lacrotte and D. Cornel and M. Monestiez and U. Bousquet and A.M. Pennarun and J.P. Rona,"The transplasmalemmal electrical gradient recorded in laticiferous cells at steady state was −113 ± 21 mV. Sucrose and glucose depolarize the plasmalemma of laticiferous cells by about 15 to 25 mV. Our results show that with depolarization due to sucrose (1 mM) or glucose (1 mM) a slight alkalinization (0.1 to 0.2 pH units) can be detected on the outer surface of the cell. Fructose and 3-O-methyl-glucose have no such effect. The extent of depolarization due to the addition of sugars is lower than the electrogenic component of the membrane potential produced by the functioning of the H+-excretion pump (vanadate sensitive-ATPase). Furthermore, in the presence of vanadate or DNP, with glucose or sucrose no shift in pH value was observed. The effect of phlorizin has been tested on the shift of the membrane potential due to sugar uptake across the plasmalemma: neither sucrose nor glucose demonstrate any further depolarization and alkalinization in the presence of phlorizin. Stimulation of the H+-pump by ethylene hyperpolarizes cells by approximately −40 mV and increases the extent of the depolarization induced by sugar transport. These results suggest an active transport of the sugars from the apoplasm towards the cytosol. Evidence for the existence of H+ cotransport with sucrose and/or glucose at the plasmalemma is discussed hereafter.",1991,1759
Effect of hormone replacement therapy on lipids in perimenopausal and early postmenopausal women,Marlies E Ossewaarde and Michiel L Bots and Annette A.A Bak and Yvonne T {Van Der Schouw} and Jacqueline C.M Witteman and Juan Planellas and Herjan J.T.Coelingh Bennink and Diederick E Grobbee,"Objective: To determine the effects of oral sequential hormone replacement therapy (HRT) on lipid-profile in perimenopausal and early postmenopausal women. Methods: We performed a single-center, randomized, placebo-controlled trial. The trial was double blind with respect to 17β-estradiol/desogestrel (17β-E-D) and placebo and open with respect to conjugated estrogens/norgestrel (CEE-N). A total of 125 healthy perimenopausal and early postmenopausal women, aged 43–58 years, were recruited from the general population in Zoetermeer, the Netherlands. The intervention consisted of 6 months treatment with 1.5 mg 17β-estradiol/0.15 mg desogestrel (n=53), 0.625 mg conjugated estrogens/0.15 mg norgestrel (n=36) or placebo (n=36). At baseline, cycle 1, 3 and 6, overnight fasting blood samples were obtained in which lipids were determined. We used linear regression analysis to calculate differences in mean change from baseline in lipids in the active treatment groups compared to placebo. Results: In both treatment groups significant (P<0.05) falls in low-density-lipoprotein (LDL)-cholesterol (17β-E-D: −7.8% and CEE-N: −8.4%) and lipoprotein(a) (17β-E-D: −11.7% and CEE-N: −28.3%) were found compared to placebo. Apolipoprotein A1 (17β-E-D: 6.8% and CEE-N: 7.3%) and HDL-cholesterol (17β-E-D: 6.4% and CEE-N: 8.0%) significantly increased compared to placebo. No significant changes were found in the other lipids. Mean changes from baseline in total cholesterol, LDL-cholesterol and apolipoprotein B were significantly more pronounced in postmenopausal women compared to perimenopausal women, adjustment for age-differences did not change the results. Conclusion: Treatment of perimenopausal and early postmenopausal women with 17β-E-D or CEE-N changes their lipid-profile in a potentially anti-atherogenic direction. Changes appear to be more pronounced in postmenopausal women compared to perimenopausal women.",2001,1760
Chapter 4 - Synthesizing Knowledge from Software Development Artifacts,Olga Baysal and Oleksii Kononenko and Reid Holmes and Michael W. Godfrey,"When software practitioners make day-to-day design decisions about their projects, they are guided by not only their intuition and experience, but also by the variety of software artifacts that are available to them. This chapter describes how lifecycle models can be used to build a useful and intuitive model of these development artifacts. Lifecycle models capture the dynamic nature of how such artifacts change over time in a graphical form that can be easily understood and communicated. We show how lifecycle models can be generated, and we present two industrial case studies where we applied lifecycle models to assess a project’s code review process.",2015,1761
Flavonoid variability within and between natural populations of Pinus uncinata,Josiane Lauranson and Philippe Lebreton,"A total of 120 individual trees representative of Pinus uncinata were analysed for their foliar flavonoid content. The ratio of two extracted anthocyanidins, prodelphinidin and procyanidin, is a constant. Among the flavonols quercetin, kaempfrerol and isorhamnetin, the percentage of quercetin discriminates the five studied populations. Its distribution conforms to the Hardy-Weinberg law, but the ratio of the two implicit alleles proves the originality of each population, and leads to the recognition of distinct chemotypes.",1991,1762
MoreThanSentiments: A text analysis package,Jinhang Jiang and Karthik Srinivasan,"Text mining on a large corpus of data has gained utility and popularity over recent years owing to advancements in information retrieval and machine learning methods. However, popular text mining software packages mainly focus on either sentiment analysis or semantic meaning extraction, requiring pretraining on a large corpus of text data. In comparison, MoreThanSentiments provides computation of newer text attribution measures, including boiler score, specificity, redundancy, and hard info, which have been proposed in accounting analytics literature. Our software package, available in Python, is flexible in terms of parameter setting and is adaptable to different applications. Through this package, we seek to simplify the process of deploying nontrivial information extraction techniques published in domain-specific text analysis research into domain-agnostic analytics applications.",2023,1763
Reactions on Twitter towards Australia's proposed import restriction on nicotine vaping products: a thematic analysis,Tianze Sun and Carmen C.W. Lim and Coral Gartner and Jason P. Connor and Wayne D. Hall and Janni Leung and Daniel Stjepanović and Gary C.K. Chan,"Objective
In June 2020, the Australian Government announced that personal importation of nicotine vaping products (NVP) would be prohibited, pending a 12‐month classification and regulation review by the Therapeutic Goods Administration. This brief report examines the themes of responses on Twitter to this announcement.
Methods
Simple random sampling was used to retrieve tweets containing keywords from 19 to 26 June 2020. Tweets were manually coded and descriptive statistics calculated for themes and policy position.
Results
The vast majority of the 1,168 tweets were anti‐policy. Themes included: criticism towards government (59.8%), activism against NVP restriction (38%), potential adverse consequences (30.8%) and support for NVP restriction (1.4%). Tweets that identified potential adverse consequences of NVP restriction cited: smoking relapse for individuals currently using NVPs (75.6%); the impact of policy enforcement (8.6%); illicit market (8.3%); panic buying (3.6%); difficulty obtaining prescriptions (2.8%); and impacts on NVP businesses (2.8%).
Conclusion
Tweets predominately objected to the policy announcement. Approximately three‐quarters of tweets that cited potential adverse consequences of the policy mentioned smoking relapse as their primary concern.
Implications for public health
User‐generated content on Twitter was primarily used to lobby against the proposed policy, which was subsequently amended.",2021,1764
Selecting and combining complementary feature representations and classifiers for hate speech detection,Rafael M.O. Cruz and Woshington V. {de Sousa} and George D.C. Cavalcanti,"Hate speech is a major issue in social networks due to the high volume of data generated daily. Recent works demonstrate the usefulness of machine learning (ML) in dealing with the nuances required to distinguish between hateful posts from just sarcasm or offensive language. Many ML solutions for hate speech detection have been proposed by either changing how features are extracted from the text or the classification algorithm employed. However, most works consider only one type of feature extraction and classification algorithm. This work argues that a combination of multiple feature extraction techniques and different classification models is needed. We propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme. Source code, figures and dataset splits can be found in the GitHub repository: https://github.com/Menelau/Hate-Speech-MCS.",2022,1765
Mining guidelines for architecting robotics software,Ivano Malavolta and Grace A. Lewis and Bradley Schmerl and Patricia Lago and David Garlan,"Context:
The Robot Operating System (ROS) is the de-facto standard for robotics software. However, ROS-based systems are getting larger and more complex and could benefit from good software architecture practices.
Goal:
We aim at (i) unveiling the state-of-the-practice in terms of targeted quality attributes and architecture documentation in ROS-based systems, and (ii) providing empirically-grounded guidance to roboticists about how to properly architect ROS-based systems.
Methods:
We designed and conducted an observational study where we (i) built a dataset of 335 GitHub repositories containing real open-source ROS-based systems, and (ii) mined the repositories to extract and synthesize quantitative and qualitative findings about how roboticists are architecting ROS-based systems.
Results:
First, we extracted an empirically-grounded overview of the state of the practice for architecting and documenting ROS-based systems. Second, we synthesized a catalog of 47 architecting guidelines for ROS-based systems. Third, the extracted guidelines were validated by 119 roboticists working on real-world open-source ROS-based systems.
Conclusion:
Roboticists can use our architecting guidelines for applying good design principles to develop robots that meet quality requirements, and researchers can use our results as evidence-based indications about how real-world ROS systems are architected today, thus inspiring future research contributions.",2021,1766
Forensic analysis of hook Android malware,Dominic Schmutz and Robin Rapp and Benjamin Fehrensen,"This publication presents a thorough forensic investigation of the banking malware known as Hook, shedding light on its intricate functionalities and providing valuable insights into the broader realm of banking malware. Given the persistent evolution of Android malware, particularly in the context of banking threats, this research explores the ongoing development of these malicious entities. In particular, it emphasizes the prevalent “malware as a service” (MaaS) model, which engenders a competitive environment where malware developers continually strive to enhance their capabilities. Consequently, this investigation serves as a vital benchmark for evaluating the current state of banking MaaS capabilities in July 2023, enabling researchers and practitioners to gauge the advancements and trends within the field.",2024,1767
A comprehensive survey on DDoS attacks detection & mitigation in SDN-IoT network,Chandrapal Singh and Ankit Kumar Jain,"The Internet of Things (IoT) has transformed our lives by introducing new services and enhancing productivity. However, the widespread adoption of IoT devices and communication units has posed challenges in network management. In response, there is a growing necessity to rethink and redesign IoT network control. Software-defined networking (SDN) has emerged as a promising solution, leveraging its programmability and centralized management capabilities. SDN can simplify network management, offer network abstraction, facilitate development, and efficiently handle the complexities of IoT networks. Despite these advantages, security concerns, particularly the threat of Distributed Denial of Service (DDoS) attacks, persist in the IoT landscape. This survey focuses on exploring the collaboration between SDN and IoT. It investigates various types of DDoS attacks and highlights different types of defense, detection, and mitigation methods employed to address DDoS threats in SDN-based IoT (SDN-IoT) networks.",2024,1768
An analysis of discussions in collaborative knowledge engineering through the lens of Wikidata,Elisavet Koutsiana and Gabriel Maia Rocha Amaral and Neal Reeves and Albert Meroño-Peñuela and Elena Simperl,"We study discussions in Wikidata, the world’s largest open-source collaborative knowledge graph (KG). This is important because it helps KG community managers understand how discussions are used and inform the design of collaborative practices and support tools. We follow a mixed-methods approach with descriptive statistics, thematic analysis, and statistical tests to investigate how much discussions in Wikidata are used, what they are used for, and how they support knowledge engineering (KE) activities. The study covers three core sources of discussion, the talk pages that accompany Wikidata items and properties, and a general-purpose communication page. Our findings show low use of discussion capabilities and a power-law distribution similar to other KE projects such as Schema.org. When discussions are used, they are mostly about KE activities, including activities that span across the entire KE lifecycle from conceptualisation and implementation to maintenance and taxonomy building. We hope that the findings will help Wikidata devise improved practices and capabilities to encourage the use of discussions as a tool to collaborate, improve editor engagement, and engineer better KGs.",2023,1769
MEGURU: a gesture-based robot program builder for Meta-Collaborative workstations,Cristina Nuzzi and Simone Pasinetti and Roberto Pagani and Stefano Ghidini and Manuel Beschi and Gabriele Coffetti and Giovanna Sansoni,"This paper presents the Meta-Collaborative Workstation concept and a gesture-based robot program builder software named MEGURU. The software is ROS-based, and it is publicly available on GitHub. A hand-gestures language has been developed to create a fast and easy to use communication method, where single-hand gestures are combined to create composed Commands, allowing the user to create a customized, powerful, and flexible Gestures Dictionary. Gestures are recognized using an R-FCN Object Detector model fine-tuned on a custom dataset developed for this work. The system has been tested in two experiments. The first one was aimed at evaluating the user experience of people of different age, sex, and professional background concerning the proposed communication method. The second one was aimed at comparing the traditional teach pendant programming method and MEGURU in the task of assembling a small Moka coffee maker. The results of both experiments highlight that MEGURU is a promising robot programming method, especially for non-expert users.",2021,1770
"The evolution of IoT Malwares, from 2008 to 2019: Survey, taxonomy, process simulator and perspectives",Benjamin Vignau and Raphaël Khoury and Sylvain Hallé and Abdelwahab Hamou-Lhadj,"The past decade has seen a rapidly growing interest in IoT-connected devices. But as is usually the case with computer systems and networks, malicious individuals soon realized that these objects could be exploited for criminal purposes. The problem is particularly salient since the firmware used in many Internet connected devices was developed without taking into consideration the expertise and best security practices gained over the past several years by programmers in other areas. Consequently, multiple attacks on IoT devices took place over the last decade, culminating in the largest ever recorded DDoS attack, the Mirai botnet, which took advantage of weaknesses in the security of the IoT. In this survey, we seek to shed light on the evolution of the IoT malware. We compare the characteristic features of 28 of the most widespread IoT malware programs of the last decade and propose a novel methodology for classifying malware based on its behavioral features. Our study also highlights the common practice of feature reuse across multiple malware programs.",2021,1771
BeCAPTCHA-Mouse: Synthetic mouse trajectories and improved bot detection,Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez,"We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions, and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36%, proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.",2022,1772
Image-Bot: Generating Synthetic Object Detection Datasets for Small and Medium-Sized Manufacturing Companies,Lukas Block and Adrian Raiser and Lena Schön and Franziska Braun and Oliver Riedel,"Training datasets for image recognition are poorly available for small and medium-sized manufacturing companies, due to the specialized products they work with, and the disproportionate investment to generate their own ones. Thus, we investigate a new approach: The Image-Bot consists of a physical apparatus and a processing pipeline to generate training datasets from real-world objects easily. It takes pictures of the objects in front of a green screen and blends them with random backgrounds. The approach was tested with 23 objects and a YOLOv5 algorithm. It creates a state-of-the-art training dataset with about 2,000 images per object in under 45 min.",2022,1773
TS-IDS: Traffic-aware self-supervised learning for IoT Network Intrusion Detection,Hoang Nguyen and Rasha Kashef,"With recent advances in the Internet of Things (IoT) technology, more people can have instant and easy access to the IoT network of vast and diverse interconnected devices (e.g., surveillance cameras, motion sensors, or smart watches). This trend leads to a significant increase in the frequency and complexity of cyber attacks in the IoT network. Further, these attacks inflict severe financial and privacy damages to individuals and evince the need to develop a more effective and robust network intrusion detection system (NIDS). Network Intrusion Detection (NID) aims to identify the attacks in the networked devices, which is an essential task to protect and maintain Cyber Security. Although recent Machine Learning-based methods have developed and provided more efficient non-human intervention solutions to this problem, these methods still have some unsolved issues. One of the main limitations of existing solutions is that most focus on extracting the features at the flow level independently and ignore their interactions in the network, which impacts the detection performance. To address this problem, in this paper, we propose a Traffic-aware Self-supervised learning for IoT Network Intrusion Detection System, namely TS-IDS, which aims to capture the flow relationships between the network entities. Our approach leverages both node and edge features for improved performance. Additionally, we incorporate auxiliary property-based self-supervised learning (SSL) to enhance the graph representation, even in the absence of labelled data. We conducted experiments on two real-world datasets, NF-ToN-IoT and NF-BoT-IoT. We compared the proposed model with state-of-the-art baseline models to demonstrate the potential of our proposed framework.",2023,1774
A federated society of bots for smart contract testing,Emanuele Viglianisi and Mariano Ceccato and Paolo Tonella,"Smart contracts are a new type of software that allows its users to perform irreversible transactions on a distributed persistent data storage called the blockchain. The nature of such contracts and the technical details of the blockchain architecture give raise to new kinds of faults, which require specific test behaviours to be exposed. In this paper we present SoCRATES, a generic and extensible framework to test smart contracts running in a blockchain. The key properties of SoCRATES are: (1) it comprises bots that interact with the blockchain according to a set of composable behaviours; (2) it can instantiate a society of bots, which can trigger faults due to multi-user interactions that are impossible to expose with a single bot. Our experimental results show that SoCRATES can expose known faults and detect previously unknown faults in contracts currently published in the Ethereum blockchain. They also show that a society of bots is often more effective than a single bot in fault exposure.",2020,1775
User influence analysis for Github developer social networks,Yan Hu and Shanshan Wang and Yizhi Ren and Kim-Kwang Raymond Choo,"Github, one of the largest social coding platforms, offers software developers the opportunity to engage in social activities relating to software development and to store or share their codes/projects with the wider community using the repositories. Analysis of data representing the social interactions of Github users can reveal a number of interesting features. In this paper, we analyze the data to understand user social influence on the platform. Specifically, we propose a Following-Star-Fork-Activity based approach to measure user influence in the Github developer social network. We first preprocess the Github data, and construct the social network. Then, we analyze user influence in the social network, in terms of popularity, centrality, content value, contribution and activity. Finally, we analyze the correlation of different user influence measures, and use Borda Count to comprehensively quantify user influence and verify the results.",2018,1776
A survey on near-human conversational agents,Satwinder Singh and Himanshu Beniwal,"Conversational AI intends for machine-human interactions to appear and feel more natural and inclined to communicate in a near-human context. Chatbots, also known as conversational agents, are typically divided into two use-cases: task-oriented bots and social friend-bots. Task-oriented bots are often used to do activities such as answering questions or solving basic queries. Furthermore, social-friend-bots are designed to communicate like humans, where the user can speak freely and the bot answers organically while maintaining the conversation’s ambience. This paper analyses recent works in the conversational AI domain examining the exclusive methodologies, existing frameworks or tools, evaluation metrics, and available datasets for building robust conversational agents. Finally, a mind-map encompassing all the stated elements and qualities of chatbots is created.",2022,1777
Survey of approaches and features for the identification of HTTP-based botnet traffic,Dilara Acarali and Muttukrishnan Rajarajan and Nikos Komninos and Ian Herwono,"Botnet use is on the rise, with a growing number of botmasters now switching to the HTTP-based C&C infrastructure. This offers them more stealth by allowing them to blend in with benign web traffic. Several works have been carried out aimed at characterising or detecting HTTP-based bots, many of which use network communication features as identifiers of botnet behaviour. In this paper, we present a survey of these approaches and the network features they use in order to highlight how botnet traffic is currently differentiated from normal traffic. We classify papers by traffic types, and provide a breakdown of features by protocol. In doing so, we hope to highlight the relationships between features at the application, transport and network layers.",2016,1778
Domain-specific NLP system to support learning path and curriculum design at tech universities,Nhi N.Y. Vo and Quang T. Vu and Nam H. Vu and Tu A. Vu and Bang D. Mach and Guandong Xu,"The tech sector has been growing at a rapid speed, demanding a higher level of expertise from its labor force. New skills and programming languages are introduced and required by the industry every day, while the university courses are not updated adequately. Finding the high-demand skills and relevant courses to study has become essential to both students and faculty members at tech universities, which leads to a growing research interest in building an intelligence system to support decision making. Leveraging recent development in Natural Language Processing, we built an NLP-based course recommendation system specifically for the computer science (CS) and information technology (IT) fields. In particular, we built (1) a Named Entity Recognition (CSIT-NER) model to extract tech-related skills and entities, then used these skills to build (2) a personalized multi-level course recommendation system using a hybrid model (hybrid CSIT-CRS). Our CSIT-NER model, trained and fine-tuned on a large corpus of text extracted from StackOverflow and GitHub, can accurately extract the relevant skills and entities, outperforming state-of-the-art models across all evaluation metrics. Our hybrid CSIT-CRS can provide recommendations on multiple individualized levels of university courses, career paths with job listings, and industry-required with suitable online courses. The whole system received good ratings and feedback from users from our survey with 201 volunteers who are students and faculty members of tech universities in Australia and Vietnam. This research is beneficial to students, faculty members, universities in CS/IT higher education sector, and stakeholders in tech-related industries.",2022,1779
IoT Botnet Forensics: A Comprehensive Digital Forensic Case Study on Mirai Botnet Servers,Xiaolu Zhang and Oren Upton and Nicole Lang Beebe and Kim-Kwang Raymond Choo,"Internet of Things (IoT) bot malware is relatively new and not yet well understood forensically, despite its potential role in a broad range of malicious cyber activities. For example, it was abused to facilitate the distributed denial of service (DDoS) attack that took down a significant portion of the Internet on October 21, 2016, keeping millions of people from accessing over 1200 websites, including Twitter and NetFlix for nearly an entire day. The widespread adoption of an estimated 50 billion IoT devices, as well as the increasing interconnectivity of those devices to traditional networks, not to mention to one another with the advent of fifth generation (5G) networks, underscore the need for IoT botnet forensics. This study is the first published, comprehensive digital forensic case study on one of the most well known families of IoT bot malware - Mirai. Past research has largely studied the botnet architecture and analyzed the Mirai source code (and that of its variants) through traditional static and dynamic malware analysis means, but has not fully and forensically analyzed infected devices or Mirai network devices. In this paper, we set up a fully functioning Mirai botnet network architecture and conduct a comprehensive forensic analysis on the Mirai botnet server. We discuss forensic artifacts left on the attacker's terminal, command and control (CNC) server, database server, scan receiver and loader, as well as the network packets therefrom. We discuss how a forensic investigator might acquire some of these artifacts remotely, without direct physical access to the botnet server itself. This research provides findings tactically useful to forensic investigators, not only from the perspective of what data can be obtained (e.g., IP addresses of bot members), but also important information about which device they should target for acquisition and investigation to obtain the most investigatively useful information.",2020,1780
How transparent are transparency reports? Comparative analysis of transparency reporting across online platforms,Aleksandra Urman and Mykola Makhortykh,"Over the last decade, transparency reports have been adopted by most large information technology companies. These reports provide important information on the requests tech companies receive from state actors around the world and the ways they respond to these requests, including what content the companies remove from platforms they own. In theory, such reports shall make inner workings of companies more transparent, in particular with respect to their collaboration with state actors. They shall also allow users and external entities (e.g., researchers or watchdogs) to assess to what extent companies adhere to their own policies on user privacy and content moderation as well as to the principles formulated by global entities that advocate for the freedom of expression and privacy online such as the Global Network Initiative or Santa Clara Principles. However, whether the current state of transparency reports actually is conducive to meaningful transparency remains an open question. In this paper, we aim to address this through a critical comparative analysis of transparency reports using Santa Clara Principles 2.0 (SCP 2.0) as the main analytical framework. Specifically, we aim to make three contributions: first, we conduct a comparative analysis of the types of data disclosed by major tech companies and social media platforms in their transparency reports. The companies and platforms analyzed include Google (incl. YouTube), Microsoft (incl. its subsidiaries Github and LinkedIn), Apple, Meta (prev. Facebook), TikTok, Twitter, Snapchat, Pinterest, Reddit and Amazon (incl. subsidiary Twitch). Second, we evaluate to what degree the released information complies with SCP 2.0 and how it aligns with different purposes of transparency. Finally, we outline recommendations that could improve the level of transparency within the reports and beyond, and contextualize our recommendations with regard to the Digital Services Act (DSA) that received the final approval of the European Council in October 2022.",2023,1781
Reviewer recommendation for pull-requests in GitHub: What can we learn from code review and bug assignment?,Yue Yu and Huaimin Wang and Gang Yin and Tao Wang,"Context: The pull-based model, widely used in distributed software development, offers an extremely low barrier to entry for potential contributors (anyone can submit of contributions to any project, through pull-requests). Meanwhile, the project’s core team must act as guardians of code quality, ensuring that pull-requests are carefully inspected before being merged into the main development line. However, with pull-requests becoming increasingly popular, the need for qualified reviewers also increases. GitHub facilitates this, by enabling the crowd-sourcing of pull-request reviews to a larger community of coders than just the project’s core team, as a part of their social coding philosophy. However, having access to more potential reviewers does not necessarily mean that it’s easier to find the right ones (the “needle in a haystack” problem). If left unsupervised, this process may result in communication overhead and delayed pull-request processing. Objective: This study aims to investigate whether and how previous approaches used in bug triaging and code review can be adapted to recommending reviewers for pull-requests, and how to improve the recommendation performance. Method: First, we extend three typical approaches used in bug triaging and code review for the new challenge of assigning reviewers to pull-requests. Second, we analyze social relations between contributors and reviewers, and propose a novel approach by mining each project’s comment networks (CNs). Finally, we combine the CNs with traditional approaches, and evaluate the effectiveness of all these methods on 84 GitHub projects through both quantitative and qualitative analysis. Results: We find that CN-based recommendation can achieve, by itself, similar performance as the traditional approaches. However, the mixed approaches can achieve significant improvements compared to using either of them independently. Conclusion: Our study confirms that traditional approaches to bug triaging and code review are feasible for pull-request reviewer recommendations on GitHub. Furthermore, their performance can be improved significantly by combining them with information extracted from prior social interactions between developers on GitHub. These results prompt for novel tools to support process automation in social coding platforms, that combine social (e.g., common interests among developers) and technical factors (e.g., developers’ expertise).",2016,1782
Neural networks based domain name generation,Zheng Wang and Yang Guo,"Domain generation algorithm (DGA) is used by botnets to build a stealthy command and control (C&C) communication channel between the C&C server and the bots. A DGA can periodically produce a large number of pseudo-random algorithmically generated domains (AGDs), a few of which direct the bots to the C&C server. AGD detection algorithms provide a lightweight, promising solution in response to the existing DGA techniques. In the constantly evolving attacker–defender game, attackers may seek more advanced DGA techniques to gain a better chance of evading detection by defenders. In this paper, we propose a new DGA, namely a neural networks-based domain name generation (NDG) architecture. NDG is based on a variational autoencoder (VAE), where the encoder and decoder networks use stacked gated convolutional neural networks (GCNNs) to learn the contextual structure hierarchically. NDG is experimentally validated using a set of state-of-the-art AGD detection algorithms. The existing DGAs of different classes following a DGA taxonomy are used to benchmark NDG. NDG shows the best overall anti-detection performance among all tested DGAs. We also demonstrate that NDG is effective in benchmarking AGD detection algorithms.",2021,1783
Detecting and mitigating DDoS attacks with moving target defense approach based on automated flow classification in SDN networks,Marcos Aurélio Ribeiro and Mauro Sergio {Pereira Fonseca} and Juliana {de Santi},"The Distributed Denial of Service (DDoS) coordinates synchronized attacks on systems on the Internet using a set of infected hosts (bots). Bots are programmed to attack a determined target by firing a lot of synchronized requests, causing slowness or unavailability of the service. This type of attack has recently grown in magnitude, diversity, and economic cost. Thus, this paper presents a DDoS detection and mitigation architecture based on Software Defined Networking (SDN). It considers the Moving Target Defense (MTD) approach, redirecting malicious floods to expendable low-capacity servers to protect the main server while discouraging the attacker. The redirecting decision is based on a sensor, that employs Machine Learning (ML) algorithms for flow classification. When malicious flows are detected, the sensor notifies the SDN controller to include them in the malicious hosts lists and to realize the redirection. The validation and evaluation of the proposed architecture are conducted by simulation. Results considering different classification models (probabilistic, linear model, neural networks, and trees) and attack types indicate that the proposed architecture is efficient in detecting and mitigating DDoS attacks in approximately 3 seconds.",2023,1784
Automating ontology engineering support activities with OnToology,Ahmad Alobaid and Daniel Garijo and María Poveda-Villalón and Idafen Santana-Perez and Alba Fernández-Izquierdo and Oscar Corcho,"Due to the increasing uptake of semantic technologies, ontologies are now part of a good number of information systems. As a result, software development teams that have to combine ontology engineering activities with software development practices are facing several challenges, since these two areas have evolved, in general, separately. In this paper we present OnToology, an approach to manage ontology engineering support activities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based application that builds on top of Git-based environments and integrates existing semantic web technologies. We have validated OnToology against a set of representative requirements for ontology development support activities in distributed environments, and report on a survey of the system to assess its usefulness and usability.",2019,1785
The war against AI web scraping,Matthew Sparkes,"Elon Musk and Reddit are leading a new wave of objections to the long-accepted practice of scraping content from websites, discovers Matthew Sparkes",2023,1786
Saliency infused dialogue response generation: Improving task oriented text generation using feature attribution,Ratnesh Kumar Joshi and Arindam Chatterjee and Asif Ekbal,"Challenges persist in dialogue scenarios, particularly in multi-turn dialogues where response generation often disregards contextual information beyond the last user utterance, resulting in fluent yet inadequate responses. This paper addresses these issues by identifying and resolving common shortcomings in base model responses during response generation and proposes methods to enhance response quality in unannotated dialogue settings. Our approach involves augmenting information from multiple sources, including keywords, salient features, and knowledge graph triples. We compare the effectiveness of these methods against both the base model and human annotation, which includes dialogue acts and entities. Our findings demonstrate that appending extracted tokens significantly enhances response quality compared to annotated information. In task-oriented dialogue, models perform best when infused with saliency and knowledge graph triples, as shown in the MultiWOZ dataset. Conversely, focusing solely on saliency yields better results for open-domain dialogue, as demonstrated with the DailyDialog dataset. For contextual relevance, the information infusion could also approach the performance of the LLama2 model with only a tenth of the available parameters.",2024,1787
Secret digital coin mining and trading is a threat to your business,Jesse Sampson,"Digital coin software could be infecting your desktops and servers with malware, opening the doors to hackers. They could be after your customer lists, your passwords, your databases. Or they could be looking to turn your computers and devices into bots. Jesse Sampson of Ziften explains the nature of the threat and what to do about it. Bitcoin? Monero? Ethereum? It doesn't matter. Coin mining and trading activities by employees – or by hackers – is a huge security problem that every organisation needs to address.",2018,1788
The strength of weak bots,Marijn A. Keijzer and Michael Mäs,"Some fear that social bots, automated accounts on online social networks, propagate falsehoods that can harm public opinion formation and democratic decision-making. Empirical research, however, resulted in puzzling findings. On the one hand, the content emitted by bots tends to spread very quickly in the networks. On the other hand, it turned out that bots’ ability to contact human users tends to be very limited. Here we analyze an agent-based model of social influence in networks explaining this inconsistency. We show that bots may be successful in spreading falsehoods not despite their limited direct impact on human users, but because of this limitation. Our model suggests that bots with limited direct impact on humans may be more and not less effective in spreading their views in the social network, because their direct contacts keep exerting influence on users that the bot does not reach directly. Highly active and well-connected bots, in contrast, may have a strong impact on their direct contacts, but these contacts grow too dissimilar from their network neighbors to further spread the bot’s content. To demonstrate this effect, we included bots in Axelrod’s seminal model of the dissemination of cultures and conducted simulation experiments demonstrating the strength of weak bots. A series of sensitivity analyses show that the finding is robust, in particular when the model is tailored to the context of online social networks. We discuss implications for future empirical research and developers of approaches to detect bots and misinformation.",2021,1789
Generic signature development for IoT Botnet families,Syed Ghazanfar Abbas and Fabiha Hashmat and Ghalib A. Shah and Kashif Zafar,"As the source code of various IoT botnet families including Mirai has been made publicly available, the adversaries are drastically introducing new variants of these IoT Botnet families. However, there is a lack of generic mechanism for the detection of these emerging variants. As a consequence, it is infeasible for security solution providers to effectively identify new variants of IoT botnets. In this paper, we have done static code analysis of 17 IoT botnet variants of family Mirai and Qbot in order to dig out the attacker's perspective, generic behavior, employed technologies and implemented techniques. With the help of this analysis, we have identified generic behavioral patterns of IoT botnets and have developed generic signatures for the identification of IoT botnets. These signatures includes identification on the basis of CPU architectures, Bot control commands, Bot scanning commands, obfuscation methods, botnet specific exploits and attacks. A comparative analysis of analyzed IoT-Botnet families has been presented. For the evaluation of identified signatures, we first tested them on unknown Mirai and Qbot variants and gained a detection rate of 100% for both the variants. Secondly, we tested those signatures on other IoT-Botnet families: IRC-Bot, Perl ShellBot, Trick-Bot and gained a detection rate of 98%, 96.79% and 98.2% respectively. Further, we have presented open research challenges in the field of IoT-Botnet detection. This research will enhance IoT botnets understanding and pave the way for generic detection and prevention methods of IoT botnets.",2021,1790
Security provision for protecting intelligent sensors and zero touch devices by using blockchain method for the smart cities,Khaleeq {Un Nisa} and Adi Alhudhaif and Kashif Naseer Qureshi and Hassan Jalil Hadi and Gwanggil Jeon,"Internet of Things (IoT) networks has gained popularity due to their amazing and cost-effective services and one of the main areas in smart cities. The stability of these networks is based on stable and secure data transmission without any vulnerabilities present used devices. Distributed Denial of Services (DDoS) attacks have brought critical interruptions in IoT services and significantly damage the network. In DDoS attacks, attackers utilize botnets, with the capability of frequently exploiting the millions of IoT devices around the globe. After the source code of Mirai malware is loaded on GitHub, the threats are significantly increased. Manufacturer Usage Description (MUD) is an embedded software standard for IoT device makers to advertise device specifications, including the intended communication patterns when it connects to the network. Even though the MUD mechanism is promising exertion, still there is a need for evaluating its viability, recognize its limits, and upgrade its architecture to reduce shortcomings in its architecture as well as to increase its effectiveness. This standard neither identifies the vulnerability path before the creation of the MUD profile. Thus, it is possible to exploit an IoT device even after the MUD profile is issued to the device by manipulating the vulnerabilities in the device. By keeping in mind this situation, this paper discusses the limitations of MUD in detail and proposed a framework to identify the patch and default vulnerabilities by using blockchain method before the generation/creation of MUD profiles. The proposed framework can also mitigate open ports, DDoS attacks, and Brute force attacks. The experiment results show the identification, elimination, and sharing of vulnerability report with vendors and significantly minimized the risk of IoT device exploitation.",2022,1791
Generation of a dataset for DoW attack detection in serverless architectures,José Manuel {Ortega Candel} and Francisco José {Mora Gimeno} and Higinio {Mora Mora},"Denial of Wallet (DoW) attacks refers to a type of cyberattack that aims to exploit and exhaust the financial resources of an organization by triggering excessive costs or charges within their cloud or serverless computing environment. These attacks are particularly relevant in the context of serverless architectures due to characteristics like pay-as-you-go model, auto-scaling, limited control and cost amplification. Serverless computing, often referred to as Function-as-a-Service (FaaS), is a cloud computing model that allows developers to build and run applications without the need to manage traditional server infrastructure. Serverless architectures have gained popularity in cloud computing due to their flexibility and ability to scale automatically based on demand. These architectures are based on executing functions without the need to manage the underlying infrastructure. However, the lack of realistic and representative datasets that simulate function invocations in serverless environments has been a challenge for research and development of solutions in this field. The aim is to create a dataset for simulating function invocations in serverless architectures, that is a valuable practice for ensuring the reliability, efficiency, and security of serverless applications. Furthermore, we propose a methodology for the generation of the dataset, which involves the generation of synthetic data from traffic generated on cloud platforms and the identification of the main characteristics of function invocations. These characteristics include SubmitTime, Invocation Delay, Response Delay, Function Duration, Active Functions at Request, Active Functions at Response. By generating this dataset, we expect to facilitate the detection of Denial of Wallet (DoW) attacks using machine learning techniques and neural networks. In this way, this dataset available in Mendeley data repository could provide other researchers and developers with a dataset to test and evaluate machine learning algorithms or use other techniques based on the detection of attacks and anomalies in serverless environments.",2024,1792
Using Linked Building Data for managing temporary construction items,Alexander Schlachter and Mads Holten Rasmussen and Jan Karlshøj,"For decades, the construction industry has experienced poor productivity due to challenges such as increasing project complexity and a fragmented project environment. Even though some technological innovations around Building Information Modeling (BIM) might have the potential to overcome these challenges, data integration across disciplines, companies and software solutions is yet to be solved entirely. Trending advancements try to enrich existing BIM data using Linked Data technologies to semantically describe the building information and facilitate data integration. By that, project data from different data sources is made available in an accessible format, so project participants can use it for their planning efforts. In this paper we explore the use of Linked Building Data (LBD) on a specific use case to answer the question of how the planning of Temporary Construction Items (TCIs) can be improved by integrating data and automating the demand calculation. A literature review concludes that TCIs only experience little attention in the current planning of construction projects but have a critical impact on the outcome of a project. Thus, the objective of this paper is to develop standard ontologies to provide a semantically rich terminology of the data and to propose a framework for TCI consideration within a BIM based project delivery system. A prototype solution is developed, taking formwork as a TCI representative. The result is a process for automatically creating a TCI utilization plan that quantifies the precise time- and location-based on-site TCI demand by integrating data from BIM, Location-Based Scheduling (LBS) and TCI information. Based on the results of prototyping and findings from expert interviews, this research integrates the solution into the process of construction and finally proposes two implementation scenarios for the solution – one being based on the current industry situation and one exploring the future vision of a more integrated and decentralized project delivery in the construction industry.",2022,1793
Programming with ChatGPT: How far can we go?,Alessio Bucaioni and Hampus Ekedahl and Vilma Helander and Phuong T. Nguyen,"Artificial intelligence (AI) has made remarkable strides, giving rise to the development of large language models such as ChatGPT. The chatbot has garnered significant attention from academia, industry, and the general public, marking the beginning of a new era in AI applications. This work explores how well ChatGPT can write source code. To this end, we performed a series of experiments to assess the extent to which ChatGPT is capable of solving general programming problems. Our objective is to assess ChatGPT’s capabilities in two different programming languages, namely C++ and Java, by providing it with a set of programming problem, encompassing various types and difficulty levels. We focus on evaluating ChatGPT’s performance in terms of code correctness, run-time efficiency, and memory usage. The experimental results show that, while ChatGPT is good at solving easy and medium programming problems written in C++ and Java, it encounters some difficulties with more complicated tasks in the two languages. Compared to code written by humans, the one generated by ChatGPT is of lower quality, with respect to runtime and memory usage.",2024,1794
The evolution of Mirai botnet scans over a six-year period,Antonia Affinito and Stefania Zinno and Giovanni Stanco and Alessio Botta and Giorgio Ventre,"The proliferation of Internet of Things devices has resulted in an increase in security vulnerabilities and network attacks. The Mirai botnet is a well-known example of a network used for malicious activities, detected for the first time by the white-hat research group in August 2016. Since then, Mirai initiated massive DDoS attacks by scanning for and exploiting vulnerabilities in network devices. In this paper, we investigate the evolution of the Mirai botnet over a six-year period, analyzing the TCP SYN packets using Mirai signature, i.e. with TCP sequence number equal to the destination IP address. Our analysis stands out as we extensively investigate the evolution of Mirai scans over a prolonged six-year period (2016–2022). Our findings reveal that the Mirai signature is still implemented by malicious actors today, in contrast with previous works. Moreover, we observe that the number of hijacked devices and TCP SYN packets involved in the scanning phase have increased over time. We also confirm that cybercriminals generally target Telnet port 23, followed by fewer requests on Telnet port 2323. Conversely, the number of probes on the SSH ports decreases over time, followed by a subsequent increase in 2022. Lastly, we identify several ports that had not been contacted until 2018 but have since received a large number of TCP SYN packets that verify the Mirai’s signature. These ports are linked with the emergence of new variants of the Mirai botnet.",2023,1795
StarCraft adversary-agent challenge for pursuit–evasion game,Xun Huang,"A reinforcement learning environment with adversary agents is proposed in this work for pursuit–evasion game in the presence of fog of war, which is of both scientific significance and practical importance in aerospace applications. One of the most popular learning environments, StarCraft, is adopted here and the associated mini-games are analyzed to identify its potential applications and limitations for training adversary agents. The key contribution includes the analysis of the best performance that an intelligent agent could be achieved by incorporating control and differential game theory into the specific reinforcement learning environment, and the development of a StarCraft adversary-agents challenge (SAAC) environment by extending the current StarCraft mini-games. The subsequent study showcases the use of this learning environment and the effectiveness of an adversary agent for evaders. Overall, along with rapidly-emerging reinforcement learning technologies, the proposed SAAC environment should benefit pursuit–evasion studies in particular and aerospace applications in general. Last but not least, the corresponding code is available at GitHub.",2023,1796
A framework for cognitive chatbots based on abductive–deductive inference,Carmelo Fabio Longo and Paolo Marco Riela and Daniele Francesco Santamaria and Corrado Santoro and Antonio Lieto,"This paper presents a framework based on natural language processing and first-order logic aiming at instantiating cognitive chatbots. The proposed framework leverages two types of knowledge bases interacting with each other in a meta-reasoning process. The first one is devoted to the reactive interactions within the environment, while the second one to conceptual reasoning. The latter exploits a combination of axioms represented with rich semantics and abduction as pre-stage of deduction, dealing also with some of the state-of-the-art issues in the natural language ontology domain. As a case study, a Telegram chatbot system has been implemented, supported by a module which automatically transforms polar and wh-questions into one or more likely assertions, so as to infer Boolean values or snippets with variable length as factoid answer. The conceptual knowledge base is organized in two layers, representing both long- and short-term memory. The knowledge transition between the two layers is achieved by leveraging both a greedy algorithm and the engine’s features of a NoSQL database, with promising timing performance if compared with the adoption of a single layer. Furthermore, the implemented chatbot only requires the knowledge base in natural language sentences, avoiding any script updates or code refactoring when new knowledge has to income. The framework has been also evaluated as cognitive system by taking into account the state-of-the art criteria: the results show that AD-Caspar is an interesting starting point for the design of psychologically inspired cognitive systems, endowed of functional features and integrating different types of perception.",2023,1797
Hard-style Selective Context Utilization for dialogue generation based on what user just said,Yanxiang Ling and Zheng Liang and Tianqi Wang and Fei Cai and Honghui Chen,"Dialogue is a process of information exchanging, where global background is stable while local focuses are transiting. Thus, at the ongoing dialogue turn, there are both relevant and irrelevant semantics existing in dialogue contexts. How to filter out noises and selectively utilize context can pave the way to successful dialogue generation. Current work on dialogue context utilization either processes contexts as vanilla monologue text ignoring dynamic conversation flows, or depends on weighted strategies to fuse all contexts where irrelevant utterances cannot be filter out even may overwhelm relevant ones. To deal with this, this paper proposes a Hard-style Selective Context Utilization method (HardSCU). We first define and measure the information density of the last utterance (query) of a dialogue, marking it as “strong” or “weak”. For a dialogue with strong query, HardSCU directly inputs the query into a RNN-based or T5-based encoder–decoder framework to generate a response; for a dialogue with weak query, HardSCU conducts a selective context utilization for dialogue generation, where a semantic interaction module introduces relevant semantics of context to enrich the query and the co-reference relations existing in dialogue are extracted to promote the learning process of response decoder. Extensive experiments on two benchmark conversation corpora verify that our HardSCU method can outperform competitive baselines on generating appropriate responses for chit-chat-bots with yielding strong robustness to the variations of dialogue lengths.",2022,1798
Recommending tags for pull requests in GitHub,Jing Jiang and Qiudi Wu and Jin Cao and Xin Xia and Li Zhang,"Context
In GitHub, contributors make code changes, then create and submit pull requests to projects. Tags are a simple and effective way to attach additional information to pull requests and facilitate their organization. However, little effort has been devoted to study pull requests’ tags in GitHub.
Objective
Our objective in this paper is to propose an approach which automatically recommends tags for pull requests in GitHub.
Method
We make a survey on the usage of tags in pull requests. Survey results show that tags are useful for developers to track, search or classify pull requests. But some respondents think that it is difficult to choose right tags and keep consistency of tags. 60.61% of respondents think that a tag recommendation tool is useful. In order to help developers choose tags, we propose a method FNNRec which uses feed-forward neural network to analyze titles, description, file paths and contributors.
Results
We evaluate the effectiveness of FNNRec on 10 projects containing 68,497 tagged pull requests. The experimental results show that on average, FNNRec outperforms approach TagDeepRec and TagMulRec by 62.985% and 24.953% in terms of F1−score@3, respectively.
Conclusion
FNNRec is useful to find appropriate tags and improve tag setting process in GitHub.",2021,1799
ompTG: From OpenMP Programs to Task Graphs,Jinghao Sun and Tao Jin and Yekai Xue and Liwei Zhang and Jinrong Liu and Nan Guan and Quan Zhou,"Real-time systems are shifting them from single-core to multi-core processors. Software must be parallelized to fully utilize the computation power of multi-core architectures. OpenMP is a promising framework to develop parallel real-time software on multi-cores. OpenMP programs keep certain similarity to real-time task graph models, and this motivates much recent work done on real-time scheduling of OpenMP tasks. However, these studies conduct evaluations with randomly generated task graphs, which cannot well capture the structure features of realistic OpenMP programs. To fill the gap between theoretical real-time scheduling research and the OpenMP software reality, we develop an ompTG tool for transforming OpenMP programs into parallel task graphs. ompTG prepares a way to exhibit OpenMP such that the researchers in real-time community can easily understand: An OpenMP system consists of a set of tasks. There are interdependencies among tasks, and each task has an intra structure of the control-flow graph. Besides the topology of OpenMP tasks, we also provide a safe WCET for each vertex of OpenMP task graphs by using static WCET analysis techniques. Moreover, we derive the flow facts, e.g, infeasible path and loop bounds for the task graph, which is necessary information for real-time scheduling and analysis. As a case study, we collect 12 OpenMP programs from the BOTS benchmark, and transform them into task graphs, demonstrating the usage of ompTG.",2022,1800
U-YOLOv7: A network for underwater organism detection,Guoyan Yu and Ruilin Cai and Jinping Su and Mingxin Hou and Ruoling Deng,"Detecting and monitoring underwater organisms is very important for sea aquaculture. The human eye struggles to quickly distinguish between aquatic species due to their variety and dense dispersion. In this paper, a deep learning object detection algorithm based on YOLOv7 is used to design a new network, called Underwater-YOLOv7 (U-YOLOv7), for underwater organism detection. This model satisfies the requirements with regards to both speed and accuracy. First, a network combining CrossConv and an efficient squeeze-excitation module is created. This network increases the extraction of channel information while reducing parameters and enhancing the feature fusion of the network. Second, a lightweight Content-Aware ReAssembly of FEatures (CARAFE) operator is used to obtain more semantic information about underwater images before feature fusion. A 3D attention mechanism is incorporated to improve the anti-interference ability of the model in underwater recognition. Finally, a decoupling head using hybrid convolution is designed to accelerate convergence and improve the accuracy of underwater detection. The results show that the network proposed in this paper obtains an improvement of 3.2% in accuracy, 2.3% in recall, and 2.8% in the mean average precision value and runs at up to 179 fps, far outperforming other advanced networks. Moreover, it has a higher estimation accuracy than the YOLOv7 network.",2023,1801
Encrypted and covert DNS queries for botnets: Challenges and countermeasures,Constantinos Patsakis and Fran Casino and Vasilios Katos,"There is a continuous increase in the sophistication that modern malware exercise in order to bypass the deployed security mechanisms. A typical approach to evade the identification and potential take down of a botnet command and control server is domain fluxing through the use of Domain Generation Algorithms (DGAs). These algorithms produce a vast amount of domain names that the infected device tries to communicate with to find the C&C server, yet only a small fragment of them is actually registered. This allows the botmaster to pivot the control and make the work of seizing the botnet control rather difficult. Current state of the art and practice considers that the DNS queries performed by a compromised device are transparent to the network administrator and therefore can be monitored, analysed, and blocked. In this work, we showcase that the latter is a strong assumption as malware could efficiently hide its DNS queries using covert and/or encrypted channels bypassing the detection mechanisms. To this end, we discuss possible mitigation measures based on traffic analysis to address the new challenges that arise from this approach.",2020,1802
"Repeated rock, paper, scissors play reveals limits in adaptive sequential behavior",Erik Brockbank and Edward Vul,"How do people adapt to others in adversarial settings? Prior work has shown that people often violate rational models of adversarial decision-making in repeated interactions. In particular, in mixed strategy equilibrium (MSE) games, where optimal action selection entails choosing moves randomly, people often do not play randomly, but instead try to outwit their opponents. However, little is known about the adaptive reasoning that underlies these deviations from random behavior. Here, we examine strategic decision-making across repeated rounds of rock, paper, scissors, a well-known MSE game. In experiment 1, participants were paired with bot opponents that exhibited distinct stable move patterns, allowing us to identify the bounds of the complexity of opponent behavior that people can detect and adapt to. In experiment 2, bot opponents instead exploited stable patterns in the human participants’ moves, providing a symmetrical bound on the complexity of patterns people can revise in their own behavior. Across both experiments, people exhibited a robust and flexible attention to transition patterns from one move to the next, exploiting these patterns in opponents and modifying them strategically in their own moves. However, their adaptive reasoning showed strong limitations with respect to more sophisticated patterns. Together, results provide a precise and consistent account of the surprisingly limited scope of people’s adaptive decision-making in this setting.",2024,1803
Freedom of speech or freedom of reach? Strategies for mitigating malicious content in social networks,Saurav Chakraborty and Sandeep Goyal and Annamina Rieder and Agnieszka Onuchowska and Donald J. Berndt,"Malicious content threatens the integrity and quality of content in social networks. Research and practice have experimented with network intervention strategies to curb malicious content propagation. These strategies lack efficiency, target malicious content propagators, and abridge freedom of speech. We draw upon the preferential attachment literature and cognitive load theory to employ the mechanisms of network formation, information sharing, and limited human cognitive capacities to propose an alternative feed management strategy—Preferentiality Dampened Feed Management. We compare and contrast this strategy against other established strategies using an agent-based model that utilizes empirical data from Twitter and findings from the prior literature. The results from our two experiments suggest that our proposed strategy is more effective in curbing malicious content propagation than other established strategies. Our work has important implications for the network interventions literature and practical implications for platform providers, social media users, and society.",2024,1804
Explainable machine learning pipeline for Twitter bot detection during the 2020 US Presidential Elections,Alexander Shevtsov and Christos Tzagkarakis and Despoina Antonakaki and Sotiris Ioannidis,"This study introduces a novel, reproducible and reusable Twitter bot identification system. The system uses a machine learning (ML) pipeline, fed with hundreds of features extracted from a Twitter corpus. The main objective of the proposed ML pipeline is to train and validate different state-of-the-art machine learning models, where the eXtreme Gradient Boosting (XGBoost) model is selected since it achieves the highest detection performance. The Twitter dataset was collected during the 2020 US Presidential Elections, and additional experimental evaluation on distinct Twitter datasets demonstrates the superiority of our approach, in terms of high bot detection accuracy.",2022,1805
Robust detection of unknown DoS/DDoS attacks in IoT networks using a hybrid learning model,Xuan-Ha Nguyen and Kim-Hung Le,"The fourth industrial revolution is marked by the rapid growth of Internet of Things (IoT) technology, leading to an increase in the number of IoT devices. Unfortunately, this also makes these devices more susceptible to cyber threats, especially DoS/DDoS attacks. While supervised learning models have been adopted to detect and mitigate these threats, they have limitations in detecting unknown attacks that can cause severe consequences. This research aims to address those limitations and provide better protection for IoT networks against DoS/DDoS attacks. We propose a new approach that combines a soft-ordering convolutional neural network (SOCNN) model with local outlier factor (LOF) and isolation-based anomaly detection using nearest-neighbor ensembles (iNNE) models that use both supervised and unsupervised learning methods. We evaluated our approach on three benchmark datasets with varying unknown attack scenarios, and our hybrid model achieved high accuracy in detecting unknown attacks with an average F1-score of 98.94%, 91.68%, and 96.07%, respectively, on BoT-IoT, CIC-IDS-2017, and CIC-IDS-2018 datasets, outperforming state-of-the-art competitors. Our model also showed resilience against adversarial attacks such as the fast gradient sign method (FGSM) and Carlini Wagner (CW) adversarial attacks, highlighting the potential of our approach to enhance IoT network security against DoS/DDoS attacks in unknown attack scenarios.",2023,1806
Robotic Process Automation and Artificial Intelligence in Industry 4.0 – A Literature review,Jorge Ribeiro and Rui Lima and Tiago Eckhardt and Sara Paiva,"Taking into account the technological evolution of the last decades and the proliferation of information systems in society, today we see the vast majority of services provided by companies and institutions as digital services. Industry 4.0 is the fourth industrial revolution where technologies and automation are asserting themselves as major changes. Robotic Process Automation (RPA) has numerous advantages in terms of automating organizational and business processes. Allied to these advantages, the complementary use of Artificial Intelligence (AI) algorithms and techniques allows to improve the accuracy and execution of RPA processes in the extraction of information, in the recognition, classification, forecasting and optimization of processes. In this context, this paper aims to present a study of the RPA tools associated with AI that can contribute to the improvement of the organizational processes associated with Industry 4.0. It appears that the RPA tools enhance their functionality with the objectives of AI being extended with the use of Artificial Neural Network algorithms, Text Mining techniques and Natural Language Processing techniques for the extraction of information and consequent process of optimization and of forecasting scenarios in improving the operational and business processes of organizations.",2021,1807
De-CAPTCHA: A novel DFS based approach to solve CAPTCHA schemes,Aditya Atri and Ankita Bansal and Manju Khari and S. Vimal,"CAPTCHA stands for Completely Automated Public Turing Test to Tell Computers and Humans Apart. CAPTCHAs are used as security mechanism in web applications to differentiate between real users and automated users, also known as bots. Text-based CAPTCHAs are the popularly used CAPTCHA schemes due to their simplicity and thus, they are still being used despite the proposal of several attack mechanisms. In this work, the authors have proposed a novel approach to solve CAPTCHA schemes. In this approach, the authors have used Depth First Search algorithm for the extraction of characters from CAPTCHAs and Convolutional Neural Network for recognizing these extracted characters. The proposed approach was validated on 3000+ CAPTCHA schemes and proved to be efficient by providing an average accuracy of more than 92.0% in detecting CAPTCHA schemes.",2022,1808
Real-time bot infection detection system using DNS fingerprinting and machine-learning,Vicente Quezada and Fabian Astudillo-Salinas and Luis Tello-Oquendo and Paul Bernal,"In today’s cyberattacks, botnets are used as an advanced technique to generate sophisticated and coordinated attacks. Infected systems connect to a command and control (C&C) server to receive commands and attack. Thus, detecting infected hosts makes it possible to protect the network’s resources and prevent them from illicit activities toward third parties. This research elaborates on the design, implementation, and results of a bot infection detection system based on Domain Name System (DNS) traffic events for a network corporation. An infection detection feasibility analysis is performed by creating fingerprints. The traces are generated from a numerical analysis of 13 attributes. These attributes are obtained from the DNS logs of a DNS server. It looks for fingerprint anomalies using Isolation Forest to label a host as infected or not. In addition, on the traces cataloged as anomalous, a search will be carried out for queries to domains generated by Domain Generation Algorithms (DGA). Then, Random Forest generates a model that detects future bot infections on hosts. The devised system integrates the ELK stack and Python. This integration facilitates the management, transformation, and storage of events, generation of fingerprints, machine learning application, and analysis of fingerprint classification results with a precision greater than 99%.",2023,1809
Automating discussion structure re-organization for GitHub issues,Shuotong Bai and Lei Liu and Chenkun Meng and Huaxiao Liu,"As a popular social code hosting platform, GitHub encourages developers to discuss and leave opinions on issues. However, the linear format of GitHub issue discussions makes popular discussions difficult for developers to organize and extract useful information effectively. In this paper, we propose an issue discussion re-organization approach, aiming at converting an issue discussion with the linear structure into a discussion tree with key information. First, we conduct a motivational study to investigate the current situation of issue discussions in GitHub. Further, to re-organize discussion structures, we employ a Transformer-based model with transfer learning to predict the response relationship between comments for re-building structures and utilize TF–IDF to extract key information from the content with different topics. The experimental results show that our approach outperforms other baselines, and achieves an average improvement of 14.54% on metrics in the task of predicting response relationships, as well as getting an average improvement of 27.19% in terms of metrics of the re-organizing task. To investigate our re-organized results from actual perspectives, we also conduct a human evaluation. The results show that our approach can predict the accurate response relationships for 80.74% of comments from actual perspectives and 63% of topics extracted by our approach are highly rated. Moreover, 90.00% of newcomers from the open-source community approve of re-organized discussion structures.",2023,1810
Cultivating Metanoia in Twitter Publics: Analyzing and Producing Bots of Protest in the #GamerGate Controversy,Steve Holmes and Rachael Graham Lussos,"This article examines the unique rhetorical affordances of Twitter bots as a way to offer student writers the kairotic means of understanding how networked writing functions in social media public spheres. Specifically, this article discusses how to theorize and construct protest bots in the ad hoc and post hoc Twitter publics of the GamerGate controversy. In addition to kairos, we suggest that the supplementary concept of metanoia offers a highly relevant lens to understand Twitter bots' anonymity and persistence in relationship to the ways in which publics spheres adapt and change over time.",2018,1811
A test amplification bot for Pharo/Smalltalk,Mehrdad Abdi and Henrique Rocha and Alexandre Bergel and Serge Demeyer,"Test amplification exploits the knowledge embedded in an existing test suite to strengthen it. A typical test amplification technique transforms the initial tests into additional test methods that increase the mutation coverage. Although past research demonstrated the benefits, additional steps need to be taken to incorporate test amplifiers in the everyday workflow of developers. This paper describes a proof-of-concept bot integrating Small-Amp with GitHub-Actions. The bot decides for itself which tests to amplify and does so within a limited time budget. To integrate the bot into the GitHub-Actions workflow, we incorporate three special-purpose features: (i) prioritization (to fit the process within a given time budget), (ii) sharding (to split lengthy tests into smaller chunks), and (iii) sandboxing (to make the amplifier crash-resilient). We evaluate our approach by installing the proof-of-concept extension of Small-Amp on five open-source projects deployed on GitHub. Our results show that a test amplification bot is feasible at a project level by integrating it into the build system. Moreover, we quantify the impact of prioritization, sharding, and sandboxing so that other test amplifiers may benefit from these special-purpose features. Our proof-of-concept demonstrates that the entry barrier for adopting test amplification can be significantly lowered.",2024,1812
UTL_DGA22 - a dataset for DGA botnet detection and classification,Tong Anh Tuan and Nguyen Viet Anh and Tran Thi Luong and Hoang Viet Long,"The DGA botnet prevention is a burning topic in cybersecurity, with two problems: detection and classification. The DGA botnet dataset plays an essential role in the research allowing researchers to evaluate their proposed solutions. This study introduces a new dataset on DGA botnets named UTL_DGA22. Our proposed dataset not only inherits previous datasets' results but also has got own advantages. First, our new dataset includes only domain records and no other raw network traffic, helping to address the DGA botnet problem. Second, we removed duplicated botnet DGA families and added new botnet families for a total of 76 DGA botnet families presented. Third, we propose a valuable set of attributes as input for classification algorithms. Our experiments using the proposed features with several machine learning algorithms have had good results. It shows that our proposed attributes are firmly suitable for the input of the DGA botnet solution. Finally, we carefully compiled the dataset and attribute description documents to make it easy for researchers to use. The UTL_DGA22 dataset can serve as a database for researchers to develop their algorithms while objectively evaluating different solutions.",2023,1813
Augment reality chatbot using cloud,Viswanath Matukumalli and Sai {Naga Sasidhar Maddi} and Kushwanth {Krishna Angirekula} and Vivek {Reddy Pulicherla} and A.M. {Senthil kumar} and T. Maridurai and T. Sathish and D. Kasinathan,"A chat is a program that simulates the conversation with humans through text or voice commands. Now days, every company or organization is trying to reduce the manpower in many ways to gain some more profits and to increase the efficiency of the output. Chabot is one of the way to reduce the manpower and to reduce the human intervention. In early days websites and organizations use people to guide their clients and users to their respective outcome. But now days to reduce the manpower organizations are using the chatbots to interact their clients. In our paper, we want to create a chatbot using Augmented Reality and using cloud technologies. Our theme is to create a chatbot which interact with humans like there is another human talking to him and to increase interaction. This makes the organization to produce more accurate results for their clients and uses less human power.",2021,1814
Phylogenomic insights into the first multicellular streptophyte,Maaike J. Bierenbroodspot and Tatyana Darienko and Sophie {de Vries} and Janine M.R. Fürst-Jansen and Henrik Buschmann and Thomas Pröschold and Iker Irisarri and Jan {de Vries},"Summary
Streptophytes are best known as the clade containing the teeming diversity of embryophytes (land plants).1,2,3,4 Next to embryophytes are however a range of freshwater and terrestrial algae that bear important information on the emergence of key traits of land plants. Among these, the Klebsormidiophyceae stand out. Thriving in diverse environments—from mundane (ubiquitous occurrence on tree barks and rocks) to extreme (from the Atacama Desert to the Antarctic)—Klebsormidiophyceae can exhibit filamentous body plans and display remarkable resilience as colonizers of terrestrial habitats.5,6 Currently, the lack of a robust phylogenetic framework for the Klebsormidiophyceae hampers our understanding of the evolutionary history of these key traits. Here, we conducted a phylogenomic analysis utilizing advanced models that can counteract systematic biases. We sequenced 24 new transcriptomes of Klebsormidiophyceae and combined them with 14 previously published genomic and transcriptomic datasets. Using an analysis built on 845 loci and sophisticated mixture models, we establish a phylogenomic framework, dividing the six distinct genera of Klebsormidiophyceae in a novel three-order system, with a deep divergence more than 830 million years ago. Our reconstructions of ancestral states suggest (1) an evolutionary history of multiple transitions between terrestrial-aquatic habitats, with stem Klebsormidiales having conquered land earlier than embryophytes, and (2) that the body plan of the last common ancestor of Klebsormidiophyceae was multicellular, with a high probability that it was filamentous whereas the sarcinoids and unicells in Klebsormidiophyceae are likely derived states. We provide evidence that the first multicellular streptophytes likely lived about a billion years ago.",2024,1815
"oTree—An open-source platform for laboratory, online, and field experiments",Daniel L. Chen and Martin Schonger and Chris Wickens,"oTree is an open-source and online software for implementing interactive experiments in the laboratory, online, the field or combinations thereof. oTree does not require installation of software on subjects’ devices; it can run on any device that has a web browser, be that a desktop computer, a tablet or a smartphone. Deployment can be internet-based without a shared local network, or local-network-based even without internet access. For coding, Python is used, a popular, open-source programming language. www.oTree.org provides the source code, a library of standard game templates and demo games which can be played by anyone.",2016,1816
Bot2Vec: A general approach of intra-community oriented representation learning for bot detection in different types of social networks,Phu Pham and Loan T.T. Nguyen and Bay Vo and Unil Yun,"Recently, due to the rapid growth of online social networks (OSNs) such as Facebook, Twitter, Weibo, etc. the number of machine accounts/social bots that mimic human users has increased. Along with the development of artificial intelligence (AI), social bots are designed to become smarter and more sophisticated in their efforts at replicating the normal behaviors of human accounts. Constructing reliable and effective bot detection mechanisms is this considered crucial to keep OSNs clean and safe for users. Despite the rapid development of social bot detection platforms, recent state-of-the-art systems still encounter challenges which are related to the model’s generalization (and whether it can be adaptable for multiple types of OSNs) as well as the great efforts needed for feature engineering. In this paper, we propose a novel approach of applying network representation learning (NRL) to bot/spammer detection, called Bot2Vec. Our proposed Bot2Vec model is designed to automatically preserve both local neighborhood relations and the intra-community structure of user nodes while learning the representation of given OSNs, without using any extra features based on the user’s profile. By applying the intra-community random walk strategy, Bot2Vec promises to achieve better user node embedding outputs than recent state-of-the-art network embedding baselines for bot detection tasks. Extensive experiments on two different types of real-word social networks (Twitter and Tagged) demonstrate the effectiveness of our proposed model. The source code for implementing the Bot2Vec model is available at: https://github.com/phamtheanhphu/bot2vec",2022,1817
Can natural language processing be effectively applied for audit data analysis in gynaecological oncology at a UK cancer centre?,Mark McGowan and Filipe {Correia Martins} and Jodi-Louise Keen and Amelia Whitehead and Ellie Davis and Pubudu Pathiraja and Helen Bolton and Peter Baldwin,"Background
The British Gynaecological Cancer Society (BGCS) has highlighted the disparity of ovarian cancer outcomes in the UK compared to other European countries. Therefore, cancer quality assurance audits and subspecialty training are important in improving the UK standard of care for these patients. The current workforce crisis afflicting the NHS creates difficulty in dedicating teams of clinicians to these audits. We present a single institution study to evaluate if NLP-generated code can improve the efficiency of ovarian cancer and subspeciality reaccreditations audits. We used the chat bot Google Bard to write Visual Basic Applications algorithms that utilise Excel files from electronic health records.
Methods
Primary ovarian cancer data from 2019 to 2022 was retrospectively collected from the Cambridge University Hospital electronic health records. The surgical subspecialty reaccreditation audit analysed the 2022 surgical database. A modular coding approach with Google Bard was applied to generate audit algorithms. The time to complete these current audits was compared against the 2016 ovarian cancer and 2020 subspeciality reaccreditation audits.
Results
The previous ovarian cancer audit conducted in 2016 required 3 clinicians for the 135 cases and data collection required 1800 min. Data analysis was completed in 300 min. The current ovarian cancer audit allocated 2 clinicians to the 600 surgical cases. Data collection was completed in 3120 min, 3360 min for code development and 720 min for testing. The 2020 subspecialty reaccreditation audit was completed in 360 min. The 2022 subspecialty reaccreditation audit was completed in 1680 min, with 960 min for code development, 240 for debugging and 480 min for testing.
Conclusion
We have demonstrated that NLP-generated code can significantly increase the efficiency of surgical quality assurance audits by eliminating the need for manual data analysis. With the current trajectory of NLP development, increasingly complex algorithms can be developed with minimal programming knowledge.",2024,1818
GPTSniffer: A CodeBERT-based classifier to detect source code written by ChatGPT,Phuong T. Nguyen and Juri {Di Rocco} and Claudio {Di Sipio} and Riccardo Rubei and Davide {Di Ruscio} and Massimiliano {Di Penta},"Since its launch in November 2022, ChatGPT has gained popularity among users, especially programmers who use it to solve development issues. However, while offering a practical solution to programming problems, ChatGPT should be used primarily as a supporting tool (e.g., in software education) rather than as a replacement for humans. Thus, detecting automatically generated source code by ChatGPT is necessary, and tools for identifying AI-generated content need to be adapted to work effectively with code. This paper presents GPTSniffer– a novel approach to the detection of source code written by AI – built on top of CodeBERT. We conducted an empirical study to investigate the feasibility of automated identification of AI-generated code, and the factors that influence this ability. The results show that GPTSniffer can accurately classify whether code is human-written or AI-generated, outperforming two baselines, GPTZero and OpenAI Text Classifier. Also, the study shows how similar training data or a classification context with paired snippets helps boost the prediction. We conclude that GPTSniffer can be leveraged in different contexts, e.g., in software engineering education, where teachers use the tool to detect cheating and plagiarism, or in development, where AI-generated code may require peculiar quality assurance activities.",2024,1819
Detecting bot-infected machines using DNS fingerprinting,Manmeet Singh and Maninder Singh and Sanmeet Kaur,"The never-ending menace of botnet is causing many serious problems on the Internet. Although there are significant efforts on detecting botnet at the global level which rely heavily on finding failed queries and domain flux information for botnet detection, there are very few efforts being made to detect bot infection at an enterprise level. Detecting bot-infected machines is vital for any organization in combating various security threats. This work proposes a novel anomaly-based detection technique which considers hourly hosts DNS fingerprint and attempts to find anomalous behavior which is quite different from normal machine behavior. This work successfully demonstrates the DNS Anomaly Detection (named BotDAD) technique for detecting bot-infected machine in a network using DNS fingerprinting.",2019,1820
Effectiveness of an ensemble technique based on the distributivity equation in detecting suspicious network activity,Ewa Rak and Jaromir Sarzyński and Rafał Rak,"With the growing complexity and frequency of cyber threats, there is a pressing need for more effective defense mechanisms. Machine learning offers the potential to analyze vast amounts of data and identify patterns indicative of malicious activity, enabling faster and more accurate threat detection. Ensemble methods, by incorporating diverse models with varying vulnerabilities, can increase resilience against adversarial attacks. This study covers the usage and evaluation of the relevance of an innovative approach of ensemble classification for identifying intrusion threats on a large CICIDS2017 dataset. The approach is based on the distributivity equation that appropriately aggregates the underlying classifiers. It combines various standard supervised classification algorithms, including Multilayer Perceptron Network, k-Nearest Neighbors, and Naive Bayes, to create an ensemble. Experiments were conducted to evaluate the effectiveness of the proposed hybrid ensemble method. The performance of the ensemble approach was compared with individual classifiers using measures such as accuracy, precision, recall, F-score, and area under the ROC curve. Additionally, comparisons were made with widely used state-of-the-art ensemble models, including the soft voting method (Weighted Average Probabilities), Adaptive Boosting (AdaBoost), and Histogram-based Gradient Boosting Classification Tree (HGBC) and with existing methods in the literature using the same dataset, such as Deep Belief Networks (DBN), Deep Feature Learning via Graph (Deep GFL). Based on these experiments, it was found that some ensemble methods, such as AdaBoost and Histogram-based Gradient Classification Tree, do not perform reliably for the specific task of identifying network attacks. This highlights the importance of understanding the context and requirements of the data and problem domain. The results indicate that the proposed hybrid ensemble method outperforms traditional algorithms in terms of classification precision and accuracy, and offers insights for improving the effectiveness of intrusion detection systems.",2024,1821
Twitter social bots: The 2019 Spanish general election data,Javier Pastor-Galindo and Mattia Zago and Pantaleone Nespoli and Sergio {López Bernal} and Alberto {Huertas Celdrán} and Manuel {Gil Pérez} and José A. Ruipérez-Valiente and Gregorio {Martínez Pérez} and Félix {Gómez Mármol},"The term social bots refer to software-controlled accounts that actively participate in the social platforms to influence public opinion toward desired directions. To this extent, this data descriptor presents a Twitter dataset collected from October 4th to November 11th, 2019, within the context of the Spanish general election. Starting from 46 hashtags, the collection contains almost eight hundred thousand users involved in political discussions, with a total of 5.8 million tweets. The proposed data descriptor is related to the research article available at [1]. Its main objectives are: i) to enable worldwide researchers to improve the data gathering, organization, and preprocessing phases; ii) to test machine-learning-powered proposals; and, finally, iii) to improve state-of-the-art solutions on social bots detection, analysis, and classification. Note that the data are anonymized to preserve the privacy of the users. Throughout our analysis, we enriched the collected data with meaningful features in addition to the ones provided by Twitter. In particular, the tweets collection presents the tweets’ topic mentions and keywords (in the form of political bag-of-words), and the sentiment score. The users’ collection includes one field indicating the likelihood of one account being a bot. Furthermore, for those accounts classified as bots, it also includes a score that indicates the affinity to a political party and the followers/followings list.",2020,1822
Dataset of open-source software developers labeled by their experience level in the project and their associated software metrics,Quentin Perez and Christelle Urtado and Sylvain Vauttier,"Developers are extracted from 17 open-source projects from GitHub. Projects are chosen that use the java programming language, the Spring framework and Maven/Gradle build tools. Along with these developers, 24 software engineering metrics are extracted for each of them. These metrics are either calculated by analyzing the source code or relative to project management metadata. Each of these developers then are manually searched for in professional social media such as LinkedIn or Twitter to be labeled with their experience level in their project. Outliers are statistically detected and manually re-assigned when needed. The resulting dataset contains 703 anonymized developers qualified by their 24 project-related software engineering metrics and labeled for their experience. It is suitable for empirical software engineering studies that need to connect developers’ level of experience to tangible software engineering metrics.",2023,1823
Understanding nuances of scholarly publishing in orthodontics: A comprehensive guide,Narayan H. Gandedkar and Veerasathpurush Allareddy and Nikhilesh R. Vaiid,"This comprehensive manuscript endeavors to furnish orthodontic researchers with the necessary tools and knowledge to adeptly navigate the multifaceted landscape of academic publishing, thereby enhancing the efficacy and reach of their scholarly endeavors. It meticulously imparts critical insights and methodologies for comprehending and leveraging publication metrics, such as citation counts, the h-index, and Journal Impact Factors, to strategically plan research trajectories. Furthermore, it offers guidance on adeptly engaging with evaluation agencies such as the American Dental Association (ADA) and the National Institute of Dental and Craniofacial Research (NIDCR), thereby optimizing alignment with grant opportunities. Through the adept utilization of orthodontic bibliometrics, researchers can gain invaluable insights into prevailing collaboration trends and emerging research domains, thus facilitating informed decision-making and prioritization of scholarly pursuits. Additionally, the manuscript delves into the nuanced optimization of publication guidelines to maximize research impact. Spanning both established domains such as biomechanics, anchorage control, and aligner therapy, as well as burgeoning frontiers including 3D printing and artificial intelligence applications in aligner treatment, this manuscript equips orthodontic researchers with the requisite acumen to embark upon a journey of impactful scholarly contributions, thereby catalyzing advancements in patient care within the discipline.",2024,1824
GRuM — A flexible model-driven runtime monitoring framework and its application to automated aerial and ground vehicles,Michael Vierhauser and Antonio Garmendia and Marco Stadler and Manuel Wimmer and Jane Cleland-Huang,"Runtime monitoring is critical for ensuring safe operation and for enabling self-adaptive behavior of Cyber-Physical Systems (CPS). Monitors are established by identifying runtime properties of interest, creating probes to instrument the system, and defining constraints to be checked at runtime. For many systems, implementing and setting up a monitoring platform can be tedious and time-consuming, as generic monitoring platforms do not adequately cover domain-specific monitoring requirements. This situation is exacerbated when the System under Monitoring (SuM) evolves, requiring changes in the monitoring platform. Most existing approaches lack support for the automated generation and setup of monitors for diverse technologies and do not provide adequate support for dealing with system evolution. In this paper, we present GRuM (Generating CPS Runtime Monitors), a framework that combines model-driven techniques and runtime monitoring, to automatically generate a customized monitoring platform for a given SuM. Relevant properties are captured in a Domain Model Fragment, and changes to the SuM can be easily accommodated by automatically regenerating the platform code. To demonstrate the feasibility and performance we evaluated GRuM against two different systems using TurtleBot robots and Unmanned Aerial Vehicles. Results show that GRuM facilitates the creation and evolution of a runtime monitoring platform with little effort and that the platform can handle a substantial amount of events and data.",2023,1825
Content queries and in-depth analysis on version-controlled software,Jannek Squar and Niclas Schroeter and Anna Fuchs and Michael Kuhn and Thomas Ludwig,"Writing scientific code usually implies the need to coordinate and conflate the contributions of several scientific programmers. Using Git hosting services eases this process, because the hosting services offer many features, which assist in collaborated work on code. The well-established hosting service GitHub has seen continuous growth in terms of number of users, repositories and commits over the last few years; therefore it offers a large data source of scientific codes as well as social interaction of associated scientific programmers. We present a tool, which allows to easily search through relevant GitHub repositories and perform more advanced analyses, which cannot be conducted solely with the GitHub API. Our tool combines benefits from online as well as offline approaches to retrieve and analyse data to optimise time of execution and consumption of storage. We discuss possible use cases and demonstrate the tool's capabilities by investigating the popularity of OpenMP directives in the scientific community.",2022,1826
Applying model-driven engineering to the domain of chatbots: The Xatkit experience,Gwendal Daniel and Jordi Cabot,"Chatbots are becoming a common component of many types of software systems. But they are typically developed as a side feature using ad-hoc tools and custom integrations. Moreover, current frameworks are efficient only when designing simple chatbot applications while they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs. In addition, the deployment of a chatbot application usually requires a deep understanding of the targeted platforms, especially back-end connections, increasing the development and maintenance costs. In this paper, we discuss our experiences building, evolving and distributing the Xatkit framework. Xatkit is a model-based framework built around a Domain-Specific Language to define chatbots (and voicebots and bots in general) in a platform-independent way. Xatkit also comes with a runtime engine that automatically deploys the chatbot application and manages the defined conversation logic over the platforms of choice. Xatkit has significantly evolved since its initial release. This paper focuses on describing the evolution and the reasons (technical and non-technical) that triggered them. We believe our lessons learned can be useful to any other initiative trying to build a successful industrial-level chatbot platform, and in general, any type of model-based solution.",2024,1827
A novel deep learning based cloud service system for automated acupuncture needle counting: a strategy to improve acupuncture safety,Tsz Ho Wong and Junyi Wei and Haiyong Chen and Bacon Fung Leung Ng,"Objective
The unintentional retention of needles in patients can lead to severe consequences. To enhance acupuncture safety, the study aimed to develop a deep learning-based cloud system for automated process of counting acupuncture needles.
Methods
This project adopted transfer learning from a pre-trained Oriented Region-based Convolutional Neural Network (Oriented R-CNN) model to develop a detection algorithm that can automatically count the number of acupuncture needles in a camera picture. A training set with 590 pictures and a validation set with 1 025 pictures were accumulated for fine-tuning. Then, we deployed the MMRotate toolbox in a Google Colab environment with a NVIDIA Tesla T4 Graphics processing unit (GPU) to carry out the training task. Furthermore, we integrated the model with a newly-developed Telegram bot interface to determine the accuracy, precision, and recall of the needling counting system. The end-to-end inference time was also recorded to determine the speed of our cloud service system.
Results
In a 20-needle scenario, our Oriented R-CNN detection model has achieved an accuracy of 96.49%, precision of 99.98%, and recall of 99.84%, with an average end-to-end inference time of 1.535 s
Conclusion
The speed, accuracy, and reliability advancements of this cloud service system innovation have demonstrated its potential of using object detection technique to improve acupuncture practice based on deep learning.",2024,1828
SmartDefense: A distributed deep defense against DDoS attacks with edge computing,Sowmya Myneni and Ankur Chowdhary and Dijiang Huang and Adel Alshamrani,"The growing number of IoT edge devices have inflicted a change in the cyber-attack space. The DDoS attacks, in particular, have significantly increased in magnitude and intensity. Of the existing DDoS solutions, while the destination-based defense mechanisms incur high false positives due to the seemingly legitimate nature of the attack traffic, defense mechanisms implemented at the source alone do not suffice due to the lack of visibility into ongoing DDoS attacks. This paper proposes a distributed DDoS detection and mitigation framework, SmartDefense, based on edge computing approaches towards detecting and mitigating DDoS attacks at and near the source. By mitigating the DDoS attacks near the source, SmartDefense significantly reduces unnecessary bandwidth otherwise consumed by DDoS traffic going from residential edge networks to the ISP edge network. Furthermore, SmartDefense demonstrates how ISPs can detect botnet devices in their customer’s network by having smart edge devices pass attributes that are processed by the botnet detection engine at the provider’s edge. The evaluation of this work shows that SmartDefense can improve the detection and mitigation rate, with over 90% of DDoS traffic caught at the source and over 97.5% of remaining DDoS traffic caught at the provider’s edge. Our experiments also demonstrate how using a botnet detection engine can further reduce the DDoS traffic by up to 51.95% by facilitating ISPs to detect bot devices in their customers’ edge network.",2022,1829
Adaptive ambulance monitoring system using IOT,S. Mahalakshmi and T. Ragunthar and N. Veena and S. Sumukha and Pranav R. Deshkulkarni,"- With the increase in the number of automobiles in urban cities, the number of accidents has increased manifold. Hence, the need for ambulances is increasing at an alarming rate. In order to increase the survival rates of the patients, an efficient communication of ambulances with the hospital and routing of the ambulances at the signal posts is very essential. Hence, the proposed architecture is distributed in nature. The system not only provides effective communication between the ambulance and the hospital but also helps the ambulance send the signal to nearby traffic signal posts to open up so that the ambulance can easily pass through saving ample amounts of time. The signal posts use a camera to detect the incoming ambulance and open up that lane so that the ambulance need not spend much time waiting for the traffic to get cleared.",2022,1830
Discovering patterns of online popularity from time series,Mert Ozer and Anna Sapienza and Andrés Abeliuk and Goran Muric and Emilio Ferrara,"How is popularity gained online? Is being successful strictly related to rapidly becoming viral in an online platform, or is it possible to acquire popularity in a steady and disciplined fashion? What are other temporal characteristics that can unveil the popularity of online content? To answer these questions, we leverage a multifaceted temporal analysis of the evolution of popular online content. We present dipm-SC: a multidimensional shape-based time-series clustering algorithm with a heuristic to find the optimal number of clusters. First, we validate the accuracy of our algorithm on synthetic datasets generated from benchmark time series models. Second, we show that dipm-SC can uncover meaningful clusters of popularity behaviors in real-world GitHub and Twitter datasets. By clustering the multidimensional time-series of the popularity of contents coupled with other domain-specific dimensions, we discover two main patterns of popularity: bursty and steady temporal behaviors. Furthermore, we find that the way popularity is gained over time has no significant impact on the final cumulative popularity.",2020,1831
Unusual events in GitHub repositories,Christoph Treude and Larissa Leite and Maurício Aniche,"In large and active software projects, it becomes impractical for a developer to stay aware of all project activity. While it might not be necessary to know about each commit or issue, it is arguably important to know about the ones that are unusual. To investigate this hypothesis, we identified unusual events in 200 GitHub projects using a comprehensive list of ways in which an artifact can be unusual and asked 140 developers responsible for or affected by these events to comment on the usefulness of the corresponding information. Based on 2,096 answers, we identify the subset of unusual events that developers consider particularly useful, including large code modifications and unusual amounts of reviewing activity, along with qualitative evidence on the reasons behind these answers. Our findings provide a means for reducing the amount of information that developers need to parse in order to stay up to date with development activity in their projects.",2018,1832
Data and replication supplement for double auction markets with snipers,Paul Brewer and Anmol Ratan,"We provide a dataset for our research article “Profitability, Efficiency and Inequality in Double Auction Markets with Snipers” [1]. This dataset [2] includes configuration files, raw output data, and replications of calculated metrics for our robot-populated market simulations. The raw data is subdivided into a hierarchy of folders corresponding to simulation treatment variables, in a 2 × 2 × 21 design for 84 treatments in total. Treatments variables include: (i) robot population ordering, either “primary” or “reverse”; (ii) two market schedules of agent's values and costs: equal-expected-profit “market 1” and unequal-expected-profit “market 2”; (iii) 21 robot populations identified by the number of Sniper Bots (0–20) on each side of the market. Each treatment directory contains a simulator input file and outputs for 10,000 periods of market data. The outputs include all acceptable buy and sell orders, all trades, profits for each agent, and market metrics such as efficiency-of-allocation, Gini coefficient, and price statistics. An additional public copy in Google Cloud is available for database query by users of Google BigQuery. The market simulator software is a private product created by Paul Brewer at Economic and Financial Technology Consulting LLC. Free open source modules are available for tech-savvy users at GitHub, NPM, and Docker Hub repositories and are sufficient to repeat the simulations. An easier-to-use paid market simulation product will eventually be available online from Econ1.Net. We provide instructions for repeating individual simulations using the free open source simulator and the free container tool Docker.",2019,1833
Algorithmically generated malicious domain names detection based on n-grams features,Alessandro Cucchiarelli and Christian Morbidoni and Luca Spalazzi and Marco Baldi,"Botnets are one of the major cyber infections used in several criminal activities. In most botnets, a Domain Generation Algorithm (DGA) is used by bots to make DNS queries aimed at establishing the connection with the Command and Control (C&C) server. The identification of such queries by monitoring the network DNS traffic is then crucial for bot detection. In this paper we present a methodology to detect DGA generated domain names based on a supervised machine learning process, trained with a dataset of known benign and malicious domain names. The proposed approach represents the domain names through a set of features which express the similarity between the 2-grams and 3-grams in a single unclassified domain name and those in domain names known as malicious or benign. We used the Kullback-Leibner divergence and the Jaccard Index to estimate the similarity, and we tested different machine learning algorithms to classify each domain name as benign or DGA-based (with both binary and multi-class approach). The results of our experiments demonstrate that the proposed methodology, which only exploits lexical features of domain names, attains a good level of accuracy and results in a general model able to classify previously unseen domains in an effective way. It is also able to outperform some of the state-of-the-art featur eless classification methods based on deep learning.",2021,1834
IoTSecSim: A framework for modelling and simulation of security in Internet of things,Kok Onn Chee and Mengmeng Ge and Guangdong Bai and Dan Dongseong Kim,"The proliferation of the Internet of Things (IoT) devices has provided attackers with tremendous opportunities to launch various cyber-attacks. It has been challenging to analyse the impact of cyber-attacks and evaluate the effectiveness of defences in real IoT environments due to the scale and heterogeneity of IoT networks. In this work, we propose a novel simulation framework and a software tool, IoT Security Simulator (IoTSecSim). IoTSecSim is operated based on a framework we propose for modelling and simulating cyber-attacks and various defences in IoT networks. IoTSecSim is not only able to support the creation of an IoT network with flexible settings of IoT devices and topology information but also models the attack behaviours, node-level, and network-level defences. Moreover, a systematic security evaluation can be performed by comparing the results based on the calculation of security metrics. We perform simulations with case studies on Mirai malware and its variants to model cyber-attack behaviours on IoT networks and evaluate the impact of these attacks and the effectiveness of defence techniques via IoTSecSim. Then, we carry out a sensitivity analysis to justify that the simulation results produced by IoTSecSim are accurate and feasible when compared with related works. We also perform a comparative performance analysis with four combinations of cyber-attack behaviours and show that these behaviours can influence IoT malware propagation in different situations. We consider multiple attacker models and deploy conventional defence techniques (including firewall, intrusion detection, and vulnerability patching) to investigate the effectiveness of defence techniques. IoTSecSim provides a generalised and extensible simulation framework that enables users to model emerging cyber-attacks against IoT networks and evaluate the effectiveness of defences against these attacks. This helps users focus on the design and performance evaluation of new defences before the actual implementation and deployment of the defences are required.",2024,1835
DAmpADF: A framework for DNS amplification attack defense based on Bloom filters and NAmpKeeper,Yunwei Dai and Tao Huang and Shuo Wang,"Domain Name System (DNS) amplification attacks exploit botnets and open recursive DNS servers to launch Distributed Denial of Service (DDoS) attacks. During an attack, the attacker leverages infected computers (bots) to perpetually send small spoofed DNS queries to numerous open recursive DNS servers. The servers, in turn, respond with lots of large DNS responses, which are reflected back to the victim. Such responses are usually several times larger than the original queries, and could exhaust the resources of CPUs, memory, and network bandwidth, rendering them unavailable for benign users. However, most in-network DDoS mitigation systems today inevitably cause normal DNS responses to be discarded while scrubbing traffic, as they do not distinguish between legitimate and malicious responses. To address this issue, some existing solutions employ Bloom filters to filter out unsolicited DNS responses, utilizing the “one-to-one mapping” relationship between DNS queries and responses. In this work, we present a framework called DAmpADF, designed to defend against DNS amplification attacks. The framework employs Bloom filters at the edge or core routers of Internet Service Providers (ISPs) or organizations to filter out malicious DNS responses. To reduce the false positives of the Bloom filters, we propose a novel data structure called the Non-amplifier Keeper (NAmpKeeper), which maintains the most frequently queried DNS servers that are not DNS amplifiers. By excluding queries to non-amplifiers from the Bloom filters, the false positives of Bloom filters are decreased significantly. Experimental results show that DAmpADF outperforms previous methods and achieves a superior filtration ratio of illegitimate DNS responses. Furthermore, the proposed approach incurs small, constant processing and memory overhead, enabling support for high line rates.",2024,1836
A ground-truth dataset and classification model for detecting bots in GitHub issue and PR comments,Mehdi Golzadeh and Alexandre Decan and Damien Legay and Tom Mens,"Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots.",2021,1837
Chatbot for communicating with university students in emergency situation,Antonio Balderas and Roberto Fermín García-Mena and Milagros Huerta and Nestor Mora and Juan Manuel Dodero,"Chatbots have arrived in higher education, and professors are trying to make the most of them. Typically, chatbots are used to help students learn academic subjects. In times of crisis, such as the COVID-19 pandemic, students who were not living with their families during the course, especially international students, were isolated and in critical situations. The student services offices were in constant contact with these students to solve problems, advise them and support them during their stay, within the constraints of confinement and the guidelines dictated by the country at the time. The student services offices were overwhelmed trying to help these students because, although the students' problems were very recurrent, the government guidelines changed from one day to the next. This article proposes the use of a chatbot to provide initial support to students during crisis situations, and facilitate communication between them and the university. The chatbot was tested by more than 160 students and student services staff. The findings support the use of chatbots as a potential tool to facilitate communication with students in emerging emergency situations, and encourage universities to adopt these types of smart tools to be prepared to respond quickly and efficiently to students in times of crisis.",2023,1838
Initialization of profile and social network analyses robot and platform with a concise systematic review,Burak Omer Saracoglu,"This paper presents profile and social network analyses on concise systematic review corpora. It suggests two new robots and platforms for profile and social network analyses, that will serve previously proposed data, expert, and event-driven robots and platforms for energy and power industry. The literature is collected and stored in three topic clusters “location”, “investment”, and “DEMATEL” to prepare corpora. Twenty-five publications are selected in each sample corpus. A sample dataset of each corpus is prepared for thirty-one features such as “author’s full name and surname”, “applied methods”, and “publisher”. Afterward, “authors network matrices” are prepared in spreadsheet software. Data input files (*.csv) are prepared for each dataset. Gephi 0.9.2 201709241107 (free open-source software) is used for social network analyses with built-in layout and statistics algorithms on a desktop Windows 10 Pro, Intel(R) Core(TM) i5 CPU 650 @ 3.20 GHz, 6,00 GB RAM personal computer in an offline and active cybersecurity software environment. Force Atlas, Force Atlas 2, Fruchterman–Reingold, OpenOrd, Yifan Hu, and Yifan Hu Proportional layout algorithms with Noverlap layout algorithm are run one by one. Runtimes range 2–120 s. All default statistic algorithms are run for several metrics like average degree, average weighted degree, betweenness centrality, closeness centrality, harmonic closeness centrality, eccentricity, and density. Authors in “location” cluster have a centralized network, but authors in “investment” and “DEMATEL” clusters have distributed networks. General profile analyses are conducted based on authors’ publications in the literature without any data and information on social media sites and platforms. Two new profile analysis metrics are proposed as “researcher’s past research focus index”, and “researcher’s future research focus prediction index”. Detailed profile analysis is performed for only Burak Omer Saracoglu. All analyses and findings are compared and summarized in the end.",2022,1839
Public attitudes and sentiments toward ChatGPT in China: A text mining analysis based on social media,Ying Lian and Huiting Tang and Mengting Xiang and Xuefan Dong,"ChatGPT, an innovative artificial intelligence language model, is attracted significant attention around the world, sparking both enthusiasm and controversy, but identifying its societal impact and addressing its potential concerns necessitate an understanding of the prevailing public's attitudes toward the tool. In this study, we leverage text mining techniques to analyze the sentiments and themes prevalent among Chinese social media discussions of ChatGPT. In total, 96,435 comment data and 55,186 repost data were used, and the results show that public discussions mainly focused on ChatGPT's technical support, AI-related effectiveness, impact on human work, and effects on education and technology. Concerns were related to disinformation risks, technological unemployment, and the human–computer relationship. In addition, we found that social media played a prominent role in information dissemination, while official media and government units demonstrated a limited influence. The insights obtained through this study can inform policymakers, industry stakeholders, and the public of the public's prevailing attitude toward AI technologies, and they can facilitate informed decision-making.",2024,1840
A review of amplification-based distributed denial of service attacks and their mitigation,Salih Ismail and Hani Ragab Hassen and Mike Just and Hind Zantout,"The rise of Distributed Denial of Service (DDoS) attacks have been steady in terms of the frequency and the impact of the attack. Traditionally, the attackers required control of a huge amount of resources to launch an attack. This has changed with the use of reflectors and amplifiers in DDoS attacks. A recent shift consisted of using other protocols than the traditional NTP and DNS protocols which were heavily used for ADDoS. In this paper, we review and organize amplification-based DDoS (ADDoS) attacks and associated countermeasures into a new taxonomy. Furthermore, we present a modus operandi of ADDoS attacks and analyze how it differs from traditional DDoS attacks. We also investigate how accessible ADDoS are for attackers with average resources. We survey readily available open-source scripts on GitHub and also the ADDoS features available in hire-to-DDoS platforms. We believe that accessibility and low-cost of hire-to-DDoS platforms are the major reasons for the increase of amplification-based DDoS attacks. Lastly, we provide a list of future directions that might be interesting for the community to focus on.",2021,1841
A modular ontology modeling approach to developing digital product passports to promote circular economy in the built environment,Rahel Kebede and Annika Moscati and He Tan and Peter Johansson,"The significant impact of the built environment on resource consumption and waste production has led to calls for a shift towards a circular economy model that maximizes the efficient use of resources. This study explores the use of digital product passports (DPPs) to improve how we manage products throughout their lifecycle. However, dealing with the complexity and large volume of data in DPPs can be challenging in terms of effective information management and utilization. We address this issue by adopting a modular ontological approach to systematically capture product lifecycle information from its origin to its end-of-life phase. To ensure interoperability and reusability of the ontology, we annotate key concepts and relationships using International Organization for Standardization (ISO) standards that promote circular economy. Our research led to the development of several ontology modules derived from literature reviews and interviews conducted with industry and academia experts who specialize in sustainability. These modules were then integrated to create a digital product passport ontology. The study demonstrates the feasibility of using a modular ontology approach to manage the complex information inherent in DPPs paving the way for more sustainable management practices in the built environment sector.",2024,1842
Can ChatGPT provide intelligent diagnoses? A comparative study between predictive models and ChatGPT to define a new medical diagnostic bot,Loredana Caruccio and Stefano Cirillo and Giuseppe Polese and Giandomenico Solimando and Shanmugam Sundaramurthy and Genoveffa Tortora,"Intelligent diagnosis processes rely on Artificial Intelligence (AI) techniques to provide possible diagnoses by analyzing patient data and medical information. To make accurate and quick diagnoses, it is possible to use AI tools to efficiently analyze huge amounts of data and find patterns that a clinician might miss. In recent years, new large language models (LLMs), such as ChatGPT and Google BARD, have shown remarkable capabilities in several domains, including intelligent diagnostics. This research aims to compare the performances of ChatGPT and traditional machine learning models for making diagnoses of low- and medium- risk diseases only based on their symptoms. On the basis of our study, we defined four research questions: RQ1) What are the benefits and limitations of using ChatGPT in intelligent diagnosis? RQ2) How do traditional machine learning approaches compare to ChatGPT for intelligent diagnosis? RQ3) How does ChatGPT compare with other LLMs and domain-specific natural language processing models in the intelligent diagnosis tasks?, and RQ4) What are the implications of the predictive models and ChatGPT for healthcare, and how can they be used to support people?. To answer these RQs, we first evaluate the performances of different engines of ChatGPT, also introducing a new prompt engineering methodology specifically tailored for achieving accurate diagnostic outcomes. Moreover, we compare these results with those achieved by different predictive models trained for intelligent diagnosis tasks, i.e., Google BARD, and two domain-specific NLP models. Finally, we propose a new interactive bot available for users that relies on the best-performing models evaluated in the previous steps. The experiments have been conducted using two medical datasets for disease prediction consisting of more than 100 symptoms associated with several diagnoses.",2024,1843
The uphill journey of FaaS in the open-source community,Nafise Eskandani and Guido Salvaneschi,"Since its introduction in 2014 by Amazon, the Function as a Service (FaaS) model of serverless computing has set the expectation to fulfill the promise of on-demand, pay-as-you-go, infrastructure-independent processing, originally formulated by cloud computing. Yet, serverless applications are fundamentally different than traditional service-oriented software in that they pose specific performance (e.g., cold start), design (e.g., stateless), and development challenges (e.g., debugging). A growing number of cloud solutions have been continuously attempting to address each of these challenges as a result of the increasing popularity of FaaS. Yet, the characteristics of this model have been poorly understood; therefore, the challenges are poorly tackled. In this paper, we assess the state of FaaS in open-source community with a study on almost 2K real-world serverless applications. Our results show a jeopardized ecosystem, where, despite the hype of serverless solutions in the last years, a number of challenges remain untackled, especially concerning component reuse, support for software development, and flexibility among different platforms — resulting in arguably slow adoption of the FaaS model. We believe that addressing the issues discussed in this paper may help researchers shaping the next generation of cloud computing models.",2023,1844
A secure annuli CAPTCHA system,Jie Zhang and Min-Yen Tsai and Kotcharat Kitchat and Min-Te Sun and Kazuya Sakai and Wei-Shinn Ku and Thattapon Surasak and Tipajin Thaipisutikul,"Many websites and applications rely on CAPTCHA for protection from bot attacks. Otherwise, users and businesses will be exposed to risks. Although several different CAPTCHA systems have been proposed, the development of deep learning algorithms allows attackers to create more efficient and accurate attack methods. Many studies have shown that existing CAPTCHA systems are no longer safe, especially text-based CAPTCHA. To resolve this issue, a simple, secure, and effective annuli CAPTCHA system is proposed in this paper. In the proposed system, the annuli CAPTCHA image containing the overlapping of circles and ovals is randomly generated. The user wishing to gain access to the system is required to answer correctly the total number of circles and ovals in the image to prove that he/she is not a bot. The security of our proposed CAPTCHA system is verified by three attack methods. Additionally, the usability survey of our CAPTCHA system conducted by anonymous questionnaires shows that our system is user friendly. In other words, the proposed system maintains a high level of usability under the premise of high security. Compared with the existing CAPTCHA system, our CAPTCHA system is significantly better in terms of security, usability and ease of implementation.",2023,1845
"Multi-labeling of complex, multi-behavioral malware samples",P. García-Teodoro and J.A. Gómez-Hernández and A. Abellán-Galera,"The use of malware samples is usually required to test cyber security solutions. For that, the correct typology of the samples is of interest to properly estimate the exhibited performance of the tools under evaluation. Although several malware datasets are publicly available at present, most of them are not labeled or, if so, only one class or tag is assigned to each malware sample. We defend that just one label is not enough to represent the usual complex behavior exhibited by most of current malware. With this hypothesis in mind, and based on the varied classification generally provided by automatic detection engines per sample, we introduce here a simple multi-labeling approach to automatically tag the usual multiple behavior of malware samples. In the paper, we first analyze the coherence between the behaviors exhibited by a specific number of well-known malware samples dissected in the literature and the multiple tags provided for them by our labeling proposal. After that, the automatic multi-labeling scheme is executed over four public Android malware datasets, the different results and statistics obtained regarding their composition and representativeness being discussed. We share in a GitHub repository the multi-labeling tool developed, for public usage.",2022,1846
Multimodal prediction of profanity based on speech analysis,Ivan Smirnov and Anastasia Laushkina,"With increasing multimedia content and social activities, moderation problems increase. There are different approaches to moderation and automation. However, they have limitations in terms of usage in real-time. The analysis of scientific papers revealed that most of the more common approaches solve the task of detection instead of prediction by considering the final utterance. For this reason, calls are unprotected in toxic languages, and online broadcasts can be unpredictable. In this work, a new way for automatic speech moderation in terms of dynamic word prediction was suggested. The considered task involves the analysis of the auditory and textual channels of speech. Words can have different meanings depending on the context, so in solving the problem it is planned to consider profanity, which is socially unacceptable regardless of the context. In this paper approaches for working with speech stream in the task of profanity prediction were proposed. It can be possible to have smaller latency with usage of audio features. We also suggest the pipeline for real-time (with the ability to predict the sequence with a higher duration than the latency of the processing) prediction for multimodal prediction, which compensates the latency of ASR systems. As a result, in this paper, we compared different solutions for the next color prediction task for English speech and reached the F1 score of 86.6 for 3 class prediction.",2023,1847
A Deep Learning Ensemble Approach to Detecting Unknown Network Attacks,Rasheed Ahmad and Izzat Alsmadi and Wasim Alhamdani and Lo'ai Tawalbeh,"The majority of the intrusion detection solutions proposed using machine learning and deep learning approaches are based on known attack classes only. Comprehensive threat detection systems should consider both known and unknown attacks. Rapidly changing network environment and the advanced tools and techniques used by adversaries to launch new sophisticated attacks highlight a growing need to build intrusion detection systems that are more realistic, diverse, and robust to detect known and unknown attacks. We employed deep-learning models in our experiments to detect unknown threats, never introduced before to the model. This paper also studied the bias issues in connection with unknown threats detection. Many recent research studies based on conventional machine learning may report biased results and restricted training due to relying only on a single dataset; thus, there are existing threats that the model is unaware of, although the model may have high accuracy (in the known territories). This study presents a realistic IDS approach in which a deep learning classifiers' ensemble is trained on four benchmark IDS datasets for testing the unknown attack instances. Specifically, the model has no prior knowledge of some labels and traffic patterns in those experiments. The architecture proposed builds a deep learning ensemble using classifiers well-known to process and produce good results for sequential data. Our empirical results indicate that the proposed ensemble model can detect a range of unknown attacks with reasonable performance measures and a practical approach towards building a comprehensive IDS solution.",2022,1848
The influence of social media affordances on drug dealer posting behavior across multiple social networking sites (SNS),Michael Robert Haupt and Raphael Cuomo and Jiawei Li and Matthew Nali and Tim K. Mackey,"Social media has been documented as widely used for initiating online sales of illicit drugs such as opioids. However, not much is known about how affordances of social networking sites (SNS) influence how dealers advertise their supplies. To explore this topic, social media posts across 5 online platforms (Google Groups, Instagram, Twitter, Reddit, and Tumblr) were collected during 2020–2021. Biterm topic modeling (BTM) was used to identify signal posts specifically associated with the illegal online sale of opioids from drug selling social media accounts. Posts were analyzed by conducting a word count for drug names or slang terms associated with 5 categories: Opioids, Non-Opioid Prescription Controlled Drugs (e.g., Xanax, Valium), Other Illicit Drugs (e.g., Meth, Cocaine), Synthetic Opioids (Fentanyl), and Synthetic Marijuana. Number of mentions per post were calculated for each drug category and compared across platforms. Identifiers (e.g., publicly available email address) associated with posts were used to track dealers across different user accounts. Platforms with affordances for longer messages (e.g., Tumblr) had higher concentrations of drug mentions per post and higher variety of drug type mentions compared to SNS platforms Instagram and Twitter. Google Groups had the most drug mentions per post across all 5 categories. Additionally, each identifier was associated with multiple user accounts on a given platform. These results indicate that affordances of anonymity and message length may influence how drug dealers advertise their services on different platforms. Public health implications and strategies to counteract drug dealers and illicit drug diversion via SNS are also discussed.",2022,1849
TZMon: Improving mobile game security with ARM trustzone,Sanghoon Jeon and Huy Kang Kim,"As the game industry is moving from PC to smartphone platforms, security problems related to mobile games are becoming critical. Considering the characteristics of mobile games such as having short life-cycles and high communication costs, the server/network-side security technologies designed for PC games are not appropriate for mobile games. In this study, we propose TZMon, a client-side game protection mechanism based on the ARM TrustZone, which protects the confidentiality and integrity of mobile games. TZMon is composed of application integrity protocol, secure update protocol, data hiding protocol, and timer synchronization protocol. To adequately safeguard game codes and data, TZMon is designed considering an environment of frequent communications with the game server, a stand-alone operation environment, and an unreliable environment using a rooted OS. Furthermore, flexibility is provided to game application developers who apply security policies by using the Java Native Interface (JNI). In this study, we use Android and the Open Portable Trusted Execution Environment (OPTEE) as the OS platforms for Normal World and Secure World, respectively. After implementing a full-featured prototype of TZMon, we apply it to several open-source mobile games. We prove through the experiments that the application of the proposed TZMon does not cause any noticeable performance degradation and can detect major cheating techniques of mobile games.",2021,1850
WaveMAP for identifying putative cell types from in vivo electrophysiology,Kenji Lee and Nicole Carr and Alec Perliss and Chandramouli Chandrasekaran,"Summary
Action potential spike widths are used to classify cell types as either excitatory or inhibitory; however, this approach obscures other differences in waveform shape useful for identifying more fine-grained cell types. Here, we present a protocol for using WaveMAP to generate nuanced average waveform clusters more closely linked to underlying cell types. We describe steps for installing WaveMAP, preprocessing data, and clustering waveform into putative cell types. We also detail cluster evaluation for functional differences and interpretation of WaveMAP output. For complete details on the use and execution of this protocol, please refer to Lee et al. (2021).1",2023,1851
Vulnerability retrospection of security solutions for software-defined Cyber–Physical System against DDoS and IoT-DDoS attacks,Manish Snehi and Abhinav Bhandari,"The wide dispersion of the Internet of Things (IoT), Software-defined Networks and Cloud Computing have given the wings to Cyber–Physical System adoption. The newfangled society relies so much on Cyber–Physical Systems, such as Smart Cities, Smart Agriculture, Medical Cyber System, that a dearth to any of the available services may lead to severe concerns. The IoT devices are unwittingly contributing to the denial of service attacks. Though the neoteric Software-defined Anything (SDx) paradigm has offered effective solution approaches to catastrophic IoT-based DDoS attacks, the novel designed solutions confront various vulnerabilities due to less secure IoT devices, high-volume real-time network traffic generated by the colossal amount of IoT devices, etc. In this paper, we present a comprehensive survey on vulnerability analysis of security solutions for Software-defined Cyber–Physical System. The paper delineates the architectural details of the Software-defined Cyber–Physical System and recommends amalgamation of Fog Computing as one of the architectural layers for overcoming a number of vulnerabilities. As contemporary technologies like IoT, Software-defined Networking and Cloud Computing are the soup ingredients of the Software-defined Cyber–Physical System, each of the individual components has been auscultated individually for security vulnerabilities with a focus on Distributed Denial of Service (DDoS and IoT-based DDoS) attacks. To anticipate the future recasting of the novel paradigm, we discuss the ongoing research and detailed vulnerability analysis with a focus on resiliency, performance, and scalability. Last but not least, we discuss the lessons learned and prospects to conclude.",2021,1852
A generic framework for federated CDEs applied to Issue Management,Jeroen Werbrouck and Oliver Schulz and Jyrki Oraskari and Erik Mannens and Pieter Pauwels and Jakob Beetz,"This paper analyses the requirements for managing interoperable building data in a federated Common Data Environment (CDE). We discuss the need for generic (meta)data storage patterns, semantic query interfaces, decentral authentication, data aggregation, and adaptation and prove that their combination is feasible with current-day technologies. We illustrate the mechanisms of such federated CDE by considering the topic of digital Issue Management, one of the primary functions of a CDE. In an exemplary data flow process, we show how generic (federated, Semantic Web-based) data patterns for Issue Management can be aggregated and restructured to match existing industry standards like buildingSMART’s BIM Collaboration Format (BCF) API. Finally, we show the methodology is compatible with current-day practice by implementing this process in a proof of concept. The main contribution of this research is a generic, federated framework for project-related, interdisciplinary collaboration for CDEs.",2023,1853
Applying machine learning to assess emotional reactions to video game content streamed on Spanish Twitch channels,Noemí Merayo and Rosalía Cotelo and Rocío Carratalá-Sáez and Francisco J. Andújar,"This research explores for the first time the application of machine learning to detect emotional responses in video game streaming channels, specifically on Twitch, the most widely used platform for broadcasting content. Analyzing sentiment in gaming contexts is difficult due to the brevity of messages, the lack of context, and the use of informal language, which is exacerbated in the gaming environment by slang, abbreviations, memes, and jargon. First, a novel Spanish corpus was created from chat messages on Spanish video game Twitch channels, manually labeled for polarity and emotions. It is noteworthy as the first Spanish corpus for analyzing social responses on Twitch. Secondly, machine learning algorithms were used to classify polarity and emotions offering promising evaluations. The methodology followed in this work consists of three main steps: (1) Extracting Twitch chat messages from Spanish streamers’ channels related to gaming events and gameplays; (2) Processing and selecting the messages to form the corpus and manually annotating polarity and emotions; and (3) Applying machine learning models to detect polarity and emotions in the created corpus. The results have shown that a Bidirectional Encoder Representation from Transformers (BERT) based model excels with 78% accuracy in polarity detection, while deep learning and Random Forest models reach around 70%. For emotion detection, the BERT model performs best with 68%, followed by deep learning with 55%. It is worth noting that emotion detection is more challenging due to the subjective interpretation of emotions in the complex communicative context of video gaming on platforms such as Twitch. The use of supervised learning techniques, together with the rigorous corpus labeling process and the subsequent corpus pre-processing methodology, has helped to mitigate these challenges, and the algorithms have performed well. The main limitations of the research involve category and video game representation balance. Finally, it is important to stress that the integration of machine learning in video games and on Twitch is innovative, by allowing the identification of viewers’ emotions on streamers’ channels. This innovation could bring benefits such as a better understanding of audience sentiment, improving content and audience retention, providing personalized recommendations and detecting toxic behavior in chats.",2024,1854
In brief,,,2018,1855
A collaborative approach to early detection of IoT Botnet,Giang L. Nguyen and Braulio Dumba and Quoc-Dung Ngo and Hai-Viet Le and Tu N. Nguyen,"With the rapid growth of threats and diversity in the manner of attack, Internet of things (IoT) systems has major challenges in providing methods to detect security vulnerabilities and attacks. There have been increasing developments of many detection tools and methods using full-time series data during malware execution based on machine learning/deep learning. However, the effectiveness of existing works is tightly bound by the requirement to use full-time series data. On the other hand, an earlier detection would help propose better solutions to respond to the IoT Botnet. Therefore, it mitigating the damage from potential attacks. In this paper, going beyond the full-time series data-based methods, we propose a collaborative machine learning model to effectively automate the early detection of IoT Botnet based on many features. The proposed model is 99.37% accurate on a dataset of 5023 IoT botnet and 3888 benign samples.",2022,1856
How Digital Tools Align with Organizational Agility and Strengthen Digital Innovation in Automotive Startups,Dulce Gonçalves and Magnus Bergquist and Sverker Alänge and Richard Bunk,"Digital tools can be an enabler for automotive startups to strengthen their digital innovation capability. Still, few empirical studies describe how automotive startups apply digital tools to do this. Digital innovation capability is essential for survival in a volatile global digital marketplace. Therefore, we conducted a qualitative study based on 23 interviews with nine global automotive startups to understand how they apply digital tools to strengthen their digital innovation. The results showed that automotive startups use cloud services almost exclusively for their business. We conclude that startups choose to use digital tools as SaaS to strengthen their organizational agility and digital innovation initiatives. It harmonizes with their agile culture, effectively enabling innovation collaborations between employees internally and with external actors enabling rapidness to market. SaaS providers’ startup programs enabled startups to remain focused on their innovation initiatives and not worry about scalability since the solutions scaled from the start.",2022,1857
Crediting pull requests to open source research software as an academic contribution,Hartwig Anzt and Eileen Kuehn and Goran Flegar,"Like any other scientific discipline, the High Performance Computing community suffers under the publish or perish paradigm. As a result, a significant portion of novel algorithm designs and hardware-optimized implementations never make it into production code but are instead abandoned once they served the purpose of yielding (another) publication. At the same time, community software packages driving scientific research lack the addition of new technology and hardware-specific implementations. This results in a very unsatisfying situation where researchers and software developers are working independently, and the traditional peer reviewing is reaching its capacity limits. A paradigm shift that accepts high-quality software pull requests to open source research software as conference contributions may create incentives to realize new and/or improved algorithms in community software ecosystems. In this paper, we propose to complement code reviews on pull requests to scientific open source software with scientific reviews, and allow the presentation and publication of high quality software contributions that present an academic improvement to the state-of-the-art at scientific conferences.",2021,1858
A unifying framework for the systematic analysis of Git workflows,Julio César {Cortés Ríos} and Suzanne M. Embury and Sukru Eraslan,"Context:
Git is a popular distributed version control system that provides flexibility and robustness for software development projects. Several workflows have been proposed to codify the way project contributors work collaboratively with Git. Some workflows are highly prescriptive while others allow more leeway but do not provide the same level of code quality assurance, thus, preventing their comparison to determine the most suitable for a specific set of requirements, or to ascertain if a workflow is being properly followed.
Objective:
In this paper, we propose a novel feature-based framework for describing Git workflows, based on a study of 26 existing instances. The framework enables workflows’ comparison, to discern how, and to what extent, they exploit Git capabilities for collaborative software development.
Methods:
The framework uses feature-based modelling to map Git capabilities, regularly expressed as contribution guidelines, and a set of features that can be impartially applied to all the workflows considered. Through this framework, each workflow was characterised based on their publicly available descriptions. The characterisations were then vectorised and processed using hierarchical clustering to determine workflows’ similarities and to identify which features are most popular, and more relevant for discriminatory purposes.
Results:
Comparative analysis evidenced that some workflows claiming to be closely related, when described and then characterised, turned out to have more differences than similarities. The analysis also showed that most workflows focus on the branching and code integration strategies, whilst others emphasise subtle differences from other popular workflows or describe a specific development route and are, thus, widely reused.
Conclusion:
The characterisation and clustering analysis demonstrated that our framework can be used to compare and analyse Git workflows.",2022,1859
Epigenetic landscape in the kick-and-kill therapeutic vaccine BCN02 clinical trial is associated with antiretroviral treatment interruption (ATI) outcome,Bruna Oriol-Tordera and Anna Esteve-Codina and María Berdasco and Míriam Rosás-Umbert and Elena Gonçalves and Clara Duran-Castells and Francesc Català-Moll and Anuska Llano and Samandhy Cedeño and Maria C. Puertas and Martin Tolstrup and Ole S. Søgaard and Bonaventura Clotet and Javier Martínez-Picado and Tomáš Hanke and Behazine Combadiere and Roger Paredes and Dennis Hartigan-O'Connor and Manel Esteller and Michael Meulbroek and María Luz Calle and Alex Sanchez-Pla and José Moltó and Beatriz Mothe and Christian Brander and Marta Ruiz-Riol,"Summary
Background
The BCN02-trial combined therapeutic vaccination with a viral latency reversing agent (romidepsin, RMD) in HIV-1-infected individuals and included a monitored antiretroviral pause (MAP) as an efficacy read-out identifying individuals with an early or late (< or > 4weeks) viral-rebound. Integrated -omics analyses were applied prior treatment interruption to identify markers of virus control during MAP.
Methods
PBMC, whole-genome DNA methylation and transcriptomics were assessed in 14 BCN02 participants, including 8 Early and 4 Late viral-rebound individuals. Chromatin state, histone marks and integration analysis (histone-3 acetylation (H3Ac), viral load, proviral levels and HIV-specific T cells responses) were included. REDUC-trial samples (n = 5) were included as a control group for RMD administration alone.
Findings
DNA methylation imprints after receiving the complete intervention discriminated Early versus Late viral-rebound individuals before MAP. Also, differential chromatin accessibility and histone marks at DNA methylation level were detected. Importantly, the differential DNA methylation positions (DMPs) between Early and Late rebounders before MAP were strongly associated with viral load, proviral levels as well as the HIV-specific T-cell responses. Most of these DMPs were already present prior to the intervention and accentuated after RMD infusion.
Interpretation
This study identifies host DNA methylation profiles and epigenetic cascades that are predictive of subsequent virus control in a kick-and-kill HIV cure strategy.
Funding
European Union Horizon 2020 Framework Programme for Research and Innovation under Grant Agreement N°681137-EAVI2020 and N°847943-MISTRAL, the Ministerio de Ciencia e Innovación (SAF2017_89726_R), and the National Institutes of Health–National Institute of Allergy and Infectious Diseases Program Grant P01-AI131568.",2022,1860
A hands-on gaze on HTTP/3 security through the lens of HTTP/2 and a public dataset,Efstratios Chatzoglou and Vasileios Kouliaridis and Georgios Kambourakis and Georgios Karopoulos and Stefanos Gritzalis,"Following QUIC protocol ratification on May 2021, the third major version of the Hypertext Transfer Protocol, namely HTTP/3, was published around one year later in RFC 9114. In light of these consequential advancements, the current work aspires to provide a full-blown coverage of the following issues, which to our knowledge have received feeble or no attention in the literature so far. First, we provide a complete review of attacks against HTTP/2, and elaborate on if and in which way they can be migrated to HTTP/3. Second, through the creation of a testbed comprising the at present six most popular HTTP/3-enabled servers, we examine the effectiveness of a quartet of attacks, either stemming directly from the HTTP/2 relevant literature or being entirely new. This scrutiny led to the assignment of at least one CVE ID with a critical base score by MITRE. No less important, by capitalizing on a realistic, abundant in devices testbed, we compiled a voluminous, labeled corpus containing traces of ten diverse attacks against HTTP and QUIC services. An initial evaluation of the dataset mainly by means of machine learning techniques is included as well. Given that the 30 GB dataset is made available in both pcap and CSV formats, forthcoming research can easily take advantage of any subset of features, contingent upon the specific network topology and configuration.",2023,1861
An Empirical Survey of Functions and Configurations of Open-Source Capture the Flag (CTF) Environments,Stela Kucek and Maria Leitner,"Capture the Flag (CTF) is a computer security competition that is generally used to give participants experience in securing (virtual) machines and responding to cyber attacks. CTF contests have been getting larger and are receiving many participants every year (e.g., DEFCON, NYU-CSAW). CTF competitions are typically hosted in virtual environments, specifically set up to fulfill the goals and scenarios of the CTF. This article investigates the underlying infrastructures and CTF environments, specifically open-source CTF environments. A systematic review is conducted to assess functionality and game configuration in CTF environments where the source code is available on the web (i.e., open-source software). In particular, from out of 28 CTF platforms, we found 12 open-source CTF environments. As four platforms were not installable for several reasons, we finally examined 8 open-source CTF environments (PicoCTF, FacebookCTF, HackTheArch, WrathCTF, Pedagogic-CTF, RootTheBox, CTFd and Mellivora) regarding their features and functions for hosting CTFs (e.g., scoring, statistics or supported challenge types) and providing game configurations (e.g., multiple flags, points, hint penalities). Surprisingly, while many platforms provide similar base functionality, game configurations between the platforms varied strongly. For example, hint penalty, time frames for solving challenges, limited number of attempts or dependencies between challenges are game options that might be relevant for potential CTF organizers and for choosing a technology. This article contributes to the general understanding of CTF software configurations and technology design and implementation. Potential CTF organizers and participants may use this as a reference for challenge configurations and technology utilization. Based on our analysis, we would like to further review commercial and other platforms in order to establish a golden standard for CTF environments and further contribute to a better understanding of CTF design and development.",2020,1862
Platform for transverse evaluation of control strategies for multi-energy smart grids,Timothé Gronier and Erwin Franquet and Stéphane Gibout,"This paper presents the PEACEFULNESS software platform (Platform for transvErse evAluation of Control stratEgies For mULti-eNErgy Smart gridS), an open framework dedicated to multi-energy smart-grids, based on a techno-economic model that integrates economic considerations (contracts). As such, it is mainly oriented towards the evaluation of multi-energy grid supervision strategies, that is, energy management, and the corresponding policies and legal organization. The main goal is then to highlight the various possible behaviors and strategies to organize the probable future interconnections between the different energy carriers. In particular, it aims at investigating how to maximize the use of renewable energy sources (RES), using Demand Side Management (DSM) techniques and energy storage, in a shared economy context. The open-source tool PEACEFULNESS, written in Python, is described here in detail. It combines a top-down description of the energy networks and connections between the various agents (energy providers, distribution system operators, aggregators, consumers, producers, prosumers, etc.), together with a techno-economic bottom-up description for all devices. Here, both public databases and users’ data (basic heating demands or based on building modeling) can be used, as well as generic or more specific models (e.g., PV panels with constant or temperature-dependent efficiency). One of its major unique features compared with other tools is that it extends the use of DSM techniques to various energy grids which can also interact together. Furthermore, different economic models can be set for both the aggregators and the customers, and even within these groups. As a last competitive advantage, PEACEFULNESS allows the user to simulate the operation and supervision of tens up to hundreds of thousands of agents. It also provides a reporting system giving access to all the data, with a configurable granularity and frequency for the retained indicators. Finally, several validation cases are presented, followed by a series of test cases with increasing size: a smart home, a smart district (2 000 dwellings) and a smart community (50 000 dwellings).",2022,1863
PBCNN: Packet Bytes-based Convolutional Neural Network for Network Intrusion Detection,Lian Yu and Jingtao Dong and Lihao Chen and Mengyuan Li and Bingfeng Xu and Zhao Li and Lin Qiao and Lijun Liu and Bei Zhao and Chen Zhang,"Network intrusion detection system (IDS) protects the target network from the threats of data breaches and the insecurity of people’s privacy. However, most of existing researches on network intrusion detection cannot fulfil effectively the protection of targets, especially, depending heavily on the statistical features that are manually designed with domain experts’ knowledge and experiences, and failing to address the few sample data problem. Network traffic has a hierarchical structure, i.e., byte-packet-flow, which is similar to phrase-sentence-article in an article. This paper proposes a hierarchical packet byte-based CNN, called PBCNN, where the first level extracts abstract features automatically from bytes in a packet in raw Pcap files, and then the second level further constructs the representation from packets in a flow or session, instead of using feature-ready CSV files, to make full use of original data information. Multiple convolution-pooling modules are cascaded with byte-friendly sizes of multiple filters, and one-layer TextCNN to obtain the representation of traffic flow, feeding the representation to 3 layers of fully connected networks for intrusion classification. PBCNN-based few shot learning is applied to improve the detection reliability of network attack categories with the few sample problem. Several experiments are performed and the results show that the evaluation metrics are superior to the existing researches in regard to CIC-IDS2017 and CSE-CIC-IDS2018 datasets.",2021,1864
Public sentiment analysis and topic modeling regarding COVID-19 vaccines on the Reddit social media platform: A call to action for strengthening vaccine confidence,Chad A. Melton and Olufunto A. Olusanya and Nariman Ammar and Arash Shaban-Nejad,"Background
The COVID-19 pandemic fueled one of the most rapid vaccine developments in history. However, misinformation spread through online social media often leads to negative vaccine sentiment and hesitancy.
Methods
To investigate COVID-19 vaccine-related discussion in social media, we conducted a sentiment analysis and Latent Dirichlet Allocation topic modeling on textual data collected from 13 Reddit communities focusing on the COVID-19 vaccine from Dec 1, 2020, to May 15, 2021. Data were aggregated and analyzed by month to detect changes in any sentiment and latent topics.
Results
Polarity analysis suggested these communities expressed more positive sentiment than negative regarding the vaccine-related discussions and has remained static over time. Topic modeling revealed community members mainly focused on side effects rather than outlandish conspiracy theories.
Conclusion
Covid-19 vaccine-related content from 13 subreddits show that the sentiments expressed in these communities are overall more positive than negative and have not meaningfully changed since December 2020. Keywords indicating vaccine hesitancy were detected throughout the LDA topic modeling. Public sentiment and topic modeling analysis regarding vaccines could facilitate the implementation of appropriate messaging, digital interventions, and new policies to promote vaccine confidence.",2021,1865
Wizard: Unsupervised goats tracking algorithm,Jehan-Antoine Vayssade and Xavier Godard and Mathieu Bonneau,"Computer vision is an interesting tool for animal behavior monitoring, mainly because it limits animal handling and it can be used to record various traits using only one sensor. From previous studies, this technic has shown to be suitable for various species and behavior. However it remains challenging to collect individual information, i.e. not only to detect animals and behavior on the video frames, but also to identify them. Animal identification is a prerequisite to gather individual information in order to characterize individuals and compare them. A common solution to this problem, known as multiple objects tracking, consists in detecting the animals on each video frame, and then associate detections to a unique animal ID. Association of detections between two consecutive frames are generally made to maintain coherence of the detection locations and appearances. To extract appearance information, a common solution is to use a convolutional neural network (CNN), trained on a large dataset before running the tracking algorithm. For farmed animals, designing such network is challenging as far as large training dataset are still lacking. In this article, we proposed an innovative solution, where the CNN used to extract appearance information is parameterized using offline unsupervised training. The algorithm, named Wizard, was evaluated for the purpose of goats monitoring in outdoor conditions. 17 annotated videos were used, for a total of 4H30, with various number of animals on the video (from 3 to 8) and different level of color differences between animals. First, the ability of the algorithm to track the detected animals was evaluated. When animals were detected, the algorithm found the correct animal ID in 94.82% of the frames. When tracking and detection were evaluated together, we found that Wizard found the correct animal ID in 86.18% of the video length. In situations where the animal detection rate could be high, Wizard seems to be a suitable solution for individual behavior analysis experiments based on computer vision.",2023,1866
Automated cryptocurrency trading approach using ensemble deep reinforcement learning: Learn to understand candlesticks,Liu Jing and Yuncheol Kang,"Despite their high risk, cryptocurrencies have gained popularity as viable trading options. Cryptocurrencies are digital assets that experience significant fluctuations in a market operating 24 h a day. Recently, considerable attention has been paid to developing trading bots using machine-learning-based artificial intelligence. Previous studies have employed machine learning techniques to predict financial market trends or make trading decisions, primarily using numerical data extracted from candlesticks. However, these data often overlook the temporal and spatial information of candlesticks, leading to a limited understanding of their significance. In this study, we utilize multi-resolution candlestick images containing temporal and spatial information. Our rationale for using visual information from candlestick charts is to replicate the decision-making processes of human trading experts. To achieve this, we employ deep reinforcement learning algorithms to generate trading signals based on a state vector that includes embedded candlestick-chart images. The trading signal is generated using a multi-agent weighted voting ensemble approach. We test the proposed approach on two BTC/USDT datasets under both bullish and bearish market scenarios. Additionally, we use an attention-based technique to identify significant areas in the candlestick images targeted by the proposed approach. Our findings demonstrate that models using candlestick images 'as-is', outperform those using raw numeric data and other baseline models.",2024,1867
Vibration Free Flexible Object Handling with a Robot Manipulator Using Learning Control,Daniele Ronzani and Shamil Mamedov and Jan Swevers,"Many industries extensively use flexible materials. effective approaches for handling flexible objects with a robot manipulator must address residual vibrations. Existing solutions rely on complex models, use additional instrumentation for sensing the vibrations, or do not exploit the repetitive nature of most industrial tasks. This paper develops an iterative learning control approach that jointly learns model parameters and residual dynamics using only the interoceptive sensors of the robot. The learned model is subsequently utilized to design optimal point-to-point (PTP) trajectories that accounts for residual vibration, nonlinear kinematics of the manipulator and joint limits. We experimentally show that the proposed approach reduces the residual vibrations by an order of magnitude compared with optimal vibration suppression using the analytical model and threefold compared with the available state-of-the-art method. These results demonstrate that effective handling of a flexible object does not require neither complex models nor additional instrumentation.",2023,1868
Migrants vs. stayers in the pandemic – A sentiment analysis of Twitter content,Olga Czeranowska and Karol Chlasta and Piotr Miłkowski and Izabela Grabowska and Jan Kocoń and Krzysztof Hwaszcz and Jan Wieczorek and Agata Jastrzębowska,"In this paper, we propose a sentiment analysis of Twitter data focused on the attitudes and sentiments of Polish migrants and stayers during the pandemic. We collected 9 million tweets and retweets between January and August 2021, and analysed them using MultiEmo, the multilingual, multilevel, multi-domain sentiment analysis corpus. We discovered that the sentiment of tweets differs between migrants and stayers over time, and it relates to the country of migration. The general sentiment is similar for migrants and stayers, but a more detailed analysis reveals that hashtags related to staying safe and staying at home, as well as vaccinations are more polarised for migrants than for stayers, and they reflect the general development trend of the pandemic in Europe. In addition to comparing migrants with stayers, we also compared migrants staying in different countries. amongst the countries of migration, for which we collected at least 3000 tweets, the most positive sentiment of Polish migrants’ tweets was observed in Belgium, with the most negative sentiment coming from Estonia. We also observed that the sentiment of tweets written in Polish by stayers in Poland is less negative when compared to Polish migrants in most of the countries with the highest number of tweets.",2023,1869
Information and communication technology platforms as an experimental paradigm in cyber-bystander research: A critique of methodology,Pooja Megha Nagar and Victoria Talwar,"The investigation of bystander behavior in response to cyberbullying is a developing area of research that is still in its infancy. To advance this area of inquiry, researchers can use information and communication technology (ICT) platforms, such as simulated social media websites, as an experimental paradigm to facilitate and measure the behavior change of cyber-bystanders in a controlled virtual environment. However, this is a method that remains under-utilized by researchers and it remains unclear why. Thus, the purpose of this paper is to use the '5 principles of cyberbullying research' as an informed and empirical framework to systematically identify the methodological shortcomings that contribute to the underutilization of ICT platforms in cyber-bystander research. The final section of the paper builds on these 5 principles by critically analyzing the unique features of ICT platforms to outline ways in which researchers can design paradigms that are informed by both theory and practice. Overall, this paper aims to further develop the types of experimental methods that are used in the field of cyberbullying to create new avenues of research.",2021,1870
Threatwatch,,,2021,1871
Cutting through the noise to motivate people: A comprehensive analysis of COVID-19 social media posts de/motivating vaccination,Ashiqur Rahman and Ehsan Mohammadi and Hamed Alhoori,"The COVID-19 pandemic exposed significant weaknesses in the healthcare information system. The overwhelming volume of misinformation on social media and other socioeconomic factors created extraordinary challenges to motivate people to take proper precautions and get vaccinated. In this context, our work explored a novel direction by analyzing an extensive dataset collected over two years, identifying the topics de/motivating the public about COVID-19 vaccination. We analyzed these topics based on time, geographic location, and political orientation. We noticed that while the motivating topics remain the same over time and geographic location, the demotivating topics change rapidly. We also identified that intrinsic motivation, rather than external mandate, is more advantageous to inspire the public. This study addresses scientific communication and public motivation in social media. It can help public health officials, policymakers, and social media platforms develop more effective messaging strategies to cut through the noise of misinformation and educate the public about scientific findings.",2024,1872
"ADLES: Specifying, deploying, and sharing hands-on cyber-exercises",Daniel {Conte de Leon} and Christopher E. Goes and Michael A. Haney and Axel W. Krings,"Hands-on tutorials and exercises are recognized as an effective means for gaining much needed cybersecurity and communication and information technology skills. These exercises must be performed in dedicated and virtually isolated computing environments or laboratories, most of which make use of virtualization technology. Building, modifying, and deploying the virtual environments that enable hands-on instruction is currently very time consuming. A new complete exercise instance must be deployed and configured for each course or module, tutorial or exercise, and student. In addition, efficient sharing and reuse of hands-on exercises between organizations is currently extremely difficult, unless the computing resources and virtualization environment are also shared. ADLES is a specification language and associated deployment system created to address these issues up-front. ADLES enables: (1) the formal specification of hands-on virtual computing, networking, and cybersecurity exercises, (2) the automated deployment of specified exercises, and (3) the efficient sharing of such exercises and their computing environment. In this article, we describe in detail the ADLES specification language and deployment system. We also demonstrate ADLES capabilities using two case studies: a pentesting tutorial and a cyber defense competition. The ADLES system is open source and available for all educators to use and improve.",2018,1873
Defending Root DNS Servers against DDoS Using Layered Defenses (Extended),ASM Rizvi and Jelena Mirkovic and John Heidemann and Wesley Hardaker and Robert Story,"Distributed Denial-of-Service (DDoS) attacks exhaust resources, leaving a server unavailable to legitimate clients. The Domain Name System (DNS) is a frequent target of DDoS attacks. Since DNS is a critical infrastructure service, protecting it from DoS is imperative. Many prior approaches have focused on specific filters or anti-spoofing techniques to protect generic services. DNS root nameservers are more challenging to protect, since they use fixed IP addresses, serve very diverse clients and requests, receive predominantly UDP traffic that can be spoofed, and must guarantee high quality of service. In this paper we propose a layered DDoS defense for DNS root nameservers. Our defense uses a library of defensive filters, which can be optimized for different attack types, with different levels of selectivity. We further propose a method that automatically and continuously evaluates and selects the best combination of filters throughout the attack. We show that this layered defense approach provides exceptional protection against all attack types using traces of ten real attacks from a DNS root nameserver. Our automated system can select the best defense within seconds and quickly reduces traffic to the server within a manageable range, while keeping collateral damage lower than 2%. We show our system can successfully mitigate resource exhaustion using replay of a real-world attack. We can handle millions of filtering rules without noticeable operational overhead.",2023,1874
On-chain analytics for sentiment-driven statistical causality in cryptocurrencies,Ioannis Chalkiadakis and Anna Zaremba and Gareth W. Peters and Michael J. Chantler,"This paper establishes a new framework for assessing multimodal statistical causality between cryptocurrency market (cryptomarket) sentiment and cryptocurrency price processes. In order to achieve this, we present an efficient algorithm for multimodal statistical causality analysis based on Multiple-Output Gaussian Processes. Signals from different information sources (modalities) are jointly modelled as a Multiple-Output Gaussian Process, and then using a novel approach to statistical causality based on Gaussian Processes (GPs), we study linear and non-linear causal effects between the different modalities. We demonstrate the effectiveness of our approach in a machine learning application by studying the relationship between cryptocurrency spot price dynamics and sentiment time-series data specific to the crypto sector, which we conjecture influences retail investor behaviour. The investor sentiment is extracted from cryptomarket news data via methods developed in the area of statistical machine learning known as Natural Language Processing (NLP). To capture sentiment, we present a novel framework for text to time-series embedding, which we then use to construct a sentiment index from publicly available news articles. We conduct a statistical analysis of our sentiment statistical index model and compare it to alternative state-of-the-art sentiment models popular in the NLP literature. In regard to the multimodal causality, the investor sentiment is our primary modality of exploration, in addition to price and a blockchain technology-related indicator (hash rate). Analysis shows that our approach is effective in modelling causal structures of variable degree of complexity between heterogeneous data sources and illustrates the impact that certain modelling choices for the different modalities can have on detecting causality. A solid understanding of these factors is necessary to gauge cryptocurrency adoption by retail investors and provide sentiment- and technology-based insights about the cryptocurrency market dynamics.",2022,1875
Using the Launcher for Executing High Throughput Workloads,Lucas A. Wilson,"For many scientific disciplines, the transition to using advanced cyberinfrastructure comes not out of a desire to use the most advanced or most powerful resources available, but because their current operational model is no longer sufficient to meet their computational needs. Many researchers begin their computations on their desktop or local workstation, only to discover that the time required to simulate their problem, analyze their instrument data, or score the multitude of entities that they want to would require far more time than they have available. Launcher is a simple utility which enables the execution of high throughput computing workloads on managed HPC systems quickly and with as little effort as possible on the part of the user. Basic usage of the Launcher is straightforward, but Launcher provides several more advanced capabilities including use of Intel® Xeon Phi™ coprocessor cards and task binding support for multi-/many-core architectures. We step through the processes of setting up a basic Launcher job, including creating a job file, setting appropriate environment variables, and using scheduler integration. We also describe how to enable use of the Intel® Xeon Phi™ coprocessor cards, take advantage of Launcher's task binding system, and execute many parallel (OpenMP/MPI) applications at once.",2017,1876
Artificial nervous systems—A new paradigm for artificial intelligence,Fredric Narcross,"Three dissimilar methodologies in the field of artificial intelligence (AI) appear to be following a common path toward biological authenticity. This trend could be expedited by using a common tool, artificial nervous systems (ANS), for recreating the biology underpinning all three. ANS would then represent a new paradigm for AI with application to many related fields.",2021,1877
A systematic comparison and evaluation of building ontologies for deploying data-driven analytics in smart buildings,Zhangcheng Qiang and Stuart Hands and Kerry Taylor and Subbu Sethuvenkatraman and Daniel Hugo and Pouya {Ghiasnezhad Omran} and Madhawa Perera and Armin Haller,"Ontologies play a critical role in data exchange, information integration, and knowledge sharing across diverse smart building applications. Yet, semantic differences between the prevailing building ontologies hamper their purpose of bringing data interoperability and restrict the ability to reuse building ontologies in real-world applications. In this paper, we propose and adopt a framework to conduct a systematic comparison and evaluation of four popular building ontologies (Brick Schema, RealEstateCore, Project Haystack, and Digital Buildings) from both axiomatic design and assertions in a use case, namely the Terminological Box (TBox) evaluation and the Assertion Box (ABox) evaluation. In the TBox evaluation, we use the SQuaRE-based Ontology Quality Evaluation (OQuaRE) framework and concede that Project Haystack and Brick Schema are more compact with respect to the ontology axiomatic design. In the ABox evaluation, we apply an empirical study with sample building data that suggests Brick Schema and RealEstateCore have greater completeness and expressiveness in capturing the main concepts and relations within the building domain. The results indicate that there is no universal building ontology for integrating Linked Building Data (LBD). We also discuss ontology compatibility and investigate building ontology design patterns (ODPs) to support ontology matching, alignment, and harmonisation.",2023,1878
"AI techniques for IoT-based DDoS attack detection: Taxonomies, comprehensive review and research challenges",Bindu Bala and Sunny Behal,"Distributed Denial of Service (DDoS) attacks in IoT networks are one of the most devastating and challenging cyber-attacks. The number of IoT users is growing exponentially due to the increase in IoT devices over the past years. Consequently, DDoS attack has become the most prominent attack as vulnerable IoT devices are becoming victims of it. In the literature, numerous techniques have been proposed to detect IoT-based DDoS attacks. However, techniques based on Artificial Intelligence (AI) have proven to be effective in the detection of cyber-attacks in comparison to other alternative techniques. This paper presents a systematic literature review of AI-based tools and techniques used for analysis, classification, and detection of the most threatening, prominent, and dreadful IoT-based DDoS attacks between the years 2019 to 2023. A comparative study of real datasets having IoT traffic features has also been illustrated. The findings of this systematic review provide useful insights into the existing research landscape for designing AI-based models to detect IoT-based DDoS attacks specifically. Additionally, the study sheds light on IoT botnet lifecycle, various botnet families, the taxonomy of IoT-based DDoS attacks, prominent tools used to launch DDoS attack, publicly available IoT datasets, the taxonomy of AI techniques, popular software available for ML/DL modeling, a list of numerous research challenges and future directions that may aid in the development of novel and reliable methods for identifying and categorizing IoT-based DDoS attacks.",2024,1879
The ReFiBot makers guide: Fostering academic open science and circularity with a robotic educational kit,Christos Pantos and Jurrian Doornbos and Gonzalo Mier and João Valente,"The advent of robotics in schools and universities curricula are preparing students to encompass new didactic fields. This article presents ReFiBot which is an education robot that has been used to increase the technical literacy on robotics and bring more awareness to open science at Wageningen University. The ReFiBot combines open-source hardware and software, integrated with a chassis made from recycled plastic from fishnets. The ReFiBot was carefully designed to be easily assembled with off-the-shelf electronic parts and programmed using the Arduino IDE. Moreover, a software library is facilitated to ease its adoption in educational activities from any curricula level. The ReFiBot has been mainly used for education but can also be used for research on swarm robotics. The CAD files, components list, software files, and tutorial within this contribution will guide the reader through the assemblage and best practices of this circular robotics kit.",2023,1880
Multi attribute auction based incentivized solution against DDoS attacks,Amrita Dahiya and B.B. Gupta,"Complexity and severity of DDoS attacks is increasing day by day. Internet has highly inconsistent structure in terms of resource distribution. Numerous technical solutions are present in this domain but solutions considering economic aspects have not been given attention. Therefore, in this paper, a multi attribute based auction mechanism to mitigate DDoS attacks has been proposed. A reputation based detection mechanism has been proposed where reputation of a user is assessed through his marginal utility. Along with detection mechanism, two payment mechanisms have been proposed for legitimate and malicious users separately. A greedy resource allocation is devised to allocate resources fairly among the legitimate users. Malicious users who manipulate their bid to acquire maximum share of limited resources are charged with penalty according to differential payment scheme. Since, this is a generalized concept to mitigate DDoS attacks on any platform, we have taken our case study on cloud computing. So, simulations have been carried out on CloudSim. Results obtained from simulations clearly showed that proposed approach performs better than existing DDoS attack mitigation techniques.",2020,1881
Chapter 27 - Natural Language Processing for Diabetes Digital Health,Alexander Turchin,"Natural language processing (NLP) is used increasingly widely in the field of diabetes, ranging from the identification of hypoglycemic events to building predictive models for adverse clinical outcomes. More recent studies have shown a relationship between quantitative characteristics of text (e.g., the length of a sentence describing a patient-provider discussion) and outcomes like glucose levels, implying that these computational characteristics likely reflect care delivered to the patient. Barriers to the broader use of NLP in diabetes care and research (e.g., scarcity of human and data resources) can be overcome by promoting the training of endocrinologists with expertise in data science and cross-institutional collaborations.",2024,1882
Optimizing Hearthstone agents using an evolutionary algorithm,Pablo García-Sánchez and Alberto Tonda and Antonio J. Fernández-Leiva and Carlos Cotta,"Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone.",2020,1883
Dynamic reorientation of tidally locked bodies: Application to Pluto,Vojtěch Patočka and Martin Kihoulou,"Planets and moons reorient in space due to mass redistribution associated with various types of internal and external processes. While the equilibrium orientation of a tidally locked body is well understood, much less explored are the dynamics of the reorientation process (or true polar wander, TPW, used here for the motion of either the rotation or the tidal pole). TPW dynamics can be non-trivial and are important for predicting the patterns of TPW-induced surface fractures, as well as for assessing whether enough time has passed for the equilibrium orientation to be reached. The only existing and relatively complex numerical method for an accurate evaluation of the reorientation dynamics of a tidally locked body was described in a series of papers by Hu et al., 2017a, Hu et al., 2017b, Hu et al., 2019. Here we demonstrate that an identical solution can be obtained with a simpler approach, denoted as oω||mMIA, because during TPW the tidal and the rotation axes closely follow respectively the minor and the major axes of the total, time-evolving inertia tensor of the body. Motivated by the presumed reorientation of Pluto, the use of the oω||mMIA method is illustrated on several test examples. In particular, we vary the load sign and the mass of the host body and analyze whether TPW paths are curved or straight. When tidal forcing is relatively small, the paths of negative anomalies (e.g. basins) towards the rotation pole are highly curved, while positive loads may reach the sub- or anti-host point straightforwardly. The obtained behavior is explained by the relative timing of longitudinal and latitudinal reorientation. Our results suggest that the Sputnik Planitia basin cannot be a negative anomaly at present day, and that the remnant figure of Pluto must have formed prior to the reorientation. Finally, the presented method is complemented with an energy balance that can be used to test the numerical solution and to quantify the changes in orbital distance due to TPW. A new release of the custom written code LIOUSHELL that is used to perform the simulations is made freely available on GitHub.",2023,1884
A national scale big data analytics pipeline to assess the potential impacts of flooding on critical infrastructures and communities,N. Donratanapat and S. Samadi and J.M. Vidal and S. {Sadeghi Tabas},"With the rapid development of the Internet of Things (IoT) and Big Data infrastructure, crowdsourcing techniques have emerged to facilitate data processing and problem solving particularly for flood emergences purposes. A Flood Analytics Information System (FAIS) has been developed as a Python Web application to gather Big Data from multiple servers and analyze flooding impacts during historical and real-time events. The application is smartly designed to integrate crowd intelligence, machine learning (ML), and natural language processing of tweets to provide flood warning with the aim to improve situational awareness for flood risk management. FAIS, a national scale prototype, combines flood peak rates and river level information with geotagged tweets to identify a dynamic set of at-risk locations to flooding. The prototype was successfully tested in real-time during Hurricane Dorian flooding as well as for historical event (Hurricanes Florence) across the Carolinas, USA where the storm made extensive disruption to infrastructure and communities.",2020,1885
Multifractal detrended fluctuation analysis based detection for SYN flooding attack,Dalia Nashat and Fatma A. Hussain,"The TCP SYN flooding (half-open connection) attack is a type of DDoS attack, which denies the services by consuming the server resources. This attack prevents legitimate users from using their desired service. The SYN flooding attack exploits the normal TCP three-way handshake by sending stream of SYN packets to the server with spoofed IP addresses. The detection of this attack is hard since the internet routing infrastructure cannot differentiate between legitimate and spoofed SYN packets. In this paper we present a new detection method for the SYN flooding attack based on Multifractal Detrended Fluctuation Analysis (MFDFA) in addition to an adaptive threshold, thus we can detect the abnormal behavior in the TCP protocol time series.",2021,1886
A comparison of four self-controlled study designs in an analysis of COVID-19 vaccines and myocarditis using five European databases,Anna Schultze and Ivonne Martin and Davide Messina and Sophie Bots and Svetlana Belitser and Juan {José Carreras-Martínez} and Elisa Correcher-Martinez and Arantxa Urchueguía-Fornes and Mar Martín-Pérez and Patricia García-Poza and Felipe Villalobos and Meritxell Pallejà-Millán and Carlo {Alberto Bissacco} and Elena Segundo and Patrick Souverein and Fabio Riefolo and Carlos E. Durán and Rosa Gini and Miriam Sturkenboom and Olaf Klungel and Ian Douglas,"Introduction
The aim of this study was to assess the possible extent of bias due to violation of a core assumption (event-dependent exposures) when using self-controlled designs to analyse the association between COVID-19 vaccines and myocarditis.
Methods
We used data from five European databases (Spain: BIFAP, FISABIO VID, and SIDIAP; Italy: ARS-Tuscany; England: CPRD Aurum) converted to the ConcePTION Common Data Model. Individuals who experienced both myocarditis and were vaccinated against COVID-19 between 1 September 2020 and the end of data availability in each country were included. We compared a self-controlled risk interval study (SCRI) using a pre-vaccination control window, an SCRI using a post-vaccination control window, a standard SCCS and an extension of the SCCS designed to handle violations of the assumption of event-dependent exposures.
Results
We included 1,757 cases of myocarditis. For analyses of the first dose of the Pfizer vaccine, to which all databases contributed information, we found results consistent with a null effect in both of the SCRI and extended SCCS, but some indication of a harmful effect in a standard SCCS. For the second dose, we found evidence of a harmful association for all study designs, with relatively similar effect sizes (SCRI pre = 1.99, 1.40 – 2.82; SCRI post 2.13, 95 %CI – 1.43, 3.18; standard SCCS 1.79, 95 %CI 1.31 – 2.44, extended SCCS 1.52, 95 %CI = 1.08 – 2.15). Adjustment for calendar time did not change these conclusions. Findings using all designs were also consistent with a harmful effect following a second dose of the Moderna vaccine.
Conclusions
In the context of the known association between COVID-19 vaccines and myocarditis, we have demonstrated that two forms of SCRI and two forms of SCCS led to largely comparable results, possibly because of limited violation of the assumption of event-dependent exposures.",2024,1887
"Distributed denial of service attack prediction: Challenges, open issues and opportunities",Anderson Bergamini {de Neira} and Burak Kantarci and Michele Nogueira,"Distributed Denial of Service (DDoS) attack is one of the biggest cyber threats. DDoS attacks have evolved in quantity and volume to evade detection and increase damage. Changes during the COVID-19 pandemic have left traditional perimeter-based security measures vulnerable to attackers that have diversified their activities by targeting health services, e-commerce, and educational services. DDoS attack prediction searches for signals of attack preparation to warn about the imminence of the attack. Prediction is necessary to handle high-volumetric DDoS attacks and to increase the time to defend against them. This survey article presents the classification of studies from the literature comprising the current state-of-the-art on DDoS attack prediction. It highlights the results of this extensive literature review categorizing the works by prediction time, architecture, employed methodology, and the type of data utilized to predict attacks. Further, this survey details each identified study and, finally, it emphasizes the research opportunities to evolve the DDoS attack prediction state-of-the-art.",2023,1888
CoVerifi: A COVID-19 news verification system,Nikhil L. Kolluri and Dhiraj Murthy,"There is an abundance of misinformation, disinformation, and “fake news” related to COVID-19, leading the director-general of the World Health Organization to term this an ‘infodemic’. Given the high volume of COVID-19 content on the Internet, many find it difficult to evaluate veracity. Vulnerable and marginalized groups are being misinformed and subject to high levels of stress. Riots and panic buying have also taken place due to “fake news”. However, individual research-led websites can make a major difference in terms of providing accurate information. For example, the Johns Hopkins Coronavirus Resource Center website has over 81 million entries linked to it on Google. With the outbreak of COVID-19 and the knowledge that deceptive news has the potential to measurably affect the beliefs of the public, new strategies are needed to prevent the spread of misinformation. This study seeks to make a timely intervention to the information landscape through a COVID-19 “fake news”, misinformation, and disinformation website. In this article, we introduce CoVerifi, a web application which combines both the power of machine learning and the power of human feedback to assess the credibility of news. By allowing users the ability to “vote” on news content, the CoVerifi platform will allow us to release labelled data as open source, which will enable further research on preventing the spread of COVID-19-related misinformation. We discuss the development of CoVerifi and the potential utility of deploying the system at scale for combating the COVID-19 “infodemic”.",2021,1889
Predicting video engagement using heterogeneous DeepWalk,Iti Chaturvedi and Kishor Thapa and Sandro Cavallari and Erik Cambria and Roy E. Welsch,"Video engagement is important in online advertisements where there is no physical interaction with the consumer. Engagement can be directly measured as the number of seconds after which a consumer skips an advertisement. In this paper, we propose a model to predict video engagement of an advertisement using only a few samples. This allows for early identification of poor quality videos. This can also help identify advertisement frauds where a robot runs fake videos behind the name of well-known brands. We leverage on the fact that videos with high engagement have similar viewing patterns over time. Hence, we can create a similarity network of videos and use a graph-embedding model called DeepWalk to cluster videos into significant communities. The learned embedding is able to identify viewing patterns of fraud and popular videos. In order to assess the impact of a video, we also consider how the view counts increase or decrease over time. This results in a heterogeneous graph where an edge indicates similar video engagement or history of view counts between two videos. Since it is difficult to find labelled samples for ‘fraud’ video, we leverage on a one-class model that can determine ‘fraud’ videos with outlier or abnormal behavior. The proposed model outperforms baselines in F-measure by over 20%.",2021,1890
On Detecting and Classifying DGA Botnets and their Families,Tong Anh Tuan and Hoang Viet Long and David Taniar,"Botnets are a frequent threat to information systems on the Internet, capable of launching denial-of-service attacks, spreading spam and malware on a large scale. Detecting and preventing botnets is very important in cybersecurity. Previous studies have suggested anomaly-based, signature-based, or HoneyNet-based botnet detection solutions. This paper presents new solutions for detecting and classifying families of Domain Generation Algorithm (DGA) botnets. Our solution can be applied in practice to disable botnets even if they have infected the computer. Our works help solve two problems, including binary classification and multiclass classification, specifically: (1) Determining whether a domain name is malicious or benign; (2) For malicious domains, identify their DGA botnet family. We proposed two deep learning models called LA_Bin07 and LA_Mul07 by combining the LSTM network and Attention layer. Our evaluation used the UMUDGA dataset recently published in 2020, with 50 DGA botnet families. The experimental results show that the LA_Bin07 and LA_Mul07 models solve the DGA botnets problem for binary and multiclass classification problems with very high accuracy.",2022,1891
Adapting agile practices in university contexts,Zainab Masood and Rashina Hoda and Kelly Blincoe,"Teaching agile practices has found its place in software engineering curricula in many universities across the globe. As a result, educators and students have embraced different ways to apply agile practices during their courses through lectures, games, projects, workshops and more for effective theoretical and practical learning. Practicing agile in university contexts comes with challenges for students and to counter these challenges, they perform some adaptations to standard agile practices making them effective and easier to use in university contexts. This study describes the constraints the students faced while applying agile practices in a university course taught at the University of Auckland, including difficulty in setting up common time for all team members to work together, limited availability of customer due to busy schedule and the modifications the students introduced to adapt agile practices to suit the university context, such as daily stand-ups with reduced frequency, combining sprint meetings, and rotating scrum master from team. In addition, it summarizes the effectiveness of these modifications based on reflection of the students. Recommendations for educators and students are also provided. Our findings and recommendations will help educators and students better coordinate and apply agile practices on industry-based projects in university contexts.",2018,1892
Detection of fake news campaigns using graph convolutional networks,Dimitrios Michail and Nikos Kanakaris and Iraklis Varlamis,"The detection of organised disinformation campaigns that spread fake news, by first camouflaging them as real ones is crucial in the battle against misinformation and disinformation in social media. This article presents a method for classifying the diffusion graphs of news formed in social media, by taking into account the profiles of the users that participate in the graph, the profiles of their social relations and the way the news spread, ignoring the actual text content of the news or the messages that spread it. This increases the robustness of the method and widens its applicability in different contexts. The results of this study show that the proposed method outperforms methods that rely on textual information only and provide a model that can be employed for detecting similar disinformation campaigns on different context in the same social medium.",2022,1893
Detection of zero-day attacks: An unsupervised port-based approach,Agathe Blaise and Mathieu Bouet and Vania Conan and Stefano Secci,"Last years have witnessed more and more DDoS attacks towards high-profile websites, as the Mirai botnet attack on September 2016, or more recently the memcached attack on March 2018, this time with no botnet required. These two outbreaks were not detected nor mitigated during their spreading, but only at the time they happened. Such attacks are generally preceded by several stages, including infection of hosts or device fingerprinting; being able to capture this activity would allow their early detection. In this paper, we propose a technique for the early detection of emerging botnets and newly exploited vulnerabilities, which consists in (i) splitting the detection process over different network segments and retaining only distributed anomalies, (ii) monitoring at the port-level, with a simple yet efficient change-detection algorithm based on a modified Z-score measure. We argue how our technique, named Split-and-Merge, can ensure the detection of large-scale zero-day attacks and drastically reduce false positives. We apply the method on two datasets: the MAWI dataset, which provides daily traffic traces of a transpacific backbone link, and the UCSD Network Telescope dataset which contains unsolicited traffic mainly coming from botnet scans. The assumption of a normal distribution – for which the Z-score computation makes sense – is verified through empirical measures. We also show how the solution generates very few alerts; an extensive evaluation on the last three years allows identifying major attacks (including Mirai and memcached) that current Intrusion Detection Systems (IDSs) have not seen. Finally, we classify detected known and unknown anomalies to give additional insights about them.",2020,1894
Building product ontology: Core ontology for Linked Building Product Data,Anna Wagner and Wendelin Sprenger and Christoph Maurer and Tilmann E. Kuhn and Uwe Rüppel,"The digitalisation of the Architecture, Engineering and Construction domain introduced new methods for digital collaboration, i.e. Building Information Modelling (BIM). While this method focuses on building data, the distribution of digital product models is still problematic, complicating uniform product searches and automated product data processing. Existing schemas, such as a subpart of the Industry Foundation Classes or the German VDI 3805, rely on rigid or template-driven schemas, that do not support the description of innovative or multi-functional products or impose a large schema overhead and complexity on manufacturers. Therefore, this article combines flexible and modular product descriptions with Semantic Web technologies and Linked Data. By applying Web-based technologies, the searchability of product data and the applicability of distributed data are expected to be enhanced. More precisely, this article proposes a concept for Linked Building Product Data and introduces the generic Building Product Ontology as a potential core schema of the concept. To demonstrate the feasibility of Linked Building Product Data and the Building Product Ontology, the authors apply both the concept and the data schema to innovative and multi-functional example products that cannot be described with the existing approaches for product descriptions. The evaluation demonstrates the flexibility, modularity and overall suitability of the presented concepts, meeting all collected requirements for digital product descriptions. Hence, Linked Building Product Data may solve existing issues with rigid product description schemas. At the same time, this approach complements the current research trend of Linked Building Data.",2022,1895
Orchestration of APT malware evasive manoeuvers employed for eluding anti-virus and sandbox defense,Amit Sharma and Brij B. Gupta and Awadhesh Kumar Singh and V.K. Saraswat,"The modern day cyber attacks are highly targeted and incorporate advanced tactics, techniques and procedures for greater stealth, impact and success. These attacks are also known as Advanced Persistent Threats(APT) because of their evasive and stealth nature along with longer foothold on the victim’s digital infrastructure. The malware involved in APT attacks are sophisticated and developed with the intention of sabotaging the victim’s digital infrastructure or performing espionage. They are capable of targeting multiple operating environments starting from desktop and server operating systems (Windows, Linux and MacOS), Mobile platforms (Android, iOS), Embedded platforms (IoT Devices), to Industrial control systems (ICS/SCADA Devices). The evolution of evasive tactics and techniques employed in such advanced malware leads to extensive research efforts to develop mechanisms that can counter these evasion techniques. The research primarily aims to demonstrate that evasive manoeuvers are currently over-weighing the security countermeasures deployed by the prevalent security solutions. This paper will first explain the evasion mechanism in a systematic manner employed in modern APT malware and aims to implement a novel Evasive Manoeuvers Re-Engineering Framework(EMRF).EMRF aims to establish and demonstrate combinations of evasive manoeuvers with much known APT malware samples to elude security solutions. The payload variants, i.e., executable, dynamic link library, and shell-code, were experimented through a research-based framework EMRF to demonstrate 36% to 96% of evasive behavior countering the majority of defender engines. The EMRF system with its dynamic user defined evasion manoeuvers is able to transform non-zero-day payloads more potent by evading majority of the modern security solutions. This research clearly demonstrates the attacker’s ability to deliver non-zero-day payloads easily rather than investing resources and time in discovering zero-day exploits and developing zero-day payloads. This important observation can potentially disrupt the Advanced Persistent Threat Defenses incorporated in modern day security solution where focus is mainly on to detect zero-day payloads and exploits. Exhibiting the threat landscape poised due to APT, the paper utilizes a dataset of 4403 APT malware samples to extract and orchestrate the prevalence of evasive manoeuvers like stealth, covert communication, and anti-analysis mechanisms. This paper will contribute towards advanced malware analysis as an avenue to analyzing intrusion, evasion, and deception to prevent detection and verification, an association of responsibility, and determination of intent.",2022,1896
A study of the relationship of malware detection mechanisms using Artificial Intelligence,Jihyeon Song and Sunoh Choi and Jungtae Kim and Kyungmin Park and Cheolhee Park and Jonghyun Kim and Ikkyun Kim,"Implementation of malware detection using Artificial Intelligence (AI) has emerged as a significant research theme to combat evolving various types of malwares. Researchers implement various detection mechanisms using shallow and deep learning models to counter new malware, and they continue to develop these mechanisms today. However, in the field of malware detection using AI, there are difficulties in collecting data, and it is difficult to compare research content and performance with related studies. Meanwhile, the number of well-organized papers is not sufficient to understand the overall research flow of these related studies. Before starting new research, researchers need to analyze the current state of research in the malware detection field they want to study. Therefore, based on these requirements, we present a summary of the general criteria related to malware detection and a classification table for detection mechanisms. Additionally, we have organized many studies in the field of various types of malware detection so that they can be viewed at a glance. We hope that the provided survey can help new researchers quickly understand the research flow in the field of AI-based malware detection and establish the direction for future research.",2024,1897
Enhancing cyber risk identification in the construction industry using language models,Dongchi Yao and Borja {García de Soto},"Modern construction projects are vulnerable to cyber-attacks due to insufficient attention to cybersecurity. Cyber risks in construction projects are not fully recognized, and the relevant literature is limited. To address this gap, the capabilities of a language model were leveraged to analyze extensive text, tailored to identify cyber risks. The model was trained using a curated corpus related to construction cybersecurity, enhanced by Supervised Fine-Tuning and Reinforcement Learning from Human Feedback techniques. The findings demonstrate advancements in the model's ability to understand cybersecurity and generate responses to cybersecurity questions. Using this model, a prioritized checklist of cyber risks across project phases was developed, establishing a new industry benchmark. This checklist can be utilized by various groups, including project managers and risk analysts. The model allows for updates with new data, ensuring the checklist remains current. The upgraded model holds significant promise for industry-wide applications, serving as an intelligent cybersecurity consultant.",2024,1898
A qualitative analysis of themes in instant messaging communication of software developers,Camila {Costa Silva} and Matthias Galster and Fabian Gilson,"Software developers use instant messaging (e.g., Slack, Gitter) to collaboratively discuss software engineering problems and solutions. This communication takes place in chat rooms that generally contain a description of the main topic of discussion and the messages exchanged. To analyze whether and how the knowledge accumulated in these chat rooms is relevant to other developers, we first need to understand the themes discussed in these chat rooms. In this paper, we used thematic analysis to manually identify software engineering themes in the description of 87 chat rooms of Gitter, an instant messaging tool for software developers. Then, we checked whether these themes also occur in 184 public chat rooms of Slack, another instant messaging tool. We identified 47 themes in Gitter chat rooms, and regarding the applicability of themes, we could relate 36 of our themes to 173 Slack chat rooms. Our results indicate that, in the context of our study, chat rooms in developer instant messaging communication are mostly about software development technologies and practices rather than development processes. Furthermore, most chat rooms are topic- rather than project-related (e.g., a chat room used by developers of a particular software development project).",2022,1899
Identification of Important Web Sources of Information on Wikipedia across various Topics and Languages,Włodzimierz Lewoniewski,"Despite the fact, that over 21 years Wikipedia is edited by volunteers from all over the world with different views, culture, education and competences, this free encyclopedia is one of the most popular source of knowledge in the Internet. Freedom to edit does not mean, that you can put there whatever content you want. One of the core rules of Wikipedia says, that information in its articles should be based on reliable sources and Wikipedia readers must be able to verify particular facts in text. However, reliability is a subjective concept and a reputation of the same source can be assessed diffidently depending on a person (or group of persons), language and topic. So each language version of Wikipedia may have own rules or criteria on how the website must be assessed before it can be used as a source in references. At the same time, nowadays there are over 1 billion websites on the Internet and only few developed language chapters of the encyclopedia contains non-exhaustive lists of less than 1 thousand popular websites with reliability assessment. Additionally, since reputation of the source can be changed during the time, such lists must be updated regularly. This study presents result of important web sources identification based on analysis of over 230 million references that were extracted from over 40 million Wikipedia article of 42 most developed language version. Additionally, general statistics on references usage for each considered Wikipedia language were counted, including average number of references, number of unique references, scientific score, number of websites in references. Next, Wikipedia articles were assigned to different topics in each considered language. This allows to find differences in reliability and popularity of the same sources of information between Wikipedia languages, as well as find important websites in specific areas of knowledge.",2022,1900
Distributed denial of service attacks in cloud: State-of-the-art of scientific and commercial solutions,Aanshi Bhardwaj and Veenu Mangat and Renu Vig and Subir Halder and Mauro Conti,"Cloud computing model provides on demand, elastic and fully managed computer system resources and services to organizations. However, attacks on cloud components can cause inestimable losses to cloud service providers and cloud users. One such category of attacks is the Distributed Denial of Service (DDoS), which can have serious consequences including impaired customer experience, service outage and in severe cases, complete shutdown and total economic unsustainability. Advances in Internet of Things (IoT) and network connectivity have inadvertently facilitated launch of DDoS attacks which have increased in volume, frequency and intensity. Recent DDoS attacks involving new attack vectors and strategies, have precipitated the need for this survey. In this survey, we mainly focus on finding the gaps, as well as bridging those gaps between the future potential DDoS attacks and state-of-the-art scientific and commercial DDoS attack defending solutions. It seeks to highlight the need for a comprehensive detection approach by presenting the recent threat landscape and major cloud attack incidents, estimates of future DDoS, illustrative use cases, commercial DDoS solutions, and the laws governing DDoS attacks in different nations. An up-to-date survey of DDoS detection methods, particularly anomaly based detection, available research tools, platforms and datasets, has been given. This paper further explores the use of machine learning methods for detection of DDoS attacks and investigates features, strengths, weaknesses, tools, datasets, and evaluates results of the methods in the context of the cloud. A summary comparison of statistical, machine learning and hybrid methods has been brought forth based on detailed analysis. This paper is intended to serve as a ready reference for the research community to develop effective and innovative detection mechanisms for forthcoming DDoS attacks in the cloud environment. It will also sensitize cloud users and providers to the urgent need to invest in deployment of DDoS detection mechanisms to secure their assets.",2021,1901
Quantum Tic-Tac-Toe - learning the concepts of quantum mechanics in a playful way,Maurice Weingärtner and Tim Weingärtner,"Quantum mechanics is a complex matter. Nonetheless, given the impending arrival of quantum computers and the necessity for quantum programming abilities, more students should get acquainted with this subject. We provide a game-based learning approach based on Tic-Tac-Toe in a quantum modified version as a result of our study. We created a prototype to demonstrate and evaluate our assumptions using the design science research technique. Qualitative user feedback provided us with vital insights and shown that this game-based method helps to cope with quantum physics in a fun way. A majority of those who took part in the study stated that their interest in the subject had increased. However, for novices, it is necessary to follow them during the initial period of their training. The comments were quite helpful in optimizing the prototype. We found that our strategy, which included the use of a virtual opponent as well as the presentation of extra information about the quantum circuit, was more effective in helping participants comprehend quantum physics than earlier Tic-Tac-Toe-based learning settings.",2023,1902
Designing accurate lightweight intrusion detection systems for IoT networks using fine-tuned linear SVM and feature selectors,Jahongir Azimjonov and Taehong Kim,"Intrusion detection systems (IDSs) play a crucial role in ensuring the security and integrity of Internet of Things (IoT) networks by blocking unwanted packets and facilitating secure traffic flow. However, traditional IDSs based on data mining, fuzzy logic, heuristics, rough sets, or conventional machine learning (ML) techniques often lack accuracy and are not energy efficient, primarily due to inappropriate feature selection or the use of all features in datasets. To address these challenges, this study proposes a lightweight, accurate, and high-performance IDSs for IoT networks using fine-tuned Linear Support Vector Machines (LSVMs) and feature selection methods. Four feature selectors, including Importance Coefficient-, Forward- and Backward-Sequential-, and Correlation Coefficient-based approaches, were applied to identify the most important and efficient features from three datasets: KDD Cup-1999, BotIoT-2018, and N-BaIoT-2021. The fine-tuned LSVMs algorithm was then trained on subsets of the selected and full features of the datasets to detect various IoT botnet attacks. Evaluation results show that the IDS models trained with subsets of relevant features outperform those trained with the full feature sets of the datasets in terms of training and test performance and accuracy. The study concludes that it is possible to develop lightweight IDSs by training them with a reduced number of features (6) instead of using the full features (40, 15, 115) in KDD Cup-1999, BotIoT-2018, and N-BaIoT-2021, respectively. The findings highlight a potential for significantly improving the efficiency and accuracy of IDSs on IoT networks using the fine-tuned feature selectors and LSVMs.",2024,1903
An exploratory study of COVID-19 misinformation on Twitter,Gautam Kishore Shahi and Anne Dirkson and Tim A. Majchrzak,"During the COVID-19 pandemic, social media has become a home ground for misinformation. To tackle this infodemic, scientific oversight, as well as a better understanding by practitioners in crisis management, is needed. We have conducted an exploratory study into the propagation, authors and content of misinformation on Twitter around the topic of COVID-19 in order to gain early insights. We have collected all tweets mentioned in the verdicts of fact-checked claims related to COVID-19 by over 92 professional fact-checking organisations between January and mid-July 2020 and share this corpus with the community. This resulted in 1500 tweets relating to 1274 false and 226 partially false claims, respectively. Exploratory analysis of author accounts revealed that the verified twitter handle(including Organisation/celebrity) are also involved in either creating(new tweets) or spreading(retweet) the misinformation. Additionally, we found that false claims propagate faster than partially false claims. Compare to a background corpus of COVID-19 tweets, tweets with misinformation are more often concerned with discrediting other information on social media. Authors use less tentative language and appear to be more driven by concerns of potential harm to others. Our results enable us to suggest gaps in the current scientific coverage of the topic as well as propose actions for authorities and social media users to counter misinformation.",2021,1904
Precursor of privacy leakage detection for individual user,Xuefeng Li and Chensu Zhao and Yi Hu and Honglin Xie and Yuhang Wang and Jingyang Zhao,"The pursuit of dividends from the burgeoning realm of data traffic has emerged as a prevailing trend. Given this current state of affairs, the safeguarding of personal information has become an urgent task. Current methods for personal privacy protection primarily offer alerts when information breaches occur, but at that stage, irreversible breaches have already transpired. Thus, the study of preemptive privacy breach prediction is a task of significant importance within the realm of privacy protection. However, the endeavor to predict privacy breaches in advance remains exceedingly challenging, owing to several factors: (i) The complexity of social networks gives rise to high-dimensional features. (ii) Concerning the comprehensive and precise capture of pathways leading to information leakage. (iii) How to employ time series data effectively to realize early predictions. This study proposes a novel approach for constructing graph neural networks and forecasting privacy breaches, centered on the context of user-generated content within specific time frames, integrates spatial graph structures with temporal series information. The integration can entirely achieve the advance prediction of users' privacy status, thereby preventing information leakage. Empirical tests on real-world datasets demonstrate that our approach surpasses traditional time series forecasting methods in privacy breach predictions, achieving a notable average improvement of 2% in F1 score, Recall, and Precision metrics.",2024,1905
The diffusion of malicious content on Twitter and its impact on security,Yaman Roumani,"While Twitter remains one of the most popular social media networks within the information security community, threat actors continue to abuse the platform to create, share, and spread malicious contents. In this study, we focus on whether Twitter- and vulnerability-related features can help predict vulnerabilities known to be actively exploited. Using a sample of 6004 tweets, results show that Twitter features (tweets and quote tweets) that combine the benefits of both content creation and sharing can predict active exploitation of vulnerabilities. Furthermore, findings show that certain technical and vulnerability-related features are also capable of predicting active exploitations.",2024,1906
Benchmark dataset of memes with text transcriptions for automatic detection of multi-modal misogynistic content,Francesca Gasparini and Giulia Rizzi and Aurora Saibene and Elisabetta Fersini,"In this paper we present a benchmark dataset generated as part of a project for automatic identification of misogyny within online content, which focuses in particular on memes. The benchmark here described is composed of 800 memes collected from the most popular social media platforms, such as Facebook, Twitter, Instagram and Reddit, and consulting websites dedicated to collection and creation of memes. To gather misogynistic memes, specific keywords that refer to misogynistic content have been considered as search criterion, considering different manifestations of hatred against women, such as body shaming, stereotyping, objectification and violence. In parallel, memes with no misogynist content have been manually downloaded from the same web sources. Among all the collected memes, three domain experts have selected a dataset of 800 memes equally balanced between misogynistic and non-misogynistic ones. This dataset has been validated through a crowdsourcing platform, involving 60 subjects for the labelling process, in order to collect three evaluations for each instance. Two further binary labels have been collected from both the experts and the crowdsourcing platform, for memes evaluated as misogynistic, concerning aggressiveness and irony. Finally for each meme, the text has been manually transcribed. The dataset provided is thus composed of the 800 memes, the labels given by the experts and those obtained by the crowdsourcing validation, and the transcribed texts. This data can be used to approach the problem of automatic detection of misogynistic content on the Web relying on both textual and visual cues, facing phenomenons that are growing every day such as cybersexism and technology-facilitated violence.",2022,1907
A comprehensive study of DDoS attacks over IoT network and their countermeasures,Pooja Kumari and Ankit Kumar Jain,"IoT offers capabilities to gather information from digital devices, infer from their results, and maintain and optimize these devices in different domains. IoT is heterogeneous in nature, which makes it prone to various security threats like confidentiality and integrity breaches, lack of availability of resources, trust issues, etc. The security concerns lead to different attacks over the system, and the Distributed Denial of Services (DDoS) bout is growing generously. DDoS is an assault that targets the availability of resources and servers of a network by flooding the communication medium from distinct locations by utilizing various IoT devices, which makes it harder to detect. Thus, analyzing and defending DDoS is a protruding field of research these days. The paper gives a thorough knowledge of DDoS over IoT. In this, we have critically analysed the existing DDoS variants, IoT Security issues, the execution of DDoS attempts, along with the exploitation of IoT devices and creation of them in Botnets or zombies. Moreover, the paper will also cover prevailing DDoS defense methodologies as well as their comparative analysis for ease of understanding.",2023,1908
A methodology for the security evaluation within third-party Android Marketplaces,William J. Buchanan and Simone Chiale and Richard Macfarlane,"This paper aims to evaluate possible threats with unofficial Android marketplaces, and geo-localize the malware distribution over three main regions: China; Europe; and Russia. It provides a comprehensive review of existing academic literature about security in Android focusing especially on malware detection systems and existing malware databases. Through the implementation of a methodology for identification of malicious applications it has been collected data revealing a 5% of them as malicious in an overall analysis. Furthermore, the analysis shown that Russia and Europe have a preponderance of generic detections and adware, while China is found to be targeted mainly by riskware and malware.",2017,1909
WFP-Collector: Automated dataset collection framework for website fingerprinting evaluations on Tor Browser,Mohamad Amar Irsyad {Mohd Aminuddin} and Zarul Fitri Zaaba,"Website Fingerprinting (WFP) is a technique that analyses browsing network traffic to infer a webpage that a user browsed in Tor Browser. A sufficiently large and clean dataset is essential for quality and accurate WFP experiments. Thus, there is a corresponding need to automate the dataset collection, filtering, and validation processes. This work introduces a new paradigm, WFP-Collector, an automatic dataset collection framework for WFP experiments. WFP-Collector enables researchers to automatically (1) create a visit database for webpage crawling, (2) collect various data and log for in-depth analysis, (3) webpage visits in tablet and mobile browsing modes, (4) throttle network bandwidth and latency performance, (5) validate and filter the collected data, (6) compress and upload the collected data to cloud storage, and (7) completion notification using Telegram and email. We developed a proof-of-concept of the WFP framework for simulation and comparison. We found that the WFP-Collector framework collects nine data items and produces over 55% larger collected data size than existing approaches. The captured packet size in tablet and mobile browsing modes is up to 57.5% smaller than in desktop mode. Moreover, the file compression in WFP-Collector can reduce up to 39.9% of the storage space required for data collection.",2023,1910
Critical Java flaw puts millions of organisations at risk,,"Threat actors are actively exploiting a critical flaw in Apache's Log4j logging library that allows them to execute arbitrary code, often with elevated privileges. The vulnerability (CVE-2021-44228) is trivially easy to exploit. And while a patch has been released, the library is in such widespread use – including by major platforms and services – that it may take some time before the threat is eliminated.",2021,1911
A semantic ontology for representing and quantifying energy flexibility of buildings,Han Li and Tianzhen Hong,"Energy flexibility of buildings can be an essential resource for a sustainable and reliable power grid with the growing variable renewable energy shares and the trend to electrify and decarbonize buildings. Traditional demand-side management technologies, advanced building controls, and emerging distributed energy resources (including electric vehicle, energy storage, and on-site power generation) enable the transition of the building stock to grid-interactive efficient buildings (GEBs) that operate efficiently to meet service needs and are responsive to grid pricing or carbon signals to achieve energy and carbon neutrality. Although energy flexibility has received growing attention from industry and the research community, there remains a lack of common ground for energy flexibility terminologies, characterization, and quantification methods. This paper presents a semantic ontology—EFOnt (Energy Flexibility Ontology)—that extends existing terminologies, ontologies, and schemas for building energy flexibility applications. EFOnt aims to serve as a standardized tool for knowledge co-development and streamlining energy flexibility related applications. We demonstrate potential use cases of EFOnt via two examples: (1) energy flexibility analytics with measured data from a residential smart thermostat dataset and a commercial building, and (2) modeling and simulation to evaluate energy flexibility of buildings. The compatibility of EFOnt with existing ontologies and the outlook of EFOnt's role in the building energy data tool ecosystem are discussed.",2022,1912
"Bi-allelic NIT1 variants cause a brain small vessel disease characterized by movement disorders, massively dilated perivascular spaces, and intracerebral hemorrhage",Julie W. Rutten and Minne N. Cerfontaine and Kyra L. Dijkstra and Aat A. Mulder and Jeroen Vreijling and Mark Kruit and Roman I. Koning and Susanne T. {de Bot} and Koen M. {van Nieuwenhuizen} and Hans J. Baelde and Henk W. Berendse and Leon H. Mei and George J.G. Ruijter and Frank Baas and Carolina R. Jost and Sjoerd G. {van Duinen} and Esther A.R. Nibbeling and Gido Gravesteijn and Saskia A.J. {Lesnik Oberstein},"Purpose
To describe a recessively inherited cerebral small vessel disease, caused by loss-of-function variants in Nitrilase1 (NIT1).
Methods
We performed exome sequencing, brain magnetic resonance imaging, neuropathology, electron microscopy, western blotting, and transcriptomic and metabolic analyses in 7 NIT1-small vessel disease patients from 5 unrelated pedigrees.
Results
The first identified patients were 3 siblings, compound heterozygous for the NIT1 c.727C>T; (p.Arg243Trp) variant and the NIT1 c.198_199del; p.(Ala68∗) variant. The 4 additional patients were single cases from 4 unrelated pedigrees and were all homozygous for the NIT1 c.727C>T; p.(Arg243Trp) variant. Patients presented in mid-adulthood with movement disorders. All patients had striking abnormalities on brain magnetic resonance imaging, with numerous and massively dilated basal ganglia perivascular spaces. Three patients had non-lobar intracerebral hemorrhage between age 45 and 60, which was fatal in 2 cases. Western blotting on patient fibroblasts showed absence of NIT1 protein, and metabolic analysis in urine confirmed loss of NIT1 enzymatic function. Brain autopsy revealed large electron-dense deposits in the vessel walls of small and medium sized cerebral arteries.
Conclusion
NIT1-small vessel disease is a novel, autosomal recessively inherited cerebral small vessel disease characterized by a triad of movement disorders, massively dilated basal ganglia perivascular spaces, and intracerebral hemorrhage.",2024,1913
Random choices facilitate solutions to collective network coloring problems by artificial agents,Matthew I. Jones and Scott D. Pauls and Feng Fu,"Summary
Global coordination is required to solve a wide variety of challenging collective action problems from network colorings to the tragedy of the commons. Recent empirical study shows that the presence of a few noisy autonomous agents can greatly improve collective performance of humans in solving networked color coordination games. To provide analytical insights into the role of behavioral randomness, here we study myopic artificial agents attempting to solve similar network coloring problems using decision update rules that are only based on local information but allow random choices at various stages of their heuristic reasonings. We show that the resulting efficacy of resolving color conflicts is dependent on the implementation of random behavior of agents and specific population characteristics. Our work demonstrates that distributed greedy optimization algorithms exploiting local information should be deployed in combination with occasional exploration via random choices in order to overcome local minima and achieve global coordination.",2021,1914
MatSciRE: Leveraging pointer networks to automate entity and relation extraction for material science knowledge-base construction,Ankan Mullick and Akash Ghosh and G. Sai Chaitanya and Samir Ghui and Tapas Nayak and Seung-Cheol Lee and Satadeep Bhattacharjee and Pawan Goyal,"Material science literature is a rich source of factual information about various categories of entities (like materials and compositions) and various relations between these entities, such as conductivity, voltage, etc. Automatically extracting this information to generate a material science knowledge base is a challenging task. In this paper, we propose MatSciRE (Material Science Relation Extractor), a Pointer Network-based encoder–decoder framework, to jointly extract entities and relations from material science articles as a triplet (entity1,relation,entity2). Specifically, we target the battery materials and identify five relations to work on — conductivity, coulombic efficiency, capacity, voltage, and energy. Our proposed approach achieved a much better F1-score (0.771) than a previous attempt using ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown in Fig. 1. The material information is extracted from material science literature in the form of entity–relation triplets using MatSciRE.",2024,1915
Quantifying chatbots’ ability to learn business processes,Christoph Kecht and Andreas Egger and Wolfgang Kratsch and Maximilian Röglinger,"Chatbots enable organizations in the business-to-customer domain to respond to repetitive requests efficiently. Extant approaches in Natural Language Processing (NLP) already address the essential requirement of understanding user input and synthesizing a response as close as possible to a response a human interlocutor would give. However, we argue that the organizational adoption of chatbots further depends on the underlying model’s capability to learn and comply with organizations’ business processes, for example, authenticating a customer before providing sensitive details. To address this issue, we develop an approach that quantifies chatbots’ ability to learn business processes using standardized process mining metrics. We demonstrate our approach by training chatbots on a dataset of more than 500,000 customer service conversations from three companies on Twitter and show how our approach supports the quantification of a chatbot’s overall ability to learn business processes from the training data. Furthermore, we quantify a chatbot’s ability to learn a particular variant of the underlying process and we show how to compare the chatbot’s executed steps against a given normative process model. Our approach that seamlessly integrates with existing approaches to evaluate NLP-based chatbots mitigates the current hurdles that practitioners face and, therefore, strives to foster the adoption of chatbots in practice.",2023,1916
An evaluation of the capabilities of language models and nurses in providing neonatal clinical decision support,Chedva Levin and Tehilla Kagan and Shani Rosen and Mor Saban,"Aim
To assess the clinical reasoning capabilities of two large language models, ChatGPT-4 and Claude-2.0, compared to those of neonatal nurses during neonatal care scenarios.
Design
A cross-sectional study with a comparative evaluation using a survey instrument that included six neonatal intensive care unit clinical scenarios.
Participants
32 neonatal intensive care nurses with 5–10 years of experience working in the neonatal intensive care units of three medical centers.
Methods
Participants responded to 6 written clinical scenarios. Simultaneously, we asked ChatGPT-4 and Claude-2.0 to provide initial assessments and treatment recommendations for the same scenarios. The responses from ChatGPT-4 and Claude-2.0 were then scored by certified neonatal nurse practitioners for accuracy, completeness, and response time.
Results
Both models demonstrated capabilities in clinical reasoning for neonatal care, with Claude-2.0 significantly outperforming ChatGPT-4 in clinical accuracy and speed. However, limitations were identified across the cases in diagnostic precision, treatment specificity, and response lag.
Conclusions
While showing promise, current limitations reinforce the need for deep refinement before ChatGPT-4 and Claude-2.0 can be considered for integration into clinical practice. Additional validation of these tools is important to safely leverage this Artificial Intelligence technology for enhancing clinical decision-making.
Impact
The study provides an understanding of the reasoning accuracy of new Artificial Intelligence models in neonatal clinical care. The current accuracy gaps of ChatGPT-4 and Claude-2.0 need to be addressed prior to clinical usage.",2024,1917
ChatGPT: A meta-analysis after 2.5 months,Christoph Leiter and Ran Zhang and Yanran Chen and Jonas Belouadi and Daniil Larionov and Vivian Fresen and Steffen Eger,"ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and media attention since its release in November 2022. However, little hard evidence is available regarding its perception in various sources. In this paper, we analyze over 300,000 tweets and more than 150 scientific papers to investigate how ChatGPT is perceived and discussed. Our findings show that ChatGPT is generally viewed as of high quality, with positive sentiment and emotions of joy dominating social media. Its perception has slightly decreased since its debut, however, with joy decreasing and (negative) surprise on the rise, and it is perceived more negatively in languages other than English. In recent scientific papers, ChatGPT is characterized as a great opportunity across various fields including the medical domain, but also as a threat concerning ethics and receives mixed assessments for education. Our comprehensive meta-analysis of ChatGPT’s perception after 2.5 months since its release can contribute to shaping the public debate and informing its future development. We make our data available.11https://github.com/NL2G/ChatGPTReview.",2024,1918
"Dutch tweets before, during, and after a black lives matter demonstration in Amsterdam: Expert annotated data, protocol, and labelling tool",Laurens H.F. Müter and Christof {van Nimwegen} and Remco C. Veltkamp,"The Netherlands police are looking for measures to examine sentiment on social media related to protest demonstrations. While models exist to detect more subtle expressions of sentiment within tweets, models trained in the Dutch language are scarce. Being able to predict sentiment development during protests is relevant for parties like the Dutch government and the police to get more insight to when and where potential law enforcement is needed for public order and safety. Therefore, to analyse sentiment before, during, and after protest demonstrations, data was collected with tweets related to a Black Lives Matter protest that took place in Amsterdam during the COVID-19 pandemic. All tweets have been manually labelled by a dedicated open-source intelligence (OSINT) team within the Netherlands police following an established protocol. Both the data and the protocol are available, and interesting for researchers in natural language processing, topic detection, sentiment analysis, and protests analysis. The developed labelling tool for the labelling process is publicly available.",2024,1919
A thermo-mechanical terrestrial model of Arctic coastal erosion,Jennifer Frederick and Alejandro Mota and Irina Tezaur and Diana Bull,"Although the Arctic comprises one-third of the global coastline and has some of the fastest eroding coasts, current tools for quantifying permafrost erosion are unable to explain the episodic, storm-driven erosion events that occur in this region. In this paper, we present a novel multi-physics finite element model for the numerical simulation of Arctic coastal permafrost degradation: the terrestrial component of the Arctic Coastal Erosion (ACE) model. This model is comprised of two main ingredients: (1) a solid mechanics model that calculates the three-dimensional (3D) stress, strain and displacement fields of the underlying permafrost developing in response to a frozen water content dependent plasticity model, and (2) a novel thermal model governing the 3D heat conduction and solid–liquid phase change occurring within the permafrost. These two physics sets are coupled via a sequential thermo-mechanical coupling scheme developed within the Albany LCM open-source finite element code. Unlike prior approaches, our modeling methodology enables failure from any allowable deformation (block failure, thermo-denudation, thermo-abrasion); moreover, failure modes develop from constitutive (rather than empirical) relationships inherent in the underlying finite element model. Elements are dynamically removed from the underlying finite element mesh so as to simulate transient permafrost erosion events. Our thermo-mechanical terrestrial model is evaluated on a pseudo-realistic problem in which a slice of permafrost is exposed to realistic oceanic and atmospheric forcing boundary condition data occurring at Drew Point, Alaska in July 2018.",2021,1920
Latent representation discretization for unsupervised text style generation,Yang Gao and Qianhui Liu and Yizhe Yang and Ke Wang,"Language models, such as BART and GPT, have been shown to be highly effective at producing quality headlines. However, without clear guidelines for what constitutes a particular writing style, they may generate text that does not meet the desired style criteria (i.e., attention-grabbing), even if the resulting text is grammatically correct and semantically coherent. In this study, we introduce a novel approach called Discretized Style Transfer (DST) for unsupervised style transfer. We argue that the textual style signal is inherently abstract and separate from the text itself. Therefore, we discretize the style representation into a discrete space, where each discrete point corresponds to a particular category of style that can be elicited by the syntactic structure. To evaluate the effectiveness of our approach, we propose two new automatic evaluation metrics along with several conventional criteria, especially STR metric is nearly 0.9 in TechST, 0.87 in GYAFC datasets, and the best PPL metrics. Furthermore, we conduct thorough human evaluations by directly measuring click-through rates as an indicator of attractiveness, showing our model receives the most popularity. Our results demonstrate that DST achieves competitive performance on style transfer and can effectively capture the written structure of specified styles. This approach has the potential to significantly enhance its relevance and is capable of generating appealing content.",2024,1921
Multi-triage: A multi-task learning framework for bug triage,Thazin Win Win Aung and Yao Wan and Huan Huo and Yulei Sui,"Assigning developers and allocating issue types are two important tasks in the bug triage process. Existing approaches tackle these two tasks separately, which is time-consuming due to repetition of effort and negating the values of correlated information between tasks. In this paper, a multi-triage model is proposed that resolves both tasks simultaneously via multi-task learning (MTL). First, both tasks can be regarded as a classification problem, based on historical issue reports. Second, performances on both tasks can be improved by jointly interpreting the representations of the issue report information. To do so, a text encoder and abstract syntax tree (AST) encoder are used to extract the feature representation of bug descriptions and code snippets accordingly. Finally, due to the disproportionate ratio of class labels in training datasets, the contextual data augmentation approach is introduced to generate syntactic issue reports to balance the class labels. Experiments were conducted on eleven open-source projects to demonstrate the effectiveness of this model compared with state-of-the-art methods.",2022,1922
Including widespread geometry schemas into Linked Data-based BIM applied to built heritage,Mathias Bonduel and Anna Wagner and Pieter Pauwels and Maarten Vergauwen and Ralf Klein,"A reliable data exchange often including geometry-related data between stakeholders is crucial in construction projects. In this regard, data exchange frameworks built on Linked Data principles are very promising for combining disparate data sets. However, existing proposals to combine geometry and Linked Data either demand dedicated applications or support only a limited number of common geometry schemas. If any existing geometry schema could be used in a Linked Data context, error-prone geometry conversions are avoided and stakeholders do not need to invest in new geometry engines. In this paper, the applicability of Resource Description Framework (RDF) literals for including a wide variety of existing geometry schemas is studied and applied in a built heritage context. The uniform linking pattern and related terminology of the Ontology for Managing Geometry are used to implement this approach. Subsequently, the File Ontology for Geometry formats and Geometry Metadata Ontology are developed to ease the reuse of linked geometry descriptions. The effectiveness of the entire data structure is demonstrated in a built heritage case study project. The receiving party is able to create successfully a coordinated view using a demo web application on shared, but disparate, RDF data sets containing geometry descriptions.",2020,1923
Preserving indomitable DDoS vitality through resurrection social hybrid botnet,Chit-Jie Chew and Ying-Chin Chen and Jung-San Lee and Chih-Lung Chen and Kuo-Yu Tsai,"As explosive development of the 5th generation wireless systems (5 G), the traces of botnet could be found in a cellular phone, intelligent equipment, cloud, and even Bitcoin network. Hackers are then able to launch the distributed denial of service (DDoS) attack via mastering these vulnerable devices. No doubt that the enterprise and government must suffer from the tremendous loss once specific services cannot function normally due to the DDoS. Inspired by a counterintuitive thinking, we actively predict a variant of hybrid botnet based on social network. The addressed prototype, resurrection social hybrid botnet (RSHB), could offer cyber researcher to formulate corresponding defense strategy against botnet. According to comprehensive simulations, the survivability has demonstrated that RSHB is much more dangerous than other botnets. Consequently, RSHB might be a coming sample for cyber engineer to explore the defense methodology.",2021,1924
NEU-chatbot: Chatbot for admission of National Economics University,Trung Thanh Nguyen and Anh Duc Le and Ha Thanh Hoang and Tuan Nguyen,"In the last few years, intelligent chatbot systems have been prevalent in various application fields, especially in education. Therefore, the demand for such online consulting services like chatbots is getting higher respectively. However, most communications between potential students and universities are performed manually, which is very time-consuming procedure, becoming a burden on the head of admissions. In this paper, we introduce an AI-based chatbot where students can instantly get daily updates of curriculum, admission for new students, tuition fees, IELTS writing task II score, etc. Our chatbot was developed by Deep Learning models, which are already integrated into the Rasa framework. We also proposed a rational pipeline for Vietnamese chatbots with our data preprocessing to obtain optimal accuracy and to avoid the overfitting of the model. Our model can detect more than fifty types of questions from users' input with an accuracy of 97.1% on test set. The chatbot was applied for National Economics University's official admission Fanpage on the Facebook platform, which is the most famous social network in Vietnam. This research shows detailed guidelines on how to build an AI chatbot from scratch, and the techniques we used, which can be applied to any language globally.",2021,1925
HOW TO THINK ABOUT AI… AND HOW TO LIVE WITH IT,,,2023,1926
An answer set programming-based implementation of epistemic probabilistic event calculus,Fabio Aurelio D'Asaro and Antonis Bikakis and Luke Dickens and Rob Miller,"We describe a general procedure for translating Epistemic Probabilistic Event Calculus (EPEC) action language domains into Answer Set Programs (ASP), and show how the Python-driven features of the ASP solver Clingo can be used to provide efficient computation in this probabilistic setting. EPEC supports probabilistic, epistemic reasoning in domains containing narratives that include both an agent's own action executions and environmentally triggered events. Some of the agent's actions may be belief-conditioned, and some may be imperfect sensing actions that alter the strengths of previously held beliefs. We show that our ASP implementation can be used to provide query answers that fully correspond to EPEC's own declarative, Bayesian-inspired semantics.",2024,1927
Characterizing Linux-based malware: Findings and recent trends,J. Carrillo-Mondéjar and J.L. Martínez and G. Suarez-Tangil,"Malware targeting interconnected infrastructures has surged in recent years. A major factor driving this phenomenon is the proliferation of large networks of poorly secured IoT devices. This is exacerbated by the commoditization of the malware development industry, in which tools can be readily obtained in specialized hacking forums or underground markets. However, despite the great interest in targeting this infrastructure, there is little understanding of what the main features of this type of malware are, or the motives of the criminals behind it, apart from the classic denial of service attacks. This is vital to modern malware forensics, where analyses are required to measure the trustworthiness of files collected at large during an investigation, but also to confront challenges posed by tech-savvy criminals (e.g., Trojan Horse Defense). In this paper, we present a comprehensive characterization of Linux-based malware. Our study is tailored to IoT malware and it leverages automated techniques using both static and dynamic analysis to classify malware into related threats. By looking at the most representative dataset of Linux-based malware collected by the community to date, we are able to show that our system can accurately characterize known threats. As a key novelty, we use our system to investigate a number of threats unknown to the community. We do this in two steps. First, we identify known patterns within an unlabeled dataset using a classifier trained with the labeled dataset. Second, we combine our features with a custom distance function to discover new threats by clustering together similar samples. We further study each of the unknown clusters by using state-of-the-art reverse engineering and forensic techniques and our expertise as malware analysts. We provide an in-depth analysis of what the most recent unknown trends are through a number of case studies. Among other findings, we observe that: i) crypto-mining malware is permeating the IoT infrastructure, ii) the level of sophistication is increasing, and iii) there is a rapid proliferation of new variants with minimal investment in infrastructure.",2020,1928
Innovators and Emulators: China and Russia’s Compounding Influence on Digital Censorship,Catherine Andrzejewski and Ana Horigoshi and Abigail I. Maher and Jonathan A. Solis,"This article examines the overlapping influence of China in Russia and five countries that have experienced democratic backsliding: Azerbaijan, Nicaragua, Serbia, Turkey, and Uganda. Drawing on a wide range of data sources, including media watchdog reports, key informant interviews, and quantitative data, the paper maps the portfolio of specific digital censorship tools – legislative, institutional, and technological—that governments in China and Russia use to censor their domestic digital content. Then the digital censorship tools in the five case study countries are documented to examine where their governments’ tactics overlapped with those of the Kremlin and Beijing. These case study countries differ in their levels of development and democracy, with Russia, China, and the West all vying for influence. Key findings include the importance of timing when installing a digital censorship regime, and that Uganda and Nicaragua stand out among the case study countries.",2023,1929
A simple model of edit activity in Wikipedia,Takashi Shimada and Fumiko Ogushi and János Török and János Kertész and Kimmo Kaski,"A simple dynamical model of collective edit activity of Wikipedia articles and their content evolution is introduced. Based on the recent empirical findings, each editor in the model is characterized by an ability to make content edit, i.e., improving the article by adding content and a tendency to make maintenance edit, i.e., dealing with formal aspects and maintaining the edit flow. In addition, each article is characterized by a level of maturity as compared to a potential quality needed to comprehensively cover its topic. This model is found to reproduce the basic structure of the bipartite network between editors and articles of Wikipedia. Furthermore, the relation between the model parameters of editors and articles and the metrics of those calculated from the emergent network turns out to be robust, i.e. depending only on the rate of the introduction of new articles to the editing activity. This results provides us a way to relate observations in the real data to the hidden characteristics of editors and articles. For the nestedness of the networks, systems with weighted parameter distribution gives better match to the empirical one. This suggests the importance of high-dimensional nature of the ability of editors and quality of articles in the real system.",2023,1930
A comparative analysis of knowledge injection strategies for large language models in the scholarly domain,Andrea Cadeddu and Alessandro Chessa and Vincenzo {De Leo} and Gianni Fenu and Enrico Motta and Francesco Osborne and Diego {Reforgiato Recupero} and Angelo Salatino and Luca Secchi,"In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.",2024,1931
STCA: Utilizing a spatio-temporal cross-attention network for enhancing video person re-identification,Amran Bhuiyan and Jimmy Xiangji Huang,"Video-based re-identification (ReID) is a crucial task in computer vision that draws increasing attention due to advances in deep learning (DL) and modern computational devices. Despite recent success with CNN architectures, single models (e.g., 2D-CNNs or 3D-CNNs) alone failed to leverage temporal information with spatial cues. This is due to uncontrolled surveillance scenarios and variable poses leading to inevitable misalignment of ROIs across the tracklets, which is accompanied by occlusion and motion blur. In this context, designing temporal and spatial cues for two different models and their combinations can be beneficial, considering the global of a video-tracklet. 3D-CNNs allow encoding of temporal information while 2D-CNNs extract spatial or appearance information. In this paper, we propose a Spatio-Temporal Cross Attention (STCA) network to utilize both 2D-CNNs and 3D-CNNs that calculate the cross attention mapping both from the layer of 3D-CNNs and 2D-CNNs along a person's trajectory to gate the following layers of 2D-CNNs; and highlight relevant appearance features for the person ReID. Given an input tracklet, the proposed cross attention (CA) is able to capture the salient regions that propagate throughout the tracklet to obtain the global view. This provides a spatio-temporal attention approach that can be dynamically aggregated with spatial features of 2D-CNNs to perform finer-grained recognition. Additionally, we exploit the advantage of utilizing cosine similarity while triplet sampling as well as for calculating the final recognition score. Experimental analyses on three challenging benchmark datasets indicate that integrating spatio-temporal cross attention into the state-of-the-art video ReID backbone CNN architecture allows for improving their recognition accuracy.",2022,1932
The Westermo network traffic data set,Per Erik Strandberg and David Söderman and Alireza Dehlaghi-Ghadim and Miguel Leon and Tijana Markovic and Sasikumar Punnekkat and Mahshid Helali Moghadam and David Buffoni,"There is a growing body of knowledge on network intrusion detection, and several open data sets with network traffic and cyber-security threats have been released in the past decades. However, many data sets have aged, were not collected in a contemporary industrial communication system, or do not easily support research focusing on distributed anomaly detection. This paper presents the Westermo network traffic data set, 1.8 million network packets recorded in over 90 minutes in a network built up of twelve hardware devices. In addition to the raw data in PCAP format, the data set also contains pre-processed data in the form of network flows in CSV files. This data set can support the research community for topics such as intrusion detection, anomaly detection, misconfiguration detection, distributed or federated artificial intelligence, and attack classification. In particular, we aim to use the data set to continue work on resource-constrained distributed artificial intelligence in edge devices. The data set contains six types of events: harmless SSH, bad SSH, misconfigured IP address, duplicated IP address, port scan, and man in the middle attack.",2023,1933
Information security of Internet things,Dmitry Bagay,"The development of methods of information protection against cyber threats is a priority and the most labor-intensive direction in the development of the IoT sector. The relevance of the topic is growing due to the growing user interest in the Internet of things. Basic for studying in this work was the traditional IoT architecture format, which consists of three ""slices"". This perception, network and application levels. Each ""cut"" characterizes its key problems in the field of information security. The most difficult part is the network layer. The arising difficulties are provoked by the features of the structure (multivariance of things, different methods of networks) and a high numerical index of objects. The Internet of things accumulates information data from a huge number of devices that have different formats and various characteristics. As a result, there are failures of DoS, which arise due to a heavy load on the network, as well as disruptions in the operation of programs.",2020,1934
Ontology for building permit authorities (OBPA) for advanced building permit processes,Judith Fauth and Sebastian Seiß,"Building permit processes lie on the divide between architecture, engineering, and construction (AEC) and public administration. To ensure consistent and effective digitization in building permitting, it is necessary to consider and merge both areas. Hence, for advanced building permit processes, foundations must be developed, which begins with understanding and formalizing building permit authorities’ organizational structures and processes. Therefore, this study developed an ontology that covers a semantic representation of a building permit authority along with a subprocess of the building permit process called the assignment process. The assignment process describes how and on what basis building applications are assigned to appropriate building officials. Proposing a semantic representation of the assignment process, tacit knowledge from previously conducted data sets was analyzed and implemented in the ontology. As a case study, a sample building permit authority was described and implemented in the ontology. On the one hand, the developed ontology serves as a basis for decision support for building permit processes, while on the other hand, it enables a fully automated assignment process in a building permit authority. The approach not only makes the assignment process more objective and transparent for all parties involved in the building permit process but also allows time and personnel capacities to be used in its other subprocesses.",2023,1935
Current practices and infrastructure for open data based research on occupant-centric design and operation of buildings,Mikkel B. Kjærgaard and Omid Ardakanian and Salvatore Carlucci and Bing Dong and Steven K. Firth and Nan Gao and Gesche Margarethe Huebner and Ardeshir Mahdavi and Mohammad Saiedur Rahaman and Flora D. Salim and Fisayo Caleb Sangogboye and Jens Hjort Schwee and Dawid Wolosiuk and Yimin Zhu,"Many new tools for improving the design and operation of buildings try to realize the potential of big data. In particular, data is an important element for occupant-centric design and operation as occupants’ presence and actions are affected by a high degree of uncertainty and, hence, are hard to model in general. For such research, data handling is an important challenge, and following an open science paradigm based on open data can increase efficiency and transparency of scientific work. This article reviews current practices and infrastructure for open data-driven research on occupant-centric design and operation of buildings. In particular, it covers related work on open data in general and for the built environment in particular, presents survey results for existing scientific practices, reviews technical solutions for handling data and metadata, discusses ethics and privacy protection and analyses principles for the sharing of open data. In summary, this study establishes the status quo and presents an outlook on future work for methods and infrastructures to support the open data community within the built environment.",2020,1936
The HITRAN2020 molecular spectroscopic database,I.E. Gordon and L.S. Rothman and R.J. Hargreaves and R. Hashemi and E.V. Karlovets and F.M. Skinner and E.K. Conway and C. Hill and R.V. Kochanov and Y. Tan and P. Wcisło and A.A. Finenko and K. Nelson and P.F. Bernath and M. Birk and V. Boudon and A. Campargue and K.V. Chance and A. Coustenis and B.J. Drouin and J.–M. Flaud and R.R. Gamache and J.T. Hodges and D. Jacquemart and E.J. Mlawer and A.V. Nikitin and V.I. Perevalov and M. Rotger and J. Tennyson and G.C. Toon and H. Tran and V.G. Tyuterev and E.M. Adkins and A. Baker and A. Barbe and E. Canè and A.G. Császár and A. Dudaryonok and O. Egorov and A.J. Fleisher and H. Fleurbaey and A. Foltynowicz and T. Furtenbacher and J.J. Harrison and J.–M. Hartmann and V.–M. Horneman and X. Huang and T. Karman and J. Karns and S. Kassi and I. Kleiner and V. Kofman and F. Kwabia–Tchana and N.N. Lavrentieva and T.J. Lee and D.A. Long and A.A. Lukashevskaya and O.M. Lyulin and V.Yu. Makhnev and W. Matt and S.T. Massie and M. Melosso and S.N. Mikhailenko and D. Mondelain and H.S.P. Müller and O.V. Naumenko and A. Perrin and O.L. Polyansky and E. Raddaoui and P.L. Raston and Z.D. Reed and M. Rey and C. Richard and R. Tóbiás and I. Sadiek and D.W. Schwenke and E. Starikova and K. Sung and F. Tamassia and S.A. Tashkun and J. {Vander Auwera} and I.A. Vasilenko and A.A. Vigasin and G.L. Villanueva and B. Vispoel and G. Wagner and A. Yachmenev and S.N. Yurchenko,"The HITRAN database is a compilation of molecular spectroscopic parameters. It was established in the early 1970s and is used by various computer codes to predict and simulate the transmission and emission of light in gaseous media (with an emphasis on terrestrial and planetary atmospheres). The HITRAN compilation is composed of five major components: the line-by-line spectroscopic parameters required for high-resolution radiative-transfer codes, experimental infrared absorption cross-sections (for molecules where it is not yet feasible for representation in a line-by-line form), collision-induced absorption data, aerosol indices of refraction, and general tables (including partition sums) that apply globally to the data. This paper describes the contents of the 2020 quadrennial edition of HITRAN. The HITRAN2020 edition takes advantage of recent experimental and theoretical data that were meticulously validated, in particular, against laboratory and atmospheric spectra. The new edition replaces the previous HITRAN edition of 2016 (including its updates during the intervening years). All five components of HITRAN have undergone major updates. In particular, the extent of the updates in the HITRAN2020 edition range from updating a few lines of specific molecules to complete replacements of the lists, and also the introduction of additional isotopologues and new (to HITRAN) molecules: SO, CH3F, GeH4, CS2, CH3I and NF3. Many new vibrational bands were added, extending the spectral coverage and completeness of the line lists. Also, the accuracy of the parameters for major atmospheric absorbers has been increased substantially, often featuring sub-percent uncertainties. Broadening parameters associated with the ambient pressure of water vapor were introduced to HITRAN for the first time and are now available for several molecules. The HITRAN2020 edition continues to take advantage of the relational structure and efficient interface available at www.hitran.org and the HITRAN Application Programming Interface (HAPI). The functionality of both tools has been extended for the new edition.",2022,1937
Device for measuring part adhesion in FFF process,Daniel Laumann and Dieter Spiehl and Edgar Dörsam,"The adhesion of parts to the build surface plays a central role in the Fused Filament Fabrication (FFF) process. Without sufficient adhesion, the part will deform (so called warping) due to thermal shrinkage, so that no defined geometries can be created. Nevertheless, there is no established method to measure the adhesion of printed parts and therefore it is not possible to targeted improve it. This article presents a measurement method based on the DIN EN 28510-1 standard and a corresponding test device which makes it possible to identify the optimum build surface for a filament and also to improve the process parameters in a targeted manner. The test device combines a FFF printer with a measuring unit so that all common filaments can be tested close to the process up to a processing temperature of 400 °C in the nozzle and around 150 °C on the build platform. The test device uses only open-source parts and software and costs about 1700€.",2022,1938
Leveraging Conflicting Constraints in Solving Vehicle Routing Problems,Sabino Francesco Roselli and Remco Vader and Martin Fabian and Knut Åkesson,"The Conflict-Free Electric Vehicle Routing Problem (CF-EVRP) is a combinatorial optimization problem of designing routes for vehicles to visit customers such that a cost function, typically the number of vehicles or the total travelled distance, is minimized. The CF-EVRP involves constraints such as time windows on the delivery to the customers, limited operating range of the vehicles, and limited capacity on the number of vehicles that a road segment can simultaneously accommodate. In previous work, the compositional algorithm ComSat was introduced and that solves the CF-EVRP by breaking it down into sub-problems and iteratively solve them to build an overall solution. Though ComSat showed good performance in general, some problems took significant time to solve due to the high number of iterations required to find solutions that satisfy the road segments’ capacity constraints. The bottleneck is the Path Changing Problem, i.e., the sub-problem of finding a new set of shortest paths to connect a subset of the customers, disregarding previously found shortest paths. This paper presents an improved version of the PathsChanger function to solve the Path Changing Problem that exploits the unsatisfiable core, i.e., information on which constraints conflict, to guide the search for feasible solutions. Experiments show faster convergence to feasible solutions compared to the previous version of PathsChanger.",2022,1939
Chapter Five - Intelligent agents in games: Review with an open-source tool,Matej Vitek and Peter Peer,"The field of artificial intelligence has come a long way in the last 50 years, and studies of its methods soon expanded to a field in which they are of great practical value—computer games. The concept of intelligent agents provides a much needed theoretical background for the comparison of various different approaches to intelligent, rational behavior of computer-controlled characters in games. By combining rationality with certain limitations to the capabilities of our agents, we can achieve behavior resembling that of a human player, which is also desirable in games. The goal of this article is to introduce various types of agents that are used in games, show how to implement meaningful, reasonable limitations to agent capabilities into the game world, and provide a freely available, open-source application for the comparison of such agents. Additionally, in this article we show that even the simplest agents can succeed in their tasks in certain task environments, whereas more difficult task environments often require a more sophisticated agent architecture. Our application consists of two task environments with nine agents in total but could easily be extended with additional task environments and agent implementations. In the end, we find the addition of goals into the agent architecture has the biggest impact on the agent's behavior and performance, whereas the state-based approach helps keep our implementation simple and compact.",2020,1940
An empirical study on bugs in JavaScript engines,Ziyuan Wang and Dexin Bu and Nannan Wang and Sijie Yu and Shanyi Gou and Aiyue Sun,"Context:
JavaScript is a prototype-based dynamic type scripting language. The correct running of a JavaScript program depends on the correctness of both the program and the JavaScript engine.
Objective:
An in-depth understanding of the characteristics of bugs in JavaScript engines can help detect and fix them.
Methods:
We conduct an empirical study on the bugs in three mainstream JavaScript engines: V8, SpiderMonkey, and Chakra. Such an empirical study involves 19,019 bug reports, 16,437 revisions, 805 test cases, and root causes of randomly selected 540 bugs.
Results:
(1) The Compiler and the DOM are the most buggy component in V8 and SpiderMonkey, respectively. Most of the source files contain only one bug. (2) The scales of the testing programs that reveal bugs are usually small. Most bug fixes involve only limited modifications since the number of modified source files and lines of code modified are small. (3) Most bugs can be fixed within half a year (80.33% for V8 and 91.9% for SpiderMonkey). Only 4.33% of SpiderMonkey bugs need more than a year to fix. Bugs in SpiderMonkey are usually fixed faster than bugs in V8. (4) High priority tends to be assigned to Infrastructure bugs in V8 and Release Automation bugs in SpiderMonkey. The duration of bugs is not strictly correlated with their priorities. (5) Semantic bugs are the most common root causes of bugs. And among semantic bugs, the processing bugs, missing features bugs and function call bugs are more than others.
Conclusion:
This study deepens our understanding of bugs in JavaScript engines, and empirical results could indicate some potential problems during the detecting and fixing of bugs in JavaScript engines, assist developers of JavaScript engines in improving their development quality, assist maintainers in detecting and fixing bugs more effectively, and suggest users of JavaScript evade potential risks.",2023,1941
Probabilistic design of optimal sequential decision-making algorithms in learning and control,Émiland Garrabé and Giovanni Russo,"This survey is focused on certain sequential decision-making problems that involve optimizing over probability functions. We discuss the relevance of these problems for learning and control. The survey is organized around a framework that combines a problem formulation and a set of resolution methods. The formulation consists of an infinite-dimensional optimization problem. The methods come from approaches to search optimal solutions in the space of probability functions. Through the lenses of this overarching framework we revisit popular learning and control algorithms, showing that these naturally arise from suitable variations on the formulation mixed with different resolution methods. A running example, for which we make the code available, complements the survey. Finally, a number of challenges arising from the survey are also outlined.",2022,1942
Futures of artificial intelligence through technology readiness levels,Fernando Martínez-Plumed and Emilia Gómez and José Hernández-Orallo,"Artificial Intelligence (AI) offers the potential to transform our lives in radical ways. However, the main unanswered questions about this foreseen transformation are its depth, breadth and timelines. To answer them, not only do we lack the tools to determine what achievements will be attained in the near future, but we even ignore what various technologies in present-day AI are capable of. Many so-called breakthroughs in AI are associated with highly-cited research papers or good performance in some particular benchmarks. However, research breakthroughs do not directly translate into a technology that is ready to use in real-world environments. In this paper, we present a novel exemplar-based methodology to categorise and assess several AI technologies, by mapping them onto Technology Readiness Levels (TRL) (representing their depth in maturity and availability). We first interpret the nine TRLs in the context of AI, and identify several categories in AI to which they can be assigned. We then introduce a generality dimension, which represents increasing layers of breadth of the technology. These two dimensions lead to the new readiness-vs-generality charts, which show that higher TRLs are achievable for low-generality technologies, focusing on narrow or specific abilities, while high TRLs are still out of reach for more general capabilities. We include numerous examples of AI technologies in a variety of fields, and show their readiness-vs-generality charts, serving as exemplars. Finally, we show how the timelines of several AI technology exemplars at different generality layers can help forecast some short-term and mid-term trends for AI.",2021,1943
Secure pharmaceutical supply chain using blockchain in IoT cloud systems,Mangala N. and Naveen D.R. and B. Eswara Reddy and Rajkumar Buyya and Venugopal K.R. and S.S. Iyengar and L.M. Patnaik,"Supply Chain Management (SCM) systems require time sequencing, coordination and tracking the movement of goods and processes. Internet of Things (IoT) and Blockchain technologies are useful to develop a secure automated SCM. IoT devices with in-built sensors and actuators help to keep track of the state, location, temperature or other parameters of an entity, and control the automation of routine as well as hazardous tasks. Blockchain technology supports time-stamping, authentication, process coordination, non-repudiation, commercial transactions, and also provides security for transactions and storage. The pharmaceutical SCM demands accurate, immediate and secure control system. Additionally, the supply chain process data from IoTs is stored and processed in Cloud by Analytics applications, for business planning and improvement. An efficient and secure IoT-Cloud-Blockchain based system for both SCM automation and analytics has been proposed in this work. It leverages a hierarchical IoT, Mist, Edge, Fog, Cloud computing (IMEFC) architecture to enhance Communication-Response-Compute-Security-Storage (CRCSS) in the system. Blockchain technology provides security for the SCM transactions and data. The efficiency of the Blockchain is measured in terms of upload time, download time and transaction fees for Bitcoin, Ethereum and Filecoin platforms. The Filecoin blockchain platform is quicker and cost-effective for larger file sizes, compared to Ethereum and Bitcoin, making it suitable for Pharmaceutical SCM systems.",2024,1944
"The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance",Mohammad Mahdi {Jahani Yekta},"Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.",2024,1945
An entity-weights-based convolutional neural network for large-sale complex knowledge embedding,Zhengdi Wang and Lvqing Yang and Zhenfeng Lei and Anwar {Ul Haq} and Defu Zhang and Shuangyuan Yang and Akindipe Olusegun Francis,"Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications (e.g., risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing. With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.",2022,1946
"Chapter 27 - Artificial intelligence, machine learning, and deep learning driving big data",Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia,"Artificial Intelligence (AI) is one of those technologies that seem to be expanding outward in every direction, and this expansion is driven by sheer volume of data that we are encountering in our daily routine life in these days, where technology is deviating from its tradition format to the more modern world of electronic gadget. In today's modern technological world, we are accumulating so much data in real time with speed of electron through the world of Internet of Things (IoT), and it comes to us in Omni-Direction space. Dealing with these data in the form of structured and unstructured, we have no choice except to turn to AI for assistant, in order to stay on top of all the information that is collected from these data for us to have a knowledge that allows us to consequently have power of right and accurate decision-making in real time in our daily routine as stakeholder. However, comes with AI needs its two other integrated components, namely Machine Learning (ML) and Deep Learning (DL) to be more effective to us as human being, where we are relying on Artificial Intelligence to run our day-to-day operation in very resilience form.",2022,1947
2 - The Present,Kim Holmberg,"In the second part the current altmetric research is presented. This part begins with an overview of scholarly communication on the web, its potential, and current status. This is followed by an overview of some of the sources of these new online metrics. The service providers or aggregators of altmetrics are briefly presented, followed by a discussion of the different stakeholders. There are many stakeholders connected to altmetrics, all of whom can use them somewhat differently and benefit from them in various ways. Some of the earlier research of altmetrics will be presented, research that has pushed the development of altmetrics forward and continues to push it as the web evolves and the way we use the web changes.",2016,1948
"Blockchain for healthcare systems: Architecture, security challenges, trends and future directions",Andrew J and Deva Priya Isravel and K. Martin Sagayam and Bharat Bhushan and Yuichi Sei and Jennifer Eunice,"Blockchain has become popular in recent times through its data integrity and wide scope of applications. It has laid the foundation for cryptocurrencies such as Ripple, Bitcoin, Ethereum, and so on. Blockchain provides a platform for decentralization and trust in various applications such as finance, commerce, IoT, reputation systems, and healthcare. However, prevailing challenges like scalability, resilience, security and privacy are yet to be overcome. Due to rigorous regulatory constraints such as HIPAA, blockchain applications in the healthcare industry usually require more stringent authentication, interoperability, and record sharing requirements. This article presents an extensive study to showcase the significance of blockchain technology from both application and technical perspectives for healthcare domain. The article discusses the features and use-cases of blockchain in different applications along with the healthcare domain interoperability. The detailed working operation of the blockchain and the consensus algorithms are presented in the context of healthcare. An outline of the blockchain architecture, platforms, and classifications are discussed to choose the right platform for healthcare applications. The current state-of-the-art research in healthcare blockchain and available blockchain based healthcare applications are summarized. Furthermore, the challenges and future research opportunities along with the performance evaluation metrics in realizing the blockchain technology for healthcare are presented to provide insight for future research. We also layout the various security attacks on the blockchain protocol with the classifications of threat models and presented a comparative analysis of the detection and protection techniques. Techniques to enhance the security and privacy of the blockchain network is also discussed.",2023,1949
Mining user reviews of COVID contact-tracing apps: An exploratory analysis of nine European apps,Vahid Garousi and David Cutting and Michael Felderer,"Context:
More than 78 countries have developed COVID contact-tracing apps to limit the spread of coronavirus. However, many experts and scientists cast doubt on the effectiveness of those apps. For each app, a large number of reviews have been entered by end-users in app stores.
Objective:
Our goal is to gain insights into the user reviews of those apps, and to find out the main problems that users have reported. Our focus is to assess the “software in society” aspects of the apps, based on user reviews.
Method:
We selected nine European national apps for our analysis and used a commercial app-review analytics tool to extract and mine the user reviews. For all the apps combined, our dataset includes 39,425 user reviews.
Results:
Results show that users are generally dissatisfied with the nine apps under study, except the Scottish (“Protect Scotland”) app. Some of the major issues that users have complained about are high battery drainage and doubts on whether apps are really working.
Conclusion:
Our results show that more work is needed by the stakeholders behind the apps (e.g., app developers, decision-makers, public health experts) to improve the public adoption, software quality and public perception of these apps.",2022,1950
Static analysis of cloud elasticity,Abel Garcia and Cosimo Laneve and Michael Lienhardt,"We propose a static analysis technique that computes upper bounds of virtual machine usages in a concurrent language with explicit acquire and release operations of virtual machines. In our language it is possible to delegate other (ad-hoc or third party) concurrent code to release virtual machines (by passing them as arguments of invocations). Our technique is modular and consists of (i) a type system associating programs with behavioural types that record relevant information for resource usage (creations, releases, and concurrent operations), (ii) a translation function that takes behavioural types and returns cost equations, and (iii) an automatic off-the-shelf solver for the cost equations. A soundness proof of the type system establishes the correctness of our technique with respect to the cost equations. We have experimentally evaluated our technique using a cost analysis solver and we report some results.",2017,1951
IoT data analytics in dynamic environments: From an automated machine learning perspective,Li Yang and Abdallah Shami,"With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.",2022,1952
Neural networks for intelligent multilevel control of artificial and natural objects based on data fusion: A survey,Tianxing Man and Vasily Yu. Osipov and Nataly Zhukova and Alexey Subbotin and Dmitry I. Ignatov,"Today the tasks of complex artificial and natural objects control have come to the fore in the majority of subject domains. The efficiency and effectiveness of solving these tasks directly depends of the efficiency and effectiveness of data fusion (DF). Data fusion methods are designed to integrate data from multiple sources and transform it in order to produce more consistent, accurate, and useful information than that provided by any individual data source. Although DF has been extensively studied for a considerable period of time it is still hardly applicable in practice in the processes of the control of the real world objects with complex structure and behavior as the data produced by the objects is, in the majority of cases, heterogeneous, multimodal, and imperfect, has huge volume. To ensure proper response to the changes in the state and behavior of the controlled objects that can be caused by both internal and external influencing factors the data should be processed with high accuracy and with minimum delays. Despite the importance of the tasks of complex objects control till now there are no researches that clarify to what extent the DF problem has been solved from the perspective of its application in the processes of objects control based on the data received from the objects. In the survey we define the requirements to DF in the interests of the control of complex artificial and natural objects, consider the structure of the multilevel process of intelligent object control, identify the neural networks that can be used in the control process for data fusion. Despite the wide capabilities of the existing NN we reveal that they still do not meet all the requirements to DF for complex objects control. Based on the analysis of NN architectures, we define requirements for advanced NN architectures and discuss future research directions. To facilitate our literature analyses, we also perform conceptual exploration of collected papers with lattices of closed itemsets and implications from Formal Concept Analysis and Data Mining used for knowledge processing in similar large-scale studies.",2024,1953
Detecting coherent explorations in SQL workloads,Verónika Peralta and Patrick Marcel and Willeme Verdeaux and Aboubakar Sidikhy Diakhaby,"This paper presents a proposal aiming at better understanding a workload of SQL queries and detecting coherent explorations hidden within the workload. In particular, our work investigates SQLShare (Jain et al., 2016), a database-as-a-service platform targeting scientists and data scientists with minimal database experience, whose workload was made available to the research community. According to the authors of (Jain et al., 2016), this workload is the only one containing primarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed this workload by extracting features that characterize SQL queries and we investigate three different machine learning approaches to use these features to separate sequences of SQL queries into meaningful explorations. The first approach is unsupervised and based only on similarity between contiguous queries. The second approach uses transfer learning to apply a model trained over a dataset where ground truth is available. The last approach uses weak labeling to predict the most probable segmentation from heuristics meant to label a training set. We ran several tests over various query workloads to evaluate and compare the proposed methods.",2020,1954
Why is my code change abandoned?,Qingye Wang and Xin Xia and David Lo and Shanping Li,"Context: Software developers contribute numerous changes every day to the code review systems. However, not all submitted changes are merged into a codebase because they might not pass the code review process. Some changes would be abandoned or be asked for resubmission after improvement, which results in more workload for developers and reviewers, and more delays to deliverables. Objective: To understand the underlying reasons why changes are abandoned, we conduct an empirical study on the code review of four open source projects (Eclipse, LibreOffice, OpenStack, and Qt). Method: First, we manually analyzed 1459 abandoned changes. Second, we leveraged the open card sorting method to label these changes with reasons why they were abandoned, and we identified 12 categories of reasons. Next, we further investigated the frequency distribution of the categories across projects. Finally, we studied the relationship between the categories and time-to-abandonment. Results: Our findings include the following: (1) Duplicate changes are the majority of the abandoned changes; (2) the frequency distribution of abandoned changes across the 12 categories is similar for the four open source projects; (3) 98.39% of the changes are abandoned within a year. Conclusion: Our study concluded the root causes of abandoned changes, which will help developers submit high-quality code changes.",2019,1955
Global Flow Table: A convincing mechanism for security operations in SDN,Xiaofeng Qiu and Kai Zhang and Qiuzheng Ren,"One of the key challenges of network security is that security middle boxes, such as firewalls and Intrusion Detection Systems (IDSs), only have local view of the network. This lowers the efficiency of security detection and makes it difficult to locate the sources of the threats. There have been growing demands for security operations and appliances that are aware of the distribution and behavior of flows in the whole network; logically centralized control ability of Software-Defined Network (SDN) makes it possible for the network controller to acquire the global view of the network. In this paper, we propose a mechanism named Global Flow Table (GFT) which can provide security appliances and operators with paths of all the flows in SDN network, in addition to their sources, destinations, setup and terminate time, traffic volume and directions. A weak vertex cover based GFT algorithm which sacrifices less than 5% accuracy is also provided to improve scalability. Tests with different network topologies of cloud computing center and enterprise networks show promising performance. Utilizing the Global Flow Table, we built several applications to illustrate how GFT could benefit the security operations.",2017,1956
Bridging the gap between awareness and trust in globally distributed software teams,Erik H. Trainer and David F. Redmiles,"Trust remains a key challenge for globally distributed teams despite decades of research. Awareness, a key component of collaboration, has even more research around it. However, detailed accounts of the interrelationship of awareness and trust are still lacking in the literature, particularly in the setting of software teams. The gap we seek to fill with this article is to examine how software tool support for awareness can engender trust among globally distributed software developers. We highlight qualitative results from a previous and extensive field study that shows how trust is still a problem in contemporary teams. These results motivate a specific examination of how developers form attributions of one another. We describe a collection of visualization widgets designed to address the specific issues found in the field. To evaluate their effectiveness, we performed a controlled laboratory study with 28 students and 12 professional software developers who used these visualizations collected into a tool environment called Theseus. The results show that in general, participants using the visualizations make more accurate attributions, and their perceived trustworthiness of their remote teammates more accurately reflects actual circumstances. We conclude with a discussion of the implications of our results for theory and practice.",2018,1957
Pathology Visions 2022 Overview,,,2023,1958
An emotion and cognitive based analysis of mental health disorders from social media data,Ana-Sabina Uban and Berta Chulvi and Paolo Rosso,"Mental disorders can severely affect quality of life, constitute a major predictive factor of suicide, and are usually underdiagnosed and undertreated. Early detection of signs of mental health problems is particularly important, since unattended, they can be life-threatening. This is why a deep understanding of the complex manifestations of mental disorder development is important. We present a study of mental disorders in social media, from different perspectives. We are interested in understanding whether monitoring language in social media could help with early detection of mental disorders, using computational methods. We developed deep learning models to learn linguistic markers of disorders, at different levels of the language (content, style, emotions), and further try to interpret the behavior of our models for a deeper understanding of mental disorder signs. We complement our prediction models with computational analyses grounded in theories from psychology related to cognitive styles and emotions, in order to understand to what extent it is possible to connect cognitive styles with the communication of emotions over time. The final goal is to distinguish between users diagnosed with a mental disorder and healthy users, in order to assist clinicians in diagnosing patients. We consider three different mental disorders, which we analyze separately and comparatively: depression, anorexia, and self-harm tendencies.",2021,1959
Secure HPC: A workflow providing a secure partition on an HPC system,Hendrik Nolte and Nicolai Spicher and Andrew Russel and Tim Ehlers and Sebastian Krey and Dagmar Krefting and Julian Kunkel,"Driven by the progress of data and compute-intensive methods in various scientific domains, there is an increasing demand from researchers working with highly sensitive data to have access to the necessary computational resources to be able to adapt those methods in their respective fields. To satisfy the computing needs of those researchers cost-effectively, it is an open quest to integrate reliable security measures on existing High Performance Computing (HPC) clusters. The fundamental problem with securely working with sensitive data is, that HPC systems are shared systems that are typically trimmed for the highest performance—not for high security. For instance, there are commonly no additional virtualization techniques employed, thus, users typically have access to the host operating system. Since new vulnerabilities are being continuously discovered, solely relying on the traditional Unix permissions is not secure enough. In this paper, we discuss Secure HPC, a workflow allowing users to transfer, store and analyze data with the highest privacy requirements. Our contributions are the design of a multi-node secure workflow with parallel I/O, a strict security model enforced by the system and network features, and lastly the demonstration of a medical use case. In our experiments, we see an advantage in the asynchronous execution of IO requests in dm_crypt, while reaching 80% of the ideal performance. When comparing eCryptFS with GoCryptFS as two representative filesystem-level encryption stacks, eCryptFS was twice as fast. In a real use case, we observed on average 97% of the native performance.",2023,1960
Machine-Learning-enhanced systemic risk measure: A Two-Step supervised learning approach,Ruicheng Liu and Chi Seng Pun,"This paper explores ways to improve the existing systemic risk measures by incorporating machine learning algorithms into the measurement. We aim to overcome the shortcomings of existing methods that rely on restricted modeling and are difficult to tap into various data resources. To this end, this paper unifies a dynamic quantification framework for systemic risk and links it to a two-step supervised learning problem, which allows for hierarchical structure of the systemic event and the return dependence. We leverage the generalization and predictive powers of machine learning to statistically model the tail events and the co-movements of the equity returns during the shocks to the macro-economy. Our results show that most machine learning algorithms enhance the systemic risk measure’s predictive power. Numerous comparative and sensitivity backtesting studies for United States and Hong Kong markets are conducted, from which we recommend the best machine learning algorithm for systemic risk measurement.",2022,1961
"A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions",Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan,"This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.",2024,1962
"A Comprehensive Survey on Attacks, Security Issues and Blockchain Solutions for IoT and IIoT",Jayasree Sengupta and Sushmita Ruj and Sipra {Das Bit},"In recent years, the growing popularity of Internet of Things (IoT) is providing a promising opportunity not only for the development of various home automation systems but also for different industrial applications. By leveraging these benefits, automation is brought about in the industries giving rise to the Industrial Internet of Things (IIoT). IoT is prone to several cyberattacks and needs challenging approaches to achieve the desired security. Moreover, with the emergence of IIoT, the security vulnerabilities posed by it are even more devastating. Therefore, in order to provide a guideline to researchers, this survey primarily attempts to classify the attacks based on the objects of vulnerability. Subsequently, each of the individual attacks is mapped to one or more layers of the generalized IoT/IIoT architecture followed by a discussion on the countermeasures proposed in literature. Some relevant real-life attacks for each of these categories are also discussed. We further discuss the countermeasures proposed for the most relevant security threats in IIoT. A case study on two of the most important industrial IoT applications is also highlighted. Next, we explore the challenges brought by the centralized IoT/IIoT architecture and how blockchain can effectively be used towards addressing such challenges. In this context, we also discuss in detail one IoT specific Blockchain design known as Tangle, its merits and demerits. We further highlight the most relevant Blockchain-based solutions provided in recent times to counter the challenges posed by the traditional cloud-centered applications. The blockchain-related solutions provided in the context of two of the most relevant applications for each of IoT and IIoT is also discussed. Subsequently, we design a taxonomy of the security research areas in IoT/IIoT along with their corresponding solutions. Finally, several open research directions relevant to the focus of this survey are identified.",2020,1963
Medical deep learning—A systematic meta-review,Jan Egger and Christina Gsaxner and Antonio Pepe and Kelsey L. Pomykala and Frederic Jonske and Manuel Kurz and Jianning Li and Jens Kleesiek,"Deep learning has remarkably impacted several different scientific disciplines over the last few years. For example, in image processing and analysis, deep learning algorithms were able to outperform other cutting-edge methods. Additionally, deep learning has delivered state-of-the-art results in tasks like autonomous driving, outclassing previous attempts. There are even instances where deep learning outperformed humans, for example with object recognition and gaming. Deep learning is also showing vast potential in the medical domain. With the collection of large quantities of patient records and data, and a trend towards personalized treatments, there is a great need for automated and reliable processing and analysis of health information. Patient data is not only collected in clinical centers, like hospitals and private practices, but also by mobile healthcare apps or online websites. The abundance of collected patient data and the recent growth in the deep learning field has resulted in a large increase in research efforts. In Q2/2020, the search engine PubMed returned already over 11,000 results for the search term ‘deep learning’, and around 90% of these publications are from the last three years. However, even though PubMed represents the largest search engine in the medical field, it does not cover all medical-related publications. Hence, a complete overview of the field of ‘medical deep learning’ is almost impossible to obtain and acquiring a full overview of medical sub-fields is becoming increasingly more difficult. Nevertheless, several review and survey articles about medical deep learning have been published within the last few years. They focus, in general, on specific medical scenarios, like the analysis of medical images containing specific pathologies. With these surveys as a foundation, the aim of this article is to provide the first high-level, systematic meta-review of medical deep learning surveys.",2022,1964
"Fine-tuning Deep RL with Gradient-Free Optimization⁎⁎This work is part of the research programme Deep Learning for Robust Robot Control (DL-Force) with project number 656.000.003, which is (partly) financed by the Netherlands Organisation for Scientific Research (NWO).",Tim {de Bruin} and Jens Kober and Karl Tuyls and Robert Babuška,"Deep reinforcement learning makes it possible to train control policies that map high-dimensional observations to actions. These methods typically use gradient-based optimization techniques to enable relatively efficient learning, but are notoriously sensitive to hyperparameter choices and do not have good convergence properties. Gradient-free optimization methods, such as evolutionary strategies, can offer a more stable alternative but tend to be much less sample efficient. In this work we propose a combination, using the relative strengths of both. We start with a gradient-based initial training phase, which is used to quickly learn both a state representation and an initial policy. This phase is followed by a gradient-free optimization of only the final action selection parameters. This enables the policy to improve in a stable manner to a performance level not obtained by gradient-based optimization alone, using many fewer trials than methods using only gradient-free optimization. We demonstrate the effectiveness of the method on two Atari games, a continuous control benchmark and the CarRacing-v0 benchmark. On the latter we surpass the best previously reported score while using significantly fewer episodes.",2020,1965
MSIDN: Mitigation of Sophisticated Interest flooding-based DDoS attacks in Named Data Networking,Ahmed Benmoussa and Abdou el Karim Tahari and Chaker Abdelaziz Kerrache and Nasreddine Lagraa and Abderrahmane Lakas and Rasheed Hussain and Farhan Ahmad,"Named Data Networking (NDN) is a promising candidate for Future Internet Architecture (FIA), where the focus of communication is the content itself rather than the source of the requested content. NDN is one of the implementations of Information-Centric Networking (ICN). Among other salient features, NDN provides intrinsic security where security is provided to the content directly, rather than securing the communication channel. However, despite promising features offered by NDN, it is still susceptible to various Denial of Service (DoS) attacks, mainly Interest Flooding Attacks (IFA). Various mitigation solutions exist in the literature; however, legitimate users and their traffic are usually affected by these solutions. In this paper, we propose a lightweight mechanism called MSIDN, to mitigate sophisticated interest flooding-based DoS and Distributed DoS (DDoS) attacks in NDN. MSIDN aims to mitigate attacks at the source of communication without affecting the legitimate users. MSIDN relies on data producers’ feedback which is used by the routers to employ precise rate-limiting and block the attackers. Extensive simulations were conducted to evaluate the proposed MSIDN in terms of its robustness during various attack scenarios, dealing with malicious traffic without affecting the legitimate requests, and mitigating attacks at the source side of the communication.",2020,1966
Do altmetrics assess societal impact in a comparable way to case studies? An empirical test of the convergent validity of altmetrics based on data from the UK research excellence framework (REF),Lutz Bornmann and Robin Haunschild and Jonathan Adams,"Altmetrics have been proposed as a way to assess the societal impact of research. Although altmetrics are already in use as impact or attention metrics in different contexts, it is still not clear whether they really capture or reflect societal impact. This study is based on altmetrics, citation counts, research output and case study data from the UK Research Excellence Framework (REF), and peers’ REF assessments of research output and societal impact. We investigated the convergent validity of altmetrics by using two REF datasets: publications submitted as research output (PRO) to the REF and publications referenced in case studies (PCS). Case studies, which are intended to demonstrate societal impact, should cite the most relevant research papers. We used the MHq’ indicator for assessing impact – an indicator which has been introduced for count data with many zeros. The results of the first part of the analysis show that news media as well as mentions on Facebook, in blogs, in Wikipedia, and in policy-related documents have higher MHq’ values for PCS than for PRO. Thus, the altmetric indicators seem to have convergent validity for these data. In the second part of the analysis, altmetrics have been correlated with REF reviewers’ average scores on PCS. The negative or close to zero correlations question the convergent validity of altmetrics in that context. We suggest that they may capture a different aspect of societal impact (which can be called unknown attention) to that seen by reviewers (who are interested in the causal link between research and action in society).",2019,1967
Survey of publicly available reports on advanced persistent threat actors,Antoine Lemay and Joan Calvet and François Menet and José M. Fernandez,"The increase of cyber attacks for the purpose of espionage is a growing threat. Recent examples, such as hacking of the Democratic National Committee and indicting by the FBI of Chinese military personnel for cyber economic espionage, are testaments of the severity of the problem. Unfortunately, research on the topic of Advanced Persistent Threats (APT) is complicated due to the fact that information is fragmented across a large number of Internet resources. This paper aims at providing a comprehensive survey of open source publications related to APT actors and their activities, focusing on the APT activities, rather than research on defensive or detective measures. It is intended to serve as a quick reference on the state of the knowledge of APT actors, where interested researchers can find what primary sources are most relevant to their research. The paper covers publications related to around 40 APT groups from multiple regions across the globe. A short summary of the main findings of each publication is presented.",2018,1968
Oxidative stress from DGAT1 oncoprotein inhibition in melanoma suppresses tumor growth when ROS defenses are also breached,Daniel J. Wilcock and Andrew P. Badrock and Chun W. Wong and Rhys Owen and Melissa Guerin and Andrew D. Southam and Hannah Johnston and Brian A. Telfer and Paul Fullwood and Joanne Watson and Harriet Ferguson and Jennifer Ferguson and Gavin R. Lloyd and Andris Jankevics and Warwick B. Dunn and Claudia Wellbrock and Paul Lorigan and Craig Ceol and Chiara Francavilla and Michael P. Smith and Adam F.L. Hurlstone,"Summary
Dysregulated cellular metabolism is a cancer hallmark for which few druggable oncoprotein targets have been identified. Increased fatty acid (FA) acquisition allows cancer cells to meet their heightened membrane biogenesis, bioenergy, and signaling needs. Excess FAs are toxic to non-transformed cells but surprisingly not to cancer cells. Molecules underlying this cancer adaptation may provide alternative drug targets. Here, we demonstrate that diacylglycerol O-acyltransferase 1 (DGAT1), an enzyme integral to triacylglyceride synthesis and lipid droplet formation, is frequently up-regulated in melanoma, allowing melanoma cells to tolerate excess FA. DGAT1 over-expression alone transforms p53-mutant zebrafish melanocytes and co-operates with oncogenic BRAF or NRAS for more rapid melanoma formation. Antagonism of DGAT1 induces oxidative stress in melanoma cells, which adapt by up-regulating cellular reactive oxygen species defenses. We show that inhibiting both DGAT1 and superoxide dismutase 1 profoundly suppress tumor growth through eliciting intolerable oxidative stress.",2022,1969
"Design, implementation, and automation of a risk management approach for man-at-the-End software protection",Cataldo Basile and Bjorn {De Sutter} and Daniele Canavese and Leonardo Regano and Bart Coppens,"The last years have seen an increase in Man-at-the-End (MATE) attacks against software applications, both in number and severity. However, software protection, which aims at mitigating MATE attacks, is dominated by fuzzy concepts and security-through-obscurity. This paper presents a rationale for adopting and standardizing the protection of software as a risk management process according to the NIST SP800-39 approach. We examine the relevant constructs, models, and methods needed for formalizing and automating the activities in this process in the context of MATE software protection. We highlight the open issues that the research community still has to address. We discuss the benefits that such an approach can bring to all stakeholders. In addition, we present a Proof of Concept (PoC) decision support system that instantiates many of the discussed construct, models, and methods and automates many activities in the risk analysis methodology for the protection of software. Despite being a prototype, the PoC’s validation with industry experts indicated that several aspects of the proposed risk management process can already be formalized and automated with our existing toolbox and that it can actually assist decision making in industrially relevant settings.",2023,1970
Chapter 6 - Supervised Learning Based Detection of Malware on Android,F. Tchakounté and F. Hayata,"This chapter aims to present a new approach for detecting Android malware by relying permissions and supervised learning techniques. For that, we present security and its flaws in the Android system. Then we present concepts around machine learning and how they can be used for malware detection in general. We discuss works using permissions as key feature for the characterization of applications to detect malicious behavior. We present a detection system combining the proportion of requested permissions and risks induced on resources. This system requires the user to specify resources to protect and inform in an understandable way, activities performed in background with those permissions. We pass through some graphical interfaces of the implementation, then elucidate results concerning detection and prediction performance with the support of learning algorithms. We compare these results against well-known antiviruses and related solutions on the same collected datasets of malicious and benign applications. It is revealed that our system outperforms most of them and it is able to detect zero-day malware. Therefore it constitutes an interesting step forward to help users understanding the risks induced on resources and to help them detecting malware.",2017,1971
Deep learning for neuroimaging-based diagnosis and rehabilitation of Autism Spectrum Disorder: A review,Marjane Khodatars and Afshin Shoeibi and Delaram Sadeghi and Navid Ghaasemi and Mahboobeh Jafari and Parisa Moridian and Ali Khadem and Roohallah Alizadehsani and Assef Zare and Yinan Kong and Abbas Khosravi and Saeid Nahavandi and Sadiq Hussain and U. Rajendra Acharya and Michael Berk,"Accurate diagnosis of Autism Spectrum Disorder (ASD) followed by effective rehabilitation is essential for the management of this disorder. Artificial intelligence (AI) techniques can aid physicians to apply automatic diagnosis and rehabilitation procedures. AI techniques comprise traditional machine learning (ML) approaches and deep learning (DL) techniques. Conventional ML methods employ various feature extraction and classification techniques, but in DL, the process of feature extraction and classification is accomplished intelligently and integrally. DL methods for diagnosis of ASD have been focused on neuroimaging-based approaches. Neuroimaging techniques are non-invasive disease markers potentially useful for ASD diagnosis. Structural and functional neuroimaging techniques provide physicians substantial information about the structure (anatomy and structural connectivity) and function (activity and functional connectivity) of the brain. Due to the intricate structure and function of the brain, proposing optimum procedures for ASD diagnosis with neuroimaging data without exploiting powerful AI techniques like DL may be challenging. In this paper, studies conducted with the aid of DL networks to distinguish ASD are investigated. Rehabilitation tools provided for supporting ASD patients utilizing DL networks are also assessed. Finally, we will present important challenges in the automated detection and rehabilitation of ASD and propose some future works.",2021,1972
Visual analytics for identifying product disruptions and effects via social media,Araceli Zavala and Jose Emmanuel Ramirez-Marquez,"In the last decade, there have been high profile product safety events that captured public attention on social networks. Researchers have attempted various studies on consumers' reaction to product recalls but hardly any studies were conducted to find out a way to identify recalls by using users' comments specifically on social media. The earlier a company can detect a product disruption, the more a company can do in preparation to reduce its impact. In this paper, we propose a visualization framework capable of identifying a possible product recall via social networks, like Facebook or Twitter. Customers' comments found in data that express a negative sentiment are considered as non-conforming observations and plotted on a p-chart, which helps to identify when the proportion of negative comments get out of control and, as a result, a company can diminish the response time. To check its viability, we conducted three event studies of well-known companies that have experienced product recalls. The results show that customers’ negative sentiments could be monitored with the aim of predicting when a product might necessitate a recall as well as reducing decision-makers response time.",2019,1973
Chapter 1 - Introduction to artificial intelligence for cardiovascular clinicians,Anthony C. Chang and Alfonso Limon,"The impressive gains in deep learning (DL) started in 2012 and its successful utilization in image interpretation have led to the current momentum for artificial intelligence (AI) awareness and adoption. In 2016, Google DeepMind's AlphaGo software soundly defeated the best human Go champion Lee Sedol to introduce the capability of DL outside of image interpretation. More recently, there have been impressive exponential advances in natural language processing with transformer tools such as GPT-3, GPT-4, and now ChatGPT. DeepMind and its AlphaFold AI tool has been able to predict the three-dimensional (3D) structure of proteins since 2021 and was Science magazine's “Breakthrough of the Year.” All of these AI accomplishments heralded the recent new era in AI. Major universities with AI departments (such as Stanford, MIT, and Carnegie Mellon) and technology giants (such as IBM, Apple, Facebook, and Microsoft in the United States as well as other large companies such as Baidu, Alibaba, and Tencent [BAT] in China) are all fervidly exploring real-life applications of AI. There is also a movement to democratize AI so that “no-code platforms” can accommodate people who do not know how to code [1].",2024,1974
Deep learning in exchange markets,Rui Gonçalves and Vitor Miguel Ribeiro and Fernando Lobo Pereira and Ana Paula Rocha,"We present the implementation of a short-term forecasting system of price movements in exchange markets using market depth data and a systematic procedure to enable a fully automated trading system. Three types of Deep Learning (DL) Neural Network (NN) methodologies are trained and tested: Deep NN Classifier (DNNC), Long Short-Term Memory (LSTM) and Convolutional NN (CNN). Although the LSTM is more suitable for multivariate time series analysis from a theoretical point of view, test results indicate that the CNN has on average the best predictive power in the case study under analysis, which is the UK to Win Horse Racing market during pre-live stage in the world’s most relevant betting exchange. Implications from the generalized use of automated trading systems in betting exchange markets are discussed.",2019,1975
GeniGraph: A genetic-based novel security defense resource allocation method for interdependent systems modeled by attack graphs,Mohammad Ryiad Al-Eiadeh and Mustafa Abdallah,"We design a resource allocation framework for securing interdependent systems managed by multiple defenders. Our framework models these multi-defender interdependent systems with the notion of attack graphs. We propose three defense scenarios that are derived from the top attack paths that defenders predict, based on their system knowledge, which attackers may consider to launch their attacks. Furthermore, we propose a defense method with low sensitivity to the number of concurrent attacks, based on a graph-theoretical notion known as the Markov random field (MRF). We elucidate the advantages gained from our decision-making framework through comprehensive evaluation experiments on fourteen attack graphs (that includes multiple real-world interdependent systems). In our evaluation, we compare different defense scenarios and provide information about the most effective resource allocation approach against each attack scenario. In particular, we quantify the level of security improvement under our defense methods compared to three well-known resource allocation algorithms. Our experimental results show that our framework surpasses these resource allocation algorithms. In particular, our proposed defense leads to an average relative reduction in the expected security cost of 72% under equal initial investments on all edges. Moreover, it leads to an average relative reduction in the expected security cost of 78% under random initial investments on all edges. Under high security budget, our proposed defense approach has a relative reduction above 94% for 10 of the 14 attack graphs. These results signify notable enhancements in security resource allocation which contributes to improved security decision-making.",2024,1976
Towards ESCO 4.0 – Is the European classification of skills in line with Industry 4.0? A text mining approach,Filippo Chiarello and Gualtiero Fantoni and Terence Hogarth and Vito Giordano and Liga Baltina and Irene Spada,"ESCO is a multilingual classification of Skills, Competences, Qualifications, and Occupations created by the European Commission to improve the supply of information on skills demand in the labour market. It is designed to assist individuals, employers, universities and training providers by giving them up to date and standardized information on skills. Rapid technological change means that ESCO needs to be updated in a timely manner. Evidence is presented here of how text-mining techniques can be applied to the analysis of data on emerging skill needs arising from Industry 4.0 to ensure that ESCO provides information which is current. The alignment between ESCO and Industry 4.0 technological trends is analysed. Using text mining techniques, information is extracted on Industry 4.0 technologies from: (i) two versions of ESCO (v1.0 - v1.1.); and (ii) from the 4.0 related scientific literature. These are then compared to identify potential data gaps in ESCO. The findings demonstrate that text mining applied on scientific literature to extract technology trends, can help policy makers to provide more up-to-date labour market intelligence.",2021,1977
Robust Decentralized Switching Control of UAVs using UWB-based Localization and Cooperation⁎⁎The authors were partially supported by NSF Grant 1629949.,Joao P. Jansch-Porto and Geir E. Dullerud,"In this paper, we implement a switched decentralized controller, along with a proposed communication protocol, to control a nested multi-agent system without the need for a centralized processing node. More specifically, we apply a recently developed method for switched systems synthesis, which gives exact conditions for existence of a block-lower triangular path-dependent controller with L2 induced norm performance. The synthesis conditions are given in the form of a semidefinite program (SDP), which is computed offline for a predefined switching sequence. Each robot is equipped with a ultra-wideband (UWB) unit, which allows it to both estimate its position and communicate with other robots.",2020,1978
DDoS attack resisting authentication protocol for mobile based online social network applications,Munmun Bhattacharya and Sandip Roy and Ashok Kumar Das and Samiran Chattopadhyay and Soumya Banerjee and Ankush Mitra,"The rapid development of smartphone technology and the Internet services in mobile devices facilitates easy access to online social networking (OSN) sites anytime, anywhere. At the same time, this allures the adversaries to exploit the OSNs as a soft target for easy execution of various attacks that can quickly spread to a large number of users. In distributed denial-of-service (DDoS) attacks, an adversary aims to overwhelm the normal traffic of a targeted server with a flood of fake login messages so that the associated Internet service or website turns inoperable. In this paper, we propose a secure and lightweight authentication scheme (PRDoS) that resists DDoS and other security attacks in mobile OSN environments. We provide a multi-faceted solution towards the remedy of DDoS attacks in the OSN environment. After a certain threshold, the scheme discards further user login attempts and blocks an adversary who intends to overload the network server. We use the pre-loaded shadow identity and emergency key pairs, and a key-refilling strategy that rebuilds the essential synchronization between a blocked naive user and the OSN server. This technique restores the intended un-linkability property of the protocol. Using NS3 simulation, we study the impact of DDoS attackers on network throughput and network delay. Moreover, we validate and compare the proposed scheme against state-of-the-art solutions using the real attacks and benign datasets. We use the Canadian Institute for Cybersecurity (CIC) DoS dataset 2017, which is generated by capturing the normal and DoS attack packets separately with subsequent pre-processed for testing. We also use the machine learning (ML) algorithms, such as K-Nearest Neighbor (KNN), Gaussian Naive Bayes, and Multilayer Perceptron (MLP) to demonstrate the performance of the proposed solution in a practical attack detection scenario. We observe that these algorithms provide 97.05%, 95.48%, and 96.6% DDoS attack detection accuracy, respectively.",2022,1979
"A survey on blockchain, SDN and NFV for the smart-home security",N’guessan Yves-Roland Douha and Monowar Bhuyan and Shigeru Kashihara and Doudou Fall and Yuzo Taenaka and Youki Kadobayashi,"Due to millions of loosely coupled devices, the smart-home security is gaining the attention of industry professionals, attackers, and academic researchers. The smart home is a typical home where many sensors, actuators, and IoT devices are used to automate home users’ daily activities. Although a smart home provides comfort, safety, and satisfaction to users, it opens up multiple challenging security issues when automating and offering intelligent services. Recent studies have investigated not only blockchain but SDN and NFV to address these challenges. We present a comprehensive survey on blockchain, SDN, and NFV for smart-home security. The paper also proposes a new architecture of the smart-home security. First, we describe the features of the smart home and its current security issues. Next, we outline the characteristics of blockchain, SDN, and NFV, including their contribution to improving the smart-home security. While SDN enhances the management and access control of the home network by providing a programmable controller to home nodes, NFV implements the functions of network appliances (e.g., network monitoring, firewall) as virtual machines and ensures the high availability of the network. Blockchain reinforces IoT data’s privacy, integrity, and security and improves the trust in transactions among untrusted IoT devices. Finally, we discuss open issues and challenges in the field and propose recommendations towards high-level security for the smart home.",2022,1980
Fighting post-truth using natural language processing: A review and open challenges,Estela Saquete and David Tomás and Paloma Moreda and Patricio Martínez-Barco and Manuel Palomar,"Post-truth is a term that describes a distorting phenomenon that aims to manipulate public opinion and behavior. One of its key engines is the spread of Fake News. Nowadays most news is rapidly disseminated in written language via digital media and social networks. Therefore, to detect fake news it is becoming increasingly necessary to apply Artificial Intelligence (AI) and, more specifically Natural Language Processing (NLP). This paper presents a review of the application of AI to the complex task of automatically detecting fake news. The review begins with a definition and classification of fake news. Considering the complexity of the fake news detection task, a divide-and-conquer methodology was applied to identify a series of subtasks to tackle the problem from a computational perspective. As a result, the following subtasks were identified: deception detection; stance detection; controversy and polarization; automated fact checking; clickbait detection; and, credibility scores. From each subtask, a PRISMA compliant systematic review of the main studies was undertaken, searching Google Scholar. The various approaches and technologies are surveyed, as well as the resources and competitions that have been involved in resolving the different subtasks. The review concludes with a roadmap for addressing the future challenges that have emerged from the analysis of the state of the art, providing a rich source of potential work for the research community going forward.",2020,1981
Survey of security supervision on blockchain from the perspective of technology,Yu Wang and Gaopeng Gou and Chang Liu and Mingxin Cui and Zhen Li and Gang Xiong,"Due to decentralization, immutability, circulation, anonymity, blockchain technology has become a hot topic but also a hotbed of various cyber-crimes. Many perpetrators attack blockchain to steal cryptocurrencies or use anonymous addresses to conduct illicit financial transactions or receive ransoms while hiding their identities. Since blockchain technology has not developed for a long time, the security issues in this area cannot be well resolved through its own mechanisms, which brings great challenges to protect the security of blockchain and users. Although there are some protective measures to prevent attackers from attacking, most of them are proposed after attacks, and it is impossible to find the masterminds behind modern cyber-crimes, so it is necessary to continuously monitor suspicious nodes or users. In this paper, we first present a systematic overview of blockchain technology and security issues according to the four-layer structure, and explain the problem of security supervision of blockchain. Subsequently, we divide the key technologies for security supervision of blockchain into three aspects: node discovery technology on the network layer, data analysis technology of transaction records on the transaction layer, and network traffic analysis technology on the application layer. In terms of each aspect, we summarize the studies from various angles according to its characteristics. In the end, we discuss the relationship between blockchain and traditional law. Moreover, we present the challenges of security supervision and possible future research directions in this field.",2021,1982
On the impact of release policies on bug handling activity: A case study of Eclipse,Zeinab {Abou Khalil} and Eleni Constantinou and Tom Mens and Laurence Duchien,"Large software projects follow a continuous development process with regular releases during which bugs are handled. In recent years, many software projects shifted to rapid releases that reduce time-to-market and claim a faster delivery of fixed issues, but also have a shorter period to address bugs. To better understand the impact of rapid releases on bug handling activity, we empirically analyze successive releases of the Eclipse Core projects, focusing on the bug handling rates and durations as well as the feature freeze period. We study the impact of Eclipse’s transition from a yearly to quarterly release cycle. We confirm our findings through feedback received from five Eclipse Core maintainers. Among others, our results reveal that Eclipse’s bug handling process is becoming more stable over time, with a decreasing number of reported bugs before releases, an increasing bug fixing rate and an increasingly balanced bug handling workload before and after releases. The transition to a quarterly release cycle continued to improve bug handling. In addition, more effort is spent on bug fixing during the feature freeze period, while the bug handling rates do not differ between both periods.",2021,1983
Agriculture 4.0 and beyond: Evaluating cyber threat intelligence sources and techniques in smart farming ecosystems,Hang Thanh Bui and Hamed Aboutorab and Arash Mahboubi and Yansong Gao and Nazatul Haque Sultan and Aufeef Chauhan and Mohammad Zavid Parvez and Michael Bewong and Rafiqul Islam and Zahid Islam and Seyit A. Camtepe and Praveen Gauravaram and Dineshkumar Singh and M. {Ali Babar} and Shihao Yan,"The digitisation of agriculture, integral to Agriculture 4.0, has brought significant benefits while simultaneously escalating cybersecurity risks. With the rapid adoption of smart farming technologies and infrastructure, the agricultural sector has become an attractive target for cyberattacks. This paper presents a systematic literature review that assesses the applicability of existing cyber threat intelligence (CTI) techniques within smart farming infrastructures (SFIs). We develop a comprehensive taxonomy of CTI techniques and sources, specifically tailored to the SFI context, addressing the unique cyber threat challenges in this domain. A crucial finding of our review is the identified need for a virtual Chief Information Security Officer (vCISO) in smart agriculture. While the concept of a vCISO is not yet established in the agricultural sector, our study highlights its potential significance. The implementation of a vCISO could play a pivotal role in enhancing cybersecurity measures by offering strategic guidance, developing robust security protocols, and facilitating real-time threat analysis and response strategies. This approach is critical for safeguarding the food supply chain against the evolving landscape of cyber threats. Our research underscores the importance of integrating a vCISO framework into smart farming practices as a vital step towards strengthening cybersecurity. This is essential for protecting the agriculture sector in the era of digital transformation, ensuring the resilience and sustainability of the food supply chain against emerging cyber risks.",2024,1984
Textual keyword extraction and summarization: State-of-the-art,Zara Nasar and Syed Waqar Jaffry and Muhammad Kamran Malik,"With the advent of Web 2.0, there exist many online platforms that results in massive textual data production such as social networks, online blogs, magazines etc. This textual data carries information that can be used for betterment of humanity. Hence, there is a dire need to extract potential information out of it. This study aims to present an overview of approaches that can be applied to extract and later present these valuable information nuggets residing within text in brief, clear and concise way. In this regard, two major tasks of automatic keyword extraction and text summarization are being reviewed. To compile the literature, scientific articles were collected using major digital computing research repositories. In the light of acquired literature, survey study covers early approaches up to all the way till recent advancements using machine learning solutions. Survey findings conclude that annotated benchmark datasets for various textual data-generators such as twitter and social forms are not available. This scarcity of dataset has resulted into relatively less progress in many domains. Also, applications of deep learning techniques for the task of automatic keyword extraction are relatively unaddressed. Hence, impact of various deep architectures stands as an open research direction. For text summarization task, deep learning techniques are applied after advent of word vectors, and are currently governing state-of-the-art for abstractive summarization. Currently, one of the major challenges in these tasks is semantic aware evaluation of generated results.",2019,1985
A fast all-packets-based DDoS attack detection approach based on network graph and graph kernel,Xinqian Liu and Jiadong Ren and Haitao He and Bing Zhang and Chen Song and Yunxue Wang,"DDoS attack detection methods play a very important role in protecting computer network security. However, the existing flow-based DDoS attack detection methods face the non-negligible time delay and are not general for different types of DDoS attacks at different rates. In order to fill this research gap, a fast all-packets-based DDoS attack detection approach (FAPDD) is proposed. The FAPDD firstly designs a new time series network graph model to effectively simplify the processing of network traffic handling compared with the flow-based detections. Furthermore, it is the first time that the directed Weisfeiler-Lehman graph kernel is built for measuring the divergence between the current network graph and the normalization network graphs. Due to the new graph model and kernel measurement method to judge network changes, the different types and rates of DDoS attacks can be especially detected. In addition, the dynamic threshold and freezing mechanism are constructed to display standard traffic changes and prevent the pollution of attack traffic to the standard network. Finally, a number of real DDoS attack datasets are applied to evaluate the effectiveness of the proposed method, as well as the overall time efficiency and detection effect. Compared with other methods, the FAPDD can better meet the real-time requirements and achieve good detection effects in different types of DDoS attacks with different attack rates.",2021,1986
Next generation antivirus endowed with bitwise morphological extreme learning machines,Sidney M.L. Lima and Danilo M. Souza and Ricardo P. Pinheiro and Sthéfano H.M.T. Silva and Petrônio G. Lopes and Rafael D.T. {de Lima} and Jemerson R. {de Oliveira} and Thyago de A. Monteiro and Sérgio M.M. Fernandes and Edison de Q. Albuquerque and Washington W.A. {da Silva} and Wellington P. {dos Santos},"Background and Objective
Every second, on average, 8 (eight) new malware are created. So, our goal is to propose an antivirus, endowed with artificial intelligence, able of identifying malwares through models based on fast training and high-performance neural networks.
Methods
Our NGAV (Next Generation Antivirus) is equipped with an authorial ELM (Extreme Learning Morphological) machine. Our bmELMs (Bitwise-Morphological ELMs) are inspired by the image processing theory of Mathematical Morphology. We claim that bmELMs are able to adapt in any machine learning dataset. Inspired by Mathematical Morphology, our bmELMs are capable of modeling any form present at the decisions boundaries of neural networks.
Results
Our bmELMs results are compared with classical ELMs and evaluated through widely used classification metrics. Our antivirus, provided with Bitwise-Morphology, achieves an average accuracy of 97.88%, 93.07%, 93.07% and 91.74% in malware detection of PE (Portable Executable), Java, JavaScript and PHP, respectively.
Conclusions
Our NGAV enables high performance, large capacity of parallelism, and simple, low-power architecture with low power consumption. We concluded that our Bitwise-Morphology assists to the main requirements for the proper operation and confection of antivirus in hardware.",2021,1987
"Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",Yogesh K. Dwivedi and Laurie Hughes and Elvira Ismagilova and Gert Aarts and Crispin Coombs and Tom Crick and Yanqing Duan and Rohita Dwivedi and John Edwards and Aled Eirug and Vassilis Galanos and P. Vigneswara Ilavarasan and Marijn Janssen and Paul Jones and Arpan Kumar Kar and Hatice Kizgin and Bianca Kronemann and Banita Lal and Biagio Lucini and Rony Medaglia and Kenneth {Le Meunier-FitzHugh} and Leslie Caroline {Le Meunier-FitzHugh} and Santosh Misra and Emmanuel Mogaji and Sujeet Kumar Sharma and Jang Bahadur Singh and Vishnupriya Raghavan and Ramakrishnan Raman and Nripendra P. Rana and Spyridon Samothrakis and Jak Spencer and Kuttimani Tamilmani and Annie Tubadji and Paul Walton and Michael D. Williams,"As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.",2021,1988
Chapter 37 - Artificial intelligence: a way forward for agricultural sciences,Neeru S. Redhu and Zoozeal Thakur and Shikha Yashveer and Poonam Mor,"Artificial intelligence (AI) mimics the cognitive functions of the human brain, that is, learning from experiences using a set of algorithms by identifying hidden patterns. AI has already demonstrated its game-changing capabilities in many fields such as the banking sector, healthcare, e-commerce, among others. Recent advances in both computer hardware and large-scale generation of biological data have created room for the applicability of AI in agriculture by employing methods like machine learning, deep learning, natural language processing, artificial or convoluted neural networks either independently or in combination thereof. Currently, AI is being utilized in multiple domains of agriculture and contributing toward increasing crop yield as well as minimizing the risk associated with crop cultivation. Data generated from different agricultural domains such as weather, soil, temperature, humidity, irrigation, sowing requirements, and other crop-related criterion are being analyzed and integrated for agriculture-related predictions. An increase in crop yield is being achieved not only by predicting the best time for sowing, harvesting, and monitoring crop health but also by reducing the cost of agricultural inputs like chemicals, fertilizers, irrigation, etc., through precision farming. Correspondingly, agricultural risks are also being minimized by addressing issues such as poor rainfall, weed growth, pest attacks, and postharvest losses. Besides, AI is also being used for crop/commodity price forecasting and agricultural automation via robots. The predictive agricultural analytics and chatbots are used to disseminate information to farmers as a part of advisory services by these agribusinesses. This chapter will discuss AI technologies and their various applications for the development of better farm practices to improve plant productivity as well as technological advancements due to AI and its anticipated challenges in the future will be explored.",2022,1989
Chapter 23 - The Domain Name System,Walter Goralski,"In this chapter, you will learn how DNS gives the Internet a more user-friendly way to access resources. We’ll see how names are associated with IP addresses and how applications find this information. We will also see how the openness and necessity of DNS makes the whole Internet more vulnerable to attack. You will learn how DNS servers provide information about local networks, and how this information is distributed and shared on the Internet. We’ll also use show tools to help examine DNS.",2017,1990
Planet Braitenberg: Experiments in virtual psychology,Paul R. Smart,"Braitenberg vehicles are simple robotic platforms, equipped with rudimentary sensor and motor components. Such vehicles have typically featured as part of thought experiments that are intended to show how complex behaviours are apt to emerge from the interaction of inner control mechanisms with aspects of bodily structure and features of the wider (extra-agential) environment. The present paper describes a framework for creating Braitenberg-like vehicles, which is built on top of a widely used and freely available game engine, namely, the Unity game engine. The framework can be used to study the behaviour of virtual vehicles within a multiplicity of virtual environments. All aspects of the vehicle’s design, as well as the wider virtual environment in which the vehicle is situated, can be modified during the design phase, as well as at runtime. The result is a general-purpose simulation capability that is intended to provide the foundation for studies in so-called computational situated cognition—a field of study whose primary objective is to support the computational modelling of cognitive processes associated with the physically-embodied, environmentally-embedded, and materially-extended mind.",2020,1991
Applications of link prediction in social networks: A review,Nur Nasuha Daud and Siti Hafizah {Ab Hamid} and Muntadher Saadoon and Firdaus Sahran and Nor Badrul Anuar,"Link prediction methods anticipate the likelihood of a future connection between two nodes in a given network. The methods are essential in social networks to infer social interactions or to suggest possible friends to the users. Rapid social network growth trigger link prediction analysis to be more challenging especially with the significant advancement in complex social network modeling. Researchers implement numerous applications related to link prediction analysis in different network contexts such as dynamic network, weighted network, heterogeneous network and cross network. However, link prediction applications namely, recommendation system, anomaly detection, influence analysis and community detection become more strenuous due to network diversity, complex and dynamic network contexts. In the past decade, several reviews on link prediction were published to discuss the algorithms, state-of-the-art, applications, challenges and future directions of link prediction research. However, the discussion was limited to physical domains and had less focus on social network perspectives. To reduce the gap of the existing reviews, this paper aims to provide a comprehensive review and discuss link prediction applications in different social network contexts and analyses, focusing on social networks. In this paper, we also present conventional link prediction measures based on previous researches. Furthermore, we introduce various link prediction approaches and address how researchers combined link prediction as a base method to perform other applications in social networks such as recommender systems, community detection, anomaly detection and influence analysis. Finally, we conclude the review with a discussion on recent researches and highlight several future research directions of link prediction in social networks.",2020,1992
CRISPR Screens Uncover Genes that Regulate Target Cell Sensitivity to the Morphogen Sonic Hedgehog,Ganesh V. Pusapati and Jennifer H. Kong and Bhaven B. Patel and Arunkumar Krishnan and Andreas Sagner and Maia Kinnebrew and James Briscoe and L. Aravind and Rajat Rohatgi,"Summary
To uncover regulatory mechanisms in Hedgehog (Hh) signaling, we conducted genome-wide screens to identify positive and negative pathway components and validated top hits using multiple signaling and differentiation assays in two different cell types. Most positive regulators identified in our screens, including Rab34, Pdcl, and Tubd1, were involved in ciliary functions, confirming the central role for primary cilia in Hh signaling. Negative regulators identified included Megf8, Mgrn1, and an unannotated gene encoding a tetraspan protein we named Atthog. The function of these negative regulators converged on Smoothened (SMO), an oncoprotein that transduces the Hh signal across the membrane. In the absence of Atthog, SMO was stabilized at the cell surface and concentrated in the ciliary membrane, boosting cell sensitivity to the ligand Sonic Hedgehog (SHH) and consequently altering SHH-guided neural cell-fate decisions. Thus, we uncovered genes that modify the interpretation of morphogen signals by regulating protein-trafficking events in target cells.",2018,1993
An evaluation of agile Ontology Engineering Methodologies for the digital transformation of companies,Daniele Spoladore and Elena Pessot,"Ontologies are increasingly recognised among the key enablers of the digital transformation of knowledge management processes, but still with a low level of adoption in manufacturing companies. Because ontologies and underlying technologies are complex, Ontology Engineering Methodologies (OEMs) provide a set of guidelines to move from an informal to a formal representation of the company’s knowledge base. This study evaluates three agile OEMs, i.e. UPONLite, SAMOD and RapidOWL, in terms of their process and outcome features, i.e. the OEM steps and the expected quality of the ontological models produced. The assessment is performed from the viewpoint of developers of ontology-based technologies in real industrial use cases. Results show that the three agile OEMs reflect different features to effectively support the digital transformation of companies' knowledge management; thus, they cannot be interchangeable. UPONLite is more effective in contexts where there is a lack of skills in OE, with the need for a structured approach in involving domain experts and generating documentation. SAMOD requires a more extended development period, but with several cycles that allow to map different types of knowledge and enable a “try-and-learn” approach. Conversely, RapidOWL lacks a structured sequence of modelling activities and encourages developers to be creative, but at the same time requires higher expertise in OE. Thus, companies and personnel dedicated to OE should choose the methodology according to the main aims guiding their digitalisation process, the current development status, and the level of expertise.",2022,1994
Climate change and COP26: Are digital technologies and information management part of the problem or the solution? An editorial reflection and call to action,Yogesh K. Dwivedi and Laurie Hughes and Arpan Kumar Kar and Abdullah M. Baabdullah and Purva Grover and Roba Abbas and Daniela Andreini and Iyad Abumoghli and Yves Barlette and Deborah Bunker and Leona {Chandra Kruse} and Ioanna Constantiou and Robert M. Davison and Rahul De’ and Rameshwar Dubey and Henry Fenby-Taylor and Babita Gupta and Wu He and Mitsuru Kodama and Matti Mäntymäki and Bhimaraya Metri and Katina Michael and Johan Olaisen and Niki Panteli and Samuli Pekkola and Rohit Nishant and Ramakrishnan Raman and Nripendra P. Rana and Frantz Rowe and Suprateek Sarker and Brenda Scholtz and Maung Sein and Jeel Dharmeshkumar Shah and Thompson S.H. Teo and Manoj Kumar Tiwari and Morten Thanning Vendelø and Michael Wade,"The UN COP26 2021 conference on climate change offers the chance for world leaders to take action and make urgent and meaningful commitments to reducing emissions and limit global temperatures to 1.5 °C above pre-industrial levels by 2050. Whilst the political aspects and subsequent ramifications of these fundamental and critical decisions cannot be underestimated, there exists a technical perspective where digital and IS technology has a role to play in the monitoring of potential solutions, but also an integral element of climate change solutions. We explore these aspects in this editorial article, offering a comprehensive opinion based insight to a multitude of diverse viewpoints that look at the many challenges through a technology lens. It is widely recognized that technology in all its forms, is an important and integral element of the solution, but industry and wider society also view technology as being part of the problem. Increasingly, researchers are referencing the importance of responsible digitalization to eliminate the significant levels of e-waste. The reality is that technology is an integral component of the global efforts to get to net zero, however, its adoption requires pragmatic tradeoffs as we transition from current behaviors to a more climate friendly society.",2022,1995
AI-based multidisciplinary framework to assess the impact of gamified video-based learning through schema and emotion analysis,Anjana Junius Vidanaralage and Anuja Thimali Dharmaratne and Shamsul Haque,"Background
As a natural and continual process, the initial learning stages encompass mastering and recalling basic facts. The process proves effective with the integration of new information with pre-existing knowledge characterised as schema to facilitate memory encoding. Additionally, emotions also have the ability to modulate human cognition in terms of learning and memory. The recent advent of gamification in e-learning, which has garnered much scholarly and industrial interest, necessitates a thorough examination between video-based learning and its subsequent implications on schema, emotions, and gamification.
Objectives
The current multidisciplinary research triangulated cognitive psychology, affective science, and education technology with artificial intelligence for evaluating digital learning pedagogy based on memory retrieval accuracy, response time, and emotional valence.
Design
This three-way (2 x 2 x 2) mixed factorial experiment design with repeated measures entailed 64 healthy young adult volunteers (n = 64) with 32 in the schema congruent group and 32 in the schema incongruent group. Additionally, 27 (42%) of the volunteers were males, while 37 (58%) were females with an age range between 20 and 39 years old (mean age 27.78 years, SD = 4.77 years).
Results
The findings demonstrate that the schema congruent group attained a statistically significant and higher retrieval accuracy (p < .001). The delayed recall response time was faster than its immediate recall counterpart (p < .001). Overall, the gamified learning mode depicted more positive emotions compared to non-gamified learning, although both groups primarily portrayed more negative emotions (p = .05).
Implications
The synthesis of current research aimed to recommend an AI-based multidisciplinary framework to assess the impact on adult learners in terms of schema and evaluate their emotions in experiencing gamified or non-gamified video materials as a learning medium. The implications expedited from this research offer valuable insights for diverse stakeholders engaged in the video-based learning ecosystem.",2022,1996
The air quality management of the region of Great Casablanca (Morocco). Part 1: atmospheric emission inventory for the year 1992,A. Khatami and J.-L. Ponche and E. Jabry and Ph. Mirabel,"Within the frame of an air quality study of the Great Casablanca Area (GCA), an atmospheric emission inventory concerning the major pollutants: SO2; NOx; non-methane volatile organic compounds (NMVOC); and CO has been realized. This inventory has a spatial resolution of 1 km2 and is established for the reference year 1992. The area, which covers 2500 km2 includes a region which is very sensitive to atmospheric pollution since it is heavily populated and contains up to 60% of the industrial activities of Morocco. The results, which include both biogenic and anthropogenic sources, show as expected very large emissions of pollutants mainly due to the presence of a refinery, several power plants and, contrary to the general European situation, the production of NOx is not dominated by road traffic.",1998,1997
Neuromuscular blocking agents and skeletal muscle relaxants,O. Zuzan and M. Leuwer,"Publisher Summary
This chapter describes the adverse effects of neuromuscular blocking agents and skeletal muscle relaxants. Persistent paralysis has been reported after the long-term administration of neuromuscular blocking agents in critically ill patients. Neuromuscular transmission monitoring helps avoid the overdosing of neuromuscular blocking agents, which might prevent persistent paralysis. Glucocorticoids have been linked to acute quadriplegic myopathy and severe muscle weakness in patients receiving neuromuscular blocking agents. Heterotopic ossification or myositis ossificans has been reported after the long-term administration of neuromuscular blocking agents to patients in intensive care units. Allergic reactions to muscle relaxants result in a massive release of cell mediators, such as histamine, tryptase, serotonin, kinins, prostaglandins, and leukotrienes. This release is usually triggered by the bridging of IgE-receptor complexes with allergens. Tertiary and quaternary ammonium ions are important elements at the allergenic site of muscle relaxants that may explain cross-reactivity among muscle relaxants. Plasma tryptase concentration is a sensitive and relatively specific marker of anaphylaxis.",1999,1998
Dielectric relaxation in cyano complexes of the K4Me11(CN)6·3H2O type. II. Isotopic effect,H.A. Kołłodziej and S. Sorriso,"The dielectric relaxation of deuterated K4Me11(CN)6·3H2O doped with NH41 has been investigated in the vicinity of the transition temperature. In all cases the dielectric band appeared to be non-symmetrical and may be described by means of the Williams-Watts empirical decay function ϕ(Φ) ⇌ exp[−(−/τ-)]. A mechanism for the asymmetry of the dielectric absorption due to fluctuation of the “Glarum defects” is proposed. Also, the nature of the dielectric absorption in the crystals is discussed.",1983,1999
A Study of Thickening Phenomenon in Laser Bending Zone of a Metal Laminated Plate,Xuyue Wang and Xupeng Ma and Zihui Li and Rui Wang,"A metal laminated plate, taking stainless steel/carbon steel laminated plate (SCLP) for example, is being increasingly used in the aircraft, ship, car and automotive industries. Based on the temperature gradient and thickening mechanisms, the thickening phenomenon was investigated for the SCLP forming and application widely. The bending zone thickness increases after laser bending of the SCLP. Then, an IPP image processing software was used to evaluate the grain size and thickness changes in laser bending zone. There are three areas they are stainless steel squeezed layer on the top, carbon steel extended layer at the middle and the bottom layer of stainless steel. The research results show that the grain size increases bigger in laser bending process which means thermal expansion thickening (TET) on one hand. In the positive bending process, grains extended by laser heating are squeezed through the bending stress and extrusion thickening (ET) on the other hand. The sizes of both stainless steel at upper layer and carbon steel layer at middle get thicker, but the bottom stainless layer changed little. The research outcomes also show the thickening size consists of TET and ET, where TET effect accounts for great proportion in the range of bending angle 5-20 degree. While ET effect increases in the range of bending angle 30-60 degree, the thickening ratio of TET and ET reaches 2 to 3 at bigger angles. The researches of the thickening phenomenon provide a deeper understand on laser bending theory and technology.",2016,2000
Cell wall-conjugated phenolics from coniferae leaves,Dieter Strack and Jürgen Heilemann and Martina Mömken and Victor Wray,"The occurrence of insoluble conjugated (ester-bound) hydroxycinnamic acids (p-coumaric and ferulic acids) and flavonol glycosides (kaempferol 3-O-β-glucopyranoside and 3-O-α-rhamnopyranoside, and quercetin 3-O-β-glucopyranoside and 3-O-α-arabinofuranoside) in cell wall preparations from leaves of 54 species from various Conifer taxa has been investigated. the bound β-coumaric and ferulic acids seem to be widespread among members of the Coniferae-with β-coumaric acid as the major compounds in most species investigated-, while the presence of bound flavonol glycosides appears to be of chemotaxonomic relevance for members of the Pinaceae. The Pinaceae species generally gave kaempferol 3-O-glucoside, although Abies gave in addition kaempferol 3-O-rhamnoside and Pinus quercetin 3-0-glucoside and 3-O-arabinoside.",1988,2001
Жизнестроение,Hans Günther,,1986,2002
Human α2-HS-glycoprotein/bovine fetuin homologue in mice: identification and developmental regulation of the gene,Funmei Yang and Zi-Lian Chen and Judith M. Bergeron and Rod L. Cupples and William E. Friedrichs,"Human α2-HS-glycoprotein (AHSG) is a plasma protein synthesized in liver and selectively concentrated in bone matrix. It has been reported to be involved in bone formation and resorption as well as immune responses. Recently, AHSG was found to be the species equivalent protein of fetuin, the major fetal serum protein in cattle and sheep. The function and regulation of AHSG/fetuin in different species are not understood. We have isolated a liver cDNA clone that encodes the human AHSG/bovine fetuin homologue in the mouse. The AHSG/fetuin gene may have a role in differentiation since it is expressed in mouse limb buds and brain only at certain stages during development. Mouse liver AHSG/fetuin mRNA was present at low level at 12 days gestation but its level increased during the late part of gestation and peaked between 1 to 3 months after birth. The regulation of mouse AHSG/fetuin synthesis during development was found to be significantly different from that of sheep and bovine fetuin. Compared to fetuin, which is reduced in adult to 1 to 2% of the fetal level, mouse AHSG synthesis subsides only 50% 4 months after birth.",1992,2003
Simulations of high latitude ionospheric climatology,J.J. Sojka and R.W. Schunk,"Historically, the high latitude ionosphere has been viewed as the most complex of the ionospheric regions because it is driven by both magnetospheric and solar inputs. At lower latitudes the direct, and highly variable, magnetospheric input is relatively unimportant, which makes these other regions amenable to empirical modeling. To date, however, no empirical model of the high latitude ionosphere is available which includes these complex dependencies. On the other hand, numerical models that include the physics of this region have been developed and have proven to be successful at the climatology level. In this study we present the climatological results of one of these models, namely the Utah State University (USU) timedependent ionospheric model (TDIM). A total of 108 separate TDIM simulations for different ionospheric conditions were used to elucidate the high latitude ionospheric trends. These trends depend on solar cycle, season, universal time (UT), magnetic activity, interplanetary magnetic field (IMF) orientation, and hemisphere. The ionospheric climatology is not dominated by any one of these parameters. The solar cycle (F10.7 index), season (day), and magnetic activity (Kp index) compete on an even footing for control of the high latitude ionosphere. Mean variations of over an order of magnitude in NmF2, of over 150 km in hmF2, and of over 50 km in the transition height are present in the high latitude ionospheric climatology. The 108 simulations quantify the trends and show the UT dependence and spatial variability of the ionosphere. Some aspects of these UT trends are compared successfully with observations. Many of the simulation results are predictions that can be verified as more complete observational databases become available. The UT dependence, which at times can be a factor of two modulation of the F region densities, is a key reason for the failure of statistical models at high latitudes. At lower latitudes, statistical models based mainly on local time rather than UT have been very successful. At high latitudes, this is not so and, therefore, local time and UT (longitude) must be treated as independent variables. This fact alone explains why data sets based on a fixed ground location or satellite orbital plane cannot unravel the LT and UT dependencies at high latitudes. Also, the high latitude ionosphere is not spatially uniform; morphological features on latitudinal scales of 1–2 ° are present. These structures play a key role in identifying the ionospheric climatology.",1997,2004
Illumination and Late Hatched Pullets,R.B. THOMPSON,"ABSTRACT
Last year at the Guelph meeting I had my first good taste of illumination and liked it to the last drop. At that time I remember there was some discussion regarding what illumination would do for the late hatched pullets. At that time I had hatched five different ages of leghorn pullets for the purpose of a demonstration as to the proper or most desirable time of hatching for egg production. It was therefore an easy matter to divide these pullets into two lots and provide electric lights for one and allow the other to get along as best it could without. The pullets were therefore divided into two pens A. and B. and in each pen there were eight pullets from each of five different hatches but all hatched from eggs produced by the same pen. The hatching dates were for lot 1, March first, lot 2, March 25, . . .",1921,2005
Chapter 2 - Mobile Security: A Practitioner’s Perspective,S. Tully and Y. Mohanraj,"This chapter looks at some of the sociological aspects of the increase in the use of mobile devices as the world transforms rapidly in a move toward mobile ubiquity. It then describes the challenges surrounding mobile device security for practitioners, highlighting the key risks for individuals and the key concerns for organizations interacting with a digital native workforce who rely on these mobile devices. It offers insights on various threats, risks, issues, mitigations, and mobile security strategies, as well as sections covering privacy, forensics, and individual versus organizational impacts. Finally, it concludes with 10 suggested steps to secure mobile devices.",2017,2006
"Cyber ranges and security testbeds: Scenarios, functions, tools and architecture",Muhammad Mudassar Yamin and Basel Katt and Vasileios Gkioulos,"The first line of defense against cyber threats and cyber crimes is to be aware and get ready, e.g., through cyber security training. Training can have two forms, the first is directed towards security professionals and aims at improving understanding of the latest threats and increasing skill levels in defending and mitigating against them. The second form of training, which used to attract less attention, aims at increasing cyber security awareness among non-security professionals and the general public. Conducting such training programs requires dedicated testbeds and infrastructures that help realizing and executing the training scenarios and provide a playground for the trainees. A cyber range is an environment that aims at providing such testbeds. The purpose of this paper is to study the concept of a cyber range, and provide a systematic literature review that covers unclassified cyber ranges and security testbeds. In this study we develop a taxonomy for cyber range systems and evaluate the current literature focusing on architecture and scenarios, but including also capabilities, roles, tools and evaluation criteria. The results of this study can be used as a baseline for future initiatives towards the development and evaluation of cyber ranges in accordance with existing best practices and lessons learned from contemporary research and developments.",2020,2007
2 - DOES LIGHT LIMIT CROP PRODUCTION?,J.L. MONTEITH,,1981,2008
The activation of human platelet adenylate cyclase by vanadate,Katalin Ajtai and Katalin Tuka and E.N.A. Biró,,1983,2009
Emergence of Transient Compulsive Symptoms during Treatment with Clothiapine,PAZ TOREN and ELIAHU SAMUEL and RONIT WEIZMAN and ABIGAIL GOLOMB and SOFIA ELDAR and NATHANIEL LAOR,"ABSTRACT
Serotonergic dysregulation in obsessive-compulsive disorder has been repeatedly demonstrated. Recent reports on the emergence of obsessive-compulsive symptoms in patients treated with clozapine support a hyposerotonergic hypothesis of obsessive-compulsive disorder. The authors report the emergence of de novo compulsive symptoms in a drug-naive 8-year-old schizophrenic child, shortly after the initiation of treatment with clothiapine. Clothiapine, an atypical antipsychotic agent, shares with clozapine its strong antiserotonergic properties. It seems that antagonistic activity of atypical neuroleptics at postsynaptic serotonergic receptors might be responsible for the development of iatrogenic obsessive-compulsive symptoms.",1995,2010
"Network attacks: Taxonomy, tools and systems",N. Hoque and Monowar H. Bhuyan and R.C. Baishya and D.K. Bhattacharyya and J.K. Kalita,"To prevent and defend networks from the occurrence of attacks, it is highly essential that we have a broad knowledge of existing tools and systems available in the public domain. Based on the behavior and possible impact or severity of damages, attacks are categorized into a number of distinct classes. In this survey, we provide a taxonomy of attack tools in a consistent way for the benefit of network security researchers. This paper also presents a comprehensive and structured survey of existing tools and systems that can support both attackers and network defenders. We discuss pros and cons of such tools and systems for better understanding of their capabilities. Finally, we include a list of observations and some research challenges that may help new researchers in this field based on our hands-on experience.",2014,2011
Chapter 1 - The Foundation of Policy Management,John Strassner,"Publisher Summary
This chapter provides a brief retrospective on how policy-based network management (PBNM) was designed. It outlines two fundamental problems—the lack of use of an information model and the inability to use business rules to drive configuration of devices, services, and networks. Quality of service (QoS) is one of the primary drivers for implementing PBNM solutions. PBNM solutions require information models that contain business and system entities that can be easily implemented. The chapter introduces a unique object-oriented information model, known as DEN-ng (Directory Enabled Networks-new generation). It is being developed in the TM Forum. An object-oriented information model is a means to represent various entities in a managed environment. An entity can be a person, a computer, a router, or even a protocol message—anything that needs a uniform and consistent representation for configuration and management is a possibility for definition and representation in DEN-ng. An object-oriented information model provides a common language in which different types of management entities can be represented.",2004,2012
"Cuticular characters adapted to volcanic stress in a new Cretaceous cycad leaf from Patagonia, Argentina. Considerations on the stratigraphy and depositional history of the Baqueró Formation",Ana Archangelsky and Renato R. Andreis and Sergio Archangelsky and Analia Artabe,"The cuticle of a new cycad, Pseudoctenis ornata Archangelsky et al., sp. nov. is described and discussed in relation to the physical paleoenvironment in which the plant lived. The specimens occur in the Early Cretaceous Baqueró Formation, near Estancia El Verano in the Santa Cruz Province, Argentina. A detailed stratigraphic section records four facies, namely (1) fluvial channel, (2) flood plain, (3) lacustrine, and (4) flat and extended plains. A detail of each facies is provided. Pseudoctenis cuticles are found in the flood plain facies; the other components of the plant association are Gleichenites, Araucaria and Taeniopteris. The depositional history of this succession is related to a braided river that periodically received volcanic ash. Plants grew until complete burial by ash. Leaves of Pseudoctenis are pinnate, hypostomatic, with polycyclic stomata that form ill-defined rows. Abundant papillae and hair bases are present, especially on the lower cuticle. Comparisons are made with other Pseudoctenis species found in the same formation, and in other regions of the world. It is suggested that the paleoenvironment had a strong influence on the vegetation, especially the ash fall, and that it may have played a role in the formation of xeromorphic structures that characterize several gymnosperms present in this stratigraphic unit, including Pseudoctenis ornata. The lack of burning features on the cuticle suggests a cold ash fall. This fall probably was responsible for changes in edaphic patterns and atmospheric conditions that governed the growth and distribution of plant communities during the deposition of the Baqueró Formation.",1995,2013
“На пороге как бы двойного бытия...”: тени и призраки в лирике Тютчева,Н.Е. Меднис,,2005,2014
"In vitro phagocytosis assay of nano- and microparticles by chemiluminescence. I. Effect of analytical parameters, particle size and particle concentration",S. Rudt and R.H. Müller,"Chemiluminescence (CL) was used to study the uptake of differently sized model drug carriers (polystyrene latex particles) by human granulocytes. The polystyrene particles were incubated with the granulocytes on microtitre plates and the CL monitored for a period of 3 hours. The sensitivity of the chemiluminescence (CL) assay could be increased by optimization of the luminol concentration, cell number and particle number (mass) per well. The intensity (uptake)/time profiles were characterized by the intensity Imax (uptake velocity) and the area under the curve (AUC, total uptake of particles). The reproducibility within one cell isolation was good. To compare CL measurements from different isolations internal standards were included to compensate for biological variations in the granulocyte function. The total uptake of particles increased with increasing particle size and at low to medium particle concentrations. It levelled or even decreased at high particle concentrations reducing the sensitivity of the assay. The AUC correlated with the total mass of phagocytosed particles and could therefore be used to compare the uptake of nano- and microparticles differing in size.",1992,2015
"Proton conduction in H2Ti4O9, 1.2 H2O",E. {Krogh Andersen} and I.G. {Krogh Andersen} and E. Skou,"H2Ti4O9, xH2O was prepared from K2Ti4O9 by hydrolysis with 0.4 M nitric acid as described by Marchand and co-workers. The K2Ti4O9 was prepared by a conventional solid state reaction, and by a sol/gel procedure. The ac conductivity was measured conventionally, and also under a load of 0.5 V. The temperature range was from 20–100°C. The measuring cell was purged with gas (N2 or H2) saturated with water vapor at the measuring temperature. Carbon electrodes were used. The dc conductivity was measured on tablets with platinum black electrodes in an atmosphere of hydrogen saturated with water vapour at 75% RH. The maximum conductivity was 1.8 × 10−3 (Ω cm)−1.",1988,2016
"The role of chemical senses in seed-carrying behavior by ants: A behavioral, physiological, and morphological study",S.L. Sheridan and K.A. Iversen and H. Itagaki,"The sensory bases of seed-carrying by ants were studied through behavioral, physiological, and morphological methods. Fourteen colonies from 4 ant species [Aphaenogaster rudis Emery, Lasius alienus Foerster, Formica subsericea Say, and Camponotus ferrigineus (F.)] were behaviorally tested for olfactory and gustatory responses to seeds of A. candense. Results indicate that ants do not perceive these seeds, which have elaiosomes, by olfaction. Rather, they respond to these seeds only if the seeds are antennated. These results indicate that these ants are not “attracted” to A. canadense seeds by olfactory cues. Electroantennograms (EAGs) recorded from F. subsericea in response to olfactory and gustatory application of mixed isomer diolein, a major component of many seed elaiosomes, support the behavioral data. Olfactory delivery produced EAG responses no different from that of controls, while gustatory delivery produced responses with dose-dependence. Scanning electron microscopy (SEM) of the antennae of A. rudis, F. subsericea, and C. ferrigineus revealed the presence of sensilla chaetica, presumed to be contact chemoreceptors, and sensilla placodea and/or sensilla trichodea curvata that are believed to be olfactory receptors.",1996,2017
Proactive defense mechanism: Enhancing IoT security through diversity-based moving target defense and cyber deception,Zubaida Rehman and Iqbal Gondal and Mengmeng Ge and Hai Dong and Mark Gregory and Zahir Tari,"The Internet of Things (IoT) has become increasingly prevalent in various aspects of our lives, enabling billions of devices to connect and communicate seamlessly. However, the intricate nature of IoT connections and device vulnerabilities exposes the devices to security threats. To address the security challenges, we propose a proactive defense framework that leverages a model-based approach for security analysis and facilitates the defense strategies. Our proposed approach incorporates proactive defense mechanisms that combine Moving Target Defense techniques with cyber deception. The proposed approach involves the use of a decoy nodes as a deception technique and operating system based diversity as a moving target defense strategy to change the attack surface area of IoT networks. Additionally, we introduce a technique known as Important Measure-based Operating System Diversity to reduce defense cost. The effectiveness of the defense mechanisms was evaluated by using a graphical security model in a Software Defined Networking-based IoT network. Simulation results demonstrate the effectiveness of our approach in mitigating the impact of attacks while maintaining high performance levels in IoT networks.",2024,2018
Postharvest ripening and ethylene biosynthesis in purple passion fruit,Shinjiro Shiomi and Yasutaka Kubo and Leonard S. Wamocho and Hiroshi Koaze and Reinosuke Nakamura and Akitugu Inaba,"A study was undertaken to investigate the pattern of changes in some chemical constituents, ethylene biosynthesis, and the effect of ethylene treatment during postharvest ripening in purple passion fruit (Passiflora edulis Sims.). During ripening, sucrose content decreased while fructose and glucose contents increased. Citric and malic acid contents slightly increased during the early stage of ripening and decreased thereafter. Amino acids did not change significantly, except for proline, which increased rapidly towards the late stage of ripening. While 1-aminocyclopropane-1-carboxylic acid (ACC) content and ACC synthase activity increased in parallel with ethylene production, ACC oxidase activity was already high when harvested at the turning stage and further increased during ripening. Application of 1000 ppm ethylene for 24 h did not induce earlier onset of ethylene production when applied on harvest day, but was effective when applied one day or five days after harvest. The results indicate that purple passion fruit produces ethylene with the same biosynthetic pathway as other tissues in higher plants; its ethylene biosynthesis is regulated mainly by ACC synthase activity, and the sensitivity to ethylene might have changed after harvest as the fruit ripened.",1996,2019
13 - Annual Plants: Potential Responses to Multiple Stresses,F.A. Bazzaz and S.R. Morse,,1991,2020
The antigenic connection between two different types of foot-and-mouth disease viruses and their subparticles,C.J. {Van Oss} and Léone Dhennin and Louis Dhennin,,1964,2021
Effects of sodium dipropylacetate on the withdrawal syndrome and alcohol consumption in a free-choice situation in alcohol-dependent rats,Bernard {Le Bourhis} and Gilles Aufrère,,1980,2022
Chronic acamprosate eliminates the alcohol deprivation effect while having limited effects on baseline responding for ethanol in rats,Charles J. Heyser and Gery Schulteis and Philippe Durbin and George F. Koob,"Acamprosate (calcium-acetyl homotaurinate) is a relatively new compound developed for the treatment of alcoholism and has been shown to be effective in attenuating relapse in human alcoholics. In the current study, the effects of this drug were further examined using an animal model of oral ethanol self-administration in a limited access paradigm. Male Wistar rats were trained to respond for ethanol (10% w/v) or water in a two-lever free-choice opérant condition. Acute administration of acamprosate (400 mg/kg) reduced ethanol consumption and increased responding for water. Chronic administration of lower daily doses of acamprosate (100 and 200 mg/kg) blocked the increased ethanol consumption typically observed in rats after an imposed abstinence period. This effect of acamprosate was selective for ethanol, as responding for water was unaffected at any dose tested. These results with rats suggest a model by which to explore the mechanisms for anti-relapse effects of acamprosate.",1998,2023
"Time-resolved spectral studies of blue-green fluorescence of leaves, mesophyll and chloroplasts of sugar beet (Beta vulgaris L.)",Zoran G. Cerovic and Fermín Morales and Ismael Moya,"Synchrotron radiation and the time-correlated single-photon-counting technique were used to investigate the spectral and time-resolved characteristics of blue-green fluorescence of leaves, mesophyll and chloroplasts. Four kinetic components were resolved. Decay-associated spectra and comparative analysis showed that the fluorescence of leaves, on both sides, was dominated by the fast (0.3 ns) and the medium (1 ns) kinetic components, comprising fluorophores emitting principally in the blue and present in the epidermal layer. In the mesophyll, these two faster components have two maxima, in the blue and in the green part of the spectrum, with a shift of the blue maxima to longer wavelengths when compared to leaves. The slow component (3.5 ns) was green-related with a strong indication for the presence of flavins. The very slow component (9 ns) had a maximal fractional intensity in mesophyll and was blue-related. The excitation and emission characteristics and the effect of anaerobic atmosphere on the fractional intensities of the slow and very slow component showed that they contain fluorescence of flavin nucleotides and nicotinamide nucleotides, respectively. Time-resolved measurements could be a means to extract the information on nucleotide fluorescence from the overall leaf fluorescence, and therefore to evaluate changes in mesophyll redox sate.",1994,2024
Chapter 9 - Domain 8: Software Development Security,Eric Conrad and Seth Misenar and Joshua Feldman,"This chapter introduces Domain 8 of the CISSP®, Software Development Security. The most important aspects of this domain are related to managing the development of software and applications. Approaches to software development that attempt to reduce the likelihood of defects or flaws are a key topic in this domain. In particular, the Waterfall, Spiral, and Rapid Application Development (RAD) models of software development are considered. Another significant portion of this chapter is dedicated to understanding the principles of Object-Oriented programming and design. A basic discussion of several types of software vulnerabilities and the issues surrounding disclosure of the vulnerabilities are also a topic for this domain. Finally, databases, being a key component of many applications, are considered.",2023,2025
Effects of enhanced UV-B radiation on the growth of rice and its susceptibility to rice blast under glasshouse conditions,Maria R. Finckh and Arlene Q. Chavez and Q. Dai and Paul S. Teng,"The effects of enhanced UV-B (280–320 nm) on the susceptibility of 18 irrigated lowland rice cultivars to rice blast (Pyricularia grisea) were investigated. The rice cultivars were irradiated with UV-B 313 lamps that were either filtered with cellulose acetate of 0.13 mm thickness (light transmission greater than 290 nm) or with Mylar D of 0.13 mm thickness (light transmission greater than 320 nm). Irradiation was for 6 h daily for 21 days starting at 9 days after planting. After irradiation, plants were inoculated with one or two blast isolates. In 18 out of 36 measured interactions the number of lesions per plant was higher in the presence of UV-B than in its absence. However, only two interactions were statistically significant. In a second experiment the dose-response relationship between the cultivars IR30 and IR72 and UV-B was established in the presence and absence of disease. Increasing levels of UV-B significantly reduced leaf areas, dry weights, and heights of both cultivars. Disease severity was either unaffected or decreased by UV-B. However, disease significantly changed the effects of UV-B on plant growth and recovery from UV-B damage. IR30 but not IR72 recovered from UV-B damage in the absence of disease within 5 days. No recovery occurred when inoculated. IR72 suffered even greater reductions of leaf area and dry weight by UV-B when inoculated. Although the effects of UV-B on disease severity may be small, it appears that the tolerance of plants to disease is decreased by UV-B radiation.",1995,2026
A Review on Outsourcing with a Special Reference to Telecom Operations,Sunil Patil and Y.S. Patil,"Outsourcing is not a recent phenomenon in the industry. Companies have been outsourcing non-core services or functions for many years. Outsourcing of IT management picked up in the late 80's and since then it has mushroomed into a multi-billion dollar industry. Initially the key driver for outsourcing was controlling operational expenses but over the period of time several other factors have been prominent such as flexibility in control of investments and resources, risk sharing, acquisition of special skills and competencies, revenue sharing, establishing long term strategic relationship, etc. The decision of outsourcing attaches special significance in the strategy formulation in the company or any operation. Large numbers of research papers have been published in the past two decades on outsourcing and researchers have studied and analyzed various internal and external factors having bearing on outsourcing decision. Over a period of time new business models and frameworks have emerged helping decision makers in the process. Some researchers have adopted research methodologies that are case studies based or while others have designed empirical formulations by conducting extensive surveys. Interestingly most conclusions from these studies tend to converge on similar set of key factors. This paper provides detailed review of the published research body of knowledge on outsourcing IT management and explores relevance to telecom operations. In the case of telecom operators, it is observed that the basic set of parameters influencing the decision of outsourcing is same as rest of the industry. In the recent past, it is observed that telecom operators have extended this model by outsourcing management of network infrastructure, management of towers, billing systems, marketing, etc. This is creating new working models and relationships. The paper concludes by providing the direction for the future research work in this domain",2014,2027
Vacuum of the quantum Yang-Mills theory and magnetostatics,Heinz Pagels and E. Tomboulis,"It is argued that since in asymptotically free Yang-Mills theories the quantum ground state is not controlled by perturbation theory, there is no a priori reason to believe that individual orbits corresponding to minima of the classical action dominate the Euclidean functional integral. To examine and classify the vacua of the quantum gauge theory, we propose an effective action in which the gauge field coupling constant g is replaced by the effective coupling g(t), t = ln[Fμνa)2μ4]. The vacua of this model correspond to paramagnetism and perfect paramagnetism, for which the gauge field is Fμνa = 0, and ferromagnetism, for which (Fμνa)2 = λ2, i.e. spontaneous magnetization of the vacuum occurs. We show that there are no instanton solutions to the quantum effective action. The equations for a point classical source of color spin are solved, and we show that the field infrared energy becomes linearly divergent in the limit of spontaneous magnetization. This implies bag formation, and an electric Meissner effect confining the bag contents.",1978,2028
Bubble Oxygenation and Cardiotomy Suction Impair the Host Defense during Cardiopulmonary Bypass: A Study in Dogs,Willem {van Oeveren} and Jacob Dankert and Charles R.H. Wildevuur,"Decreased complement levels and impairment of polymorphonuclear leukocyte function increase the risk of infection during cardiopulmonary bypass (CPB). The effects of different types of oxygenator and of blood suction on this natural humoral and cellular host defense mechanism were investigated in dogs undergoing CPB during sham open-heart operations. Airborne contamination of the wound area and the CPB circuit was performed by aerosolizing Staphylococcus aureus. A membrane oxygenator in the CPB circuit maintained a normal host defense mechanism. The use of cardiotomy suction during CPB with this type of oxygenator affected the host defense to some extent. The use of a bubble oxygenator in the CPB circuit together with cardiotomy suction seriously impaired the host defense. Postoperatively bacteremia developed in no dogs in the membrane oxygenator group, whereas 8 of 15 dogs in the bubble oxygenator group had a positive blood culture for the indicator microorganism. We conclude that the use of a membrane oxygenator is helpful to maintain the host defense. Attention has to be paid to reduce the deleterious effects of cardiotomy suction.",1987,2029
"Decomposition of leaf litter of albizia (Paraserianthes falcataria), eucalypt (Eucalyptus tereticornis) and teak (Tectona grandis) in Kerala, India",K.V. Sankaran,"Ex situ decomposition of leaf litter of Paraserianthes falcataria, Eucalyptus tereticornis and Tectona grandis was studied under field and laboratory conditions for a period of 18 months using the litter-bag technique. The amount of CO2 evolved from the decaying litters and the population of fungi, bacteria and actinomycetes associated with the litters were quantified. A laboratory study was also conducted to determine the amount of organic carbon added to soil during decomposition. The dry weight loss of litter under field and laboratory conditions, respectively, were 94 and 74% for albizia, 64% and 60% for eucalypt and 96 and 92% for teak. The decay rate of the three types of litters varied significantly both in the field and in the laboratory. Teak litter decomposed rapidly as compared with others; decomposition of eucalypt litter was the slowest. Weight loss was positively correlated with litter moisture content as well as rainfall. The CO2 evolution differed significantly between the species, but in general it was highest during the southwest monsoon. The weight loss and CO2 evolution were significantly higher in the field than in the laboratory. No significant addition of organic carbon to soil from decomposing litters was evident in the present study. Eucalypt differed statistically from albizia and teak with respect to the bacterial pearl gram of litter, irrespective of the period of sampling or incubation condition. However, bacterial population did not vary significantly between albizia and teak litters. The number of fungi per gram of litter differed significantly between eucalypt and albizia, and that of actinomycetes between teak and eucalypt litters, both in the field and laboratory conditions. Between other pairs, the population and actinomycetes did not vary significantly. The CO2 evolution was positively correlated with number of fungi per gram of litter in albizia and teak in the field; it was not correlated with the population of bacteria and actinomycetes.",1993,2030
Aceraceae,G.C.S. Clarke and Marilyn R. Jones,,1978,2031
Heat transfer during the rewetting of hot horizontal channels,Martha Salcudean and T.M. Bui,"The heat transfer in the rewetting of hot horizontal channels is investigated. The physical model assumes an inclined rewetting front advancing at a uniform velocity. Precursory cooling in the dry region is considered. Three-dimensional energy equations are solved numerically by a finite difference method. Further, the axial and circumferential temperature distributions are predicted. The influence of various parameters on the rewetting velocity is analyzed, as are the variations of the different heat transfer mechanisms, convection to the fluid and conduction in the three-dimensions, as a function of time.",1980,2032
"Effect and cost-effectiveness of step-up versus step-down treatment with antacids, H2-receptor antagonists, and proton pump inhibitors in patients with new onset dyspepsia (DIAMOND study): a primary-care-based randomised controlled trial",Corine J {van Marrewijk} and Suhreta Mujakovic and Gerdine AJ Fransen and Mattijs E Numans and Niek J {de Wit} and Jean WM Muris and Martijn GH {van Oijen} and Jan BMJ Jansen and Diederik E Grobbee and J André Knottnerus and Robert JF Laheij,"Summary
Background
Substantial physician workload and high costs are associated with the treatment of dyspepsia in primary health care. Despite the availability of consensus statements and guidelines, the most cost-effective empirical strategy for initial management of the condition remains to be determined. We compared step-up and step-down treatment strategies for initial management of patients with new onset dyspepsia in primary care.
Methods
Patients aged 18 years and older who consulted with their family doctor for new onset dyspepsia in the Netherlands were eligible for enrolment in this double-blind, randomised controlled trial. Between October, 2003, and January, 2006, 664 patients were randomly assigned to receive stepwise treatment with antacid, H2-receptor antagonist, and proton pump inhibitor (step-up; n=341), or these drugs in the reverse order (step-down; n=323), by use of a computer-generated sequence with blocks of six. Each step lasted 4 weeks and treatment only continued with the next step if symptoms persisted or relapsed within 4 weeks. Primary outcomes were symptom relief and cost-effectiveness of initial management at 6 months. Analysis was by intention to treat (ITT); the ITT population consisted of all patients with data for the primary outcome at 6 months. This trial is registered with ClinicalTrials.gov, number NCT00247715.
Findings
332 patients in the step-up, and 313 in the step-down group reached an endpoint with sufficient data for evaluation; the main reason for dropout was loss to follow-up. Treatment success after 6 months was achieved in 238 (72%) patients in the step-up group and 219 (70%) patients in the step-down group (odds ratio 0·92, 95% CI 0·7–1·3). The average medical costs were lower for patients in the step-up group than for those in the step-down group (€228 vs €245; p=0·0008), which was mainly because of costs of medication. One or more adverse drug events were reported by 94 (28%) patients in the step-up and 93 (29%) patients in the step-down group. All were minor events, including (other) dyspeptic symptoms, diarrhoea, constipation, and bad/dry taste.
Interpretation
Although treatment success with either step-up or step-down treatment is similar, the step-up strategy is more cost effective at 6 months for initial treatment of patients with new onset dyspeptic symptoms in primary care.
Funding
The Netherlands Organisation for Health Research and Development.",2009,2033
Hypervisor-based cloud intrusion detection through online multivariate statistical change tracking,Abdulaziz Aldribi and Issa Traoré and Belaid Moa and Onyekachi Nwamuo,"Cloud computing is facing a multidimensional and rapidly evolving threat landscape, making intrusion detection more challenging. This paper introduces a new hypervisor-based cloud intrusion detection system (IDS) that uses online multivariate statistical change analysis to detect anomalous network behaviors. As a departure from the conventional monolithic network IDS feature model, we leverage the fact that a hypervisor consists of a collection of instances, to introduce an instance-oriented feature model that exploits the individual and correlated behaviors of instances to improve the detection capability. The proposed approach is evaluated by collecting and using a new cloud intrusion dataset that includes a wide variety of attack vectors.",2020,2034
Identification of collision risk factors perceived by ship operators in a vessel encounter situation,Do-Hoon Kim,"This study aims to elucidate the possible risk factors considering the point of view of a ship operator by identifying the maximum collision risk-bearing angle (MCRBA) and the distance at which the collision risk begins to increase significantly (DCRBIS), which are key factors affecting navigation safety in real maritime environments. Using two ships at sea in various vessel encounter situations (relative bearing angles of 000°, 045°, 090°, and 135°), the perceived ship collision risk (PSCR) was estimated by the ship operators. Then, the mean values of all the measured parameters were used to identify the bearing angle corresponding to the highest PSCR. The MCRBA was compared to the maximum frequency collision bearing angle (MFCBA) derived by investigating 200 reported ship collision cases in Korean waters. The highest experimental PSCR values were observed at a relative bearing angle of 135°, which corresponded to the results of collision case analyses stating that the MFCBA was equal to 112.5°–135°. Notably, the PSCR magnitude increased most significantly at a distance of 1.25–1 NM in experiments involving various vessel encounter situations. Collision risk factors such as the MCRBA and DCRBIS can thus be used in the development of collision prevention protocols for ship operators.",2020,2035
Pharmaceutical applications,,,1991,2036
SUBTRIBE BAMBUSINAE,D. OHRNBERGER,,1999,2037
"Ultrastructure of a Colourless Amoeboid Flagellate, Cercomonas sp.",>A.P. Mylnikov,"Summary
The ultrastructure of a colourless amoeboid flagellate, Cercomonas sp. isolated from fresh water, is considered. This minute organism possesses two heterodynamic flagella. When moving it actively crawls along the substratum, persistently forming pseudopodia of various shape and size. The vesicular nucleus with a central nucleolus is located anteriorly, close the two kinetosomes. The system of kinetosome-derived microtubules forms a conical sheath around the nucleus. The Golgi apparatus, the endoplasmic reticulum, and mitochondria with tubular cristae have usual structure. A paranuclear body surrounded by a unit membrane is observed in the cells. The cytoplasm is filled with food vacuoles and drops of storage substances. Beneath the body surface, there are stinging organelles affecting bacteria. Peculiarities of the ultrastructure relate Cercomonas with the chrysophyte algae and myxomycetes.",1986,2038
The influence of forest type on microbial-nutrient relationships in tropical mangrove sediments,Daniel M. Alongi and Paul Christoffersen and Frank Tirendi,"Microbial productivity, nutrient chemistry and rates of nutrient regeneration were examined in muds of different mangrove forests within the Fly Delta, Papua New Guinea, to assess the effect of forest type on microbial and nutrient processes, and their interactions. Three major forest types were examined: Rhizophora-Bruguiera, Nypa and Avicennia-Sonneratia forests. For most variables, variations within a forest type were as great as, or greater than, differences between forest types. A high-intertidal Nypa site was most different in edaphic characteristics compared to five low-intertidal stations (two stations in each forest type) suggesting that differences among forest types in earlier studies were mainly a function of tidal elevation rather than species-specific ability of mangroves to influence redox and nutrient status. Dissolved inorganic nutrients were dominated by high (~ 200–500 μM) concentrations of silicates, but porewater phosphate levels were usually below detection limits (<0.02μM). Measured rates of nutrient regeneration were either slow into the sediment, or undetectable, despite a high concentration gradient for some solutes such as silicate. Rates of bacterial DNA and protein synthesis, and patterns of benthic primary production, indicate uptake of nutrients at the sediment-water interface by epibenthic microalgac and sequestering of porewater solutes by very active, subsurface bacterial communities. Rapid growth of these bacteria may be partially maintained by the decomposition and release of nutrients of mangrove roots and rhizomes, as suggested by the dominance of silicate in the porewater. Correlation analysis supports the notion of nutrient (mainly P) limitation of bacteria and microalgac in mangrove muds. It appears that a close microbe-nutricnt-planl connection serves as a mechanism to conserve scarce nutrients necessary for the existence of these tropical tidal forests.",1993,2039
Imbalanced data classification: A KNN and generative adversarial networks-based hybrid approach for intrusion detection,Hongwei Ding and Leiyang Chen and Liang Dong and Zhongwang Fu and Xiaohui Cui,"With the continuous emergence of various network attacks, it is becoming more and more important to ensure the security of the network. Intrusion detection, as one of the important technologies to ensure network security, has been widely studied. However, class imbalance leads to a challenging problem, that is, the normal data is much more than the attack data. Class imbalance will lead to the deviation of decision boundary, which makes higher value attack data classification error. In the face of imbalanced data, how to make the classification model classify more effectively is called imbalanced learning problem. In this study, we propose a tabular data sampling method to solve the imbalanced learning problem, which aims to balance the normal samples and attack samples. Firstly, for normal samples, on the premise of minimizing the loss of sample information, the K-nearest neighbor method is used for effective undersampling. Then, we design a tabular auxiliary classifier generative adversarial networks model (TACGAN) for attack sample oversampling. TACGAN model is an extension of ACGAN model. We add two loss functions in the generator to measure the information loss between real data and generated data, which makes TACGAN more suitable for the generation of tabular data. Finally, the normal data after undersampling and the attack data after oversampling are mixed to balance the data. We have carried out verification experiments on three real intrusion detection data sets. Experimental results show that the proposed method achieves excellent results in Accuracy, F1, AUC and Recall.",2022,2040
Optimal Control of Manipulation Robots,L.D. Akulenko and N.N. Bolotnik and F.L. Chernousko and A.A. Kaplunov,Optimal control of manipulation robots is considered. Problems of optimal control for robots are stated and solved by means of both analytical and numerical methods. Some optimal time motions of robots with two and three degrees of freedom are presented. Some of these regimes were realized experimentally for industrial robots. The obtained results show that optimal regimes require much less time than non-optimal “natural” ones. Practical applications of optimal regimes are discussed.,1984,2041
Heterozygosis and genetic recombination in herpes simplex type 1 virus,D.A. Ritchie and S. {Moira Brown} and J.H. Subak-Sharpe and A.T. Jamieson,"Two- and three-factor crosses with temperature-sensitive (ts) and syncytial plaque morphology (syn) mutants of herpes simplex type 1 virus have been used to study the possible role of syn-syn+ mixed plaque-forming virus in genetic recombination. Under the conditions of a standard genetic cross, recombinants first appear about 6 hr after infection, the time of formation of the first infectious progeny virus, and their frequency progressively rises until about 20 hr postinfection. During this period the frequency of mixed plaques remained constant at approximately 5%. The frequency of mixed plaques and recombinants were both increased several-fold when crosses were made in the presence of the DNA synthesis inhibitor 5-fluorodeoxyuridine (FUdR). In view of the genetic instability of the partially heterozygous genomes of mixed plaque forming virus this result is interpreted to mean that mixed plaques identify virus which is an intermediate in recombinant formation and that the molecular structure of their genome is probably that of a partial heteroduplex. Measurements of deoxynucleoside triphosphate pools showed that FUdR inhibited the large increase in the dTTP pool size which normally accompanies HSV-1 infection of BHK cells.",1977,2042
Chapter 34 - Pulmonary Mast Cells,Pranita Katwa and Jared M. Brown,"Mast cells are well known as key effector cells of allergic disease, act as sentinels of innate immunity, and are regulators of adaptive immune responses. Phenotypic plasticity, characteristics and key locations of resident mast cells in a variety of host tissues contribute to the versatility of the mast cell response. While classically identified with IgE dependent allergy and innate immunity, mast cells are also known to play an indispensible role in modulating other effector cells, and delineating subsequent adverse pulmonary outcomes. The activation and function of mast cells is integral to the etiology of numerous lung pathophysiologies, exacerbation of pre-existing pulmonary conditions and detrimental toxicant exposures. Several therapeutics targeting mast cell activation and downstream effects of mediators have been developed to neutralize allergy and hypersensitivity. Advancements in mast cell biology have renewed focus on the importance of mast cells in pulmonary physiology and disease pathology.",2015,2043
A Novel Secreted Tumor Antigen with a Glycosylphosphatidylinositol-Anchored Structure Ubiquitously Expressed in Human Cancers,Haruo Onda and Shoichi Ohkubo and Yasushi Shintani and Kazuhiro Ogi and Kuniko Kikuchi and Hideyuki Tanaka and Koji Yamamoto and Isamu Tsuji and Yoshihiro Ishibashi and Takao Yamada and Chieko Kitada and Nobuhiro Suzuki and Hidekazu Sawada and Osamu Nishimura and Masahiko Fujino,"In a search for novel genes expressed in human cancers, we identified one gene from an assembled expressed sequence tag database. Northern blot analysis revealed that the gene, termed alcan, was expressed in various types of human cancer cell lines and in the fetus, but not in normal tissues. The alcan gene is located on chromosome 6 and is encoded on a 246-amino-acid protein with weak homology to classical major histocompatibility complex class I. Its gene product, ALCAN, had hydrophobic amino acid clusters at both the N- and C-terminal regions and was predicted to be a glycosylphosphatidylinositol (GPI)-anchored membrane protein. Flow cytometric analysis revealed that ALCAN was detected on the surface of human cancer cells and on alcan-transfected CHO-K1 cells. ALCAN was also secreted from these cells, suggesting that some portion of the molecules was secreted by enzymatic cleavage by, for example, phospholipases. Mutational analysis of ALCAN suggested that the GPI-anchored position was the Ser216 residue. These findings indicate that ALCAN may be a potential target for cancer diagnosis or therapy.",2001,2044
Cardenolide triosides of oleander leaves,Fumiko Abe and Tatsuo Yamauchi,"Polar glycosides from the air-dried leaves were re-examined, and gentiobiosyl-nerigoside and gentiobiosylbeaumontoside isolated along with the major trioside, gentiobiosyl-oleandrin. Minor triosides also include glycosides of8β-hydroxy- and Δ16-8β-hydroxy-digitoxigenin, and Δ16-neriagenin, along with glycosides of known cardenolides, oleandrigenin, digitoxigenin, adynerigenin, neriagenin and their Δ16-derivatives.",1992,2045
Heat-inactivation of mango pectinesterase and polygalacturonase,Azza A.S. Labib and F.A. El-Ashwah and H.T. Omran and A. Askar,"The present work involves isolation of mango pectinesterase (PE) and polygalacturonase (PG), and investigating some of its characteristics, mainly with respect to heat-stability of the enzymes. Within a reaction time of 10 min, mango PE shows its maximal activity at pH 7.5 in a reaction mixture containing 1% citrus pectin and 0.1 or 0.2 m NaCl, at 55 °C. This enzyme possesses a Z value of 18.5 °C. Mango PG reveals its maximal activity in a reaction mixture containing 0.1% Na-polygalacturonate and 0.1 m NaCl. The reaction mixture was adjusted to pH 4.8 (McIlvaine buffer of 0.1 m) and 30–35 °C. Mango PG was less heat-stable than PE, with a Z value of 12.25 °C. The stepwise running of the inactivation diagrams of PE and PG emphasise the suggestion of the existence of more than one PE and more than one PG in mango enzyme extract.",1995,2046
Liposomal membranes. XV: Importance of surface structure in liposomal membranes of glyceroglycolipids,Kiyoshi Iwamoto and Junzo Sunamoto and Keizo Inoue and Tamao Endo and Shoshichi Nojima,"To elucidate the importance of the headgroup structure of glycolipids in the physicochemical properties of liposomal membranes of glycolipids, two diglucosyldialkylglycerols containing an α(1′ → 4′) or a β(1′ → 4′) glucoside linkage, 1,2-dihexadecyl-O-β-d-maltosyl(1′ → 3)-rac-glycerol (MAL-DG) and 1,2-dihexadecyl-O-β-d-cellobiosyl(1′ → 3)-rac-glycerol (CEL-DG) were employed. The fluorescence spectra and steady-state fluorescence anisotropy of 1,6-diphenyl-1,3,5-hexatriene in the deep hydrophobic domain of these liposomal bilayers and dansylhexadecylamine in the vicinity of glycerol backbone were measured, respectively. Compared with dipalmitoylphosphatidylcholine (DPPC) liposomes, the phase-transition temperatures (Tc) of the present two glycolipid liposomes were about 11–15°C higher and the fluidity at the surface of these glycolipid liposome was considerably lower. This means that the interaction between neighboring diglucoside headgroups may be stronger than that between phosphatidylcholine headgroups. Fluorospectrometric and CPK model studies suggested that the structural difference in anomerization and epimerization of the disaccharide moiety of glyceroglycolipids is an important factor for determining the physicochemical properties of these glycolipid liposomes.",1982,2047
Privatization as a financing alternative for desalination plants in Bahrain,Ahmed Khater and Sami Dannish and Mohammed Al-Ansari,"Production cost in the desalination plants remains to be a major constraint on the development and promotion of the water processing industry. Water is heavily subsidized in those countries where the only source of water for domestic supply is desalinated water. Searching for cost reduction measures, essentially involves analysis of the financing system under which a desalination plant is constructed, operated and maintained. In Bahrain, where natural water resources are extremely limited and fresh groundwater of requisite quality is becoming scarce, desalination of water is ranked as a must. The desalinated water is blended with groundwater to provide a domestic supply within the necessary quality limits. However, to keep pace with increasing demand and minimize the need to draw water from the aquifer, new desalination plants are required. In this paper the authors discuss the role of privatization as a financing alternative in the desalination industry in Bahrain, and its influence on the production cost. Available data from the existing operated plants in Bahrain throughout the past fifteen years are analyzed with a view to assert this possibility.",1994,2048
Transient swelling of liquid level during pool boiling in an emergency condenser,K.P. Singh and J.P. Gupta,A modified kettle reboiler is planned to be used as an emergency condenser for high pressure steam in nuclear power plants. The condensing is achieved by the boiling-off of water at atmospheric pressure. The bubbles released at the tubular surface have a finite residence time in the liquid column above the tubular surface. This ‘bubble holdup’ increases the level of the two-phase boiling system. It is important to know the level change in order to determine the free space available for disengagement of the entrapped droplets and the capacity of the condenser to condense the high pressure steam. A solution to this problem is presented below alongwith an actual case study.,1981,2049
The condition monitoring of heavily loaded spur gears,M.L. Atkin and E.D. Doyle,"A short-duration high-tooth-load test on case-hardened spur gears was run on an experimental gear rig with a view to comparing the performance of various condition monitoring techniques. The condition monitoring techniques used all relied on the detection of wear debris in the oil and included atomic absorption spectrometry, X-ray fluorescence, particle counting and ferrography. They all proved to be successful in signalling accelerated wear after 100 h of operation. Examination of the gears at the termination of the test indicated that the gears had worn by a combination of surface plastic deformation and pitting.",1983,2050
Thermal-hydraulic behavior of a marine reactor during oscillations,I. Ishida and T. Kusunoki and H. Murata and T. Yokomura and M. Kobayashi and H. Nariai,"The effect of ship motion, such as heaving and rolling, on the thermal-hydraulic behavior of marine reactors was investigated. The COBRA-IV-I CODE was modified to analyse the thermal-hydraulic performance on the critical heat flux under oscillating acceleration conditions. The critical heat flux in the code was verified experimentally using freon as a comparison. The Critical Heat Flux Ratio (CHFR) at the hottest channel of the PWR subchannel was analysed using the same code. A system code RETRAN-02/MOD2-GRAV was developed by improving RETRAN-02/MOD2 to simulate the thermal hydraulic transient under ship motion. It was verified by comparison using the experimental results of both two-phase natural circulation flow under heaving motion and single-phase natural circulation flow at an inclined attitude. The code was used to analyse reactor plant behavior in the nuclear ship Mutsu. Natural circulation flow during rolling motion was investigated experimentally. The characteristics of loop flow and core flow rates were clarified. The core flow rate correlated well with the Reynolds number of rolling motion.",1990,2051
Does the spontaneous adhesion of cultured plant cells to polymer surfaces have potential as an immobilization technique?,Frank Dicosmo and Peter J. Facchini and A.Wilhelm Neumann,,1988,2052
"Dorinnotheca streelii Fairon-Demaret, gen. et sp. nov., a new early seed plant from the upper Famennian of Belgium",Muriel Fairon-Demaret,"Dorinnotheca streelii Fairon-Demaret, gen. et sp. nov. is described from three upper Famennian localities in Belgium. This new early seed plant bears cupules hanging singly at the tips of the slender ultimate axes of a pinnately branched system. Each cupule encloses a single, centrally located preovule with four free integumentary lobes. At the base of the fertile unit, the cupule segments are fused for a quarter of their total length; higher up they are free, recurved and highly dissected. The integumentary lobes appear joined to the fused part of the cupule segments. The preovule is sessile; at the apex, the nucellus shows a long, tubular, salpinx-like extension. This distinctive type of organisation is discussed in relation to the other Late Devonian preovulate structures from which it appears different.",1996,2053
British records,,,1964,2054
A systems approach to the adaptation of sunflower to new environments I. Phenology and development,W.K. Anderson and R.C.G. Smith and J.R. McWilliam,"A key for defining three developmental and five phenological stages of the sunflower has been developed and used in the study of development rate in twenty crops spanning a range of environments across northern New South Wales, Australia. Three heat sum models using different statistical methods for deriving base temperatures were tested. The base temperatures varied widely depending both on the development stage of the crop and on the statistical method of derivation. The base temperature models were found to be no more accurate for predicting development than a simple heat sum model. In general the most accurate method for predicting development was found to be a multiple regression model. This model incorporated temperature soil moisture and daylength effects during the vegetative growth stage, and temperature and soil moisture effects during the reproductive and maturation stages of growth. A test of this model against an independent set of seven crops from two widely contrasting environments confirmed the generality of this type of model for sunflower.",1978,2055
CIPA: A collaborative intrusion prevention architecture for programmable network and SDN,Xiao-Fan Chen and Shun-Zheng Yu,"Coordinated intrusion, like DDoS, Worm outbreak and Botnet, is a major threat to network security nowadays and will continue to be a threat in the future. To ensure the Internet security, effective detection and mitigation for such attacks are indispensable. In this paper, we propose a novel collaborative intrusion prevention architecture, i.e. CIPA, aiming at confronting such coordinated intrusion behavior. CIPA is deployed as a virtual network of an artificial neural net over the substrate of networks. Taking advantage of the parallel and simple mathematical manipulation of neurons in a neural net, CIPA can disperse its lightweight computation power to the programmable switches of the substrate. Each programmable switch virtualizes one to several neurons. The whole neural net functions like an integrated IDS/IPS. This allows CIPA to detect distributed attacks on a global view. Meanwhile, it does not require high communication and computation overhead. It is scalable and robust. To validate CIPA, we have realized a prototype on Software-Defined Networks. We also conducted simulations and experiments. The results demonstrate that CIPA is effective.",2016,2056
"Meeting Report: Xth International Conference on Harmful Algae, St. Pete Beach, Florida, October 21–25, 2002",Karen A. Steidinger,,2003,2057
Human-robot collaborative assembly: a use-case application,Andrea Casalino and Filippo Cividini and Andrea Maria Zanchettin and Luigi Piroddi and Paolo Rocco,"Future production plants will see more and more the presence of humans actively collaborating with robots. Typical scenarios are those related to assembly lines. In such contexts the behaviour of humans is not controllable. Therefore the problem is to produce a scheduling for robotic activities accounting for the predicted human intentions. In this work we propose a scheduling strategy based on Time Petri Nets. The sequence of commands scheduled for the robot are executed exploiting the receding horizon principle. The developed approach is applied on a realistic use-case, in which one human operator and a dual-arm robot actively collaborate to perform the assembly of a real product.",2018,2058
Analysis of PRLR and BF Genotypes Associated with Litter Size in Beijing Black Pig Population,WANG Xing-ping and WANG Li-xian and LUO RENG Zhuo-ma and SUN Shi-duo,"This study is aimed at using the DNA mutations in the prolactin receptor (PRLR) and properdin (BF) genes to determine associations between the genotype and litter size in the Beijing Black pig population. A total of 321 Beijing Black pig sows were genotyped using the polymerase chain reaction-restriction fragment length polymorphism (PCR-RFLP) method, with the Alu I and Sma I for PRLR and BF genes, respectively. Two different alleles of PRLR and BF genes were identified: allele A (0.25) and B (0.75) of the PRLR gene, allele A (0.13) and B (0.86) of the BF gene. The association analysis between the genotypes and the litter size were estimated with the method of the general linear model. The analysis results of PRLR showed that in first parity, sows with genotype AA had a larger litter size than sows with genotype AB and BB, but the difference was statistically not significant. In later parities, statistically significant (P < 0.05) differences were seen between sows with genotypes AA and AB, and BB of the PRLR gene. The associated analysis results between genotypes and litter size (total number born, TNB, and number born alive, NBA) showed that there were no significant differences in the first parity sows with different genotypes of the BF gene, but significant differences appeared in NBA between the sows of genotypes AB and BB, in later parity, for which significantly higher values were observed in the offspring of heterozygotes. Considering the consistent genotypic effect on the litter size of both sows in first parity and later parity, it was concluded that the locus of the PRLR gene, digested with Alu I, could be the gene maker for the litter size in Beijing Black pigs.",2008,2059
"Limnogeologic studies on an intertrappean continental deposit from the northern Ethiopian Plateau (37°03′E, 12°25′N)",Kedamawit Yemane and Maurice Taieb and Hugues Faure,"The Chilga lacustrine deposit from a small graben in the heart of the Northwestern Ethiopian Highlands (37°E, 12°N) is one of the ubiquitous intertrappean continental sedimentations on the Plateau. The basalt layer which makes the bottom of the basin has been dated as 8 Ma. The lacustrine sedimentation occurred contemporaneously with active volcanic phases in the region. Silicic aggregates are common in the cements and as important mineralogic constitutents of these phases whereas periods of calm sedimentation are characterized by thick lignite seams and the presence of authigenic minerals such as pyrite and vivianite. The sequence shows a general upward fining and evolution from shallow fluviatile to reduced lacustrine basin. The palynoflora from the sequence has a unique palaeofloral assemblage where the abundance of Guineo-Congolian-like pollen taxa, pteridophytes and absence of conifers imply a regional palaeoaltitude much lower than at present. The uplift of the Plateau at a rate of 0.1 mm yr−1 similarly suggests palaeotitudes of ca 900–1000 m.",1987,2060
Effects of hormones on the RNA-synthesis of Tetrahymena pyriformis,G. Csaba and L. Ubornyák,,1981,2061
Characterisation of the photosynthetic response of tobacco leaves to ozone: CO2 assimilation and chlorophyll fluorescence,Elena Degl'Innocenti and Lucia Guidi and Gian Franco Soldatini,"Summary
The present paper reports on the characterisation of the photosynthetic responses to a single pulse of ozone (150 ppb for 5 hours) in the leaves of two tobacco cultivars displaying different degrees of sensitivity to O3 (BelB, O3-resistant and BelW3, O3-sensitive). In the BelW3 cultivars, O3 induced a decrease in the photosynthetic rate, but not in actual PSII efficiency at steady-state photosynthesis. The reduction state of QA did not change in these plants while a strong decrease in intrinsic PSII efficiency was observed. The quantum yield for photosynthetic CO2 assimilation decreased more than the actual PSII efficiency, suggesting the presence of a significant fraction of electron transport to molecular oxygen or the existence of some form of cyclic electron flow. O3-treated leaves reduced the excess of absorbed light (i.e. light that couldn't be used in photosynthesis) by dissipating a large part of the light absorbed by the PSII antenna as heath. In BelB, the CO2 assimilation rate did not change in the presence of the pollutant O3 and the only gas exchange parameter that changed was the stomatal conductance, which significantly increased. Some of the Chl fluorescence parameters changed after O3 fumigation, but returned to values similar to the controls when measured 24 hours after removing the stress. The only Chl fluorescence parameter that significantly increased during the recovery phase was the 1-qP",2002,2062
Biological activity of fibrinogen adsorbed on synthetic materials,C.A Ward and D Stanga,"The adsorption isotherm for porcine fibrinogen on silicone rubber is reported for fibrinogen adsorbing from a physiological buffer. The isotherm is found to have two plateaus as a function of solution concentration. The activity of fibrinogen adsorbed at different surface concentrations in promoting platelet adhesion is investigated by preadsorbing fibrinogen on the surface of the biomaterial and subsequently exposing it to fresh porcine blood, without any exposure to air. It is found that platelet adhesion increases with increasing concentration of preadsorbed fibrinogen until the surface concentration exceeds that in the first plateau. Higher surface concentrations of preadsorbed fibrinogen do not promote further platelet adhesion. This would indicate that the fibrinogen preadsorbed in the first plateau is conformationally changed sufficiently to promote platelet adhesion, but that in the second plateau is apparently not changed to the same extent.",1986,2063
Lipase Activity in the Trunkwood of Tilia cor dataMill.,W. Höll,"Summary
Milled sawdust of the trunkwood of Tilia cordataMill., exhibits lipolytic activity when incubated with emulsified olive oil. The lipase of the peripheral trunk zone shows optimal activity at a pH of 7.5 and between 37–45 °C. The lipase is neither inhibited nor activited by sodiumdesoxycholate. Lipase activity decreases towards the centre of the trunk. Washing the peripheral wood particles with water prior to incubation results in an increased lipolytic activity. The inhibtor extracted appears to be largely water insoluble in the older wood zones. In the water extracts, only traces of lipase activity are measurable. Most of the lipase seems to be tightly associated with the woody cell walls. The acidic value of the lipids extracted from the different radial wood zones suggests that only the peripheral trunkwood tissue exhibits high lipase activity.",1975,2064
Particle segregation in fine powders by tapping as simulation of jostling during transportation,D.S. Parsons,"Segregation of powder particles by tapping or vibration is a phenomenon caused by a sifting of fines toward the bottom of a container, and has been reported in the literature for systems with particle diameters in the range of 0.2 to 12 mm. Extrapolation of this effect to micrometer-sized particle systems, with and without the presence of agglomerates, was undertaken. The particle-size distribution were measured by instruments appropriate to each powder system. Five powders, with four different absolute densities (2.8 – 19.3 g/cm3) and with FSSS numbers from 0.9 to 8.0 were sudied. Jostling of powders in containers during transportation was simulated by controlled tapping of the powders in a demountable cylinder. Segregation did take place when a wide distribution was present, unless masked by the break-up of afflomerates and consequent generation of fines during tapping.",1976,2065
A photoacoustic spectrometer for measuring heat dissipation and oxygen quantum yield at the microscopic level within leaf tissues,Tao Han and Thomas C. Vogelmann,"A photoacoustic instrument has been developed to measure photothermal signals and oxygen evolution at the microscopic level within leaves and leaf cross sections. The device utilizes lines of light, obtained from laser diodes, that are focussed on the sample and permit measurements with a spatial resolution of 40 μm. In cross sections of spinach leaves (700 μm thick) the profile for the photothermal signals is bell shaped and coincides closely with the chlorophyll content of the tissues. By contrast, the profile for oxygen evolution is relatively flat, suggesting uniform capacity for oxygen evolution in the mesophyll. Maximum values for relative quantum yield for oxygen evolution are found in palisade tissues near the adaxial leaf surface. The photoacoustic device shows that it is possible to collect thermal and oxygen signals from microscopic leaf samples and opens a new avenue for studies of leaf structure—function.",1999,2066
Permanently inserted plastic cannula for direct access to cecal contents of rats,Wesley J. Peterson,,1956,2067
The effect of the beta-mimetic drug clenbuterol on maternal and fetal behavior,J.W. Wladimiroff and P.J. Roodenburg,"The effect of the beta-mimetic compound clenbuterol on maternal and fetal behavior was studied in 12 normal primigravidae between 35 and 38 wk of gestation. None of the subjects exhibited any signs of uterine activity. Following a control period of 60 min, oral intake of 100 μg of clenbuterol resulted in a maximum increase in maternal and fetal heart rate of 13 and 7%, respectively. Maternal systolic blood pressure showed a rise of 6%, whereas diastolic blood pressure demonstrated a drop of 8%. The percentage incidence of fetal breathing and trunk movements did not show any significant change following oral clenbuterol administration, indicating the absence of any major effect of this drug on the fetal central nervous system. Maternal blood glucose levels demonstrated a fairly constant pattern during the entire study period.",1982,2068
Status report: electro-nuclear physics at NBS,Samuel Penner,,1985,2069
Using network-based text analysis to analyze trends in Microsoft's security innovations,Tabitha L. James and Lara Khansa and Deborah F. Cook and Olga Bruyaka and Kellie B. Keeling,"As the use of networked computers and digital data increase, so have the reports of data compromise and malicious cyber-attacks. Increased use and reliance on technologies complicate the process of providing information security. This expanding complexity in supplying data security requirements coupled with the increased recognition of the value of information, have led to the need to quickly advance the information security area. In this paper, we examine the maturation of the information security area by analyzing the innovation activity of one of the largest and most ubiquitous information technology companies, Microsoft. We conduct a textual analysis of their patent application activity in the information security domain since the early 2000's using a novel text analysis approach based on concepts from social network analysis and algorithmic classification. We map our analysis to focal areas in information security and examine it against Microsoft's own history, in order to determine the depth and breadth of Microsoft's innovations. Our analysis shows the relevance of using a network-based text analysis. Specifically, we find that Microsoft has increasingly emphasized topics that fall into the identity and access management area. We also show that Microsoft's innovations in information security showed tremendous growth after their Trustworthy Computing Initiative was announced. In addition, we are able to determine areas of focus that correspond to Microsoft's major vulnerabilities. These findings indicate that while Microsoft is still actively, albeit not always successfully, fighting vulnerabilities in their products, they are quite vigorously and broadly innovating in the information security area.",2013,2070
Chapter 2 - Literature Review,R. McIvor and P.K. Humphreys and A.P. Wall and A. McKittrick,"Publisher Summary
This chapter identifies the limitations of the existing outsourcing literature and practices. This also outlines the requirements for an outsourcing framework. A clear appreciation of the objectives and risks of outsourcing is an essential pre-condition to the formation of suitable outsourcing performance measurements. According to a survey in 2006, the global business process outsourcing market was worth $48.4 billion for the first three quarters of 2006, representing a rise of 34% from the same period in 2005. Outsourcing can be employed to achieve performance improvements in cost, quality, service, and time-to-market. Examples of business services include computer services, professional services (legal, accountancy, market research, technical, engineering, advertising, human resources and consultancy), research and development (R&D), recruitment agencies and call centers. Between 1984 and 2001, the growth in the business services sector accounted for around one-third of the total output growth in the UK economy. A comprehensive survey of outsourcing in 2003 in the European banking industry by McKinsey, in collaboration with IBM and the EFMA, found that 44% of banks considered efficiency gains as the primary or even sole outsourcing objective. However, failure to place outsourcing within a strategic context will lead to a piecemeal approach based solely on attempts to reduce costs. Thus, a new focus on quality and customer relationships necessitates changes in policies, cultural values, work procedures and processes, relationship between departments and interactions between buyers and suppliers.",2009,2071
Closed-loop Battery Aging Management for Electric Vehicles,Gabriele Pozzato and Matteo Corno,"In this work, a closed-loop battery aging management strategy for electric vehicles is proposed. The aging management strategy, following the model predictive control rationale, optimizes aging and vehicle performance online. The proposed formulation is based on a closed-loop term which aims at tracking a user defined aging profile. A thorough simulation study validates the approach and verifies its robustness against model uncertainties and anomalous aging phenomena.",2020,2072
Bacterial epibiosis on Bahamian and Pacific ascidians,Martin Wahl,"Interactions between epibiotic bacteria and organisms possibly play a central role in marine ecology. Despite its potential significance, this field has long time been neglected. For most aquatic taxa nothing is known about presence/absence of bacteria on their surface, much less about specific associations or potential interactions between epibiotically associated micro- and macroorganisms. Bahamian and Pacific ascidians, most of them colonial, were screened for the presence, abundance and diversity of epibiotic bacteria and macroepibionts. Only one species, Polyclinum planum, occasionally carried macroepibionts. All ascidian species exhibited varying densities of epibiotic bacteria on their surfaces. Average epibacterial abundance as assessed by plate counts on the 29 species ranged from 60 to 1.2 × 107/cm2. Significant differences in bacterial abundances were observed between species, families and geographical regions. On the family level, Polyclinidae were the most densely colonized. Bahamian species exhibited less dense epibacterial communities than Pacific species, a difference that may partly be caused by the absence of the heavily fouled Polyclinidae from the Bahamian collection. Diversity of culturable strains, evaluated for the Bahamian species only, was uniformly high on most species. I did not find any evidence for specific associations (as reflected by dominance of single strains) between culturable bacteria and ascidian species. Contrarily, direct observation by epifluorescence revealed the presence of an apparently dominant photosynthetic symbiont on several didemnid species. The presence of this symbiont correlated negatively with abundance and diversity of culturable epibionts. This negative correlation could reflect properties of the host's surface which selectively favor proliferation of the symbiont or antagonistic interactions between the symbionts and other potential bacterial colonizers.",1995,2073
Trusted Cloud Computing Architectures for infrastructure as a service: Survey and systematic literature review,Fady {A. M. Ibrahim} and Elsayed E. Hemayed,"Cloud computing is no longer the future but the present. Security and trust are critical in cloud computing, but how can cloud service tenants trust cloud service providers to store all their private data on the cloud? Trusted computing is one of the new technologies in the last decade, and the integration between cloud computing and trusted computing can create a new architecture for infrastructure as a service that motivates more cloud service tenants to trust cloud service providers. This paper provides a survey and systematic literature review on the suggested architectures for this integration.",2019,2074
Chapter 1 - Defining a Firewall,Rob Cameron and Neil R. Wyler,"Publisher Summary
This chapter discusses various aspects of firewall. A firewall is a chokepoint from one network to another network. Firewalls are also being used to create chokepoints between other networks in an enterprise environment. Various information security studies and surveys have found that the majority of attacks come from inside an organization. Hackers attempt to discover systems and gather information. In most cases, these attacks are used to gather information to set up an access or a Denial of Service (DOS) attack. Phishing, the new information gathering technique, is spreading and becoming increasingly sophisticated. Phishing e-mails either ask the victim to fill out a form or directs them to a Web page designed to look like a legitimate banking site. Application proxy firewalls are written to the specifications of the particular applications, and the selection of a particular application proxy is mostly determined by the type of application server being protected. Fine control of application proxies is lost in favor of better performance. It is suggested that stateful inspection gateways can be used internally to further protect subnets, and filter the information coming in from the Internet.",2007,2075
Absence of a ‘cholinergic link’ in the apomorphine-induced feedback inhibition of dopamine synthesis in rat striatum,Barbara J. {Van Zwieten-Boot} and Annelies Petri-Bot,"The blocking effect of apomorphine on the rise in striatal dopamine (DA) content, induced by 1-hydroxy-3-amino-pyrrolidone-2 (HA-966) was taken as a measure for the intrastriatal feedback inhibition of DA synthesis. The effects of cholinergic drugs on this feedback system were assessed in order to verify the hypothesis that this mechanism is mediated via an intrastriatal cholinergic link. We presumed that DA receptors were located on a cholinergic neuron, while the cholinergic terminals in turn made direct or indirect axon-axonal contact with the dopaminergic nigro-striatal pathway (N.S.P.). Although cholinergic agents could modify the effect of HA-966 on striatal DA content, it proved to be impossible to counteract the blocking effect of apomorphine with cholinergic drugs as was to be expected. Therefore we concluded that the effect of apomorphine was not brought about in the way which had been postulated.",1976,2076
"Ligand substitution reaction of (μ-H)Os3(CO)10(μ-COMe) with 1,1'-bis(diphenylphosphino) ferrocene",Wen-Yann Yeh and Sun-Bin Chen and Shie-Ming Peng and Gene-Hsiang Lee,"Reaction of (μ-H)Os3(CO)10(μ-COMe) with 1,1'-bis(diphenylphosphino)-ferrocene (dppf) produces (μ-H)Os3(CO)8(μ-COMe){μ-η2-(η5-C5H4PPh2)2Fe} (1) and (μ-H)2Os3(CO)7(μ-COMe){μ-η3-(η5-C5H3PPh2)Fe(η5-C5H4PPh2)} (2). Thermolysis of 1 leads quantitatively to 2. These compounds have been characterized by 1H, 31P, and 13C NMR, IR, and mass spectroscopies. Compound 2 crystallizes in space group P 21/c with a = 11.898(2), b = 21.266(3), c = 18.262(3) Å, β = 104.71(1)°, V = 4469(1) Å3, Z = 4, and RF = 0.029.",1994,2077
Chromatographic behaviour of alkaloids on thin layers of anion and cation exchangers: I. AG 1-X4 and Cellex D,L. Lepri and P.G. Desideri and M. Lepori,"The chromatographic behavior of 48 alkaloids has been studied on Bio-Rad AG 1-X4, Cellex D and microcyrstalline cellulose, eluting with solutions of different pH but constant ionic strength (0.5). Many interesting separations were effected on both AG 1-X4 and Cellex D layers. The influence of pH on the chromatographic behaviour of alkaloids has been quantitatively studied and an equation was used that expresses the behaviour of the alkaloids on both AG 1-X4 (AcOt-) and microcrystalline cellulose layers. The non-applicability of this equation to Cellex D layers is discussed.",1976,2078
Pliocene-pleistocene vegetational and climatic evolution of the south-central mediterranean,Remo Bertoldi and Domenico Rio and Robert Thunell,"The vegetational history of the south-central Mediterranean clearly reflects the major global climatic changes of the last four million years. A Mediterranean-type climate, characterized by strong seasonality and a dry summer, may have existed in this region during the early Pliocene. A short-term climatic cooling at approximately 3.2 Ma resulted in the temporary establishment of a humidity-demanding flora; the onset of major Northern Hemisphere glaciation at approximately 2.4 Ma initiated an alternation between humid (glacial) and dry (interglacial) conditions. Vegetational differences between this region and the north-west Mediterranean indicate that distinct latitudinal climatic gradients probably existed in the Mediterranean during the Pliocene-early Pleistocene.",1989,2079
21 - Intergeneric Hybrids involving the Genus Hordeum,GEORGE FEDAK,,1991,2080
The possible involvement of oscillatory cAMP signaling in multicellular morphogenesis of the cellular slime molds,Pauline Schaap and Mei Wang,"The involvement of pulsatile chemoattractant emission and signal relay in aggregation and multicellular morphogenesis of a variety of cellular slime mold species was investigated. The species differ from each other in the developmental stage when pulsatile signaling first becomes evident. In D. discoideum, D. mucoroides, and D. purpureum pulsatile signal emission starts in the preaggregative field. In D. vinaceo-fuscum, D. mexicanum, P. violaceum, and P. pallidum the aggregation centers shifts from continuous to pulsatile secretion of chemoattractant during the aggregation process. In D. minutum pulsatile signaling starts after the completion of aggregation and slightly before the onset of culmination. Tip formation is a consequence of continued attraction of amoebae inside the aggregate to the center of signal emission. The occurrence of pulsatile signaling at an early stage of development is correlated with the capacity of the tip (signaling center) to organize a relatively large number of cells into a single fruiting body. Several lines of evidence suggest that cAMP is probably involved in the coordination of morphogenetic movement in the multicellular stage of all investigated species.",1984,2081
Elimination of plastids during spermatogenesis and fertilization in the plant kingdom,Barbara B. Sears,"Ultrastructural and genetic investigations involving diverse species of plants have demonstrated that plastids may be transmitted either biparentally or maternally during sexual reproduction. In species in which plastid transmission is maternal, elimination of plastids from the paternal parent may occur in a number of ways: exclusion from the male gamete during spermatogenesis, loss from the motile sperm, exclusion during fertilization, or degradation within the zygote. These diverse ways in which maternal inheritance of plastids is achieved suggest that this inheritance pattern may have evolved independently many times in response to different selective pressures in different phyletic lineages.",1980,2082
Information System Planning: A case review,S. Gill,"Information System Planning (ISP) is a structured approach developed by IBM to assist organizations in establishing a plan to satisfy their short and long term information requirements. The ISP methodology was implemented at Tel-Aviv University. A comprehensive plan for the development of a Management Information System (MIS) was derived. This paper presents a review of the process by which the plan was obtained, a discussion of the methodology, and its ramifications.",1981,2083
Chapter 5 - Windows Forensic Analysis,Ryan D. Pittman and Dave Shaver,"Publisher Summary
This chapter provides technical methods and techniques to help practitioners extract and interpret data of investigative value from computers running Windows operating systems. An important aspect of conducting advanced forensic analysis is understanding the mechanisms underlying fundamental operations on Windows systems such as the boot process, file creation and deletion, and use of removable storage media. By understanding how to aggregate and correlate data on Windows systems, digital investigators are better able to get the “big picture” (such as an overall theory of user action and a timeline), as well as overcoming specific technical obstacles. It is not surprising that the majority of systems that digital investigators are called upon to examine run a Windows operating system. Whether investigating child pornography, intellectual property theft, or Internet Relay Chat (IRC) bot infection, it is a safe bet that knowledge of Windows operating systems, and its associated artifacts, will aid investigators in their task. It is important for forensic examiners to understand the Windows startup process for a number of reasons beyond simply interrupting the boot process to view and document the CMOS configuration. Ever since examiners figured out that there might be more to a file than meets the eye, they have been interested in Metadata, the information that describes or places data in context, without being part of the data that is the primary focus of the user. There are two types of metadata: file system metadata and application (or file) metadata.",2010,2084
"5 - Interactions between Rising CO2, Soil Salinity, and Plant Growth",Rana Munns and Grant R. Cramer and Marilyn C. Ball,"Publisher Summary
This chapter describes the global impact of salinity and the consequences of elevated CO2 on plant growth and soil salinity. The cellular and whole plant mechanisms by which salinity affects growth are described, with emphasis on the mechanism by which elevated CO2 might interact with salinity. Much of the saline land in the world has been caused by human activities: by clearing, overgrazing, or the installation of irrigation schemes. The result of these activities is termed “secondary salinization.” The long-term productivity of the global agricultural system as a whole is declining, as is ecosystem stability. Rising CO2 levels could increase the productivity of cultivated species and natural vegetation on saline soils, but it may also increase soil salinity. Elevated CO2 can increase the growth rate of crops, pastures, and trees, but because it also increases their water use efficiency, it may alter the soil-plant water balance in a way that has adverse consequences for land with saline groundwater, where any factor that causes the water table to rise will increase the rate of salinization of the topsoil.",1999,2085
Developmental response of zygotes exposed to similar mutagens,W.M. Generoso and A.G. Shourbaji and W.W. Piegorsch and J.B. Bishop,"Exposure of mouse zygotes to ethylene oxide (EtO) or ethyl methanesulfonate (EMS) led to high incidences of fetal death and of certain classes of fetal malformations (Generoso et al., 1987, 1988; Rutledge and Generoso, 1989). These effects were not associated with induced chromosomal aberrations (Katoh et al., 1989) nor are they likely to be caused by gene mutations (Generoso et al., 1990). Nevertheless, the anomalies observed in these studies resemble the large class of stillbirths and sporadic defects in humans that are of unknown etiology, such as cleft palate, omphalocoel, clubfoot, hydrops and stillbirths (Czeizel, 1985; Oakley, 1986). Therefore, we continue to study the possible mechanisms relating to induction of these types of zygote-derived anomalies in mice. Effects of zygote exposure to the compounds methyl methanesulfonate (MMS), dimethyl sulfate (DMS), and diethyl sulfate (DES), which have similar DNA-binding properties as EtO and EMS, were studied. DMS and DES, but not MMS, induced effects that are similar to those induced by EtO and EMS. Thus, no site-specific alkylation product was identifiable as the critical target for these zygote-derived anomalies. We speculate that the developmental anomalies arose as a result of altered programming of gene expression during embryogenesis.",1991,2086
On-Line Trajectory Encoding for Discrete-Observation Process Monitoring,L.E. Holloway and B.H. Krogh,"Traditional process monitoring techniques are unable to utilize the delayed, infrequent, and sometimes uncertain information available in many complex industrial processes. This paper presents the trajectory encoding method for process monitoring that is specifically designed to handle this type of information. Using a behavioral model specification of the range of admissible dynamic behaviors, a graphical representation of the system trajectories consistent with the process data is constructed on line as observations are received from the process. The monitor reports when the observations are no longer consistent with the behavioral model specification, giving timely fault-detection information to the operator. This paper describes the elements of the trajectory encoding method and its application to computer-controlled industrial processes.",1992,2087
"Campanian to paleocene spore and pollen assemblages of Seymour Island, Antarctica",Rosemary A. Askin,"Spore and pollen assemblages in Campanian to Paleocene nearshore marine to deltaic sediments of Seymour Island, Antarctica, are characterized by abundant podocarpaceous conifer pollen, diverse provincial angiosperm pollen and cryptogam spores of low diversity. These assemblages resemble coeval assemblages from New Zealand, southeastern Australia and southern South America. The palynoflora reflects primarily conifer-dominated rainforest growing in cool to warm temperate paleoclimates. The late Maastrichtian was a comparatively warm interval, with a humid equable paleoclimate. The Antarctic Peninsula cordillera probably supported altitudinally zoned plant associations and it is likely that climatic fluctuations during the Campanian to Paleocene resulted in altitudinal shifts of these vegetation zones.",1990,2088
Manipulation of terrain data for a real-time display application,JR Vaughan and GR Brookes and MA Fletcher and DPM Wills,"As part of a real-time control application, there is a need for a graphics display for data which is stored as a terrain data base. Due to space and cost limitations of the application, a system has been developed to work on a microcomputer system which includes transputers to allow parallel processing. This paper describes the data representation and manipulation which has been undertaken in order to provide a real-time graphical display. This data representation is suitable for use in a parallel processing environment.",1991,2089
Oligosaccharides related to xyloglucan: Synthesis and X-ray crystal structure of methyl 2-O-(α-l-fucopyranosyl)-β-d-galactopyranoside,Derek K. Watt and Donald J. Brasch and David S. Larsen and Laurence D. Melton and Jim Simpson,"The disaccharides methyl 2-O-(α-l-fucopyranosyl)-β-d-galactopyranoside and methyl 2-O-(α-l-fucopyranosyl)-α-d-galactopyranoside have been synthesised using the assisted halide reaction of tri-O-benzyl-α-l-fucopyranosyl bromide with methyl 3,4,6-tri-O-benzyl-β-d-galactopyranoside and methyl 3,4,6-tri-O-benzyl-α-d-galactopyranoside to construct the interresidue glycosidic linkages. A crystal structure of methyl 2-O-(α-l-fucopyranosyl)-β-d-galactopyranoside was determined using Mo-Kα X-ray data at 183 K. The space group is P1 (No. 1) with the unit cell containing two molecules of the disaccharide with unique conformations and a water molecule. The structure was refined to R = 0.0566 for 2969 reflections. The l-fucopyranosyl and d-galactopyranosyl residues have the nominal 1C4 and 4C1 conformations, respectively. The interresidue torsion angles are comparable with those generated in a recent molecular modelling study.",1996,2090
The effect of hormone replacement therapy on arterial distensibility and compliance in perimenopausal women: a 2-year randomised trial,I.C.D Westendorp and M.J.J {de Kleijn} and M.L Bots and A.A.A Bak and J Planellas and H.J.T {Coelingh Bennink} and A Hofman and D.E Grobbee and J.C.M Witteman,"A single centre randomised placebo-controlled trial was performed to assess the 2-year effects of hormone replacement therapy compared to placebo on mechanical arterial properties in 99 perimenopausal women recruited from the general population. The trial was double-blind with respect to a sequential combined regimen of oral 17β-oestradiol and desogestrel (17βE2-D) and the placebo group and open with respect to combination of conjugated equine oestrogens and norgestrel (CEE-N). At baseline, distensibility and compliance of the common carotid artery were measured non-invasively with B-mode ultrasound and a vessel wall movement detector system, and the distensibility coefficient (DC) and compliance coefficient (CC) were calculated. Measurements were repeated after 6 and 24 months. Change in DC and CC in treatment groups was compared to placebo. After 24 months, changes for 17βE2-D compared to placebo were −1.4×10−3/kPa (95% CI −4.4; 1.7, P=0.39) for DC and 0.26 mm2/kPa (95% CI −0.01; 0.53, P=0.07) for CC. Changes for CEE-N compared to placebo were 0.4×10−3/kPa (95% CI −1.0; 1.9, P=0.79) and 0.11 mm2/kPa (95% CI −0.14; 0.37, P=0.40). For systolic blood pressure (SBP), diastolic blood pressure (DBP) and arterial lumen diameter no changes were found. In this study no significant differences in changes in distensibility and compliance were found between perimenopausal women using 17βE2-D or CEE-N and women using placebo after 6 and 24 months.",2000,2091
Reviews and Analysis of Special Reports,Leslie Stebbins,,2014,2092
Autonomic protection of multi-tenant 5G mobile networks against UDP flooding DDoS attacks,Ana {Serrano Mamolar} and Pablo Salvá-García and Enrique Chirivella-Perez and Zeeshan Pervez and Jose M. {Alcaraz Calero} and Qi Wang,"There is a lack of effective security solutions that autonomously, without any human intervention, detect and mitigate DDoS cyber-attacks. The lack is exacerbated when the network to be protected is a 5G mobile network. 5G networks push multi-tenancy to the edge of the network. Both the 5G user mobility and multi-tenancy are challenges to be addressed by current security solutions. These challenges lead to an insufficient protection of 5G users, tenants and infrastructures. This research proposes a novel autonomic security system, including the design, implementation and empirical validation to demonstrate the efficient protection of the network against Distributed Denial of Service (DDoS) attacks by applying countermeasures decided on and taken by an autonomic system, instead of a human. The self-management architecture provides support for all the different phases involved in a DDoS attack, from the detection of an attack to its final mitigation, through making the appropriate autonomous decisions and enforcing actions. Empirical experiments have been performed to protect a 5G multi-tenant infrastructure against a User Datagram Protocol (UDP) flooding attack, as an example of an attack to validate the design and prototype of the proposed architecture. Scalability results show self-protection against DDoS attacks, without human intervention, in around one second for an attack of 256 simultaneous attackers with 100 Mbps bandwidth per attacker. Furthermore, results demonstrate the proposed approach is flow-, user- and tenant-aware, which allows applying different protection strategies within the infrastructure.",2019,2093
EAD-DNN: Early Alzheimer's disease prediction using deep neural networks,Preethi Thangavel and Yuvaraj Natarajan and K.R. {Sri Preethaa},"Early Alzheimer’s disease (EAD) diagnosis enables individuals to take preventative actions before irreversible brain damage occurs. Memory and thinking skills get worse in alzheimer disease, making it hard to do basic things. The abnormal buildup of amyloid and tau proteins in and around brain cells is thought to cause it. When amyloid builds up, it forms plaques around brain cells. Inside brain cells, tau tangles form when it accumulates. Healthy brain cells are damaged by the tangles and plagues, which causes them to shrink. The hippocampus, a part of the brain that aids in memory formation, appears to be the location of this damage. There are currently no methods that give the most accurate results and suggestions. With the methods we have now, alzheimer disease is not found early. So, we said that the Early Alzheimer’s disease - Deep Neural Network (EAD-DNN) method has found a way to predict alzheimer disease earlier. The Magnetic Resonance Imaging (MRI) dataset in the Comma Separated Value (CSV) format has been used by the EAD-DNN method. Convolutional Neural Network (CNN), the deep Residual Network (ResNet) has been used to train the MRI image dataset. This ResNet model can get more information from network levels with the help of Deep ResNet.The modified adam optimization has selected the best feature information from MRI scans of alzheimer patients and transferred it to another area while keeping the most important data. Using the EAD-DNN approach, a multi-class classification has been carried out. The extensive experiments show that the suggested method can achieve an accuracy rate of 98%.",2023,2094
Value drivers of blockchain technology: A case study of blockchain-enabled online community,Yujie Zheng and Wai Fong Boh,"There is growing recognition that blockchain technology has significant potential to alter how organizations and people work and communicate. However, theoretical guidance concerning how organizations leverage blockchain technology to enhance value creation for users is still limited. Grounded in the socio-technical perspective and leveraging the rich data obtained from case analyses of blockchain-enabled online communities, this paper develops a theoretical model to identify the core value drivers that blockchain enables for online communities. The core value drivers include: a reputation-value system, data ownership mechanisms, and verification & tracking mechanisms. Our findings suggest that these three value drivers enhance value creation of online communities by motivating participation and protecting contributions.",2021,2095
LogDoS: A Novel logging-based DDoS prevention mechanism in path identifier-Based information centric networks,Basheer Al-Duwairi and Öznur Özkasap and Ahmet Uysal and Ceren Kocaoğullar and Kaan Yildirim,"Information Centric Networks (ICNs) have emerged in recent years as a new networking paradigm for the next-generation Internet. The primary goal of these networks is to provide effective mechanisms for content distribution and retrieval based on in-network content caching. Several network architectures were proposed in recent years to realize this communication model. This include Named Data Networks (NDN) and Path-Identifier (PID) based ICN. This paper proposes LogDoS as a novel mechanism to address the problem of data flooding attacks in PID-based ICNs. The proposed LogDoS mechanism is a unique hybrid approach that combines the best of NDN networks and PID-based ICNs, and it is the first to employ Bloom-filter based logging approach in a novel way to filter attack traffic efficiently. In this context, we develop and model three versions of LogDoS with varying levels of storage overhead at LogDoS-enabled routers. Extensive simulation experiments show that LogDoS is very effective against DDoS attacks as it can filter more than 99.98% of attack traffic in different attack scenarios while incurring acceptable storage overhead.",2020,2096
The Robot Operating System: Package reuse and community dynamics,Pablo Estefo and Jocelyn Simmonds and Romain Robbes and Johan Fabry,"ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem.",2019,2097
Towards complex product line variability modelling: Mining relationships from non-boolean descriptions,Jessie Carbonnel and Marianne Huchard and Clémentine Nebut,"Software product line engineering relies on systematic reuse and mass customisation to reduce the development time and cost of a software system family. The extractive adoption of a product line requires to extract variability information from the description of a collection of existing software systems to model their variability. With the increasing complexity of software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of existing boolean variability models, such as multi-valued attributes or UML-like cardinalities, were proposed to enhance their expressiveness and support variability modelling in complex product lines. In this paper, we propose an approach to extract complex variability information, i.e., involving features as well as multi-valued attributes and cardinalities, in the form of logical relationships. This approach is based on Formal Concept Analysis and Pattern Structures, two mathematical frameworks for knowledge discovery that bring theoretical foundations to complex variability extraction algorithms. We present an application on product comparison matrices representing complex descriptions of software system families. We show that our method does not suffer from scalability issues and extracts all pertinent relationships, but that it also extracts numerous accidental relationships that need to be filtered.",2019,2098
A two-tiered framework for anomaly classification in IoT networks utilizing CNN-BiLSTM model,Yue Guan and Morteza Noferesti and Naser Ezzati-Jivan,"The paper introduces ACS-IoT, an Anomaly Classification System for IoT networks, structured as a two-tiered framework. In the first, it employs a decision tree classifier for anomaly detection. In the second, a CNN-BiLSTM model is utilized for more profound analysis and classification of anomaly types. To address data imbalance, SMOTE is used, and feature selection is enhanced with PSO. The approach showcases strong practical applicability in real-world industrial settings, achieving an accuracy of 88%, precision of 89%, recall of 88%, and F1-score of 88% for multi-class classification, surpassing other machine learning approaches by at least 6% in all metrics.",2024,2099
IoT botnet detection via power consumption modeling,Woosub Jung and Hongyang Zhao and Minglong Sun and Gang Zhou,"Many IoT botnets that exploit vulnerabilities of IoT devices have emerged recently. After taking over control of IoT devices, the botnets generate tremendous traffic to attack target nodes. It is also a threat to the smart health area since they have used IoT devices more and more. To detect the malicious IoT botnets, many researchers have proposed botnet detection systems; however, these are not easily applicable to resource-constrained IoT devices. Moreover, since the botnet's early stage makes marginal differences in terms of traffic, it is hard to detect when they first attack the victim nodes. However, we observe that the IoT botnets generate distinguishable power consumption patterns. Thus, we aim to classify whether the IoT device is affected by malign behaviors through power consumption patterns so that we can protect the healthcare ecosystem from the malicious IoT botnets. We propose a CNN-based deep learning model that consists of a data processing module as well as an 8-layer CNN. Prior to applying the CNN model, we segment and normalize the collected power consumption data to help our CNN model to achieve higher accuracy. The 8-layer CNN classifies the processed data into four classes including a botnet class, which is our primary target. To demonstrate the performance, we run self-evaluation, cross-device-evaluation, leave-one-device-out, and leave-one-botnet-out tests on three common types of IoT devices, which are Security Camera, Router, and Voice Assistant devices. The self-tests achieve up to 96.5% classification accuracy whereas the cross-evaluation tests perform about 90% accuracy. Leave-one-out tests also introduce higher than 90% accuracy for botnet detection.",2020,2100
Kansei engineering for new energy vehicle exterior design: An internet big data mining approach,Xinjun Lai and Sheng Zhang and Ning Mao and Jianjun Liu and Qingxin Chen,"New energy vehicles (NEVs) such as electronic cars represent a major trend in the automobile industry, where most their exterior designs still follow those of convention fuelled vehicles (FVs). It is important to investigate whether NEV users have unique requirements that differ from those of traditional users. Kansei engineering is a practical tool for perceptual demand analysis. However, the conventional method requires questionnaires or surveys to perform limited data collection. In this study, we utilised massive internet data to collect user Kansei requirements for NEV exterior design. The Scrapy crawler was adopted for data collection and a bidirectional long short-term memory, conditional random field, and multilayer perceptron framework was developed for text mining. To quantify design features and Kansei image scores, a hybrid Apriori  + structural equation model (SEM) system is proposed, where the data-driven Apriori algorithm can explore the hidden relationships in big user generated comments, while the SEM model captures the users’ behaviour and decision procedure so that to provide interpretable results. In addition, the association rules mined from user comments by Apriori can facilitate the specification of a complicated SEM model, substantially reducing the modelling and calibration effort. Goodness-of-fit results suggest that the proposed model outperforms conventional models. A case study on 1805 automobiles, 287 brands, and 369105 comments was conducted and the results suggest that some design features that would increase the Kansei image scores for conventional FVs may have the opposite effect on NEVs. Discussions on engineering and managerial insights are presented and the discovered rules and relationships are employed to develop a design-aided system.",2022,2101
"Machine learning research towards combating COVID-19: Virus detection, spread prevention, and medical assistance",Osama Shahid and Mohammad Nasajpour and Seyedamin Pouriyeh and Reza M. Parizi and Meng Han and Maria Valero and Fangyu Li and Mohammed Aledhari and Quan Z. Sheng,"COVID-19 was first discovered in December 2019 and has continued to rapidly spread across countries worldwide infecting thousands and millions of people. The virus is deadly, and people who are suffering from prior illnesses or are older than the age of 60 are at a higher risk of mortality. Medicine and Healthcare industries have surged towards finding a cure, and different policies have been amended to mitigate the spread of the virus. While Machine Learning (ML) methods have been widely used in other domains, there is now a high demand for ML-aided diagnosis systems for screening, tracking, predicting the spread of COVID-19 and finding a cure against it. In this paper, we present a journey of what role ML has played so far in combating the virus, mainly looking at it from a screening, forecasting, and vaccine perspective. We present a comprehensive survey of the ML algorithms and models that can be used on this expedition and aid with battling the virus.",2021,2102
Seeking and sharing datasets in an online community of data enthusiasts,Besiki Stvilia and Leila Gibradze,"This study examined discussions of the r/Datasets community on Reddit. It identified three activities in which the community engaged: question answering, data sharing, and community building. Members of the community used 21 types of data and information sources in their activities. The findings of this research enhance our understanding of the activity structures, data and information sources used, and challenges and problems encountered when users search for, share, and make sense of datasets on the web, outside the traditional information and data ecosystems. Data librarians and curators can use the findings of this study in the design of their data management and reference services. The typology of data sources and the metadata model developed through this study can be used in annotating and categorizing data sources and informing the design of metadata schemas and vocabularies for datasets.",2022,2103
Discovering e-commerce user groups from online comments: An emotional correlation analysis-based clustering method,Jia Ke and Ying Wang and Mingyue Fan and Xiaojun Chen and Wenlong Zhang and Jianping Gou,"Platform merchants mine user clusters and their characteristics to assist in precision marketing. In view of the information overload in e-commerce reviews, machine methods are needed to efficiently obtain clustering information from text. This study innovatively integrated the emotional correlation analysis model and Self-organizing Map (SOM) in application, to construct fine-grained user emotion vector based on review text and perform visual cluster analysis, which helped quickly mine user clustering and characteristics from review text. The result of empirical analysis based on real reviews of Amazon books showed that the proposed method had the average precision as 0.71, confirming that the clustering method integrating the emotional correlation analysis model and SOM could efficiently mine user groups and match appropriate marketing strategies, which will help platform merchants carry out precision marketing. The study makes contributions to the application and innovation of researches in the field of user clustering and e-commerce precision marketing.",2024,2104
Passive operating system fingerprinting revisited: Evaluation and current challenges,Martin Laštovička and Martin Husák and Petr Velan and Tomáš Jirsík and Pavel Čeleda,"Fingerprinting a host's operating system is a very common yet precarious task in network, asset, and vulnerability management. Estimating the operating system via network traffic analysis may leverage TCP/IP header parameters or complex analysis of hosts' behavior using machine learning. However, the existing approaches are becoming obsolete as network traffic evolves which makes the problem still open. This paper discusses various approaches to passive OS fingerprinting and their evolution in the past twenty years. We illustrate their usage, compare their results in an experiment, and list challenges faced by the current fingerprinting approaches. The hosts' differences in network stack settings were initially the most important information source for OS fingerprinting, which is now complemented by hosts' behavioral analysis and combined approaches backed by machine learning. The most impactful reasons for this evolution were the Internet-wide network traffic encryption and the general adoption of privacy-preserving concepts in application protocols. Other changes, such as the increasing proliferation of web applications on handheld devices, raised the need to identify these devices in the networks, for which we may use the techniques of OS fingerprinting.",2023,2105
From forced Working-From-Home to voluntary working-from-anywhere: Two revolutions in telework,Darja Šmite and Nils Brede Moe and Eriks Klotins and Javier Gonzalez-Huerta,"The COVID-19 outbreak has admittedly caused interruptions to production, transportation, and mobility, therefore, having a significant impact on the global supply and demand chain’s well-functioning. But what happened to companies developing digital services, such as software? How has the enforced Working-From-Home (WFH) mode impacted their ability to deliver software, if at all? This article shares our findings from monitoring the WFH during 2020 in an international software company with engineers located in Sweden, the USA, and the UK. We analyzed different aspects of productivity, such as developer job satisfaction and well-being, activity, communication and collaboration, efficiency and flow based on the archives of commit data, calendar invites, Slack communication, the internal reports of WFH experiences, and 30 interviews carried out in April/May and September 2020. We add more objective evidence to the existing COVID-19 studies the vast majority of which are based on self-reported productivity from the early months of the pandemic. We find that engineers continue committing code and carrying out their daily duties, as their routines adjust to “the new norm”. Our key message is that software engineers can work from home and quickly adjust their tactical approaches to the changes of unprecedented scale. Further, WFH has its benefits, including better work-life balance, improved flow, and improved quality of distributed meetings and events. Yet, WFH is not challenge free: not everybody feels equally productive working from home, work hours for many increased, while physical activity, socialization, pairing and opportunities to connect to unfamiliar colleagues decreased. Information sharing and meeting patterns also changed. Finally, experiences gained during the pandemic will have a lasting impact on the future of the workplace. The results of an internal company-wide survey suggest that only 9% of engineers will return to work in the office full time. Our article concludes with the InterSoft’s strategy for work from anywhere (WFX), and a list of useful adjustments for a better WFH.",2023,2106
PSI-rooted subgraph: A novel feature for IoT botnet detection using classifier algorithms,Huy-Trung Nguyen and Quoc-Dung Ngo and Doan-Hieu Nguyen and Van-Hoang Le,"It is obvious that IoT devices are widely used more and more in many areas. However, due to limited resources (e.g., memory, CPU), the security mechanisms on many IoT devices such as IP-Camera, router are low. Therefore, botnets are an emerging threat to compromise IoT devices recently. To tackle this, a novel method for IoT botnets detection plays a crucial role. In this paper, we have some contributions for IoT botnet detection: first, we present a novel high-level PSI-rooted subgraph-based feature for the detection of IoT botnets; second, we generate a limited number of features that have precise behavioral descriptions, which require smaller space and reduce processing time; third, The evaluation results show the effectiveness and robustness of PSI-rooted subgraph-based features, as with five machine classifiers consisting of Random Forest, Decision Tree, Bagging, k-Nearest Neighbor, and Support Vector Machine, each classifier achieves more than 97% detection rate and low time-consuming. Moreover, compared to other work, our proposed method obtains better performance. Finally, we publicize all our materials on Github, which will benefit future research (e.g., IoT botnet detection approach).",2020,2107
Remote Teaching of Dynamics and Control of Robots Using ROS 2,Walter Fetter Lages,"This paper presents the use of ROS 2 as a support platform for the Dynamics and Control of Robots course at the School of Engineering of the Universidade Federal do Rio Grande do Sul, Brazil. The organization of the course is presented and the autonomous activities performed by students at home using ROS 2 are described in detail, including some pitfalls that should be avoided. The results show that ROS 2 is mature enough for production use, at least in an academic environment and that students are able to use sophisticated robotic tools even in undergraduate courses.",2022,2108
A sequential deep learning framework for a robust and resilient network intrusion detection system,Soumyadeep Hore and Jalal Ghadermazi and Ankit Shah and Nathaniel D. Bastian,"Ensuring the security and integrity of computer and network systems is of utmost importance in today’s digital landscape. Network intrusion detection systems (NIDS) play a critical role in continuously monitoring network traffic and identifying unauthorized or potentially malicious activities that could compromise the confidentiality, availability, and integrity of these systems. However, traditional NIDS face a daunting challenge in effectively adapting to the evolving tactics of cyber attackers. To address this challenge, we propose a multistage artificial intelligence enabled framework for intrusion detection in network traffic, capable of handling zero-day, out-of-distribution, and adversarial evasion attacks. Our framework comprises three sequential deep neural network (DNN) architectures: one for the classifier and two for specific autoencoders, designed to effectively detect both known attack patterns and novel, previously unseen samples. We introduce an innovative transfer learning technique where specific combinations of neurons and layers in the DNN architectures are frozen during one-shot learning to enhance the framework’s robustness to novel attacks. To validate the effectiveness of our framework, we conducted extensive experimentation using publicly available benchmark intrusion detection data sets. Leveraging the one-shot learning approach in the transfer learning component of the framework, we demonstrate continuous improvement in detection accuracy for both known and novel network traffic patterns. The results demonstrate the effectiveness of the multiple stages in the framework by achieving, on average, 98.5% accuracy in detecting various attacks.",2024,2109
"Learning Locally, Communicating Globally: Reinforcement Learning of Multi-robot Task Allocation for Cooperative Transport",Kazuki Shibata and Tomohiko Jimbo and Tadashi Odashima and Keisuke Takeshita and Takamitsu Matsubara,"We consider task allocation for multi-object transport using a multi-robot system, in which each robot selects one object among multiple objects with different and unknown weights. The existing centralized methods assume the number of robots and tasks to be fixed, which is inapplicable to scenarios that differ from the learning environment. Meanwhile, the existing distributed methods limit the minimum number of robots and tasks to a constant value, making them applicable to various numbers of robots and tasks. However, they cannot transport an object whose weight exceeds the load capacity of robots observing the object. To make it applicable to various numbers of robots and objects with different and unknown weights, we propose a framework using multi-agent reinforcement learning for task allocation. First, we introduce a structured policy model consisting of 1) predesigned dynamic task priorities with global communication and 2) a neural network-based distributed policy model that determines the timing for coordination. The distributed policy builds consensus on the high-priority object under local observations and selects cooperative or independent actions. Then, the policy is optimized by multi-agent reinforcement learning through trial and error. This structured policy of local learning and global communication makes our framework applicable to various numbers of robots and objects with different and unknown weights, as demonstrated by simulations.",2023,2110
Turning captchas against humanity: Captcha-based attacks in online social media,Mauro Conti and Luca Pajola and Pier Paolo Tricomi,"Nowadays, people generate and share massive amounts of content on online platforms (e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook users posted around 150 thousand photos every minute. Content moderators constantly monitor these online platforms to prevent the spreading of inappropriate content (e.g., hate speech, nudity images). Based on deep learning (DL) advances, Automatic Content Moderators (ACM) help human moderators handle high data volume. Despite their advantages, attackers can exploit weaknesses of DL components (e.g., preprocessing, model) to affect their performance. Therefore, an attacker can leverage such techniques to spread inappropriate content by evading ACM. In this work, we analyzed 4600 potentially toxic Instagram posts, and we discovered that 44% of them adopt obfuscations that might undermine ACM. As these posts are reminiscent of captchas (i.e., not understandable by automated mechanisms), we coin this threat as Captcha Attack (CAPA). Our contributions start by proposing a CAPA taxonomy to better understand how ACM is vulnerable to obfuscation attacks. We then focus on the broad sub-category of CAPA using textual Captcha Challenges, namely CC-CAPA, and we empirically demonstrate that it evades real-world ACM (i.e., Amazon, Google, Microsoft) with 100% accuracy. Our investigation revealed that ACM failures are caused by the OCR text extraction phase. The training of OCRs to withstand such obfuscation is therefore crucial, but huge amounts of data are required. Thus, we investigate methods to identify CC-CAPA samples from large sets of data (originated by three OSN – Pinterest, Twitter, Yahoo-Flickr), and we empirically demonstrate that supervised techniques identify target styles of samples almost perfectly. Unsupervised solutions, on the other hand, represent a solid methodology for inspecting uncommon data to detect new obfuscation techniques.",2023,2111
Optimism and pessimism analysis using deep learning on COVID-19 related twitter conversations,Guillermo Blanco and Anália Lourenço,"This paper proposes a new deep learning approach to better understand how optimistic and pessimistic feelings are conveyed in Twitter conversations about COVID-19. A pre-trained transformer embedding is used to extract the semantic features and several network architectures are compared. Model performance is evaluated on two new, publicly available Twitter corpora of crisis-related posts. The best performing pessimism and optimism detection models are based on bidirectional long- and short-term memory networks. Experimental results on four periods of the COVID-19 pandemic show how the proposed approach can model optimism and pessimism in the context of a health crisis. There is a total of 150,503 tweets and 51,319 unique users. Conversations are characterised in terms of emotional signals and shifts to unravel empathy and support mechanisms. Conversations with stronger pessimistic signals denoted little emotional shift (i.e. 62.21% of these conversations experienced almost no change in emotion). In turn, only 10.42% of the conversations laying more on the optimistic side maintained the mood. User emotional volatility is further linked with social influence.",2022,2112
Multi-service threats: Attacking and protecting network printers and VoIP phones alike,Giampaolo Bella and Pietro Biondi and Stefano Bognanni,"Printing over a network and calling over VoIP technology are routine at present. This article investigates to what extent these services can be attacked using freeware in the real world if they are not configured securely. In finding out that attacks of high impact, termed the Printjack and Phonejack families, could be mounted at least from insiders, the article also observes that secure configurations do not appear to be widely adopted. Users with the necessary skills may put existing security measures in place with printers, but would need novel measures, which the article prototypes, with phones in order for a pair of peers to call each other securely and without trusting anyone else, including sysadmins.",2022,2113
Scenario-based creation and digital investigation of ethereum ERC20 tokens,Simon F. Dyson and William J. Buchanan and Liam Bell,This paper examines the Ethereum network in the context of an investigation. The validation of data sources is achieved through different client software on both the Ropsten network and the live blockchain. New scenarios are also used test common patterns in order to track for start and end points for Ethereum and ERC20 tokens.,2020,2114
Joint Graph Learning and Matching for Semantic Feature Correspondence,He Liu and Tao Wang and Yidong Li and Congyan Lang and Yi Jin and Haibin Ling,"In recent years, powered by the learned discriminative representation via graph neural network (GNN) models, deep graph matching methods have made great progresses in the task of matching semantic features. However, these methods usually rely on heuristically generated graph patterns, which may introduce unreliable relationships to hurt the matching performance. In this paper, we propose a joint graph learning and matching network, named GLAM, to explore reliable graph structures for boosting graph matching. GLAM adopts a pure attention-based framework for both graph learning and graph matching. Specifically, it employs two types of attention mechanisms, self-attention and cross-attention for the task. The self-attention discovers the relationships between features to further update feature representations over the learnt structures; and the cross-attention computes cross-graph correlations between the two feature sets to be matched for feature reconstruction. Moreover, the final matching solution is directly derived from the output of the cross-attention layer, without employing a specific matching decision module. The proposed method is evaluated on three popular visual matching benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms previous state-of-the-art graph matching methods on all benchmarks. Furthermore, the graph patterns learnt by our model are validated to be able to remarkably enhance previous deep graph matching methods by replacing their handcrafted graph structures with the learnt ones.",2023,2115
Large language models for air transportation: A critical review,Yucheng Liu,"In the past decade, Artificial Intelligence (AI) has contributed to the improvement of various aviation aspects, including flight plan optimization, the development of autonomous systems, performing predictive analytics, as well as in passenger / crew assistance systems. The latest AI technology to potentially revolutionize air transportation are so-called Large Language Models (LLMs), which have an outstanding ability to process and generate human-like text. The application areas for LLMs cover nearly all aspects of air transportation, through language processing, content generation, and problem solving. In this study, we discuss the potential of this impact with two major contributions. First, we have performed an experimental evaluation of twelve commonly-used LLMs concerning their performance of air transportation related subjects, covering fact retrieval, complex reasoning abilities, and explanation tasks. Second, we have performed a survey among graduate students at Beihang University, a leading aviation university in China, to explore the experiences and uses of LLMs. We believe that our study makes a significant contribution towards the dissemination and application of LLMs in the air transportation domain.",2024,2116
Estimating cardiomyofiber strain in vivo by solving a computational model,Luigi E. Perotti and Ilya A. Verzhbinsky and Kévin Moulin and Tyler E. Cork and Michael Loecher and Daniel Balzani and Daniel B. Ennis,"Since heart contraction results from the electrically activated contraction of millions of cardiomyocytes, a measure of cardiomyocyte shortening mechanistically underlies cardiac contraction. In this work we aim to measure preferential aggregate cardiomyocyte (“myofiber”) strains based on Magnetic Resonance Imaging (MRI) data acquired to measure both voxel-wise displacements through systole and myofiber orientation. In order to reduce the effect of experimental noise on the computed myofiber strains, we recast the strains calculation as the solution of a boundary value problem (BVP). This approach does not require a calibrated material model, and consequently is independent of specific myocardial material properties. The solution to this auxiliary BVP is the displacement field corresponding to assigned values of myofiber strains. The actual myofiber strains are then determined by minimizing the difference between computed and measured displacements. The approach is validated using an analytical phantom, for which the ground-truth solution is known. The method is applied to compute myofiber strains using in vivo displacement and myofiber MRI data acquired in a mid-ventricular left ventricle section in N=8 swine subjects. The proposed method shows a more physiological distribution of myofiber strains compared to standard approaches that directly differentiate the displacement field.",2021,2117
Framework for software development of laboratory equipment and setups integrated into large scale DAQ systems (LabBot),Alexandr Chernakov and Nikita Zhiltsov and Vasiliy Senitchenkov and Nikita Babinov and Alexander Bazhenov and Ivan Bukreev and Paul Chernakov and Anton Chernakov and Anastasia Chironova and Artem Dmitriev and Denis Elets and Nikita Ermakov and Igor Khodunov and Alexander Koval and Gleb Kurskiev and Andrei Litvinov and Alina Mitrofanova and Alexander Mokeev and Eugene Mukhin and Alexey Razdobarin and Dmitry Samsonov and Leonid Snigirev and Valeri Solovei and Ivan Tereschenko and Sergei Tolstyakov and Lidia Varshavchik and Paul Zatylkin,"The LabBot Framework project is intended to implement control of experiment and data acquisition without writing special platform codes, while achieving industrial-grade results. The Framework is intended for fast and advanced development of controlling and data acquisition software for laboratory-scale experimental setups; more sophisticated software for middle-scale laboratory equipment; industrial-grade software for self-made commercial instrumentation developed by small- and middle-scale companies. The paper presents review of the Framework top-level architecture and its implementation addressing development of ITER Divertor Thomson Scattering (DTS) diagnostic. The first examples of the Framework implementation include the set-ups for vacuum heating testbench; RF cleaning of diagnostic mirrors as well as DTS equipment, which must be controlled by the ITER CODAC system.",2020,2118
DODFMiner: An automated tool for Named Entity Recognition from Official Gazettes,Gabriel M.C. Guimarães and Felipe X.B. {da Silva} and Andrei L. Queiroz and Ricardo M. Marcacini and Thiago P. Faleiros and Vinicius R.P. Borges and Luís P.F. Garcia,"Official gazettes are documents published by governments to publicize their actions, spanning long periods of time and making an important transparency mechanism. These documents have information on laws, contracts, and bidding processes, as well as on civil servants and their careers in public service. Automatic information extraction of these documents may contribute to public transparency, with two tasks being especially useful: the classification of the different segments of these documents, the so called acts; and the Named Entity Recognition (NER) within the acts. The variety of official gazettes and their patterns brings up the necessity of constructing different tools for specific gazettes. In this paper, we propose DODFMiner, a command-line interface tool to classify acts and extract named entities from the Official Gazette of the Federal District. The tool follows a 3-step approach: the pre-processing of the input data; text classification using rule-based systems with regular expressions; and NER with Machine Learning algorithms. It allows users to input JSON files and receive CSV as output, providing information that allows users to track government procurements through years, contracts duration and total amount, among others. We also propose a set of experiments to support the choice of models included in the tool, covering the classification and NER steps. Text classification achieved a mean F1-score of 0.778, while to the NER, we compared 3 different architectures, CRF with a mean F1-score of 0.851, CNN-biLSTM-CRF with 0.787 and CNN-CNN-LSTM with 0.841.",2024,2119
Vessel-CAPTCHA: An efficient learning framework for vessel annotation and segmentation,Vien Ngoc Dang and Francesco Galati and Rosa Cortese and Giuseppe {Di Giacomo} and Viola Marconetto and Prateek Mathur and Karim Lekadir and Marco Lorenzi and Ferran Prados and Maria A. Zuluaga,"Deep learning techniques for 3D brain vessel image segmentation have not been as successful as in the segmentation of other organs and tissues. This can be explained by two factors. First, deep learning techniques tend to show poor performances at the segmentation of relatively small objects compared to the size of the full image. Second, due to the complexity of vascular trees and the small size of vessels, it is challenging to obtain the amount of annotated training data typically needed by deep learning methods. To address these problems, we propose a novel annotation-efficient deep learning vessel segmentation framework. The framework avoids pixel-wise annotations, only requiring weak patch-level labels to discriminate between vessel and non-vessel 2D patches in the training set, in a setup similar to the CAPTCHAs used to differentiate humans from bots in web applications. The user-provided weak annotations are used for two tasks: (1) to synthesize pixel-wise pseudo-labels for vessels and background in each patch, which are used to train a segmentation network, and (2) to train a classifier network. The classifier network allows to generate additional weak patch labels, further reducing the annotation burden, and it acts as a second opinion for poor quality images. We use this framework for the segmentation of the cerebrovascular tree in Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The results show that the framework achieves state-of-the-art accuracy, while reducing the annotation time by ∼77% w.r.t. learning-based segmentation methods using pixel-wise labels for training.",2022,2120
Real-Time CNN-based Computer Vision System for Open-Field Strawberry Harvesting Robot,Madis Lemsalu and Victor Bloch and Juha Backman and Matti Pastell,"Strawberry production in open-field conditions requires a lot of human labor, which is increasingly difficult to recruit. A robotic solution could potentially operate in the field around the clock with minimal supervision. For strawberry farmers, automation of harvesting would eliminate the personnel risk and provide security for operations in the long term. A robot which would be capable of replacing physical human labor in horticultural production requires an accurate and fast perception system. In this paper, we focus on the task of detecting of garden strawberries to guide the picking by a strawberry harvesting robot. We have developed a real-time implementation of strawberry and peduncle detection system that runs on an edge device. This paper outlines the vision system requirements, hardware selection, model selection, training process and results. After consideration of the overall requirements of the system, we decided to use YOLOv5 to detect both the berries and peduncles for the picking system. Training data was collected and annotated, and the detection model was trained. The network had 91.5% average precision (AP) for detecting strawberries and an 43.6% AP for detecting peduncles. One of the reasons for performance discrepancy was the difficulty to detect peduncles from afar. Overall, the vision algorithm reached the performance that was required to guide the robot to a strawberry and detect the corresponding strawberry-peduncle pairs. However, for densely clustered berries the method often failed to detect the correct peduncle and needs to be improved.",2022,2121
Machine learning approaches to IoT security: A systematic literature review ,Rasheed Ahmad and Izzat Alsmadi,"With the continuous expansion and evolution of IoT applications, attacks on those IoT applications continue to grow rapidly. In this systematic literature review (SLR) paper, our goal is to provide a research asset to researchers on recent research trends in IoT security. As the main driver of our SLR paper, we proposed six research questions related to IoT security and machine learning. This extensive literature survey on the most recent publications in IoT security identified a few key research trends that will drive future research in this field. With the rapid growth of large scale IoT attacks, it is important to develop models that can integrate state of the art techniques and technologies from big data and machine learning. Accuracy and efficiency are key quality factors in finding the best algorithms and models to detect IoT attacks in real or near real-time",2021,2122
UMUDGA: A dataset for profiling algorithmically generated domain names in botnet detection,Mattia Zago and Manuel {Gil Pérez} and Gregorio {Martínez Pérez},"In computer security, botnets still represent a significant cyber threat. Concealing techniques such as the dynamic addressing and the domain generation algorithms (DGAs) require an improved and more effective detection process. To this extent, this data descriptor presents a collection of over 30 million manually-labeled algorithmically generated domain names decorated with a feature set ready-to-use for machine learning (ML) analysis. This proposed dataset has been co-submitted with the research article ”UMUDGA: a dataset for profiling DGA-based botnet” [1], and it aims to enable researchers to move forward the data collection, organization, and pre-processing phases, eventually enabling them to focus on the analysis and the production of ML-powered solutions for network intrusion detection. In this research, we selected 50 among the most notorious malware variants to be as exhaustive as possible. Inhere, each family is available both as a list of domains (generated by executing the malware DGAs in a controlled environment with fixed parameters) and as a collection of features (generated by extracting a combination of statistical and natural language processing metrics).",2020,2123
Weaponising the Internet of Things,Steve Mansfield-Devine,"It seems that as fast as we develop defences for our networks and computing devices we also introduce new products with new flaws that criminals and others can exploit. The Internet of Things (IoT) is a case in point. In the rush to connect everything from dolls to CCTV systems to the Internet, security seems to get overlooked, making us all vulnerable. In this interview with penetration tester Ken Munro, a partner at Pen Test Partners, we discover how the most seemingly innocuous product can be turned into a cyber weapon.",2017,2124
"Understanding students’ software development projects: Effort, performance, satisfaction, skills and their relation to the adequacy of outcomes developed",Sherlock A. Licorish and Matthias Galster and Georgia M. Kapitsaki and Amjed Tahir,"Given the inclusion of (often team-based) course projects in tertiary software engineering education, it is necessary to investigate software engineering curricula and students’ experiences while undergoing their software engineering training. Previous research efforts have not sufficiently explored students perceptions around the commitment and adequacy of effort spent on software engineering projects, their project performance and skills that are developed during course projects. This gap in skills awareness includes those that are necessary, anticipated and learned, and the challenges to student project success, which may predict project performance. Such insights could inform curricula design, theory and practice, in terms of improving post-study software development success. We conducted a survey involving undergraduate across four universities in New Zealand and Cyprus to explore these issues, where extensive deductive and inductive analyses were performed. Among our findings we observe that students’ commitment of effort on software engineering project seems appropriate. Students are more satisfied with their team’s collaboration performance than technical contributions, but we found that junior students seemed to struggle with teamwork. Further, we observe that the software students developed were of higher quality if they had worked in project teams previously, had stronger technical skills and were involved in timely meetings. This study singles out mechanisms for informing good estimation of effort, mentoring technical competencies and suitable coaching for enhancing project success and student learning.",2022,2125
A novel GPU based Geo-Location Inference Attack on WebGL framework,Weixian Mai and Yinhao Xiao,"In the past few years, graphics processing units (GPUs) have become an indispensable part of modern computer systems, not only for graphics rendering but also for intensive parallel computing. Given that many tasks running on GPUs contain sensitive information, security concerns have been raised, especially about potential GPU information leakage. Previous works have shown such concerns by showing that attackers can use GPU memory allocations or performance counters to measure victim side effects. However, such an attack has a critical drawback that it requires a victim to install desktop applications or mobile apps yielding it uneasy to be deployed in the real world. In this paper, we solve this drawback by proposing a novel GPU-based side-channel Geo-Privacy inference attack on the WebGL framework, namely, GLINT (stands for Geo-Location Inference Attack). GLINT merely utilizes a lightweight browser extension to measure the time elapsed to render a sequence of frames on well-known map websites, e.g., Google Maps, or Baidu Maps. The measured stream of time series is then employed to infer geologically privacy-sensitive information, such as a search on a specific location. Upon retrieving the stream, we propose a novel online segmentation algorithm for streaming data to determine the start and end points of privacy-sensitive time series. We then combine the DTW algorithm and KNN algorithm on these series to conclude the final inference on a user’s geo-location privacy. We conducted real-world experiments to testify our attack. The experiments show that GeoInfer can correctly infer more than 83% of user searches regardless of the locations and map websites, meaning that our Geo-Privacy inference attack is accurate, practical, and robust. To counter this attack, we implemented a defense strategy based on Differential Privacy to hinder obtaining accurate rendering data. We found that this defense mechanism managed to reduce the average accuracy of the attack model by more than 70%, indicating that the attack was no longer effective. We have fully implemented GLINT and open-sourced it for future follow-up research.",2023,2126
Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers,Marco Siino and Ilenia Tinnirello and Marco {La Cascia},"With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models – including modern Transformers – get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique – in place of the worst – can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Naïve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing.",2024,2127
Predicting continuous integration build failures using evolutionary search,Islem Saidani and Ali Ouni and Moataz Chouchen and Mohamed Wiem Mkaouer,"Context: Continuous Integration (CI) is a common practice in modern software development and it is increasingly adopted in the open-source as well as the software industry markets. CI aims at supporting developers in integrating code changes constantly and quickly through an automated build process. However, in such context, the build process is typically time and resource-consuming which requires a high maintenance effort to avoid build failure. Objective: The goal of this study is to introduce an automated approach to cut the expenses of CI build time and provide support tools to developers by predicting the CI build outcome. Method: In this paper, we address problem of CI build failure by introducing a novel search-based approach based on Multi-Objective Genetic Programming (MOGP) to build a CI build failure prediction model. Our approach aims at finding the best combination of CI built features and their appropriate threshold values, based on two conflicting objective functions to deal with both failed and passed builds. Results: We evaluated our approach on a benchmark of 56,019 builds from 10 large-scale and long-lived software projects that use the Travis CI build system. The statistical results reveal that our approach outperforms the state-of-the-art techniques based on machine learning by providing a better balance between both failed and passed builds. Furthermore, we use the generated prediction rules to investigate which factors impact the CI build results, and found that features related to (1) specific statistics about the project such as team size, (2) last build information in the current build and (3) the types of changed files are the most influential to indicate the potential failure of a given build. Conclusion: This paper proposes a multi-objective search-based approach for the problem of CI build failure prediction. The performances of the models developed using our MOGP approach were statistically better than models developed using machine learning techniques. The experimental results show that our approach can effectively reduce both false negative rate and false positive rate of CI build failures in highly imbalanced datasets.",2020,2128
Towards End-to-End Automated Microscopy Control using Holotomography: Workflow Design and Data Management,Henning Zwirnmann and Dennis Knobbe and Sami Haddadin,"Microscopy has been a key tool involved in many discoveries in the life sciences over the past centuries. In the last 30 years in particular, enormous progress has been made in developing this measurement technique further to make researchers working with it more effective. To combine gains in reproducibility and efficiency resulting from these advancements in different research areas, we present for the first time a unified and comprehensive concept for an end-to-end automated microscopy workflow. To this end, we employ both robotic and computational methods as well as holotomography microscopy. Considering the physical preparation and cleanup of a measurement, the image acquisition, and the management and analysis of the resulting data, we give a fine-grained workflow description. We present the robotic system to perform the manual process steps and a Python package to standardize the resulting proprietary image (meta)data. For the other tasks, we identify suitable open-source tools to execute them and apply them to our setup. The choice of holotomography as a suitable microscopy technique to realize this workflow is elucidated. We envision that the adoption of an automated workflow paves the way toward a future life science laboratory where microscopy-based research is carried out more efficiently and reproducibly than in the past.",2023,2129
Factors affecting the use of artificial intelligence generated content by subject librarians: A qualitative study,Xiaowen Yang and Jingjing Ding and Haibo Chen and Hanzhen Ji,"To explore the factors affecting the use of artificial intelligence generated content (AIGC) by subject librarians through understanding their perceptions of AIGC. Interpretive phenomenological analysis (IPA) and technology acceptance model (TAM) were used in semi-structured interviews to explore the external variables of perceived ease of use and perceived usability of AIGC application in subject librarians. The perceptions of subject librarians towards AIGC included performance, risk perceptions, ability enhancement, and affective attitude. Attentions were paid to AIGC's performances in providing customized services, optimizing collection resources and improving cost efficiency. The risk perception involved technical stability, data security, user acceptance and occupational risk, the ability enhancement involved the improvement of personal literacy, innovative ability, and self-confidence through the use of AIGC technology, and the affective attitudes included not only excitement and anticipation for the technical potential of AIGC, but also concerns and skepticism about it, and critical attitudes toward its application in academic settings and the ethical issues it may raise. TAM analysis on the factors affecting the use of AIGC by subject librarians indicates that the external influencing factors of perceived ease of use include personal literacy, innovative ability, self-confidence enhancement and affective attitude; the external influencing factors of perceived usability include precise service, collection resource optimization, cost-effectiveness, technological risk, user acceptance and occupational risk. These factors constitute a theoretical framework for understanding and promoting the acceptance and effective use of AIGC by subject librarians. TAM analysis combined with IPA exploration on the external variables of perceived ease of use and perceived usability of AIGC application can identify the key factors affecting subject librarians' perceptions of AIGC, propose strategies for optimizing librarians' roles, enhancing information recognition ability and privacy protection, thus providing guidance for effective use of AIGC in library.",2024,2130
"Data Collection, data mining and transfer of learning based on customer temperament-centered complaint handling system and one-of-a-kind complaint handling dataset",Ching-Hung Lee and Xuejiao Zhao,"One of the most significant sources of information from customers is customer complaints. Successful and effective complaint management can end complaint crises and ensure client loyalty, which is a sign of great service performance. In this paper, we proposed a novel customer temperament-centered and e-CCH system-based data collection and data mining method titled “3D” model for customer complaint data analysis. Three phases are (1) Development and launch of e-Customer Complaint Handling system, (2) Data collection and transfer of learning by e-Customer Complaint Handling system, and (3) Data mining by e-Customer Complaint Handling system. An advanced electronic Customer Complaint Handling System called the e-CCH system was then developed and launched. This system adapts the seasonal associations model based on Hippocrates's customer temperament theory to the whole stages of customer complaint reporting and handling. With this system, we conducted a dataset collection work from restaurant chains of two brands over four years. As a result, we collect thousands of real-world temperament-centred customer complaint cases by four years to form the one-of-a-kind CCH dataset. This one-of-a-kind CCH dataset was open-sourced with detailed customer complaint attributes and heuristic decision-making for valuable industrial handling manner. After further analysis of this dataset, we found that customers with different temperament types tend to have different types of complaints. In addition, adapting the temperament theory to the e-CCH system can classify customer types better and provide personalized solutions. To our best knowledge, this rich and the one-of-a-kind CCH dataset reported in this paper is the first comprehensive study of customer complaint handling in an industrial service management context. Meanwhile, data mining with cross analysis and correspondence analysis and an ChatGPT experiment for transfer of learning based on this yearly and one-of-a-kind industrial customer complaint dataset was analyzed and discussed. In addition, how this dataset may contribute to more realistic complaint-handling theoretic studies for better service failure recovery and interactive marketing is discussed in-depth.",2024,2131
A Method of Monitoring and Detecting APT Attacks Based on Unknown Domains,Do Xuan Cho and Ha Hai Nam,"The increasing coverage of Internet has created opportunities and advantages for different aspects of society. However, there come new threats and challenges to information security. One of the typical types of attacks that has increasingly occurred is the APT attack (Advanced Persistent Threat). APT is dangerous with clear purposes. APT attacks employ different sophisticated methods and techniques attacking targets in order to steal confidential and sensitive information. In the past, hackers attacked information systems with personal and financial motives. However, there are nowadays other motives such as political ones and they are potentially backed by governments or nations. Nations that own advanced technologies such as United States, India, Russia, UK are also suffering from special purpose attacks. APT is an advanced type of attacks that consists of many stages and concrete strategies. Besides, techniques and technologies employed in APT attack are usually new and developed by hackers in order to break through the monitoring of security software. However, APT is normally implemented through concrete steps and stages. If one of the steps or stages fails, the entire APT attack will fail. This paper presents a method of detecting APT attacks based on monitoring accesses to unknown domains. This detection method results into high effectiveness in the initial stage of APT attacks.",2019,2132
Issues and challenges in DNS based botnet detection: A survey,Manmeet Singh and Maninder Singh and Sanmeet Kaur,"Cybercrimes are evolving on a regular basis and as such these crimes are becoming a greater threat day by day. Earlier these threats were very general and unorganized. In the last decade, these attacks have become highly sophisticated in nature. This higher level of coordination is possible mainly due to botnets, which are clusters of infected hosts controlled remotely by an attacker (botmaster). The number of infected machines is continuously rising, thereby resulting in botnets with over a million infected machines. This powerful capability gives the botmaster a lethal weapon to launch various security attacks. As a result, botnet detection techniques received greater research focus. The Domain Name System (DNS) is a large scale distributed database on the Internet, which is being abused as a botnet communication channel. While there are numerous survey and review papers on botnet detection, there are two survey papers on DNS-based botnet detection which are neither comprehensive nor take into consideration various parameters vital for effective comparison. This survey presents a new classification for DNS-based botnet detection techniques and provides a deep analysis of each technique within the category.",2019,2133
The impact of markets on moral reasoning: Evidence from an online experiment,Jonas Gehrlein and Ann-Kathrin Crede and Nana Adrian,"This paper investigates the impact of markets on moral reasoning. Whereas the current literature focuses on morally relevant decisions that arise in markets, little is known about whether the exposition to markets shapes subsequent moral reasoning. To close this gap, we run a large-scale online experiment with 3 conditions: In Baseline, participants make a choice in a moral dilemma. In the other two conditions, participants are exposed to either a Non-market or Market environment, before facing the identical choice in the moral dilemma. We hypothesize that being exposed to Market induces cost-benefit considerations, which translate into modified reasoning in the subsequent moral dilemma. Compared to the baseline distribution, we indeed find a substantial effect in Market. However, similar choices can be observed in Non-market. We discuss potential explanations for these results, and suggest avenues for future research.",2020,2134
FoamPi: An open-source raspberry Pi based apparatus for monitoring polyurethane foam reactions,Harry C. Wright and Duncan D. Cameron and Anthony J. Ryan,"Adiabatic temperature rise is an important method for determining isocyanate conversion in polyurethane foam reactions as well as many other exothermic chemical reactions. Adiabatic temperature rise can be used in conjunction with change in height and mass measurements to gain understanding into the blowing and gelling reactions that occur during polyurethane foaming as well as give important information on cell morphology. FoamPi is an open-source Raspberry Pi device for monitoring polyurethane foaming reactions. The device effectively monitors temperature rise, change in foam height as well as changes in the mass during the reaction. Three Python scripts are also presented. The first logs raw data during the reaction. The second corrects temperature data such that it can be used in adiabatic temperature rise reactions for calculating isocyanate conversion; additionally this script reduces noise in all the data and removes erroneous readings. The final script extracts important information from the corrected data such as maximum temperature change and maximum height change as well as the time to reach these points. Commercial examples of such equipment exist however the price (>£10000) of these equipment make these systems inaccessible for many research laboratories. The FoamPi build presented is inexpensive (£350) and test examples are shown here to indicate the reproducibility of results as well as precision of the FoamPi.",2022,2135
A review of the security vulnerabilities and countermeasures in the Internet of Things solutions: A bright future for the Blockchain,Hossein Pourrahmani and Adel Yavarinasab and Amir Mahdi Hosseini Monazzah and Jan {Van herle},"The current advances in the Internet of Things (IoT) and the solutions being offered by this technology have accounted IoT among the top ten technologies that will transform the global economy by 2030. IoT is a state-of-the-art paradigm that has developed traditional living into a high-tech lifestyle. The current study aims to provide a comprehensive review and analysis of the existing cybersecurity attacks and vulnerabilities in IoT, offering suitable countermeasures with a focus on describing the impact of emerging technologies on IoT devices and protocol layers. The main vulnerabilities across different layers of the IoT reference model are discussed and categorized, and suitable countermeasures (such as separating IT and IoT network traffic, enhancing physical security, implementing encryption and secure messaging protocols, etc.) are suggested. In addition, the hardware, communication, application, web, and cloud vulnerabilities are introduced, then the corresponding safeguards and protections are presented. Furthermore, Information Assurance (IA) has been deliberately defined and the adoption of the NIST framework and IA model is recommended as a metric to ensure security for IoT solutions considering the five pillars of availability, integrity, authentication, confidentiality, and non-repudiation. Finally, Blockchain technology, known for its use in securing cryptocurrencies, is suggested to facilitate secure data exchange, identification, authentication, and communication for IoT devices by various avenues including ensuring the integrity of sensor data, eliminating the need for intermediaries, reducing costs, and enabling direct addressability of IoT devices.",2023,2136
Life history matters: Differential effects of abomasal parasites on caribou fitness,Eleanor R Dickinson and Karin Orsel and Christine Cuyler and Susan J Kutz,"Parasites can impact wildlife populations through their effects on host fitness and survival. The life history strategies of a parasite species can dictate the mechanisms and timing through which it influences the host. However, unravelling this species-specific effect is difficult as parasites generally occur as part of a broader community of co-infecting parasites. Here, we use a unique study system to explore how life histories of different abomasal nematode species may influence host fitness. We examined abomasal nematodes in two adjacent, but isolated, West Greenland caribou (Rangifer tarandus groenlandicus) populations. One herd of caribou were naturally infected with Ostertagia gruehneri, a common and dominant summer nematode of Rangifer sspp., and the other with Marshallagia marshalli (abundant; winter) and Teladorsagia boreoarcticus (less abundant; summer), allowing us to determine if these nematode species have differing effects on host fitness. Using a Partial Least Squares Path Modelling approach, we found that in the caribou infected with O. gruehneri, higher infection intensity was associated with lower body condition, and that animals with lower body condition were less likely to be pregnant. In caribou infected with M. marshalli and T. boreoarcticus, we found that only M. marshalli infection intensity was negatively related to body condition and pregnancy, but that caribou with a calf at heel were more likely to have higher infection intensities of both nematode species. The differing effects of abomasal nematode species on caribou health outcomes in these herds may be due to parasite species-specific seasonal patterns which influence both transmission dynamics and when the parasites have the greatest impact on host condition. These results highlight the importance of considering parasite life history when testing associations between parasitic infection and host fitness.",2023,2137
DeL-IoT: A deep ensemble learning approach to uncover anomalies in IoT,Enkhtur Tsogbaatar and Monowar H. Bhuyan and Yuzo Taenaka and Doudou Fall and Khishigjargal Gonchigsumlaa and Erik Elmroth and Youki Kadobayashi,"Internet of Things (IoT) devices are inherently vulnerable due to insecure design, implementation, and configuration. Aggressive behavior changes, due to increased attacker’s sophistication, and the heterogeneity of the data in IoT have proven that securing IoT devices trigger multiple challenges. It includes complex and dynamic attack detection, data imbalance, data heterogeneity, real-time response, and prediction capability. Most researchers are not focusing on the class imbalance, dynamic attack detection, and data heterogeneity problems together in Software-Defined Networking (SDN) enabled IoT anomaly detection. Thus, to address these challenging tasks, we propose DeL-IoT, a deep ensemble learning framework for IoT anomaly detection and prediction using SDN, having three primary modules including anomaly detection, intelligent flow management, and device status forecasting. The DeL-IoT employs deep and stacked autoencoders to extract handy features for stacking into an ensemble learning model. This framework yields efficient detection of anomalies, manages flows dynamically, and forecasts both short and long-term device status for early action. We validate the proposed DeL-IoT framework with testbed and benchmark datasets. We demonstrate that in even a 1% imbalanced dataset, the performance of our proposed method, deep feature extraction with a deep ensemble learning model, is around 3% better than the single model. The extensive experimental results show that our models have a better and more reliable performance than the competing models showcased in the relevant related work.",2021,2138
Classification of user attitudes in Twitter -beginners guide to selected Machine Learning libraries,Marta Sokolowska and Maciej Mazurek and Marcin Majer and Michal Podpora,"This paper presents an interesting use case for learning as well as teaching basics of Machine Learning systems. Starting from a brief historical outline of the ML, the authors propose and compare a set of popular ML libraries in an interesting exemplary implementation, to present their usability. The paper also describes text classification methods, the aim of which is to distinguish positive and negative labels of particular messages within the Twitter social network. The study is summarized by a brief comparison of the quality of the classification of the libraries and methods used, as an assessment of their suitability. Final thoughts on the importance of teaching ML are included.",2019,2139
TauRunner: A public Python program to propagate neutral and charged leptons,Ibrahim Safa and Jeffrey Lazar and Alex Pizzuto and Oswaldo Vasquez and Carlos A. Argüelles and Justin Vandenbroucke,"In the past decade IceCube's observations have revealed a flux of astrophysical neutrinos extending to 107GeV. The forthcoming generation of neutrino observatories promises to grant further insight into the high-energy neutrino sky, with sensitivity reaching energies up to 1012GeV. At such high energies, a new set of effects becomes relevant, which was not accounted for in the last generation of neutrino propagation software. Thus, it is important to develop new simulations which efficiently and accurately model lepton behavior at this scale. We present TauRunner, a Python-based package that propagates neutral and charged leptons. TauRunner supports propagation between 10GeV and 1012GeV. The package accounts for all relevant secondary neutrinos produced in charged-current tau neutrino interactions. Additionally, tau energy losses of taus produced in neutrino interactions are taken into account, and treated stochastically. Finally, TauRunner is broadly adaptable to divers experimental setups, allowing for user-specified trajectories and propagation media, neutrino cross sections, and initial spectra.
Program summary
Program title: TauRunner CPC Library link to program files: https://doi.org/10.17632/82nyd9skhj.1 Developer's repository link: https://github.com/icecube/TauRunner Licensing provisions: GNU General Public License 3 Programming language: Python Nature of problem: Propagation of ultra-high energy neutrinos in dense media accounting for various effects associated with ντ and τ± energy losses. Solution method: Monte Carlo methods.",2022,2140
Intercepting Hail Hydra: Real-time detection of Algorithmically Generated Domains,Fran Casino and Nikolaos Lykousas and Ivan Homoliak and Constantinos Patsakis and Julio Hernandez-Castro,"A crucial technical challenge for cybercriminals is to keep control over the potentially millions of infected devices that build up their botnets, without compromising the robustness of their attacks. A single, fixed C&C server, for example, can be trivially detected either by binary or traffic analysis and immediately sink-holed or taken-down by security researchers or law enforcement. Botnets often use Domain Generation Algorithms (DGAs), primarily to evade take-down attempts. DGAs can enlarge the lifespan of a malware campaign, thus potentially enhancing its profitability. They can also contribute to hindering attack accountability. In this work, we introduce HYDRAS, the most comprehensive and representative dataset of Algorithmically-Generated Domains (AGD) available to date. The dataset contains more than 100 DGA families, including both real-world and adversarially designed ones. We analyse the dataset and discuss the possibility of differentiating between benign requests (to real domains) and malicious ones (to AGDs) in real-time. The simultaneous study of so many families and variants introduces several challenges; nonetheless, it alleviates biases found in previous literature employing small datasets which are frequently overfitted, exploiting characteristic features of particular families that do not generalise well. We thoroughly compare our approach with the current state-of-the-art and highlight some methodological shortcomings in the actual state of practice. The outcomes obtained show that our proposed approach significantly outperforms the current state-of-the-art in terms of both classification performance and efficiency.",2021,2141
A reputation score policy and Bayesian game theory based incentivized mechanism for DDoS attacks mitigation and cyber defense,Amrita Dahiya and Brij B. Gupta,"DDoS attack is one of the most powerful cyber-weapons as it does not wait for a specific server configuration or particular network state to attack or to disrupt any operation of the target machine. Further, it does not require any huge investment and can cause enormous reputational and financial loss to the organization. Additionally, the uneven distribution of resources and incentives on Internet has paved an easy path for attackers to take the repercussions of DDoS attack to a challenging level. Malicious users cannot be assumed to obey network protocols or algorithms. In fact, they tried to take advantage of their knowledge about network to disrupt other users and to gain a maximum share of resources. Therefore, in this paper, we propose a Bayesian game theory-based solution to empower service provider to maximize the social welfare by employing incentives and pricing rules on the users of a network. The service provider and legitimate users are assumed to observe the network for a long time and gain probabilistic knowledge about another user being malicious or not. This probabilistic knowledge is utilized by the service provider and legitimate users to amend their actions to counteract malicious users present in the network. Considering these assumptions and facts, we propose Bayesian pricing and auction mechanism to achieve Bayesian Nash Equilibrium points in different scenarios where probabilistic information proves beneficial for legitimate users and service provider. Further, we propose a reputation assessment and updating mechanism where payment and participation parameters are considered to quantify user’s reliability. Extensive experimentation has been carried out using MatLab. We consider the rate of social welfare degradation and variation in user’s utility as parameters to validate the proposed model.",2021,2142
Reinforcement learning based web crawler detection for diversity and dynamics,Yang Gao and Zunlei Feng and Xiaoyang Wang and Mingli Song and Xingen Wang and Xinyu Wang and Chun Chen,"Crawler detection is always an important research topic in network security. With the development of web technology, crawlers are constantly updating and changing, and their types are becoming diverse. The diversity and dynamics of crawlers pose significant challenges for feature applicability and model robustness. Existing crawler detection methods can only detect a limited number of crawlers by predefined rules and can not cover all types of crawlers; worse, they can be completely invalidated by the emergence of new types of crawlers. In this paper, we propose a reinforcement learning based web crawler detection method for diversity and dynamics (WC3D), which is composed of a feature selector and a session classifier. The feature selector selects the appropriate feature set for different types of crawlers with deep deterministic policy gradient. The session classifier makes crawler detection and provides rewards to the feature selector. The two modules are trained jointly to optimize the feature selection and session classification processes. Extensive experiments demonstrate the existence of crawler diversity and that the proposed method is still highly robust against the new type of crawlers and achieves state-of-the-art performance even without considering the dynamics of the crawlers.",2023,2143
Smart defense against distributed Denial of service attack in IoT networks using supervised learning classifiers,B.B. Gupta and Pooja Chaudhary and Xiaojun Chang and Nadia Nedjah,"From smart home to industrial automation to smart power grid, IoT- based solutions penetrate into every working field. These devices expand the attack surface and turned out to be an easy target for the attacker as resource constraint nature hinders the integration of heavy security solutions. Because IoT devices are less secured and operate mostly in unattended scenario, they perfectly justify the requirements of attacker to form botnet army to trigger Denial of Service attack on massive scale. Therefore, this paper presents a Machine Learning-based attack detection approach to identify the attack traffic in Consumer IoT (CIoT). This approach operates on local IoT network-specific attributes to empower low-cost machine learning classifiers to detect attack, at the local router. The experimental outcomes unveiled that the proposed approach achieved the highest accuracy of 0.99 which confirms that it is robust and reliable in IoT networks.",2022,2144
Discovering emerging business ideas based on crowdfunded software projects,Won Sang Lee and So Young Sohn,"User-centered innovation has attracted considerable interest for exploiting emerging business ideas. We suggest a novel framework for discovering emerging business ideas and their combination with user-centered innovation in the software industry based on design thinking processes. We apply topic modeling to projects on Kickstarter which is one of the largest crowdfunding platforms in the world. We adopt conjoint analysis to find which topics are most preferred upon the platform in terms of the amount of funding that they have received. From our findings, the convergence of smart assistant services with various domains, such as tutoring mathematics and seeking job opportunities, is recommended as an emerging idea for software businesses. We also find that the ideas preferred in the US are different from those preferred in other countries. Our findings can be exploited effectively for decision support in establishing a new business model. Finally, this study contributes to discovering emerging business ideas by connecting user-centered innovation with a design thinking perspective.",2019,2145
"Mapping the Dutch vaccination debate on Twitter: Identifying communities, narratives, and interactions",Roel O. Lutkenhaus and Jeroen Jansz and Martine P.A. Bouman,"In recent years, vaccination rates in the Netherlands have declined slightly, but steadily. The Dutch National Institute for Public Health and the Environment (RIVM) commissioned a Committee for Vaccine Willingness (VWC) to study the societal context of the decline. One of the societal contexts is the Internet, where audiences discuss vaccination and refer to sources of health-related information of varying quality. Working for the VWC, we have explored the Dutch vaccination debate on Twitter in order to: (1) identify online communities in the vaccination debate, (2) identify vaccine-related narratives; and (3) understand how the online communities interact with each other. We identified seven different communities, including (public) health professionals, writers and journalists, anti-establishment, and international vaccination advocates. The debate is spearheaded by the writers & journalists community, while the health- and anti-establishment communities try to influence it. The health community circulates facts, figures and scientific studies, while negative messages about vaccination – either from a homeopathy or conspiracy perspective – are most prevalent in the anti-establishment. The facts and figures shared by the health community hardly reach other communities, whereas the myths introduced by the anti-establishment do spill over to other communities. Our study provides further evidence that negative perceptions about vaccination might be rooted in a wider sentiment of distrust of traditional institutions. We argue that Dutch health organizations should try to address questions, doubts, and worries among the general audience more actively, and present scientific information in a simpler and more attractive way.",2019,2146
Automated evolution of feature logging statement levels using Git histories and degree of interest,Yiming Tang and Allan Spektor and Raffi Khatchadourian and Mehdi Bagherzadeh,"Logging—used for system events and security breaches to describe more informational yet essential aspects of software features—is pervasive. Given the high transactionality of today's software, logging effectiveness can be reduced by information overload. Log levels help alleviate this problem by correlating a priority to logs that can be later filtered. As software evolves, however, levels of logs documenting surrounding feature implementations may also require modification as features once deemed important may have decreased in urgency and vice-versa. We present an automated approach that assists developers in evolving levels of such (feature) logs. The approach, based on mining Git histories and manipulating a degree of interest (DOI) model,1 transforms source code to revitalize feature log levels based on the “interestingness” of the surrounding code. Built upon JGit and Mylyn, the approach is implemented as an Eclipse IDE plug-in and evaluated on 18 Java projects with ∼3 million lines of code and ∼4K log statements. Our tool successfully analyzes 99.22% of logging statements, increases log level distributions by ∼20%, and increases the focus of logs in bug fix contexts ∼83% of the time. Moreover, pull (patch) requests were integrated into large and popular open-source projects. The results indicate that the approach is promising in assisting developers in evolving feature log levels.",2022,2147
Massive stereo-based DTM production for Mars on cloud computers,Y. Tao and J.-P. Muller and P. Sidiropoulos and Si-Ting Xiong and A.R.D. Putri and S.H.G. Walter and J. Veitch-Michaelis and V. Yershov,"Digital Terrain Model (DTM) creation is essential to improving our understanding of the formation processes of the Martian surface. Although there have been previous demonstrations of open-source or commercial planetary 3D reconstruction software, planetary scientists are still struggling with creating good quality DTMs that meet their science needs, especially when there is a requirement to produce a large number of high quality DTMs using “free” software. In this paper, we describe a new open source system to overcome many of these obstacles by demonstrating results in the context of issues found from experience with several planetary DTM pipelines. We introduce a new fully automated multi-resolution DTM processing chain for NASA Mars Reconnaissance Orbiter (MRO) Context Camera (CTX) and High Resolution Imaging Science Experiment (HiRISE) stereo processing, called the Co-registration Ames Stereo Pipeline (ASP) Gotcha Optimised (CASP-GO), based on the open source NASA ASP. CASP-GO employs tie-point based multi-resolution image co-registration, and Gotcha sub-pixel refinement and densification. CASP-GO pipeline is used to produce planet-wide CTX and HiRISE DTMs that guarantee global geo-referencing compliance with respect to High Resolution Stereo Colour imaging (HRSC), and thence to the Mars Orbiter Laser Altimeter (MOLA); providing refined stereo matching completeness and accuracy. All software and good quality products introduced in this paper are being made open-source to the planetary science community through collaboration with NASA Ames, United States Geological Survey (USGS) and the Jet Propulsion Laboratory (JPL), Advanced Multi-Mission Operations System (AMMOS) Planetary Data System (PDS) Pipeline Service (APPS-PDS4), as well as browseable and visualisable through the iMars web based Geographic Information System (webGIS) system.",2018,2148
Does negative campaign advertising stimulate uncivil communication on social media? Measuring audience response using big data,Toby Hopp and Chris J. Vargo,"Using the 2012 presidential election as a case study, this work set out to understand the relationship between negative political advertising and political incivility on Twitter. Drawing on the stimulation hypothesis and the notion that communication with dissimilar others can encourage incivility, it was predicted that (1) heightened levels of negative campaign advertising would be associated with increased citizen activity on Twitter, (2) increased citizen activity would predict online incivility, and (3) that increases in citizen activity would facilitate a positive indirect relationship between negative advertising volume and citizen incivility. This theoretical model was tested using data collected from over 140,000 individual Twitter users located in 206 Designated Market Areas. The results supported the proposed model. Additional analyses further suggested that the relationship between negative political advertising and citizen incivility was conditioned by contextual levels of economic status. These results are discussed in the context of political advertising and democratic deliberation.",2017,2149
MONDEO-Tactics5G: Multistage botnet detection and tactics for 5G/6G networks,Bruno Sousa and Duarte Dias and Nuno Antunes and Javier Cámara and Ryan Wagner and Bradley Schmerl and David Garlan and Pedro Fidalgo,"Mobile malware is a malicious code specifically designed to target mobile devices to perform multiple types of fraud. The number of attacks reported each day is increasing constantly and is causing an impact not only at the end-user level but also at the network operator level. Malware like FluBot contributes to identity theft and data loss but also enables remote Command & Control (C2) operations, which can instrument infected devices to conduct Distributed Denial of Service (DDoS) attacks. Current mobile device-installed solutions are not effective, as the end user can ignore security warnings or install malicious software. This article designs and evaluates MONDEO-Tactics5G - a multistage botnet detection mechanism that does not require software installation on end-user devices, together with tactics for 5G network operators to manage infected devices. We conducted an evaluation that demonstrates high accuracy in detecting FluBot malware, and in the different adaptation strategies to reduce the risk of DDoS while minimising the impact on the clients' satisfaction by avoiding disrupting established sessions.",2024,2150
Drug efflux and lipid A modification by 4-L-aminoarabinose are key mechanisms of polymyxin B resistance in the sepsis pathogen Enterobacter bugandensis,Inmaculada García-Romero and Mugdha Srivastava and Julia Monjarás-Feria and Samuel O. Korankye and Lewis MacDonald and Nichollas E. Scott and Miguel A. Valvano,"ABSTRACT
Objectives
A concern with the ESKAPE pathogen, Enterobacter bugandensis, and other species of the Enterobacter cloacae complex, is the frequent appearance of multidrug resistance against last-resort antibiotics, such as polymyxins.
Methods
Here, we investigated the responses to polymyxin B (PMB) in two PMB-resistant E. bugandensis clinical isolates by global transcriptomics and deletion mutagenesis.
Results
In both isolates, the genes of the CrrAB-regulated operon, including crrC and kexD, displayed the highest levels of upregulation in response to PMB. ∆crrC and ∆kexD mutants became highly susceptible to PMB and lost the heteroresistant phenotype. Conversely, heterologous expression of CrrC and KexD proteins increased PMB resistance in a sensitive Enterobacter ludwigii clinical isolate and in the Escherichia coli K12 strain, W3110. The efflux pump, AcrABTolC, and the two component regulators, PhoPQ and CrrAB, also contributed to PMB resistance and heteroresistance. Additionally, the lipid A modification with 4-L-aminoarabinose (L-Ara4N), mediated by the arnBCADTEF operon, was critical to determine PMB resistance. Biochemical experiments, supported by mass spectrometry and structural modelling, indicated that CrrC is an inner membrane protein that interacts with the membrane domain of the KexD pump. Similar interactions were modeled for AcrB and AcrD efflux pumps.
Conclusion
Our results support a model where drug efflux potentiated by CrrC interaction with membrane domains of major efflux pumps combined with resistance to PMB entry by the L-Ara4N lipid A modification, under the control of PhoPQ and CrrAB, confers the bacterium high-level resistance and heteroresistance to PMB.",2024,2151
"Adaptive gamification in Collaborative systems, a systematic mapping study",María {Dalponte Ayastuy} and Diego Torres and Alejandro Fernández,"Mass collaboration mediated by technology is now commonplace (Wikipedia, Quora, TripAdvisor). Online, mass collaboration is also present in science in the form of Citizen Science. These collaboration models, which have a large community of contributors coordinated to pursue a common goal, are known as Collaborative systems. This article introduces a study of the published research on the application of adaptive gamification to collaborative systems. The study focuses on works that explicitly discuss an approach of personalization or adaptation of the gamification elements in this type of system. It employs a systematic mapping design in which a categorical structure for classifying the research results is proposed based on the topics that emerged from the papers review. The main contributions of this paper are a formalization of the adaptation strategies and the proposal of a new taxonomy for gamification elements adaptation. The results evidence the lack of research literature in the study of adapting gamification in the field of collaborative systems. Considering the underlying cultural diversity in those projects, the adaptability of gamification design and strategies is a promissory research field.",2021,2152
The allometric propagation of COVID-19 is explained by human travel,Rohisha Tuladhar and Paolo Grigolini and Fidel Santamaria,"We analyzed the number of cumulative positive cases of COVID-19 as a function of time in countries around the World. We tracked the increase in cases from the onset of the pandemic in each region for up to 150 days. We found that in 81 out of 146 regions the trajectory was described with a power-law function for up to 30 days. We also detected scale-free properties in the majority of sub-regions in Australia, Canada, China, and the United States (US). We developed an allometric model that was capable of fitting the initial phase of the pandemic and was the best predictor for the propagation of the illness for up to 100 days. We then determined that the power-law COVID-19 exponent correlated with measurements of human mobility. The COVID-19 exponent correlated with the magnitude of air passengers per country. This correlation persisted when we analyzed the number of air passengers per US states, and even per US metropolitan areas. Furthermore, the COVID-19 exponent correlated with the number of vehicle miles traveled in the US. Together, air and vehicular travel explained 70% of the variability of the COVID-19 exponent. Taken together, our results suggest that the scale-free propagation of the virus is present at multiple geographical scales and is correlated with human mobility. We conclude that models of disease transmission should integrate scale-free dynamics as part of the modeling strategy and not only as an emergent phenomenological property.",2022,2153
The risk perception of nanotechnology: evidence from twitter,Finbarr Murphy and Ainaz Alavi and Martin Mullins and Irini Furxhi and Arash Kia and Myles Kingston,"ABSTRACT
Nanotechnology governance, particularly in relation to human and environmental concerns, remains a contested domain. In recent years, the creation of both a risk governance framework and council has been actively pursued. Part of the function of a governance framework is the communication to external stakeholders. Existing descriptions on the public perceptions of nanotechnology are generally positive with the attendant economic and societal benefits being forefront in that thinking. Debates on nanomaterials' risk tend to be dominated by expert groupings while the general public is largely unaware of the potential hazards. Communicating via social media has become an integral part of everyday life facilitating public connectedness around specific topics that was not feasible in the pre-digital age. When civilian passive stakeholders become active their frustration can quickly coalesce into a campaign of resistance, and once an issue starts to develop into a campaign it is difficult to ease the momentum. Simmering discussions with moderate local attention can gain international exposure resulting in pressure and it can, in some cases, quickly precipitate legislative action and/or economic consequences. This paper highlights the potential of such a runaway, twitterstorm. We conducted a sentiment analysis of tweets since 2006 focusing on silver, titanium and carbon-based nanomaterials. We further examined the sentiment expressed following the decision by the European Food Safety Authority (EFSA) to phase out the food additive titanium dioxide (E 171). Our analysis shows an engaged, attentive public, alert to announcements from industry and regulatory bodies. We demonstrate that risk governance frameworks, particularly the communication aspect of those structures must include a social media blueprint to counter misinformation and alleviate the potential impact of a social media induced regulatory and economic reaction.",2022,2154
DDoS goes mainstream: how headline-grabbing attacks could make this threat an organisation's biggest nightmare,Steve Mansfield-Devine,"In mid-October, a distributed denial of service (DDoS) attack hit the headlines in a big way. Targeting DNS service provider Dyn, it rendered a significant portion of the Internet inoperable and left many high-profile web services unreachable for several hours. But while this was arguably the most visible DDoS attack in history, it's only one among many. In this interview, Paul Nicholson, responsible for global product marketing and strategy at A10 Networks, talks about how DDoS is becoming an ever-growing threat and what organisations can do about it. We’ve just witnessed the biggest distributed denial of service (DDoS) attacks in history, which turned seemingly harmless devices such as video recorders into cyber-weapons. With both the scale and frequency of attacks increasing, many organisations are left wondering how they can protect themselves and how those defences should be deployed, whether on-premise or in the cloud. In this interview with Paul Nicholson of A10 Networks, we examine how DDoS is becoming an ever-growing threat and what organisations can do about it.",2016,2155
Medical Cyber Physical Systems and Its Issues,Meghna Manoj Nair and Amit Kumar Tyagi and Richa Goyal,"In the previous decade, many technologies have attracted attention from several research communities. Internet of Things (IoT) is main invention of the recent/ past decade. When these smart devices or internet connected devices are interact together, then they create a cyber infrastructure. These cyber infrastructures face several serious concerns privacy, trust, security, etc. These smart devices make an automatic environment (executed without the intervention of a human) in applications likedefense, manufacturing, e-healthcare, etc. In e-healthcare, these devices built the structure of Medical Cyber Physical System (MCPS). MCPS are facing several critical issues and challenges in current era, i.e., several attacks, issues and challenges which we require to overcome in current and next decade to provide efficient and reliable service to patients. MCPS is need of smart healthcare and require attention from several research communities towards its raised issue. Hence, this article provides a detailed study about CPS, MCPS, mitigated attacks on same architecture (CPS and MCPS), issues and challenges in CPS/ MCPS, including several research gaps in CPS/ MCPS (with opportunities for future researchers).",2019,2156
Pore-scale spatiotemporal dynamics of microbial-induced calcium carbonate growth and distribution in porous media,Na Liu and Malin Haugen and Benyamine Benali and David Landa-Marbán and Martin A. Fernø,"The naturally occurring bio-geochemical microbial-induced calcium carbonate precipitation (MICP) process is an eco-friendly technology for rehabilitating construction materials, reinforcement of soils and sand, heavy metals immobilization and sealing subsurface leakage pathways. We report pore-scale spatiotemporal dynamics of the MICP process in porous media, relevant for reduced environmental risk by leakage during CO2 geological storage. Effects of hydrodynamics and supersaturation on the MICP with Sporosarcina pasteurii stains were studied using a high-pressure, rock-on-a-chip microfluidic device. Bacterial cell numbers and variation in cementation concentration controlled the crystal size and pore-scale distribution by influencing the local supersaturation. Local pore structure determined crystal nucleation, where low velocity regions tended to nucleate more crystals. CaCO3 crystallization was observed at subsurface pressure (100 barg) with a reduced sealing performance due to the low microbial activity from elevated pressure. We identify that hydrodynamics and supersaturation determine crystal nucleation and growth in porous systems, providing important experimental evidence for subsurface environmental applications and validation of upscaled MICP models.",2023,2157
Deterministic Arterial Input Function selection in DCE-MRI for automation of quantitative perfusion calculation of colorectal cancer,Christian Tönnes and Sonja Janssen and Alena-Kathrin Golla and Tanja Uhrig and Khanlian Chung and Lothar R. Schad and Frank Gerrit Zöllner,Development of a deterministic algorithm for automated detection of the Arterial Input Function (AIF) in DCE-MRI of colorectal cancer. Using a filter pipeline to determine the AIF region of interest. Comparison to algorithms from literature with mean squared error and quantitative perfusion parameter Ktrans. The AIF found by our algorithm has a lower mean squared error (0.0022 ± 0.0021) in reference to the manual annotation than comparable algorithms. The error of Ktrans (21.52 ± 17.2%) is lower than that of other algorithms. Our algorithm generates reproducible results and thus supports a robust and comparable perfusion analysis.,2021,2158
The characteristics of rumor spreaders on Twitter: A quantitative analysis on real data,Amirhosein Bodaghi and Jonice Oliveira,"In this paper we study a dozen of rumors on Twitter to find new insights in user characteristics and macro patterns in the process of rumor spreading. The collection and curation of data has left us with 12 rumor datasets out of 56,852 tweets from 43,919 users. The analysis over data shows users with lower ratio of following-to-follower are more probable to spark the rumor diffusion while users with the higher ratio are those who keep the flame alive. Furthermore, most users participate in the process of rumor spreading only once which implies the nature of rumor spreading is not a recurrent activity. However, among those users who engage with multi posts, the extreme change of state from rumor spreader to anti-rumor spreader happens to users with higher ratio of following-to-follower. We discuss these findings by employing the theory of planned behavior. Finally, analyzing the process of rumor spreading at the macro level revealed the existence of two distinctive patterns. Further investigations showed the extent of time gap between the beginning of rumor and anti-rumor diffusion plays the major role in emerging of these patterns. This phenomenon is explained by the shift in subjective norm toward rumors on social media.",2020,2159
14 - Measuring attention: social media and altmetrics,Svetla Baykoucheva,"Scholars are communicating in many different spheres today, using social media, mobile technology, and cloud computing. Until now, research has been evaluated using citation metrics such as the impact factor (IF) and h-index. As scholarly communication has shifted now mostly to online, other methods are needed to bring attention to research and measure its impact. Altmetrics, a new field that is creating and using such alternative metrics, takes into account not just citation counts of articles published in peer-reviewed journals. It involves collecting information from different sources and measuring interest in articles, people, journals, datasets, presentations, and other artifacts by monitoring views, downloads, “likes,” and mentions in social networks and the news media. Researchers engaging in social networks now rely on recommendations from their peers about newly published articles. Altmetrics tools allow them to see what others are reading, saving, and commenting on. This chapter presents an overview of the area of altmetrics and discusses how it is affecting the dissemination of scientific information and its evaluation.",2015,2160
Prediction of initial coin offering success based on team knowledge and expert evaluation,Wei Xu and Ting Wang and Runyu Chen and J. Leon Zhao,"Initial coin offering (ICO) is a new financing method that has been widely used in cryptocurrency projects. However, it has been reported that nearly 30% of cryptocurrency projects fail during ICO, indicating an important gap in research and an opportunity for more advanced research on ICO project assessment. This study reveals that previous studies primarily used project-related factors to predict ICO success while neglecting social factors such as team information and expert evaluation. Inspired by the knowledge-based theory (KBT) of the firm, we set out to examine the impact of heterogeneous team knowledge and expert evaluation on ICO success. One primary contribution of this study is the design of novel knowledge measures based on KBT. In addition, we propose a deep-learning model – an attention-based bidirectional recurrent neural network (A-BiRNN) – to automatically extract features from online comments. We validate the proposed model on a real-world dataset, and experiments show that the accuracy of the proposed prediction model outperforms those of existing models by more than 6%, highlighting the effectiveness of the proposed approach in predicting ICO success. This study's results provide useful ideas for both investors and ICO platforms to assess the quality of cryptocurrency projects, thus improving information symmetry in ICO markets. Also, this study demonstrates the value of applying KBT in assessing firm performance in ICO markets. The generalized value of the proposed approach should be tested in more business contexts, such as crowdfunding and peer-to-peer (P2P) lending.",2021,2161
flepiMoP: The evolution of a flexible infectious disease modeling pipeline during the COVID-19 pandemic,Joseph C. Lemaitre and Sara L. Loo and Joshua Kaminsky and Elizabeth C. Lee and Clifton McKee and Claire Smith and Sung-mok Jung and Koji Sato and Erica Carcelen and Alison Hill and Justin Lessler and Shaun Truelove,"The COVID-19 pandemic led to an unprecedented demand for projections of disease burden and healthcare utilization under scenarios ranging from unmitigated spread to strict social distancing policies. In response, members of the Johns Hopkins Infectious Disease Dynamics Group developed flepiMoP (formerly called the COVID Scenario Modeling Pipeline), a comprehensive open-source software pipeline designed for creating and simulating compartmental models of infectious disease transmission and inferring parameters through these models. The framework has been used extensively to produce short-term forecasts and longer-term scenario projections of COVID-19 at the state and county level in the US, for COVID-19 in other countries at various geographic scales, and more recently for seasonal influenza. In this paper, we highlight how the flepiMoP has evolved throughout the COVID-19 pandemic to address changing epidemiological dynamics, new interventions, and shifts in policy-relevant model outputs. As the framework has reached a mature state, we provide a detailed overview of flepiMoP’s key features and remaining limitations, thereby distributing flepiMoP and its documentation as a flexible and powerful tool for researchers and public health professionals to rapidly build and deploy large-scale complex infectious disease models for any pathogen and demographic setup.",2024,2162
Traffic-flow analysis for source-side DDoS recognition on 5G environments,Marco Antonio {Sotelo Monge} and Andrés {Herranz González} and Borja {Lorenzo Fernández} and Diego {Maestre Vidal} and Guillermo {Rius García} and Jorge {Maestre Vidal},"This paper introduces a novel approach for detecting the participation of a protected network device in flooding-based Distributed Denial of Service attacks. With this purpose, the traffic flows are inspected at source-side looking for discordant behaviors. In contrast to most previous solutions, the proposal assumes the non-stationarity and heterogeneity inherent in the emergent communication environment. In particular, the approach takes advantage of the monitorization and knowledge acquisition capabilities implemented in the SELFNET (H2020-ICT-2014-2/671672) project, which facilitates its implementation as a self-organizing solution on 5G mobile networks. Monitorization, feature extraction and knowledge acquisition tasks are carried out on centralized control plane, hence the proposed architecture minimizes the impact on operational performance and prompts the end-points mobility. The preliminary results observed when considering different metrics, adjustment parameters, and a dataset with traffic observed in 61 real devices proven efficiency when distinguishing normal activities from DDoS behaviors of different intensity. With an optimal granularity selection, the highest AUC reached values close to 1.0 when measured under the most intense attacks, hence demonstrating optimal TPR and FPR relationships by adapting to the instantiated use cases.",2019,2163
Imperfect ImaGANation: Implications of GANs exacerbating biases on facial data augmentation and snapchat face lenses,Niharika Jain and Alberto Olmo and Sailik Sengupta and Lydia Manikonda and Subbarao Kambhampati,"In this paper, we show that popular Generative Adversarial Network (GAN) variants exacerbate biases along the axes of gender and skin tone in the generated data. The use of synthetic data generated by GANs is widely used for a variety of tasks ranging from data augmentation to stylizing images. While practitioners celebrate this method as an economical way to obtain synthetic data to train data-hungry machine learning models or provide new features to users of mobile applications, it is unclear whether they recognize the perils of such techniques when applied to real world datasets biased along latent dimensions. Although one expects GANs to replicate the distribution of the original data, in real-world settings with limited data and finite network capacity, GANs suffer from mode collapse. First, we show readily-accessible GAN variants such as DCGANs ‘imagine’ faces of synthetic engineering professors that have masculine facial features and fair skin tones. When using popular GAN architectures that attempt to address mode-collapse, we observe that these variants either provide a false sense of security or suffer from other inherent limitations due to their design choice. Second, we show that a conditional GAN variant transforms input images of female and nonwhite faces to have more masculine features and lighter skin when asked to generate faces of engineering professors. Worse yet, prevalent filters on Snapchat end up consistently lightening the skin tones in people of color when trying to make face images appear more feminine. Thus, our study is meant to serve as a cautionary tale for practitioners and educate them about the side-effect of bias amplification when applying GAN-based techniques.",2022,2164
Detecting cryptocurrency pump-and-dump frauds using market and social signals,Huy Nghiem and Goran Muric and Fred Morstatter and Emilio Ferrara,"The cryptocurrency market has gained significant traction in the last decade, becoming an alternative finance platform to traditional stock market trading. Despite its rapid evolution, legal regulations have not yet caught up to the cryptocurrency market’s progress, attracting the attention of scammers looking to exploit legal loopholes for profits. Pump-and-dump schemes, a well-worn fraud device, has regained relevance in this new territory. In a typical pump-and-dump scheme, scammers organize and leverage media channels to artificially inflate the price of an alternative cryptocurrency, only to quickly sell them to profit off unsuspecting buyers. The disruptive nature of pump-and-dump schemes necessitates a system to reliably forecast pump targets and the magnitude of its success. In this paper, we propose an approach to predict the target cryptocurrency for each pump before its announcement using market and social media signals using Neural Network-based architectures while offering interpretable insights into their black-box nature. Additionally, we construct models that are capable of forecasting the highest price induced by the pump after the cryptocurrency’s identity is revealed within 6.1% error margin. We examine the optimal temporal windows and describe the limitations of social data to predict the manipulations in cryptocurrency trade. Our experimental results serve as proof of a feasible forecasting expert system for identifying cryptocurrency pump-and-dump frauds using publicly available data.",2021,2165
"A survey on Blockchain solutions in DDoS attacks mitigation: Techniques, open challenges and future directions",Rajasekhar Chaganti and Bharat Bhushan and Vinayakumar Ravi,"With the proliferation of new technologies such as the Internet of Things (IoT) and Software-Defined Networking (SDN) in recent years, the Distributed Denial of Service (DDoS) attack vector has broadened and opened new opportunities for more sophisticated DDoS attacks on the targeted victims. The new attack vector includes unsecured and vulnerable IoT devices connected to the internet, and denial of service vulnerabilities like southbound channel saturation in the SDN architecture. Given the high-volume and pervasive nature of these attacks, it is beneficial for stakeholders to collaborate in detecting and mitigating the denial of service attacks promptly. Blockchain technology is considered to improve the security aspects owing to the decentralized design, secured distributed storage, and privacy. A thorough exploration and classification of blockchain techniques used for DDoS attack mitigation are not explored in the prior art. This paper reviews and categorizes state-of-the-art DDoS mitigation solutions based on blockchain technology. The DDoS mitigation techniques are classified based on the solution deployment location i.e. network-based, near attacker location, near victim location, and hybrid solutions in the network architecture with emphasis on the IoT and SDN architectures. Additionally, based on our study, the research challenges and future directions to implement the blockchain based DDoS mitigation solutions are discussed.",2023,2166
Predicting and analyzing the popularity of false rumors in Weibo,Yida Mu and Pu Niu and Kalina Bontcheva and Nikolaos Aletras,"Malicious online rumors with high popularity, if left undetected, can spread very quickly with damaging societal implications. The development of reliable computational methods for early prediction of the popularity of false rumors is very much needed, as a complement to related work on automated rumor detection and fact-checking. Besides, detecting false rumors with higher popularity in the early stage allows social media platforms to timely deliver fact-checking information to end users. To this end, we (1) propose a new regression task to predict the future popularity of false rumors given both post and user-level information; (2) introduce a new publicly available dataset in Chinese that includes 19,256 false rumor cases from Weibo, the corresponding profile information of the original spreaders and a rumor popularity score as a function of the shares, replies and reports it has received; (3) develop a new open-source domain adapted pre-trained language model, i.e., BERT-Weibo-Rumor and evaluate its performance against several supervised classifiers using post and user-level information. Our best performing model (KG-Fusion) achieves the lowest RMSE score (1.54) and highest Pearson’s r (0.636), outperforming competitive baselines by leveraging textual information from both the post and the user profile. Our analysis unveils that popular rumors consist of more conjunctions and punctuation marks, while less popular rumors contain more words related to the social context and personal pronouns. Our dataset is publicly available: https://github.com/YIDAMU/Weibo_Rumor_Popularity.",2024,2167
"A comprehensive survey of fake news in social networks: Attributes, features, and detection approaches",Medeswara Rao Kondamudi and Somya Ranjan Sahoo and Lokesh Chouhan and Nandakishor Yadav,"The explosion of online social networks in recent decades has significantly improved in which the way individuals communicate with one another. People trust social networks bluntly without knowing the origin and genuinity of the information passed through these networks. Sometimes, unreliable information on online social networks misleads the viewers, and it brings unremovable stains to humanity. Online social networks transform even the original information of the government, which create confusion among the people and people loses confidence over the government. Various types of research have been conducted to identify fake news with high efficiency. In this survey, we describe the basic theories of fake news, investigate and analyze the perspective on fake news, attribute misleading information, an in-depth analysis of disinformation, and methods that have been established for detection. To our knowledge, this research article will assist in facilitating collaborative activities among technical experts, political campaigns, online purchases, and other disciplines that are being used to investigate fake messages.",2023,2168
Humans are still better than ChatGPT: Case of the IEEEXtreme competition,Anis Koubaa and Basit Qureshi and Adel Ammar and Zahid Khan and Wadii Boulila and Lahouari Ghouti,"Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark—a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we found that the average score obtained by ChatGPT on the set of IEEExtreme programming problems is 3.9 to 5.8 times lower than the average human score, depending on the programming language. This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.",2023,2169
An automatic distillation sequence synthesis framework based on a preorder traversal algorithm,Anqing Wang and Alexander Guzman-Urbina and Hajime Ohno and Yasuhiro Fukushima,"Distillation energy consumption dominates the process industry; hence, the selection of the distillation sequence will substantially affect the separation energy consumption. We propose a framework for the automatic synthesis and optimization of distillation sequences by integrating the Aspen Plus with the MATLAB programming platform. The framework combines the concept of binary trees in data structures and uses a preorder traversal algorithm to generate a network superstructure in the simulator. In the next step, it automatically formulates and solves a mixed-integer linear programming model based on calculation results; finally, the distillation model is optimized using an improved quadratic interpolation algorithm. Two case studies for C5 alkane and dimethyl carbonate separation showed that the optimal solution obtained by the framework reduced the total annual costs by 13.18 % and 2.88 %, respectively, compared with the processes without systematic optimization. The framework also allowed screening out promising alternatives by rapid evaluation of different process routes.",2024,2170
TrollBOT: A Spontaneous Networking Tool Facilitating Rapid Prototyping of Wirelessly Communicating Products,Torjus Steffensen and Sampsa Kohtala and Håvard Vestad and Martin Steinert,"In early stages of product development, prototyping is an invaluable tool which allows designers to generate learnings and uncover unknown challenges which can be used to further construct design requirements. While generous use of prototyping early in the design process might reduce the risk of premature design decisions, it also demands significant investments in terms of resources such as time, material, and skills. Tools that allow designers to rapidly implement and test new functionalities are therefore desired. With wirelessly communicating products having become ubiquitous in modern society, designers should be comfortable designing products utilizing these technologies. In this paper we present an Arduino library, named TrollBOT, that facilitates rapid implementation of wireless communication between two or more Arduinos. The Arduinos form nodes in a tree topology using inexpensive nRF24-based radio transceivers. The library is constructed in such a way that a minimal amount of new language syntax must be learned. All nodes can be programmed from a single master node in an intuitive manner, significantly reducing the amount of code that needs to be written as compared to similar existing solutions.",2020,2171
The vision of on-demand architectural knowledge systems as a decision-making companion,Maryam Razavian and Barbara Paech and Antony Tang,"Cobbler’s children do not wear shoes. Software engineers build sophisticated software but we often cannot find the needed information and knowledge for ourselves. Issues are the amount of development information that can be captured, organizing that information to make them useable for other developers as well as human decision-making issues. Current architectural knowledge management systems cannot handle these issues properly. In this paper, we outline a research agenda for intelligent tools to support the knowledge management and decision making of architects. The research agenda consists of a vision and research challenges on the way to realize this vision. We call our vision on-demand architectural knowledge systems (ODAKS). Based on literature review, analysis, and synthesis of past research works, we derive our vision of ODAKS as decision-making companions to architects. ODAKS organize and provide relevant information and knowledge to the architect through an assistive conversation. ODAKS use probing to understand the architects’ goals and their questions, they suggest relevant knowledge and present reflective hints to mitigate human decision-making issues, such as cognitive bias, cognitive limitations, as well as design process aspects, such as problem-solution co-evolution and the balance between intuitive and rational decision-making. We present the main features of ODAKS, investigate current potential technologies for the implementation of ODAKS and discuss the main research challenges.",2023,2172
“Born in Rome” or “Sleeping Beauty”: Emergence of hashtag popularity on the Chinese microblog Sina Weibo,Hao Cui and János Kertész,"To understand the emergence of hashtag popularity in online social networking complex systems, we study the largest Chinese microblogging site Sina Weibo, which has a Hot Search List (HSL) showing in real time the ranking of the 50 most popular hashtags based on search activity. We investigate the prehistory of successful hashtags from 17 July 2020 to 17 September 2020 by mapping out the related interaction network preceding the selection to HSL. We have found that the circadian activity pattern has an impact on the time needed to get to the HSL. When analyzing this time we distinguish two extreme categories: (a) “Born in Rome”, which means hashtags are mostly first created by superhubs or reach superhubs at an early stage during their propagation and thus gain immediate wide attention from the broad public, and (b) “Sleeping Beauty”, meaning the hashtags gain little attention at the beginning and reach system-wide popularity after a considerable time lag. The evolution of the repost networks of successful hashtags before getting to the HSL show two types of growth patterns: “smooth” and “stepwise”. The former is usually dominated by a superhub and the latter results from consecutive waves of contributions of smaller hubs. The repost networks of unsuccessful hashtags exhibit a simple evolution pattern.",2023,2173
Multifactor door locking systems: A review,Yashraj Motwani and Saambhavi Seth and Devang Dixit and A. Bagubali and R. Rajesh,"Security has become very important, but along with that, people also need a system that is not very expensive and can be customized to meet our needs. As conventional door locks can be easily opened, this makes people vulnerable to security threats. This study attempts a comparative analysis of pre-existing researches, made in the field of security control system developed and improvised over the span of time with multifactor authentication technique’s evolvement. The components, hardware complications, work efficiency and algorithms used in each of the model is drawn as a comparison to other to provide an idea of systematic development in this regard. With each passing day, security systems are advancing and new technology is being developed. Security systems or door locking mechanics have evolved from metallic door locks of primitive type keys, to advanced controlling structure with up to four or five step authentications to ensure utmost safety.",2021,2174
Availability of datasets for digital forensics – And what is missing,Cinthya Grajeda and Frank Breitinger and Ibrahim Baggili,"This paper targets two main goals. First, we want to provide an overview of available datasets that can be used by researchers and where to find them. Second, we want to stress the importance of sharing datasets to allow researchers to replicate results and improve the state of the art. To answer the first goal, we analyzed 715 peer-reviewed research articles from 2010 to 2015 with focus and relevance to digital forensics to see what datasets are available and focused on three major aspects: (1) the origin of the dataset (e.g., real world vs. synthetic), (2) if datasets were released by researchers and (3) the types of datasets that exist. Additionally, we broadened our results to include the outcome of online search results. We also discuss what we think is missing. Overall, our results show that the majority of datasets are experiment generated (56.4%) followed by real world data (36.7%). On the other hand, 54.4% of the articles use existing datasets while the rest created their own. In the latter case, only 3.8% actually released their datasets. Finally, we conclude that there are many datasets for use out there but finding them can be challenging.",2017,2175
Chapter Three - NoSQL Web Crawler Application,Ganesh Chandra Deka,"With the advent of Web technology, the Web is full of unstructured data called Big Data. However, these data are not easy to collect, access, and process at large scale. Web Crawling is an optimization problem. Site-specific crawling of various social media platforms, e-Commerce websites, Blogs, News websites, and Forums is a requirement for various business organizations to answer a search quarry from webpages. Indexing of huge number of webpage requires a cluster with several petabytes of usable disk. Since the NoSQL databases are highly scalable, use of NoSQL database for storing the Crawler data is increasing along with the growing popularity of NoSQL databases. This chapter discusses about the application of NoSQL database in Web Crawler application to store the data collected by the Web Crawler.",2018,2176
Beware of the hierarchy — An analysis of ontology evolution and the materialisation impact for biomedical ontologies,Romana Pernisch and Daniele Dell’Aglio and Abraham Bernstein,"Ontologies are becoming a key component of numerous applications and research fields. But knowledge captured within ontologies is not static. Some ontology updates potentially have a wide ranging impact; others only affect very localised parts of the ontology and their applications. Investigating the impact of the evolution gives us insight into the editing behaviour but also signals ontology engineers and users how the ontology evolution is affecting other applications. However, such research is in its infancy. Hence, we need to investigate the evolution itself and its impact on the simplest of applications: the materialisation. In this work, we define impact measures that capture the effect of changes on the materialisation. In the future, the impact measures introduced in this work can be used to investigate how aware the ontology editors are about consequences of changes. By introducing five different measures, which focus either on the change in the materialisation with respect to the size or on the number of changes applied, we are able to quantify the consequences of ontology changes. To see these measures in action, we investigate the evolution and its impact on materialisation for nine open biomedical ontologies, most of which adhere to the EL++ description logic. Our results show that these ontologies evolve at varying paces but no statistically significant difference between the ontologies with respect to their evolution could be identified. We identify three types of ontologies based on the types of complex changes which are applied to them throughout their evolution. The impact on the materialisation is the same for the investigated ontologies, bringing us to the conclusion that the effect of changes on the materialisation can be generalised to other similar ontologies. Further, we found that the materialised concept inclusion axioms experience most of the impact induced by changes to the class inheritance of the ontology and other changes only marginally touch the materialisation.",2021,2177
A comparative study of retrieval-based and generative-based chatbots using Deep Learning and Machine Learning,Sumit Pandey and Srishti Sharma,"Increased screen time may cause significant health impacts, including harmful effects on mental health. Studies on the association between technological obsessions and their influence on health have been conducted using Deep Learning (DL) and Machine Learning (ML) techniques. The deployment of chatbots in different industries has been proven as a game-changer. We study conversational Artificial Intelligence (AI) systems enabling operators to conduct conversations with machines that resemble those with humans. We design and develop two retrieval-based and generative-based chatbots, each with six designs. Among the retrieval-based chatbots, Vanilla Recurrent Neural Network (RNN) has an accuracy of 83.22%, Long Short Term Memory (LSTM) is 89.87% accurate, Bidirectional LSTM (Bi-LSTM) is 91.57% accurate, Gated Recurrent Unit (GRU) is 65.57% accurate, and Convolution Neural Network (CNN) is 82.33% accurate. In comparison, generative-based chatbots have encoder–decoder designs that are 94.45% accurate. The most significant distinction is that while generative-based chatbots can generate new text, retrieval-based chatbots are restricted to responding to inputs that match the best of the outputs they already know.",2023,2178
The Rise of GoodFATR: A Novel Accuracy Comparison Methodology for Indicator Extraction Tools,Juan Caballero and Gibran Gomez and Srdjan Matic and Gustavo Sánchez and Silvia Sebastián and Arturo Villacañas,"To adapt to a constantly evolving landscape of cyber threats, organizations actively need to collect Indicators of Compromise (IOCs), i.e., forensic artifacts that signal that a host or network might have been compromised. IOCs can be collected through open-source and commercial structured IOC feeds. But, they can also be extracted from a myriad of unstructured threat reports written in natural language and distributed using a wide array of sources such as blogs and social media. There exist multiple indicator extraction tools that can identify IOCs in natural language reports. But, it is hard to compare their accuracy due to the difficulty of building large ground truth datasets. This work presents a novel majority vote methodology for comparing the accuracy of indicator extraction tools, which does not require a manually-built ground truth. We implement our methodology into GoodFATR, an automated platform for collecting threat reports from a wealth of sources, extracting IOCs from the collected reports using multiple tools, and comparing their accuracy. GoodFATR supports 6 threat report sources: RSS, Twitter, Telegram, Malpedia, APTnotes, and ChainSmith. GoodFATR continuously monitors the sources, downloads new threat reports, extracts 41 indicator types from the collected reports, and filters non-malicious indicators to output the IOCs. We run GoodFATR over 15 months to collect 472,891 reports from the 6 sources; extract 978,151 indicators from the reports; and identify 618,217 IOCs. We analyze the collected data to identify the top IOC contributors and the IOC class distribution. We apply GoodFATR to compare the IOC extraction accuracy of 7 popular open-source tools with GoodFATR’s own indicator extraction module.",2023,2179
Live Sentiment Analysis Using Multiple Machine Learning and Text Processing Algorithms,Andrew Motz and Elizabeth Ranta and Adan Sierra Calderon and Quin Adam and Fadi Alzhouri and Dariush Ebrahimi,"Due to the massive amount of data being generated on the platform, Twitter has been the subject of numerous sentiment analysis studies. Such social network services generate massive unstructured data streams which make working with them very challenging. The aim of this study is to reliably analyze the sentiment of trending tweets in the Twitter API data stream using a combination of different algorithms to achieve a consensus. The methods we implemented include Support-Vector Machine, Naive Bayes, Textblob, and Lexicon Approach. The hypothesis is that using these methods together would enable us to get more accurate results. Using a labeled dataset to test our model, the results show that the combination of these four algorithms all together performed best with an overall accuracy of 68.29%. We conclude that our combination method of analysis is suitable and fast enough for our data stream and also accurate for analyzing sentiment.",2022,2180
Applications of Blockchain Technology in marketing—A systematic review of marketing technology companies,Valerio Stallone and Martin Wetzels and Michael Klaas,"Given the emerging nature of integrating Blockchain Technology (BCT) into several business fields concerning the interaction between companies and their customers, this study aims to investigate the applications of BCT in marketing through an accurate procedure of locating, selecting and analyzing existing companies using BCT in marketing. A sample that consists of 800 companies was identified using web-scraping methods. The data set was collected from initial coin offerings (ICO) websites as well as from an existing, older landscape of applications. The data set was then intensively analyzed in order to be categorized into five fields of marketing technology. Advertising and ecommerce outgrew the other fields of social & relationship, content & experience and data in absolute numbers, revealing the focus of practitioners in the past as well as gaps for the future. The authors provided future directions for researchers on and development of tools to systematically generate knowledge and improve the application of BCT and the work of practitioners in marketing.",2021,2181
Fortifying home IoT security: A framework for comprehensive examination of vulnerabilities and intrusion detection strategies for smart cities,Akashdeep Bhardwaj and Salil Bharany and Anas W. Abulfaraj and Ashraf {Osman Ibrahim} and Wamda Nagmeldin,"Smart home devices have brought in a disruptive, revolutionary Internet-based ecosystem that enhanced our daily lives but has pushed private data from inside our homes to external public sources. Threats and attacks mounted against IoT deployments have only increased in recent times. There have been several proposals to secure home automation environments, but there is no full protection against Cybersecurity threats for our home IoT platforms. This research investigates attack attempts on smart home environments, focusing on firmware, brute force, and DoS attacks on the Internet of Things (IoT) network which were successful in bringing down the device in less than a minute. Weak passwords were cracked using Brute Force techniques related to HTTP, SSH, Telnet, and FTP protocols, and an unknown service port to reveal backdoor access. Cross-site scripting vulnerability was detected on IoT devices that could allow running malicious scripts on the devices. The authors also exploited the unknown services to reveal backdoors and access sensitive device details and potentially exploited them to add new ports or rules to turn the IoT devices into a router to attack other devices. To detect and mitigate such attacks, the authors present an IoT-based intrusion detection and prevention system to secure smart home network devices. The authors compared the proposed framework with other similar research based on Precision, Accuracy, F-measure, and Recall. The proposed model outperforms all the other known models reporting a high of 95% for identifying malicious attack packets, while others reported 58% and 71% detection percentage.",2024,2182
BIM4EarlyLCA: An interactive visualization approach for early design support based on uncertain LCA results using open BIM,Kasimir Forth and Alexander Hollberg and André Borrmann,"To meet the European climate goals in the building sector, a holistic optimization of embodied greenhouse gas (GHG) emissions using the method of life cycle assessments (LCA) are necessary. The early design stages have high impact on the final performance of the buildings and are characterized by high uncertainty due to the lack of information and not yet taken decisions. Furthermore, most current LCA approaches based on Building Information Models (BIM) require high expertise and experience in both BIM and LCA and do not follow an intuitive visualization approach for other stakeholders and non-experts. This paper presents a novel design-decision-making approach for reducing embodied GHG emissions by interactive, model-based visualizations of uncertain LCA results. The proposed workflow is based on open BIM data formats, such as Industry Foundation Classes (IFC) and BIM Collaboration Format (BCF), and is developed for decision support for non-LCA experts in the early design stages. With the help of a user study, the prototypical implementation is tested by 103 participants with different levels of experience in BIM and LCA based on a case study. We evaluate the proposed approach regarding the support of open BIM data formats, different LCA visualization strategies, and the intuitiveness of different approaches to visualizing uncertain LCA results. The user study results show a broad acceptance and need for open BIM data formats and model-based LCA visualization but less for visualizing uncertainties, which needs further research. In conclusion, this interactive, model-based visualization approach using color coding supports non-LCA experts in the design decision-making process in early design stages.",2023,2183
Disinformed social movements: A large-scale mapping of conspiracy narratives as online harms during the COVID-19 pandemic,Philipp Darius and Michael Urquhart,"The COVID-19 pandemic caused high uncertainty regarding appropriate treatments and public policy reactions. This uncertainty provided a perfect breeding ground for spreading conspiratorial anti-science narratives based on disinformation. Disinformation on public health may alter the population’s hesitance to vaccinations, counted among the ten most severe threats to global public health by the United Nations. We understand conspiracy narratives as a combination of disinformation, misinformation, and rumour that are especially effective in drawing people to believe in post-factual claims and form disinformed social movements. Conspiracy narratives provide a pseudo-epistemic background for disinformed social movements that allow for self-identification and cognitive certainty in a rapidly changing information environment. This study monitors two established conspiracy narratives and their communities on Twitter, the anti-vaccination and anti-5G communities, before and during the first UK lockdown. The study finds that, despite content moderation efforts by Twitter, conspiracy groups were able to proliferate their networks and influence broader public discourses on Twitter, such as #Lockdown in the United Kingdom.",2021,2184
Selected bibliography on information theory applications to information science and related subject areas,Pranas Zunde,,1984,2185
Tables of thermonuclear-reaction-rate data for neutron-induced reactions on heavy nuclei,J.A. Holmes and S.E. Woosley and William A. Fowler and B.A. Zimmerman,"The results of statistical model calculations of (n,γ), (n,p), and (n,α) cross sections and reaction-rate factors are presented in tabular form for over 500 target nuclei in the range 36 ≤ Z ≤ 83 (krypton to bismuth). Included in these tables is information on (i) the reaction cross section as a function of energy for the exoergic channel in the range 0.01 ≤ E(MeV) ≤ 3.0; (ii) the thermally averaged reaction-rate factor, NA〈σv〉 and the nuclear partition function G(T) for temperatures in the range 108 ≤ T(oK) ≤ 3 × 109; (iii) analytic fits to the reaction-rate factors and partition functions as functions of temperature; and (iv) nuclear level-density parameters and formulas for their extrapolation. Two types of reaction-rate factors have been computed. One, which may be called the “laboratory rate factor,” is based on the assumption that the target nuclei occupy only their ground states. The other, which shall be termed the “stellar rate factor,” is based on the more realistic assumption that the target nuclei occupy a thermal distribution of excited states at temperature T. A brief discussion of theory and instructions for usage of the tables are included. New fitting forms for statistical-model thermonuclear reaction rates are presented and justified.",1976,2186
Data card listings,,,1982,2187
Contents of volumes,,,1967,2188
CHAPTER 5 - THE PAPERS,,"Publisher Summary
This chapter focuses on the Taylor series method for the solution of systems of ordinary differential equations, which is one of the oldest and most reliable procedures for integrating systems of differential equations. The method can be formulated in general for differential systems that are reducible to rational form. In addition, it can be programmed as a general purpose algorithm that requires no previous manipulations on the differential system, accepting only the defining system as input. Experience over a wide range of problems and required accuracies have indicated that it does not suffer from numerical instability. It is a flexible, variable-step method that produces a piecewise polynomial solution, valid throughout the entire domain of integration. In many problems, it can attain a significantly greater maximum accuracy than the Runge–Kutta–Gill method, and it proceeds with unusually large step lengths. Throughout a wide range of accuracy, the Bulrisch–Stoer method often requires five times the amount of computing time taken by the Taylor series method.",1971,2189
GERMAN MINIATURE ELECTRONIC COMPONENTS AND ASSEMBLIES,,,1965,2190
Cumulative indexes of volumes 450–505: Volume contents,,,1995,2191
Key numbers and key words,,,1975,2192
List of Languages,,,2006,2193
"883. Will no one crack this egg?: Szepsenwol, J. (1965). The carcinogenic effect of egg yolk in mice of the C57 Bl. strain. Proc. Am. Ass. Cancer Res.6, 63",,,1965,2194
Bibliography of liquid column chromatography 1971–1973 and survey of applications,,,1976,2195
"Section 2 - Directory of agents and representatives, country by country",,,1993,2196
Abstracts of Papers Submitted to the American Gastroenterological Association,,,1979,2197
Protein secondary structure determination by NMR Application with recombinant human cyclophilin,Kurt Wüthrich and Claus Spitzfaden and Klaus Memmert and Hans Widmer and Gerhard Wider,It is a unique trait of the NMR method for protein structure determination that a description of the polypeptide secondary structure can be obtained at an early stage and quite independently of the complete structure calculation. In this paper the procedures used for secondary structure determination are reviewed and placed in perspective relative to the other steps in a complete three-dimensional structure determination. As an illustration the identification of the regular secondary structure elements in human cyclophilin is described.,1991,2198
Inelastic buckling of continuous steel beam-columns,P.E. Cuk and D.F. Rogers and N.S. Trahair,"This paper describes an experimental study of the inelastic flexural-torsional buckling behaviour of restrained continuous beam-columns typical of those found in multi-storey steel frames. Nine specimens were tested (and five re-tested) under a variety of loading conditions to provide experimental data on the effects of force and moment distribution, continuity, and end restraints. The experimental data will provide benchmark results for future theoretical analyses on the inelastic buckling behaviour of continuous beam-columns. Results presented include failure load combinations, variations of in-plane and out-of-plane displacements with applied load, measurements of geometrical and material imperfections, and results of subsidiary tests to determine the full plastic moment and squash load. A preliminary assessment of present design rules for beam-columns is carried out.",1986,2199
Lac repressor and Lac operator,Benno Müller-Hill,,1976,2200
The effect of the gaseous phase on pressures in a cylindrical silo,J.R. Johanson and A.W. Jenike,When a fine powder is filled into a silo the gas trapped by the particles will take an appreciable time to escape and will cause an increase in the lateral pressure on the walls of the silo. A mathematical analysis of the problem is presented and numerical solutions are provided. The theoretical results are compared with model measurements and the implications for hopper design are discussed.,1972,2201
Struktur und 13C-NMR-spektroskopie von chlorhaltigen flechtenxanthonen,Siegfried Huneck and Gerhard Höfle,"Zusammenfassung
From a Pertusaria spec. (lichen) the following chloroxanthones have been isolated and structurally elucidated: 2,4 - dichloro - 3,6 - di - O -methylnorlichexanthone, 2,5 - dichloro - 3,6 - di -O - methyl-norlichexanthone, and 2,4,5 - trichloro - 3,6 - di - O - methylnorlichexanthone. Revised structures are given for thuringione (2,4,5 - trichloro - 3 - O - methylnorlichexanthone), arthothelin (2,4,5 - trichloronorlichexanthone), and vinetorin (5 - chloro - 3 - O - methylnorlichexanthone). The synthesis of 7 - chloronoriichexanthone is described and detailed 13C-NMR data are reported for 9 chloroxanthones. Erythrommone from Haematomma erythromma (Nyl.) Zahlbr. is identical with 2,4,5 - trichloro-3,6 - di - O - acetylnorlichexanthone.",1978,2202
Caracteristiques des humoacides et des humines de deux rankers Pyreneens,C. Fallek and G. Ghiglione and R. Negre,"Resume
Sonte´tudie´s les humoacides, les humines extractibles et re´siduelles d'un ne´oranker et d'un pale´orankera`moder pre´leve´s l'una`2600 m, l'autrea`2200 m en Pyre´ne´es, pouvanteˆtre conside´re´s comme des pe´doclimax stationnels et tre`s distincts par leurs caracte`res morphologiques et physico-chimiques d'ensemble, le ne´orankera`moder acide et fulviquee´tant beaucoup plus riche en fer, aluminium, silice que le pale´orankera`moder humique et moins acide. Les fractions humiques ne renferment pas de copule lignique. Elles sont plus riches en Al, Fe, Mg dans le ne´oranker, mais ces me´taux sont plus abondants dans les humoacides que dans les humines bien que celles-ci soient plus riches en cendres totales. Elles diffe`rent quantitativement par les teneurs en me´taux, carbone, azotes divers, sucres et qualitativement par la nature des aminoacides et leuraˆge pre´cise´au14C.
The authors studied humic acids and remaining and extractable humines of a Moder on paleo and neo-rankers at two stations in the Pyrenean mountains at altitudes of 2600 and 2100 meters. Very distinct in their morphological and physio-chemical characters, they can be considered as a stational pedoclimax. The neo-rankers have an acid and fulvic moder and are more rich in iron, aluminium and silica, than the paleo-rankers having a humic and less acid moder. The humic fraction does not contain any lignin core and contains more aluminium, iron and magnesium than humines even though the latter has a higher ash ratio. Quantitatively they differ in their contents of minerals, carbon, different nitrogens and sugar compounds and qualitatively in their contents of amino-acids and in their age determined by C14.",1978,2203
Validation of a flow cytometric in vitro DNA repair (UDS) assay in rat hepatocytes,Jules R. Selden and Frank Dolbeare and James H. Clair and Judith E. Miller and Katherine McGettigan and John A. DiJohn and Gary R. Dysart and John G. DeLuca,"An in vitro flow cytometric (FCM) DNA repair assay has been developed and validated by comparison to conventional autoradiography (ARG). Both assays measure unscheduled DNA synthesis (UDS). Cultures of hepatocytes from young male Sprague-Dawley rats were exposed to a battery of 26 chemicals plus bromodeoxyuridine (BrdUrd) or 3H-thymidine (3H-dT) for 18–20 h before harvest. Selection of test chemicals was based upon both their genotoxicity classifications and carcinogenicity bioassay results in male rats. DNA repair in chemically treated cultures was detected flow cytometrically by measuring the uptake of BrdUrd in non-replicating (G1, G2, mitotic and 4C) cells. Intracellular levels of incorporated BrdUrd were visualized by immunochemical labeling with fluorescein isothiocyanate (FITC), and total cellular DNA content was simultaneously estimated by counterstaining samples with the nucleic acid intercalator, propidium iodide (PI). Information was obtained from 104 cells/sample. Since repairing cells incorporate significantly less BrdUrd per unit of time than replicating cells, low intensity BrdUrd-FITC fluorescent signals from repairing cells are readily discriminated from high intensity signals from replicating cells when displayed on linear univariate histograms. Further distinction between repairing and replicating cells was achieved by displaying the DNA contents of all cells on linear bivariate histograms. Thus, repairing cells were resolved without subjecting these cultures to agents which suppress replicative synthesis (e.g, hydroxyurea). Results from these concurrent FCM and ARG investigations include the following: (1) conclusions (DNA repair positive or negative) were in agreement, with one exception, cinnamyl anthranilate, for which cytotoxic doses produced a positive FCM response, but lack of intact hepatocytes in parallel ARG preparations prevented analysis; (2) similar sensitivities for most of the positive chemicals were reported; (3) a high correlation (85%) exists between the reported genotoxicity classification and these DNA repair results in the absence of overt cytotoxicity; (4) a poor correlation exists between these DNA repair results and hepatocarcinogenesis (only 4/11 liver carcinogens tested positive) or overall carcinogenesis in the male rat (only 9/21 carcinogens tested positive). This FCM assay provides a rapid, sensitive, safe and reliable means of identifying agents which induce DNA repair in mammalian cells.",1994,2204
The electronic relaxation of biacetyl in the vapor phase,Renie {van der Werf} and Jan Kommandeur,"The emissions of biacetyl after pulsed dye-laser excitation were studied at pressures down to 0.05 mtorr. At all energies the time-resolved fluorescence was composed of a nanosecond and a microsecond component. At “zero” pressure the long lived phosphorescence was absent while the “hot” phosphorescence has the same time characteristics as the slow fluorescence. By increasing the pressure the slow fluorescence was quenched while the milisecond phosphorescence was induced. We determined the low-pressure emission characteristics and the pressure effects as a function of excitation energy. From our data we obtained the parameters describing the intermediate type singlet-triplet coupling, the radiative and non-radiative relaxation rates from the singlet and triplet levels and the cross sections for the slow fluorescence quenching, all as a function of energy. Strong evidence is obtained for the participation of rotational states in the intra-molecular relaxation. The important difference between the situation where the singlet levels are isolated (low energy) and where the singlet level widths overlap (at higher energies) is demonstrated. In the former situation very large fluorescence quenching cross sectios were found. It is further shown that for high energies at least two effective collisions are needed to obtain a thermalized triplet; the mean energy removed per effective collision is 2200 cm−1.",1976,2205
Authors index,,,1952,2206
Early effects of epinephrine on aqueous humor dynamics in vervet monkeys (Cercopithecus ethiops),Anders Bill,"The early effects of topical administration of 2% l-epinephrine bitartrate on the dynamics of aqueous humor were studied in 15 vervet monkeys. In 10 animals the eye pressure was not interfered with; in 5 the eye pressure was stabilized at about 22 mmHg using reservoirs. [131I]Albumin was used to determine the net formation of aqueous humor in one eye; [125I]albumin was used in the other eye. The average net rate of aqueous formation in the treated eyes and that in the controls were the same 1·61 ± 0·14 μl/min. The rate of flow into the general circulation was 0·34 ± 0·10 μl/min less in the treated eyes than in the controls. The uveo-scleral drainage was enhanced in the treated eyes. The facility of outflow was 0·35 ± 0·07 μl. min−1. mmHg−1 higher in the treated eyes than in the controls. There was little effect on pseudofacility, probably due to a high arterial blood pressure caused by absorption of the drug. Epinephrine tended to raise the recipient venous pressure. It is suggested that epinephrine has a facility reducing effect that is due to relaxation of the ciliary muscle and a facility increasing effect that may be due to some change in the endothelium of the canal of Schlemm. Since epinephrine has several antagonistic effects the effect on the intraocular pressure can be expected to vary even qualitatively.",1969,2207
MIGRATION AND UNEMPLOYMENT DURATION AMONG YOUNG ADULTS,Adrian J. Bailey,"ABSTRACT
The relationship between migration and unemployment duration is examined. Standard job predictors of spell length (replacement income, labor force experience, personal characteristics and economic conditions) are included as control variables alongside measures of migration in a Weibull hazard model. The model is estimated using data from the National Longitudinal Survey of Youth. Young adults who migrated while unemployed had longer durations of unemployment than those who did not migrate. The rate at which they found jobs was also linked to how long they had been unemployed, to being laid off, being African American, to going to college, having a mortgage and of national unemployment conditions.",1994,2208
CHAPTER 5 - SHELF-LIFE OF FRUITS,DAVID C. LEWIS and TAKAYUKI SHIBAMOTO,,1986,2209
"Is esmolol cardioprotective? Tolerance of pacing tachycardia, acute afterloading and hemodilution in dogs with coronary stenosis",Donat R. Spahn and Peter E. Frasco and William D. White and L.Richard Smith and Robert L. McRae and Bruce J. Leone,"Objectives. The aim of this study was to determine whether esmolol, an ultrashort-acting beta-adrenergic antagonist, possesses cardioprotective properties unrelated to a concomitant decrease in heart rate. Background. Previous studies have demonstrated beneficial effects of beta-adrenergic blocking agents with unchanged heart rates. Methods. The effect of esmolol (100 μg/kg per min) on the response of global cardiovascular and regional myocardial contractile function (sonomicrometry) to pacing-induced tachycardia and acute left ventricular afterloading was assessed in dogs with a critical stenosis of the left anterior descending coronary artery (LAD). These responses were observed at the baseline hemoglobin level (12.5 ± 0.3 g/100 ml) as well as after hemodilution-induced mild regional contractile dysfunction (7.4 ± 0.4 g/100 ml) in the area supplied by this artery (LAD area). Data were analyzed by using a repeated measures multivariate analysis of variance with complete block design treating pacing rate and afterloading, respectively, as the repeated measure. Results. Esmolol decreased the maximal first derivative of left ventricular pressure (dP/dtmax); global cardiovascular and regional myocardial contractile function were otherwise unchanged. Esmolol did not alter the response of global cardiovascular or regional myocardial function to pacing-induced tachycardia or to acute left ventricular afterloading, both at the baseline hemoglobin level as well as during mild hemodilution-induced LAD area contractile dysfunction. Conclusions. At an infusion rate of 100 μg/kg per min we were unable to demonstrate cardioprotective esmolol effects in a canine model of critical coronary stenosis with controlled heart rate and identical loading conditions.",1993,2210
Observations of physical and biological parameters at the transition between the southern and central North Sea,J.J.M. {Van Haren} and J.C.A. Joordens,"In the area between the 30 and 40 m isobaths, just north of the Netherlands, a transition from Channel water to central North Sea water is found. Observations obtained in May and June 1986 show a predominantly along-isobath directed sub-tidal current. In the vertical cross-isobath plane a quasi-permanent upwelling zone overlying the steepest bottom slope is inferred from observed cross-isobath currents. In the same area Creutzberg (1985) observed a persistent chlorophyll a (chl a) maximum. Our observations show a chl a maximum extending from the bottom towards the pycnocline over a larger area in cross-isobath direction and with larger amounts of chl a than found by Creutzberg (1985). This chl a maximum is found above a zone of large amounts of benthic particulate organic carbon. The observed chl a distributions are compared with current and density observations via an advection-diffusion equation. Only rough estimates of the terms in this equation are obtained, which indicates that a balance between vertical advection and mixing, i.e. local generation, is most probable. The chl a distribution gives no evidence for an upwelling zone.",1990,2211
Side-scan sonar and subbottom profiling in the northern Adriatic Sea,Robert S Newton and Antonio Stefanon,"Side-scan sonar and subbottom profiling, coupled with bottom control through diving and sampling, have revealed new seafloor features in the northern Adriatic Sea and contributed to a better understanding of previously researched morphological and sedimentological features. Reef rock was found to be more significant than the previously known submerged beachrock exposures. Calcareous algae, madreporaria, bryozoa and serpulidae are the dominant reef builders. Single exposures of organic rock protruding above the seafloor range in size from a few cubic decimeters to several hundred thousand cubic meters. Such reefs have been tentatively divided into three types: (a) reef “sensu strictu” where organisms have built the entire feature; (b) cap reef, where the reef rock represents only a hard cap over a sedimentary core; and (c) coating reef, where the organic rock represents a thin coating over other rock types (mainly beachrock). Subbottom reflections both under and within features which at the surface appear to be reefs, confirm the existence of (b). Large beachrock exposures have been found between Lignano and Grado. In at least one case, beachrock seems to have supported reef growth. Other calcareous sandstones have been found in the area, but their thickness coupled with the absence of sedimentary structures and shell remains make it questionable whether the rock should be regarded as beachrock. A discontinuous, dense mattress of dead Posidonia roots has been found covering a large area (ca. 5 by 30 km) northeast of Venice in water depths ranging from 15 to 23 m. Such Posidonia mats are usually covered by a few centimeters of sand and have been cut and eroded by wave-induced turbulence to form “terraces” and a generally irregular bottom morphology. Mattresses of Posidonia roots form a protective cover along the crests of sandwaves south of Caorle in 20 m of water and protect them from wave erosion. Periodic lineations of very coarse sediment spaced 15 to 20 m apart on an otherwise fine, sandy bottom, have been found 20 miles east of Venice in 29 m of water. These are the result of sediment sorting due to exceptional storm waves coming from a southeasterly direction.",1982,2212
Some experiments on grafting frozen corneal tissue in rabbits,F.O. Mueller and Audrey U. Smith,"A technique is described for homografting the full thickness of the cornea in rabbits. It gave excellent results in all experiments using fresh, untreated corneal tissue. Either 7·5% or 15% dimethyl sulphoxide in serum was injected into the anterior chambers of donor eyes. Some of the eyes were suspended in serum containing 7·5% dimethyl sulphoxide, others in serum containing 10% glycerol. They were either kept at 20°C for 1–2 hr, or cooled to −79°C. Grafts of cornea from the unfrozen control eyes gave good results in 27 out of 30 rabbits. Twenty-nine clear grafts were obtained from 41 frozen eyes in which the corneal epithelium had been in contact with glycerol and the endothelium in contact with dimethyl sulphoxide. Twelve of these clear grafts were from 19 eyes kept for 1–2 hr at −79°C. The other 17 clear grafts were from 22 eyes kept for several days, weeks or months at −79°C.",1963,2213
"Time-budgeting and foraging strategy of the stoplight parrotfish Sparisoma viride Bonnaterre, in Jamaica",Fred Hanley,"Quantitative data on the ways in which the different phases of the stoplight parrotfish (Sparisomaviride Bonnaterre) distribute their time among various activities in different habitats are presented. Individuals spent from 84–97% of their diurnal time swimming, feeding, and hovering. Additionally, large adults spent a significant amount of time sheltering among crevices. Phase-related differences in these activities are statistically significant, as are differences in duration and rates of change of the activities. Large individuals spent more time swimming, while small individuals spent more time hovering. In addition, large individuals performed longer bouts of activity and switched activities less frequently than small individuals. Adult males and females spent approximately equal proportions of time in each of the activity states. Stochastic analyses of behavioural sequences show second order Markov chain dependencies, suggesting that preceding activity states affect subsequent behaviour. Possible relationships between behavioural sequencing and the species foraging strategy are discussed, and it is suggested that the sequence of behavioural activities can provide an estimation of the distribution of food resources in the environment.",1984,2214
Poster Presentations***Presenter,,,2018,2215
Biochemical Events Related to Phagocytosing Cells,Michèle Markert and J. Frei,"Publisher Summary
This chapter discusses the biochemical events related to phagocytosing cells. Phagocytosing cells include the polymorphonuclear leukocytes or neutrophils, the mononuclear leukocytes that can give rise to macrophages in tissues, and the eosinophils. Neutrophils are the most abundant phagocytes that ensure the early tissue response to infection. A decrease or increase in a number of leukocyte enzyme activities can be demonstrated in pathological situations, but the specificity of such changes and the way in which enzymes are brought about are not elucidated. The metabolic pathways are studied as multienzyme systems or by characterizing their individual enzymes. The enzymatic equipment of the polymorphonuclear cell is complete and the main metabolic pathways are present in white cells. Biochemical disorders in leukocytes may present a picture of metabolic impairment of the white cells themselves or that of general pathological manifestations in the human organism (Fl). Therefore, leukocytes have become a valuable tool for the study or even the diagnosis of inborn errors and acquired deficiencies and disorders of ubiquitous enzymes and metabolic systems because it can be isolated and purified with relative ease.",1981,2216
Chapter 4 - The Concept of a Solute Pump,A. KLEINZELLER,,1995,2217
Ion transport in damaged lenses and by isolated lens epithelium,M.V. Riley,"Uptake and efflux of 86Rb by lenses subjected to damage of varying degree have been investigated. Incisions were made through the capsule, anteriorly or posteriorly, with a pointed scalpel. Small incisions did not disturb either uptake or efflux when compared with intact lenses. More severe damage led to impaired efficiency in accumulation and intereased efflux, but additional changes could be superimposed in the presence of ouabain. An isolated preparation of anterior capsule plus epithelium was used in an attempt to differentiate between the ion transporting capabilities of the epithelium and of the fibres. 86Rb could not be concentrated across this membrane and it was concluded that it was excessively leaky, due to the isolation procedure, although ion movements were susceptible to ouabain. Evidence is presented that the fibres are capable of controlling, by means of a sodium pump, their own internal ionic content, but only to the extent that they are protected from the large ionic differential presented in the aqueous humour by the considerably greater pumping capacity of the epithelium.",1970,2218
Mechanics' register,,,1837,2219
The development of a knowledge-based system supporting the diagnosis of reading and spelling problems,Edward {van Aarle} and John {van den Bercken},"In this paper we argue that the quality of decision making in psychoeducational practice is likely to profit considerably from directives derived from a so-called normative diagnostic-prognostic framework, and that a computerized decision-support system is a promising tool for facilitation of the actual use of such directives. Accordingly, we describe the design and function of a knowledge-based system that is intended to support the initial stages of decision making in diagnosis of reading and spelling problems: (a) specifying the child's task performance in concrete behavioral terms, (b) identifying behavioral syndromes in these learning behaviors, and (c) generating candidate explanations, or diagnostic hypotheses, for these behaviors and syndromes. The systematic and formal foundation of the description of learning-behavior problems required for accomplishing these stages is provided by the facet definition approach. We present a mapping sentence that serves to define reading and spelling problems and that at the same time is used as a vehicle for communication between user and system. In order to show the latter function of the facet definition in the user interface of the system, a prototypical implementation of the system is described. Finally, some problems in constructing the required knowledge bases are discussed.",1992,2220
"Ultrastructural identities of Mastigamoeba punctachora, Mastigamoeba simplex and Mastigella commutans and assessment of hypotheses of relatedness of the pelobionts (Protista)",Giselle Walker and Alastair G.B. Simpson and Virginia Edgcomb and Mitchell L. Sogin and David J. Patterson,"The ultrastructural appearances of Mastigamoeba punctachora, Mastigamoeba simplex and Mastigella commutansare described. All three species have electron-dense membrane-bounded bodies, suggestive of mitochondrial homologues. All species have a single basal body giving rise to conical arrays of microtubules and to a single ribbon-like microtubular root. The proximal portion of the root is associated with a sheet of dense material. All species have ‘9+2’ flagellar axonemes, with basal bodies composed of triplets of microtubules, but axonemal outer dynein arms appear to be absent. All have a cylinder at the base of the transition zone. The transition zone of Mastigamoeba punctachora is elongate and also contains a column of electron-dense material. Ultrastructural data are compiled and analysed to assess two alternative views that the pelobionts are a primitive group of eukaryotes and the source of the other eukaryotes, or that they are related to eumycetozoan slime moulds.",2001,2221
Denitrification: its importance in a river draining an intensively cropped watershed,A.R. Hill,"Nitrogen transport was studied during summer low flows in a 20-km reach of the Nottawasaga River which drains an intensively cropped sand plain which has an underlying shallow water-table aquifer. Nitrogen inputs to the river were measured on days in May to October of 1977-81. These data indicated that about 38% of the daily nitrate input entered the river through ground water. The magnitude of this input is a consequence of widespread contamination of the shallow aquifer by nitrogen fertilizer. Ground water entering the river from springs and seeps near fertilized fields frequently contained more than 10 mg 1−1 of NO3-N. Mass balance studies of nitrogen transport in the river revealed an average daily nitraof 46 ± 23 kg N. This rate of nitrate removal represented about 40% of the ground water input to the river from the sand plain. Analysis of a mass balance for total Kjeldahl nitrogen revealed an essentially balanced budget, whereas chloride showed a small daily gain of about 5%. Laboratory experiments involving the incubation of stream sediment cores and the use of the acetylene block technique suggested that the bulk of the nitrate loss during river transport was caused by denitrification in bottom sediments.",1983,2222
Co-existence of non-histone messenger RNA species lacking and containing polyadenylic acid in sea urchin embryos,Martin Nemer and Melissa Graham and Lewis M. Dubroff,"The non-histone messenger RNAs of sea urchin embryos have been separated on oligo(T)-cellulose into fractions containing poly(A) and those entirely lacking poly(A). Proof of the existence of the class of mRNA lacking poly(A) is afforded by the following demonstrations: (1) the pulse-labeled RNA analyzed is bound to polyribosomes and has no non-polysomal contamination, (ii) The methods of extraction, as tested by mixing experiments between cytoplasmic extracts of sea urchin embryo and mammalian tissue culture cells, preclude partial degradation of mRNA containing poly(A) to yield artifactual species lacking poly(A), (iii) The non-histone mRNA lacking poly(A) has a mean sedimentation coefficient of 22 S, as measured in a denaturing solvent. It is shown not to consist of molecular aggregates of the putative histone 9 S mRNA, and its base composition differs markedly from those of both 9 S and ribosomal RNA, but resembles closely that of poly(A)-containing mRNA. (iv) Although the non-histone mRNAs lacking and containing poly(A) have similar base compositions and sizes (approximately 22 S), they differ widely in their nucleotide sequences. Complementary DNA, prepared with reverse transcriptase instructed by poly(A)-containing mRNA, hybridized to a negligible extent with RNA lacking poly(A).",1974,2223
Accounting for global dispersion of current accounts,Yongsung Chang and Sun-Bin Kim and Jaewoo Lee,"We develop a multi-country quantitative model of the global distribution of current account and external balances. Countries accumulate domestic capital and foreign assets to smooth consumption over time against exogenous productivity shocks in the presence of liquidity constraints. In equilibrium, optimal consumption and investment responses to persistent productivity shocks imply a degree of intertemporal substitution across countries that can explain up to one-third of the current account dispersion in the data.",2013,2224
"Advanced digital forensics and anti-digital forensics for IoT systems: Techniques, limitations and recommendations",Jean-Paul A. Yaacoub and Hassan N. Noura and Ola Salman and Ali Chehab,"Recently, the number of cyber attacks against IoT domains has increased tremendously. This resulted into both human and financial losses at all IoT levels especially individual and organization levels. Recently, cyber-criminals have kept on leveraging new skills and capabilities by conducting anti-forensics activities and employing techniques and tools to cover their tracks to evade any possible detection of the attack’s events, which has targeted either the IoT system or/and its component(s). Consequently, IoT cyber-attacks are becoming more efficient and more sophisticated with higher risks and threat levels based on their more frequent likelihood to occur and their impact. However, traditional security and forensics solutions are no longer enough to prevent nor investigate such cyber attacks, especially in terms of acquiring evidence for attack investigation. Hence, the need for well-defined, sophisticated, and advanced forensics investigation techniques is highly required to prevent anti-forensics techniques and track down cyber criminals. This paper reviews the different forensics and anti-forensics methods that can be applied in the IoT domain including tools, techniques, types, and challenges, while also discussing the rise of the anti-anti-forensics as a new forensics protection mechanism against anti-forensics activities. This would help forensics investigators to better understand the different anti-forensics tools, methods and techniques that cyber criminals employ while launching their attacks. Moreover, the limitations of the current forensics techniques are discussed, especially in terms of issues and challenges. Finally, this paper presents a holistic view from a literature point of view over the forensics domain in general and for IoT in particular.",2022,2225
Chapter 5 - Electromechanical Devices,Edward P. Furlani,"Publisher Summary
This chapter discusses the theory of electromechanical devices, as well as various practical applications. Electromechanical devices convert electrical energy to mechanical energy. Electromechanical devices consist of three subsystems: the electrical drive circuitry, an electromechanical coupling subsystem, and a mechanical subsystem. Electromechanical devices are governed by a coupled system of electrical and mechanical equations. The electrical equations follow from quasi-static field theory, and the mechanical equations follow from Newton's laws. Furthermore, the chapter derives equations of motion for electromechanical devices that execute either linear or rotational motion. These equations are applied to the analysis of various practical devices including magnetic circuit actuators, linear, rotational and resonant actuators, axial-field motors, and stepper motors. The focus is on magnetically linear, singly excited (single pair of electrical terminals), electromechanical devices with a single degree of mechanical freedom.",2001,2226
The cyanobacteriales: A legitimate order based on the type strain Cyanobacterium stanieri?,R. Rippka and G. Cohen-Bazire,"Summary
As a logical consequence of the definition of a bacterium (Stanier and van Niel, 1962), R. Y. Stanier created the name «cyanobacteria as a replacement for «blue-green algae. As such, cyanobacteria entered the 8th issue of Bergey's Manual of Determinative Bacteriology 1974 as members of the Procaryotae Murray 1968, this kingdom being composed of two divisions, Cyanobacteria and Bacteria. An even tighter integration of cyanobacteria with other bacteria was proposed by Gibbons and Murray (1978) for the next edition of Bergey's Manual. These authors suggested that the cyanobacteria be integrated as an order Cyanobacteriales in the class Photobacteria. However, this proposal was doomed to failure by constrants imposed under present rules of the Bacteriological Code (Lapage et al., 1976), one of which is that the type of an order is the genus upon whose name the higher taxon is based. A genus Cyanobacterium did not exist when Gibbons and Murray made their proposal, and a subsequent special request by the same authors for an exemption from this rule was not granted (Judicial Commission of the International Committee on Systematic Bacteriology, Holt, 1978). We present here a revised classification for unicellular cyanobacteria dividing in one plane wherein we propose, among other changes, the creation of two new genera, Cyanobium and Cyanobaceterium. With the creation of the latter genus, the requirement for recognition of cyanobacteria as a legal order Cyanobacteriales under the Bacteriological Code should be fulfilled. We suggest that the type species of this genus be Cyanobacterium stanieri, in honor of the late Roger Y. Stanier.
Résumé
La définition d'une «bactérie proposée par Stanier et van Niel en 1962 [48] a eu pour conséquence logique la création, par R. Y. Stanier, du nom «cyanobactéries pour remplacer celui d' «algues bleues. C'est sous cette dénomination que ces organismes entrèrent dans la troisième édition du manuel de détermination bactériologique de Bergey en 1974 [4]. Dans ce manuel, les cyanobactéries étaient traitées comme membres des Procaryotae Murray 1968, cet embranchement étant constitué par deux groupes: les Cyanobactéries et les Bactéries. Un rapproachement encore plus étroit des cyanobactéries des autres bactéries a été proposé par Gibbons et Murray [16] pour la prochaine édition du manuel de Bergey. Ces auteurs ont proposé que les cyanobactéries soient intégrées dans la classe des Photobactéries, en tant qu'ordre des Cyanobacteriales. Cette proposition fut refusée comme ne satisfaisant pas les règles du Code Bactériologigue [28], l'une de ces règles étant que le type d'un ordre est le genre sous le nom duquel le taxon d'ordre plus élevé est basé. Le genre Cyanobacterium n'existait pas lorsque Gibbons et Murray ont fait leur proposition, et une demande ultérieure d'exception à cette règle [17] fut également refusée. Nous proposons ici une nouvelle classification des cyanobactéries unicellulaires qui se divisent sur un seul plan. Entre autres changements, nous proposons la création de deux nouveaux genres: Cyanobium et Cyanobacterium. La création de ce dernier genre devrait lever l'objection qui empêchait de reconnaître les Cyanobacteriales comme ordre légitime suivant les règles du Code Bactériologique. Nous proposons que l'espèce type de ce genre soit appelée Cyanobacterium stanieri pour honorer la mémoire de Roger Y. Stanier.",1983,2227
Stimulation by estrogens of ornithine and S-adenosylmethionine decarboxylases in the immature rat uterus,Alvin M. Kaye and Isaac Icekson and H.R. Lindner,,1971,2228
The ecology of a land drainage channel—I. Oxygen balance,E.J.P. Marshall,"In shallow land drainage channels day-time solar heating during the summer produced vertical temperature and density gradients. These facilitated the development of marked gradients of dissolved oxygen, with maximum sub-surface values exceeding 300% air saturation and deoxygenated water near the sediments. Night-time cooling promoted mixing of the water column. Rates of community photosynthesis and respiration, calculated from dissolved oxygen distributions by two methods, were high.",1981,2229
Мотив несостоявшегося счастья у Достоевского и Островского: (Об одной возможной перекличке),В.Н. Топоров,,1986,2230
"The differential action of metronidazole on nitrogen fixation, hydrogen metabolism, photosynthesis and respiration in Anabaena and Scenedesmus",Richard M. Tetley and Norman I. Bishop,"Metronidazole (2-methyl-5-nitroimidazole-1-ethanol) at 1–2 mM levels has been shown to be a selective inhibitor of nitrogenase activity in Anabaena. Two constitutive hydrogenases and photosynthesis are insensitive to metronidazole at these same concentrations. At higher concentrations metronidazole inhibits photosynthesis in Anabaena while photoreduction and to a lesser extent photohydrogen production are retarded in Scenedesmus. Respiration is slightly stimulated at high metronidazole levels in both algae. The reductant source for nitrogenase in Anabaena and photohydrogen production and photoreduction electron transport in Scenedesmus are discussed. Due to the activity of metronidazole as a selective inhibitor of ferredoxin-associated processes, it should prove to be useful in N2 fixation studies and in distinguishing between ferredoxin-linked reactions of different sensitivities and other activities not associated with low reduction potential components.",1979,2231
Rare earth element geochemistry of South Atlantic deep sea sediments: Ce anomaly change at ~54 My,Y.L. Wang and Y.-G. Liu and R.A. Schmitt,"The geochemistry of the REE (rare earth elements) in oceanic sediments is discussed, based mainly on samples from DSDP Holes 530A and 530B, Leg 75, and Hole 525A, Leg 74. The proposed mechanisms for incorporation of the REE into the marine carbonate phases are adsorption, chiefly onto the carbonate minerals and on Sc, Hf, and Ta-rich Fe-Mn hydroxide flocs as carbonate coatings. The Ce anomaly of marine carbonate was used as an indicator of paleo-ocean water redox conditions: the bottom water of the Angola Basin was in a reducing condition in the Cretaceous. At ca. 54 My, the South Atlantic water condition became oxidizing, similar to the present seawater redox condition. This change was related to the improvement of circulation due to the widening of South Atlantic and the subsidence of water circulation barriers such as the Walvis Ridge and perhaps the Romanche Fracture Zone. The younger (Eocene-Recent) and older (Albian-Santonian) argillaceous sedimentary rocks from 530A (denoted as YSAB and OSAB respectively) show different degrees of Eu depletion with a transition period in between. The REE patterns of OSAB suggest a basaltic origin. The possible sources are Kaoko basalt in Southwest Africa or Namibia and the basaltic Walvis Ridge itself. The decrease in the area covered by Kaoko basalt due to erosion, the subsidence of the Walvis Ridge, and the improvement of water circulation led to changes in the Eu anomaly from Campanian to Paleocene, and resulted in the YSAB REE pattern. Changes in the Sm/Eu, La/Th, Th/Yb, Ti/Al2O3, FeO/Al2O3, and Hf/Al2O3 ratios suggest changes of average source rock composition from and esite to granodiorite. The REE abundances and patterns of younger sediments in the Angola Basin (YSAB) are very similar to those observed in NASC, PAAS, and ES sediments. The YSAB REE abundances and patterns may represent the average REE distribution of the exposed African continental crust. The strong resemblance of REE distributions of YSAB, NASC, PAAS and ES suggests thorough REE mixing from different sources and the uniformity of the average crustal compositions of different continents: Africa, North America, Australia, and Europe",1986,2232
Les microangiopathies thrombotiques de la grossesse,,,2016,2233
The order structure of topological -algebras of unbounded operators I,Konrad Schmüdgen,"In this paper we begin to study the order structure of topological -algebras of unbounded operators in Hilbert space with the investigation of the normality and the bounded decomposition property of the cones. We prove that for a large class of topological -algebras the normality of the wedge of positive elements is necessary and sufficient for a topological -algebra to be algebraically and topologically isomorphic to a -algebra of unbounded operators equipped with the uniform topology. From this theorem we obtain some corollaries, so for instance, well-known results of Lassner, Brooks and Grothendieck.",1975,2234
Improved Results of Vascular Reconstruction in Pediatric and Young Adult Patients with Renovascular Hypertension,Arturo Martinez and Andrew C. Novick and Robert Cunningham and Marlene Goormastic,"From 1955 to 1988, 56 patients 21 years old or younger underwent surgical treatment for renovascular hypertension at our clinic. The cause of renal artery disease was fibrous dysplasia in 53 patients, Takayasu’s arteritis in 2 or an arterial aneurysm in 1. Bilateral or branch renal artery disease, and extrarenal arterial disease were present in 16, 23 and 11 patients, respectively. The results of 28 patients treated from 1955 to 1977 (group 1) were compared to those of 28 patients treated from 1978 to 1988 (group 2). Hypertension was cured or improved postoperatively in 83% of the patients from group 1 and in 96% from group 2 (p = 0.07). However, this outcome was achieved through surgical revascularization in only 48% of the patients from group 1 compared to 96% from group 2 (p = 0.0002). A multivariate analysis revealed that the only significant variable related to clinical outcome was the era of treatment, which reflects the improved technical efficacy of revascularization during the last decade. Aortorenal bypass and renal autotransplantation have emerged as the preferred revascularization operations. It currently is possible to achieve amelioration of hypertension and preservation of renal function in most young patients with renal artery disease.",1990,2235
Childhood and childrearing in ad status sermons by later thirteenth century friars,Jenny Swanson,"It is difficult to obtain a balanced and accurate picture of medieval views of such topics as childhood, treatment of children, and the nature of family ties, whether of affection or obligation. A significant source of information on these topics, abundant but so far underused, lies in the sermons, pastoral handbooks and biblical commentaries of the period. These are abundant for the thirteenth to fifteenth centuries, allowing the historian to examine the development of ideas over time. One group of late thirteenth century ad status collections, written by friars, is particularly interesting. They were extremely popular throughout the fourteenth century, and therefore represent an important starting point for any study of developing views about the young. Comparison of their views with those of their predecessors, identifies a clear trend towards increased awareness of children as a group with specific characteristics and specific needs. Overall, the writers of these late thirteenth century ad status collections - John of Wales, Guibert de Tournai, Humbert de Romans - show decided reservations about the value of corporal punishment, and a conviction that children are intrinsically good, despite the sins characteristic of their various stages of development. This must call into question some of the conclusions reached by scholars such as Philippe Ariès and Lloyd de Mause.",1990,2236
Dry column chromatography of phospholipids,Robert C. MacDonald and Steven P. Rempas,"Separation of common phospholipids can be affected by dry column chromatography on silica gel. The method involves packing the column with dry gel and developing it in solvent mixtures used for thin-layer chromatography of the same lipids. Solvent is allowed to migrate only to the end of the column; access to the bands of separated material is obtained by using columns with a removable glass front. RF values of lipids on development columns and those on thin-layer plates are nearly identical when the column is packed with thin-layer chromatography gel. Such columns, however, develop very slowly. Columns packed with fine silica gel designed for elution column chromatography develop very rapidly and yield separations that are still quite comparable to those obtainable from thin-layer plates. Such columns are convenient for the purification of phospholipids in amounts of 10 mg to about 10 g. Column design and construction are described in detail.",1977,2237
Practical views on the proposed improvement of the Ohio River,W.Milnor Roberts,,1857,2238
Recently published papers,,,1972,2239
"Possible humidity receptor mechanisms in the clover mite, Bryobia praetiosa Koch",P.W. Winston,"Adult mites from a wild population were exposed to humidity gradients in small tubes at 20, 25, and 30°C. The humidity response was found to be almost entirely due to the steepness of the gradient in evaporation rate and essentially the same in all parts of the range. There is evidence of a negative influence of the higher humidity, but this is definitely of minor importance. The receptor is considered to be an evaporating organ and the possible mechanisms for this type are discussed: osmoreceptors, mechanoreceptors, and thermoreceptors. The similarity between the responses of the mites and of thermoreceptors to rate of change of the stimulus as well as to a constant stimulus makes close comparison possible. It is shown that an evaporating receptor of a size small enough for this mite would be cooled sufficiently to stimulate thermoreceptors and that the rate of water loss would not be too great.",1963,2240
EmoCNN: Unleashing Human Emotions with Customized CNN Using Different Optimizers,Sahana M and Praneetha Umesh and Ashwini Kodipalli and Trupthi Rao,"A key development towards enhancing computer-human interaction is emotion recognition. This publication describes a technique called EmoCNN, which uses deep learning techniques to precisely identify and classify human emotions, emphasizing improving model performance using different optimizers. Our research intends to contribute to the creation of more effective systems that improve computer-human interaction by solving the problems associated with emotion recognition. By bridging the gap between humans and robots, accurate emotion detection enables systems to perceive emotions for customized and responsive interactions. AI-powered assistants, chatbots, and social robots all benefit from emotion recognition by providing more responsive, empathic and interesting user experiences. Emotion-aware technologies can also enhance user feedback analysis, human-centered design, and monitoring of mental health. Using a human emotion detection dataset, we carried out comprehensive experiments focusing on the happy, sad, and neutral emotion classes. Constructing a customized EmoCNN model with convolutional layers, a hidden layer, ReLU activation, and max-pooling was the focus of our computational work. We investigated various optimizers and evaluated how they affected accuracy, convergence speed and loss minimization. The results demonstrated that the EmoCNN model, which had been trained using the Adam optimizer, gave the best accuracy in distinguishing between emotions. Our paper provides a comparative analysis, highlighting the superiority of EmoCNN over existing models, showcasing its ability to achieve higher validation accuracy (89%) and more efficient emotion recognition when compared to previous approaches with minimal loss. Our research advances the field of emotional computing by demonstrating how well EmoCNN can identify and categorizes various human emotions. This discovery has significant ramifications for the creation of emotion-aware computers, which can better understand and react to human emotions, enhancing computer-human interaction.",2024,2241
R-MATRIX METHOD IN ATOMIC PHYSICS,P.G. BURKE,,1987,2242
Evaluation of a distributed parameter ecohydrological model (TOPOG_IRM) on a small cropping rotation catchment,Warrick R. Dawes and Lu Zhang and Tom J. Hatton and Peter H. Reece and G.T.H. Beale and I. Packer,"A biophysically based distributed parameter ecohydrological catchment model, TOPOG_IRM, is described which predicts the dynamic interactions between soil-vegetation-atmosphere systems over a catchment. The physiological control on transpiration is formulated using canopy resistance defined as a function of the net assimilation rate, the relative humidity and CO2 concentration at the leaf surface. Rainfall infiltration, runoff and redistribution are simulated with the Richards equation and evapotranspiration is calculated using the Penman-Monteith equation. Two innovative features of the model are (1) coupling of the vegetation-atmosphere system by changing the value of the vapour pressure deficit of air in the canopy, and (2) the plant carbon balance, which allows the simulation of plant growth using a saturation rate kinetics formulation. The model was validated using evapotranspiration, soil moisture and leaf area index measurements from Wagga Wagga, N.S.W., Australia, for a period of 1992–1993. The calculated evapotranspiration was in good agreement with the observations. The soil moisture content at various depths was well simulated for two typical sites. The model also reasonably reproduced leaf area index of wheat and canola for two growing seasons. The success of the model simulations was due to the reasonably realistic treatment of the soil and canopy processes. The sensitivity analysis indicated that (1) the maximum assimilation rate of carbon affects canopy transpiration significantly, and (2) the total drainage is sensitive to the lower boundary conditions and the saturated hydraulic conductivity. TOPOG_IRM is a valuable tool in studying catchment responses under different land management practices. However, the application of the model is limited by the large amount of data required regarding soil and vegetation properties, and their spatial distribution.",1997,2243
SYMBOLS AND ABBREVIATIONS,G.R. DARBY,,1973,2244
Immunological studies on globular and elongated forms of electric eel acetylcholinesterase effects of hydrolytic enzymes,François Rieger and Philippe Benda and Annie Bauman and Jean Rossier,,1973,2245
2 - Advanced materials for architecture,Marco Casini,"Thanks to the huge progress in the field of materials science, technology solutions available today for buildings, such as advanced materials, nanomaterials, and smart materials, allow designers to reconcile the architectural features of buildings with the new challenges of energy and environmental efficiency. After a review of future trends in advanced materials for architecture, an in-depth analysis on nanotechnology and its application in the energy, environmental, and construction sectors is provided, focusing on innovative nanoproducts for architecture. A presentation of the new class of highly innovative materials, so-called smart materials, is given, addressing both property-changing materials and energy-exchanging materials, illustrating their properties and their application in the building sector. Finally, the chapter gives an overview on the enormous potential of three-dimensional printing technology for architecture, with a particular focus on the realization of building components, structural elements, and entire buildings.",2016,2246
Influence of a hot and humid environment on the patient with coronary heart disease,G.E. Burch,"A hot and humid environment can increase cardiac work as much as strenuous physical exericse. The environment should, therefore, be made comofortable not only for the patient with heart disease but also for aged people and for patients with debilitating states or any illness in which thermal regulation should be facilitated. There is definite need to consider the hot and humid environment produced by climate, industrial working conditions, or overrcrowding of the sickroom in the treatment of all patients in whom it is discred to reduce cardiac work. Greater use of air conditioning in hospitals, cardiac wards, offices, and homes will assist toward this end. Many important problems concering the influences of the hot and humid environment upon man remain unsolved and should be studied.",1956,2247
Molecular assembly of the phycobilisomes from the cyanobacterium Mastigocladus laminosus,Wolfgang Reuter and Claudia Nickel-Reuter,"Five phycocyanin- and two phycoerythrocyanin-associated linker polypeptides were resolved by two-dimensional gel electrophoresis from isolated phycobilisomes of the thermophilic cyanobacterium Mastigocladus laminosus. These linker polypeptides were located in the substructure of the phycobilisome by analysis of isolated high molecular weight allophycocyanin—phycocyanin (AP—PC) complexes and by examination of different phycobilisome “types” induced by alteration of the culture conditions. The core fractions reveal two different rod—core linker polypeptides LRC29.5PC and LRC32PC and one rod linker LR31PC associating the first and the following “trimeric” PC complexes at the profile sides of the core. The “minimal” phycobilisome type with an apparent molecular weight Mr of 4 500 000 is characterized by the occurrence of only one additional rod linker polypeptide LR34.5PC. Further sequential elongation of the rods occurs by PC and phycoerythrocyanin (PEC) “trimers” in cooperation with two low molecular weight linker polypeptides LR11PC and LR11PEC. The structure of the “maximal” phycobilisome with Mr 8 500 000 is completed by the association of PEC “hexamers” via LR34.5PEC. This sequential assembly could be demonstrated by the polypeptide composition of different “intermediate” and “maximal” phycobilisomes. The large number of linker polypeptides and the stoichiometric calculations of core fractions and of different phycobilisome “types” indicate a partial “trimeric” organization of the rod complexes. With respect to polypeptide stoichiometry, the “trimeric” arrangement of PC·LRC29.5/32, PC·LR31, PC·LR11 and PEC·LR11 in the structure of the rods is proposed. The structural and functional consequences of the study are represented in a model of the “maximal” phycobilisome.",1993,2248
Biogenic production and consumption of dimethylsulfide (DMS) and dimethylsulfoniopropionate (DMSP) in the marine epipelagic zone: a review,T. Groene,Dimethylsulfide (DMS) is the dominating sulfur gas in surface marine waters. The flux of DMS to the atmosphere plays an important role in the natural sulfur cycle and in the formation of acidic components and condensation nucleii in the remote oceanic atmosphere and is likely to be of climatic significance. This article reviews the biologic production and consumption processes of DMS and its precursor dimethylsulfoniopropionate (DMSP) in ocean surface waters. The description of relevant processes in this paper is complemented by rate estimates where such data are available. The literature on a region in the Northeast Atlantic Ocean is reviewed to provide information on DMS and DMSP pools in situ and their regional and seasonal variations.,1995,2249
Evolutions et tendances en simulation de colonnes de rectification discontinue,Joël Albet and Jean-Marc {Le Lann} and Xavier Joulia and Bernard Koehret,"This paper is devoted to the development of a general methodology for application to the simulation of multicomponent batch distillation with or without chemical reactions. After a survey of the field of batch distillation and dynamic simulation applied to batch distillation, evolutions and trends are reviewed through the presentation of the proposed methodology. This methodology is based on the formulation and development of a rigorous model in comparison with those commonly used in batch distillation. The resulting set of differential-algebraic equations (DAE) is solved using an efficient numerical strategy based on Gear's method, which is especially adapted to deal with DAE systems and numerical features arising in the batch context. Numerical difficulties due to detection of time and state events, the occurrence of discontinuities and start-up policy schemes (involving chemical reactions or not) are emphasized. Original and interesting solutions are proposed to tackle each of these problems. Finally, the flexibility, reliability and efficiency of the proposed methodology are illustrated through academic and industrial case studies, the results of which are compared with plant data.",1994,2250
SnO2 sensors: current status and future prospects,Wolfgang Göpel and Klaus Dieter Schierbaum,"A survey is given on the current status and future prospects in research and development of SnO2-based sensors. Atomistic models of molecular recognition are discussed first. They include physisorption, chemisorption, surface reaction, catalytic reaction, grain boundary reaction, bulk reaction and three-phase boundary reaction steps. The influence of contact geometry and crystallinity on the sensor response signal is outlined. A brief summary is given of the current status of sensor research and development with emphasis on ceramic, thick-film and thin-film sensors based on crystalline, polycrystalline and nanocrystalline SnO2. Three different aspects are mentioned in the outline which are expected to lead to significantly improved performances of future sensors: the improved selectivity through the modulation frequency in a.c. measurements, the improved stability through the better control of structures, and the improved selectivity and drift compensation through pattern recognition.",1995,2251
Biogenic-silica accumulation in the Ross Sea and the importance of Antarctic continental-shelf deposits in the marine silica budget,P.A Ledford-Hoffman and D.J Demaster and C.A Nittrouer,"Thirty-five box cores were collected from the continental shelf in the Ross Sea during cruises in January and February, 1983. Pb-210 and Pu-239, 240 geochronologies coupled with biogenic-silica measurements were used to calculate accumulation rates of biogenic silica. Sediment in the southern Ross Sea accumulates at rates ranging from ≤0.6 to 2.7 mm/y, with the highest values occurring in the southwestern Ross Sea. Biogenic-silica content in surface sediments ranges from 2% (by weight) in Sulzberger Bay and the eastern Ross Sea to 41% in the southwestern Ross Sea. Biogenic-silica accumulation in the southwestern Ross Sea averages 2.7 × 10−2 g/cm2/y and is comparable to accumulation rates in high-productivity, upwelling environments from low-latitude continental margins (e.g., Gulf of California, coast of Peru). The total rate of biogenic-silica accumulation in the southern Ross Sea is approximately 0.2 × 1014 g/y, with most of the accumulation occurring in basins (500–1000 m water depth). If biogenic-silica accumulation in the southern Ross Sea continental shelf is typical of other basins on the Antarctic continental shelf, as much as 1.2 × 1014 g/y of silica could be accumulating in these deposits. Biogenic-silica accumulation on the Antarctic continental shelf may account for as much as a fourth of the dissolved silica supplied to the world ocean by rivers and hydrothermal vents.",1986,2252
Effect of noradrenalin on intraocular pressure and outflow in cynomolgus monkeys,Friedrich Hoffmann,"The effect of noradrenalin on the aqueous dynamics of anaesthetized monkey eyes was studied using the method of perfusion at constant pressure. Values obtained suggest similar changes to those found in rabbits. Noradrenalin injected in normal monkey eyes caused a decrease in intraocular pressure, while the gross facility increased. In denervated eyes the intraocular pressure stayed at the same level as in normal eyes when the same amount of drug was injected, while the gross facility was significantly different. These results suggest that there is an increase in secretion together with an increase in gross facility. Test series in which gross facilities were measured at different pressure differentials, suggested that low facilities resulted from high pressure differentials and vice versa. In those experiments where the reservoir pressure was kept constant at 16 mmHg above P0 for some 20 min, the gross facility increased by some 50%. The results suggest that the law ΔF = C. ΔP, which gives a constant value of C in a fixed system of tubes, cannot be applied to the living monkey eye.",1968,2253
"Computational intelligence intrusion detection techniques in mobile cloud computing environments: Review, taxonomy, and open research issues",Shahab Shamshirband and Mahdis Fathi and Anthony T. Chronopoulos and Antonio Montieri and Fabio Palumbo and Antonio Pescapè,"With the increasing utilization of the Internet and its provided services, an increase in cyber-attacks to exploit the information occurs. A technology to store and maintain user's information that is mostly used for its simplicity and low-cost services is cloud computing (CC). Also, a new model of computing that is noteworthy today is mobile cloud computing (MCC) that is used to reduce the limitations of mobile devices by allowing them to offload certain computations to the remote cloud. The cloud environment may consist of critical or essential information of an organization; therefore, to prevent this environment from possible attacks a security solution is needed. An intrusion detection system (IDS) is a solution to these security issues. An IDS is a hardware or software device that can examine all inside and outside network activities and recognize doubtful patterns that may demonstrate a network attack and automatically alert the network (or system) administrator. Because of the ability of an IDS to detect known/unknown (inside/outside) attacks, it is an excellent choice for securing cloud computing. Various methods are used in an intrusion detection system to recognize attacks more accurately. Unlike survey papers presented so far, this paper aims to present a comprehensive survey of intrusion detection systems that use computational intelligence (CI) methods in a (mobile) cloud environment. We firstly provide an overview of CC and MCC paradigms and service models, also reviewing security threats in these contexts. Previous literature is critically surveyed, highlighting the advantages and limitations of previous work. Then we define a taxonomy for IDS and classify CI-based techniques into single and hybrid methods. Finally, we highlight open issues and future directions for research on this topic.",2020,2254
Studies on a highly active anticoagulant fraction of high molecular weight isolated from porcine sodium heparin,E. Sache and M. Maillard and H. Bertrand and M. Maman and M. Kunz and J. Choay and J. Fareed and H. Messmore,"We have studied heparin fractionation using gel filtration and ion-exchange chromatographic methods. The starting material was commercial grade porcine mucosal sodium heparin (PSH). The fractionation was monitored employing synthetic substrates for assaying both antithrombin (with H-D-Phe-Pip-Arg-pNA ; S-2238) and anti-FXa (with Bz-Ileu-Glu-Gly-Arg-pNA ; S-2222) activities. The resulting fractions were evaluated in different amidolytic and coagulation methods used to determine heparin potency by comparison with PSH. By gel filtration of PSH on Ultrogel AcA 54, both strong anti-FXa and antithrombin activities were associated with the fractions eluted in the high molecular weight range (MW ⋍ 20 × 103). These fractions also had potent anticoagulant action when assayed by conventional clotting methods. PSH was also subjected to fractionation by an ion-exchange technique (DEAE-Sephacel) with increasing salt molarity. The patterns for antithrombin and anti-FXa activities were again closely related, if not identical. Four fractions were usually distinguished, with respectively negligible, intermediate, high and very high activities when compared to PSH. The very highly active fraction (HAF), approximately 15% by weight, was eluted at high salt molarity (> 0.8 M NaCl). On a weight basis its anticoagulant activity was ⋍ 2–3 times that of PSH as determined by amidolytic as well as clotting methods. Intravenous injection of HAF to rabbits and dogs (1.0 and 2.5 mg/kg) produced a much stronger anticoagulant response than PSH, also showing an effect which persisted for a longer duration.",1982,2255
A general isoparametric finite element program SDRC SUPERB,V.T. Nicolas and E. Citipitioglu,"SDRC SUPERB is a general purpose finite element program that performs linear static, dynamic and steady state heat conduction analyses of structures made of isotropic and/or orthotropic elastic materials having temperature dependent properties. The finite element library of SUPERB contains isoparametric plane stress, plane strain, flat plate, curved shell, solid type curved shell and solid elements in addition to conventional beam and spring elements. Linear, quadratic and cubic interpolation functions are available for all isoparametric elements. Independent parameters such as displacements and temperatures are obtained from SUPERB using the stiffness method of analysis. The remaining dependent parameters, such as stresses and strains, are evaluated at element gauss points and extrapolated to nodal locations. Averaged values are given as final output. The graphic capabilities of SUPERB consists of geometry and distorted geometry plotting, and stress, strain and temperature contouring. Contours are plotted at user defined cutting planes for solids and at top, middle or bottom surfaces for plate and shell types of structures. In the first part of this paper, the program capabilities of SUPERB are summarized. Extrapolation techniques used for determining dependent nodal parameters and for contour plotting are explained in the second part of the paper. Behavior of standard, wedge and transition type isoparametric elements and the effect of interpolation function orders on accuracy are discussed in the third part. The results of several illustrative problems are included.",1977,2256
Pollutant dispersion near roadways — Experiments and modeling,David P. Chock,"Since the 1970's, many field and wind tunnel experiments have been conducted to study pollutant dispersion from roadways. For an at-grade situation, field experiments have revealed that mechanical mixing dominates effects due to ambient stability, that plume rise is important under very low crossroad winds, that regions of large shear enhance the mixing volume, and that the wake region grows rather slowly in the vertical direction. The models that have been developed based on recent experimental results are briefly described. For the street canyon situation, both field and wind tunnel experiments have revealed that ambient stability does not play an important role, that corner vortices near an intersection cause an increase in pollutant concentrations near the bottom corners of the leeside buildings, that in the midsection of a street block the vortex circulation causes high pollutant concentrations to be advected toward and up the leeside wall. No general street canyon models are available except an empirical model for the midsection of the street block. The complicated flow field must first be ascertained before a reliable concentration model can be developed.",1982,2257
"Inhibition of adenovirus replication by 5,6-Dichloro-1-,β-d-ribofuranosylbenzimidazole",Marietta Brötz and Walter Doerfler and Igor Tamm,"The halogenated benzimidazole derivative 5,6-dichloro-1-β-d-ribofuranosylbenzimidazole (DRB) inhibits reversibly the replication of human adenovirus type 2 (Ad2) and its DNA in human KB cells. Viral DNA replication is almost completely blocked when the drug is added earlier than 4 hr postinfection in concentrations between 50 and 150 μM. Replication of viral DNA in all four size-classes (>100 S, 50–90 S, 34 S, and <20 S) is inhibited. The replication block is reversible upon withdrawal of the inhibitor. When DRB is administered at the time of maximal viral DNA replication, 16–18 hr postinfection, the inhibitor has no apparent effect on viral DNA synthesis. In the presence of 150 μM DRB, synthesis of early virus-specific RNA in the nucleus is reduced by approximately 90% and the appearance of virus-specific RNA sequences in the cytoplasm is reduced by >95%, as demonstrated by DNA-RNA filter hybridization. Thus, the block in viral DNA replication is best explained by the inhibition of the synthesis of early virus-specific RNA.",1978,2258
Microbial attachment to food and food contact surfaces,Joseph F Frank,"Publisher Summary
The ability of microorganisms to adhere to surfaces has significant implications for food science. Microorganisms attached to plant and animal tissues can affect food safety and spoilage. Microorganisms can adhere firmly and are therefore difficult to remove or inactivate without damaging the underlying tissue. This is not of concern for most processed foods, but it is for foods that are to be sold as raw or minimally processed. Disease outbreaks associated with Salmonella on chicken and fresh produce and Escherichia coli 0157:H7 in apple juice, alfalfa seed sprouts, and lettuce may be related to the inability of sanitizer and washing treatments to remove or inactivate attachedpathogens. Microbial attachment to food contact surfaces is also of significance for food safety and spoilage. Microorganisms attached to processing equipment may escape cleaning and sanitizing procedures and proceed to contaminate processed product. Pathogens originating with raw products can attach to food preparation surfaces, which, if not adequately cleaned before reuse, can serve to recontaminate cooked foods. The ability to attach to and subsequently detach from surfaces is a characteristic of all microorganisms. Attachment is advantageous and perhaps necessary for survival in the natural environment, as it allows microorganisms to exert some control over their nutritional environment, and offers protection from environmental stresses. Attachment is also the initial event in microbial infections, for if a cell fails to adhere it will be carried away from its potential host. Therefore, microbial attachment is a process that has been extensively studied.",2001,2259
Revertants of temperature-sensitive mutants of reovirus: Evidence for frequent extragenic suppression,Robert F. Ramig and Bernard N. Fields,"Twenty-eight independently isolated, spontaneous revertants isolated from temperature-sensitive (ts) mutants of reovirus type 3 representing all the known mutant groups, were examined to determine whether they were intragenic revertants or contained extragenic suppressor mutations. Analysis of the progeny of backcrosses of the revertants to wild type, showed that 25 of the 28 revertants contained is lesions. This result indicated that 25 of the 28 revertants were suppressed pseudorevertants with the suppressor mutation in a gene that could be separated from the parental is lesion by recombination. The nature of the is lesion(s) was examined for a number of the is clones derived from back-crosses. In every case, except one, the parental is lesion was found. In five of the ten suppressed pseudorevertants examined, nonparental is lesions could also be rescued. Two of the nonparental is lesions were in the previously defined recombinant groups. Five of the nonparental is lesions represented a new recombination group or groups since they recombined with the prototype mutants of all of the defined recombination groups. Recombination analysis indicated that the five new mutants fall into two recombination groups for which we propose the designations H and I. The nonparental is lesions rescued from suppressed pseudorevertants may represent suppressor mutations with is phenotype. However, the majority of the suppressor mutations identified had no temperature phenotype and were identified only by their effect on the phenotype of the original is lesion. The fact that a large proportion of revertants were suppressed by extragenic suppressor mutations suggests that mutation events leading to extragenic suppression occur at a much higher frequency than do intragenic events leading to revertant phenotype. These results indicate a general mechanism by which RNA viruses can bypass the effects of deleterious mutations in the absence of intramolecular recombination.",1979,2260
DERIVATIVES OF MONOACETONE XYLOSE,P.A. Levene and Albert L. Raymond,,1933,2261
A design study into the delamination behaviour of tapered composites,D.M. Thomas and J.P.H. Webber,"A fracture mechanics based analysis has been used to predict the tensile delamination load of tapered laminated plates. Simple laminate examples are used to show the effect of dropped ply thickness, number of delaminating surfaces, and dropped ply axial stiffness on the delamination load. Using these trends and acknowledged guidelines, a design is presented for a complex tapered plate with a view to maximising the onset of delamination.",1994,2262
"A review of late quaternary pollen studies in East, Central and Southern Africa",E.M. {Van Zinderen Bakker} and J.A. Coetzee,"Results of fossil pollen studies in Eastern, Central and Southern Africa covering the last 32,000 years are surveyed in this paper. This research, conducted between 1951 and 1985, was mostly concentrated in East and Southern Africa, but a number of pollen sequences from intervening sites provide some links between the data of these two regions. In all this immense region, of half the African continent, information from fossil pollen is available only from some 30 sites. In addition to problems with absolute dating, the interpretation of the results in terms of former vegetation poses difficult questions. While it is far too early to draw detailed maps of former vegetation some very general conclusions can be inferred from the data presented. At two sites in East Africa pollen evidence has been found for the existence of a warmer and more humid period from ca. 32,000 to 28,000 yr B.P., an episode known as the Kalambo Interstadial. During the period from ca. 28,000 to 20,000 yr B.P. the climate in East and Central Africa was fairly similar to that during the Holocene moist period. In Southern Africa during the same period palynologic and geologic results indicate that cold episodes occurred while higher rainfall in East Africa, the Kalahari and adjacent regions caused high lake levels. During the last glacial maximum from ca. 20,000 to 16,000–14,000 yr B.P. aridity spread over nearly the whole region. The Zaire rain forest was considerably reduced and the tree line on the East African mountains was depressed by 900–1100 m, indicating a drop in mean temperature of 5–8°C. Lakes in East Africa and the Kalahari dried out, except for the southern Kalahari, its surroundings, and the SW Cape, where humidity was high. The general causes for aridity were the low evaporation at the ocean surface and the strong upwelling of colder waters. More humid conditions which have been postulated for the central part of southern AFrica could have been the result of lower evaporation in lake basins while the penetration of winter rainfall in the area has also been proposed. An abrupt change to warmer and more humid conditions in East Africa at ca. 12,000 yr B.P. reversed all the former processes. Rainfall also penetrated the Kalahari from the NW, NE and SW sides. Remarkable fluctuations in climate at this juncture could be demonstrated by pollen evidence in South Africa only at Aliwal North. During the Holocene, humid conditions persisted in East Africa until ca. 4000 yr B.P., when the climate became drier. Insufficient dated evidence in southern Africa makes it difficult to compare the various climate chronologies. In general the climate may have been wetter until about 4000 B.P. when a dry interval occurred. In East Africa pollen data point to deforestation by man during the last two millennia.",1988,2263
Taxonomy induction based on a collaboratively built knowledge repository,Simone Paolo Ponzetto and Michael Strube,"The category system in Wikipedia can be taken as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexico-syntactic matching. The result is a large scale taxonomy. For evaluation we propose a method which (1) manually determines the quality of our taxonomy, and (2) automatically compares its coverage with ResearchCyc, one of the largest manually created ontologies, and the lexical database WordNet. Additionally, we perform an extrinsic evaluation by computing semantic similarity between words in benchmarking datasets. The results show that the taxonomy compares favorably in quality and coverage with broad-coverage manually created resources.",2011,2264
"Angle dependent interaction potentials for NOAr, NOKr and NOXe derived from various total collision cross section data",H.H.W. Thuis and S. Stolte and J. Reuss and J.J.H. {Van Den Biesen} and C.J.N. {Van Den Meijdenberg},Three independent sources of information are used to analyze the angle dependent potential for NOAr: (a) the glory structure of the total collision cross section; (b) the relative difference in the total collision cross section for two different orientations of NO in the 2Π32 state; (c) the absolute value of the total collision cross section. The sudden approximation employed for the calculation of the various properties is discussed. For NOAr a fit to the total collision cross section data is obtained on the basis of an extended Maitland—Smith potential containing a Pt anisotropy in the repulsion and a P2 anisotropy in the repulsion and attraction. A comparison is made with the theoretical potential for NOAr recently by Nielson et al. and the extended Lennard-Jones potential employed in the earlier analysis. For NOKr and NOXe similar Maitland—Smith potentials are obtained by assuming the Pt anisotropy parameter for these systems to be equal to that for NOAr. In a separate appendix is analyzed which intermolecular distances are probed through measurements of the anisotropy in the total collision cross section.,1980,2265
An empirical analysis of software and hardware spending,Vijay Gurbaxani and Haim Mendelson,"The growth in information systems budgets and in their primary components, hardware and software effort, are analyzed empirically. It is demonstrated that while a large component of the growth is due to technology related factors, these expenditures, and in particular, hardware spending, are sensitive to the growth rate of the economy and fluctuate around the technology-driven growth path due to general business conditions. The validity of the popular belief that software effort (including both software-development and maintenance) represents a growing proportion of information systems expenditures is tested versus the competing view that software effort and hardware expenditures consume relatively constant budget shares. It is shown that after controlling for macroeconomic effects, hardware and software expenditures grow exponentially at the same rate. The analysis also suggests that in the aggregate, it is primarily the hardware outlays that adjust in response to unexpected business conditions.",1992,2266
Long-range memory effects in flows involving abrupt changes in geometry: Part I: flows associated with I-shaped and T-shaped geometries,M.G.N. Perera and K. Walters,"This is a theoretical paper which attempts to study for the first time the effect of high elasticity in flow situations involving elastico-viscous liquids and abrupt changes in geometry. It is argued that implicit rheological models are essential in this exercise and, accordingly, the numerical method of solution is forced to recognise the equations of continuity, the stress equations of motion and the rheological equations as separate equations involving velocity, pressure and stress variables with appropriate boundary conditions on these variables. The present paper is concerned with L-shaped and T-shaped geometries, and the effect of elasticity is assessed by comparing the numerical predictions for an elastic liquid with those for an inelastic liquid with the same “viscosity” behaviour. This comparison is facilitated by a simple limiting procedure outlined in Section 2. The main conclusions from the work are that, in general terms, elasticity works against inertia, reducing the pressure drop caused by the abrupt change in geometry and reducing the area of influence of the bend (for finite Reynolds numbers). So far as the stress fields are concerned most interest centres on the corner region, as one would expect, but there is also a region of normal-stress activity, which is generated by “stretching” rather than “shearing”. In an appendix, some consideration is given to the entry-length and exit-length problems. It is concluded that the overall problem is a complex one, since it depends to a large measure on the criterion one uses for “fully-developed” flow. If a fairly crude criterion is used, fluid elasticity is found to decrease the entry-length and increase the exit-length.",1977,2267
Patterns in tropical leaf litter and implications for angiosperm paleobotany,Robyn J. Burnham,"One hectare of undisturbed Amazonian forest, containing about 175 species of trees larger than 10 cm diameter at breast height, was studied to determine the relationship between high-richness forest and the autochthonous litter produced by the forest. Litter samples contained up to 52 species, of which one-third represented epiphytes, vines, and lianas. These modern leaf litter studies from southeast Amazonian Peru indicate that reconstructions of ancient high-diversity forests are possible using autochthonous leaf litter deposits. In comparison to temperate litter samples, however, more sampling must be done to recreate fairly simple descriptors of ancient communities such as species richness and heterogeneity. Samples must be large, relatively closely spaced, and maintained as distinct collecting localities to retrieve the maximum amount of data from rich, angiosperm-dominated localities. There are many advantages justifying more intensive collections. For example, biomass contribution of major life-form categories in the source forest is reflected in leaf litter accumulating under tropical forest canopies. Tropical forests, because of their extreme heterogeneity, also can provide the opportunity to reconstruct individual species characteristics from litter signatures. The relative rarity of most species creates distinct leaf shadows from which the canopy breadth and volume of many individuals can be estimated. The principles derived from modern tropical litter studies can be applied to existing fossil collections; however, their power lies with those collections originating from autochthonous assemblages, for which spatial control during collecting has been maintained, and time averaging has been kept to a minimum. These reflections of community structure available from the leaf litter provide a means for paleobiologists to contribute significantly to the study of community evolution and stability.",1994,2268
Parallel sessions: posters,,,2014,2269
Two Circulatory Routes Within the Human Corpus Cavernosum Penis: A Scanning Electron Microscopic Study of Corrosion Casts,Yoshiaki Banya and Tatsuo Ushiki and Hiroshi Takagane and Hikaru Aoki and Takashi Kubo and Tsutomu Ohhori and Chizuka Ide,"The microvascular architecture of the human corpus cavernosum penis was studied by scanning electron microscopy of vascular corrosion casts. The corpus cavernosum was supplied by the penile deep artery. It gave off branches to become either arteries distributed within the corpus cavernosum or those directly supplying the corpus spongiosum urethrae. The former arteries further divided into small arteries which fell into two categories: 1) arteries breaking up into capillaries, and 2) arteries draining directly into the cavernous sinuses. The capillaries were collected into venular networks just beneath the tunica albuginea (the subalbugineal venular plexus), while the cavernous sinuses were collected into venules at the periphery of the corpus cavernosum. These postcavernous venules also received venules from the subalbugineal venular plexus, and left the corpus cavernosum. Thus, two circulatory routes are evident within the corpus cavernosum. These findings suggested that the penile erectile cycle is controlled by hemodynamic changes between these two routes within the corpus cavernosum. (J. Urol, 142: 879–883, 1989)",1989,2270
Chapter 6 - Network Devices,Johnny Long and Aaron W. Bayles and James C. Foster and Chris Hurley and Mike Petruzzi and Noam Rathaus and  SensePost and Mark Wolfgang,"Publisher Summary
The chapter discusses the most common vulnerabilities and configuration errors on routers and switches. Routers and switches perform the most fundamental actions on a network. They route and direct packets on a network and enable communications at the lowest layers. No penetration test is complete without including network devices. Before conducting a penetration test on a network device, one must first identify the device. The chapter discusses penetration testing a network device from two aspects: internal and external. The chapter presents a number of tools and techniques to be used in security level of a network device. The chapter primarily focuses on the tools included on the Auditor bootable CD, but where appropriate, other tools are also described, including Windows applications that are both commercial and open source. Different types of scanning tools and techniques are discussed that deal with network devices.",2005,2271
Biochemical changes in tissues of goldfish acclimated to high and low temperatures—I. protein synthesis,Asit B. Das and C.L. Prosser,,1967,2272
"Stromatolites of the upper Siyeh Limestone (Middle Proterozoic), Belt Supergroup, Glacier National Park, Montana",Robert J. Horodyski,"Calcareous stromatolites of the upper Siyeh Limestone (ca. 1.1 ∘ 109 years old) were studied in the central part of Glacier National Park, Montana. The stromatolites, mound- and dome-shaped structures deposited in a shallow, generally submerged, tidally influenced setting, were formed by a combination of in situ carbonate precipitation and organic stabilization of detrital material. Well-developed, 1–4 cm diameter, branched columns occur in a single stromatolite bed. Physical factors, including the size and shape of sediment-surface irregularities upon which the stromatolites developed, the rate of sedimentation between stromatolites, and the water depth, played a major role in controlling stromatolite macrostructure. Deposition of non-organically stabilized detritus on stromatolite growth surfaces inhibited the development of small-diameter columns by smoothing over developing growth features. Columnar structures are absent in stromatolites that contain abundant non-organically stabilized sediment. In contrast, they are well-developed in a stromatolite bed that is relatively deficient in such material. “Molar-tooth” structures are common in the impure dolomitic limestones, and the abundant sheet-shaped forms appear to be sparry-calcite-filled syneresis cracks.",1976,2273
Glucose Transport in Isolated Brush Border Membrane from Rat Small Intestine,Ulrich Hopfer and Kristine Nelson and Joseph Perrotto and Kurt J. Isselbacher,"Uptake of labeled d- and l-glucose has been shown to occur with highly purified brush border membranes from the epithelial cells of rat small intestine using a Millipore filtration technique. An intact glucose carrier system in the isolated membranes was demonstrated as evidenced by the following. (a) d-Glucose was taken up and released faster than l-glucose. (b) Sodium ions increased initial rate and extent of d-glucose uptake 3- to 5-fold; no other cation showed this effect. (c) d-Glucose uptake and release was inhibited by phlorizin. (d) Countertransport of d-glucose was demonstrated. (e) d- and l-glucose reached the same level of uptake after prolonged incubation. (f) Uptake of labeled d-glucose was inhibited by higher concentrations of unlabeled d-galactose and vice versa. Glucose uptake by membrane vesicles represented entry into an intravesicular aqueous space rather than binding to the membrane. Exposure of the membrane to increasing cellobiose concentrations led to osmotic shrinkage of the intravesicular space and decreased glucose uptake. Infinite medium osmolarity and therefore zero intravesicular space resulted in no glucose uptake. Sodium in the medium (but not in the intravesicular spaces) stimulated d-glucose transport. It is concluded that isolated brush border membranes of intestinal epithelial cells retain the glucose carrier system. The reported findings are consistent with the concept that (a) glucose transport across the brush border membrane represents “facilitated diffusion”; (b) the glucose carrier is dependent on sodium ions; and (c) high extracellular, but not intracellular sodium concentrations lead to increased glucose transport.",1973,2274
"POSSIBLE ROUTES TO INCREASE THE CONVERSION OF SOLAR ENERGY TO FOOD AND FEED BY GRAIN LEGUMES AND CEREAL GRAINS (CROP PRODUCTION): CO2 AND N2 FIXATION, FOLIAR FERTILIZATION, AND ASSIMILATE PARTITIONING",R.W.F. Hardy and U.D. Havelka,,1977,2275
The inhibition of nitrate assimilation by ammonium in chlorella,P.J. Syrett and I. Morris,Chlorella vulgaris growing with ammonium nitrate as nitrogen source preferentially assimilates ammonium. Nitrate assimilation ceases completely when ammonium is added and recommences as soon as ammonium has disappeared. Ammonium does not inhibit nitrate reduction by cells unable to assimilate ammonium because they lack a carbon source. Thus the inhibition is not due to ammonium per se but is connected with its assimilation. The inhibition is not thought to result from competition for reduced pyridine nucleotide because nitrate reductase of Chlorella is specific for DPN while glutamic dehydrogenase is specific for TPN. Nitrite addition also inhibits nitrate assimilation completely but ammonium only partially inhibits nitrite assimilation.,1963,2276
Recycling and reducing packaging waste: How the United States compares to other countries,James E. McCarthy,"This report summarizes current developments in the United States and 18 other industrial countries regarding packaging waste. It presents available data concerning the types, amounts, and methods of managing such waste and provides information concerning the policies established or under consideration to reduce the amount of such waste being disposed. The countries discussed are all members of the Organisation for Economic Co-operation and Development (OECD). In recent years, waste disposal capacity has become more scarce in most OECD countries. As a result, waste management policies have focused on efforts to reduce and recycle major components of the waste stream. Packaging represents about one-third of municipal solid waste in many countries. Because of this, measures to reduce the amount and toxicity of packaging and to encourage its recycling are currently being considered in at least 18 OECD countries. In addition, the EC and the Nordic Council are developing programs to address packaging on a regional basis. The report is divided into four main sections. Section I summarizes available information for the OECD countries. The second section discusses waste generation and recycling rates for six types of packaging material: paper, glass, metal, plastic, wood, and composites. The third section discusses key questions raised by the information presented in the report. The fourth briefly discusses packaging waste issues facing the Congress. In general, the report finds, other countries use less packaging than the United States, recycle more of it, and are considering policy measures stronger than the measures generally being considered in America. As noted in detail, other countries have adopted or are developing requirements that: &#x02022;• set mandatory requirements for packaging waste reduction;&#x02022;• require reusable or refillable packaging;&#x02022;• impose taxes to discourage single-use packages;&#x02022;• prohibit the use of non-recyclable packaging,&#x02022;• prohibit or limit disposal of packaging, and&#x02022;• require manufacturers of packaging materials to collect and recycle post-consumer waste. Perhaps the most fundamental issue raised by these approaches is whether local governments will continue to bear responsibility for funding and operating recycling programs or whether all or some of this responsibility might be shifted to industry. To date, this issue has not been joined in the Congress directly; however, there is a growing consensus in other countries concerning the advantages of industry responsibility.",1993,2277
4 - Aspects of Windows Client Security,Jan {De Clercq} and Guido Grillenmeier,"Publisher Summary
This chapter focuses on important client security aspects that include least privilege, Internet Explorer (IE) security, security changes in XP Service Pack 2, spyware protection, and the Trusted Platform Module (TPM). It also covers the upcoming client security features in Internet Explorer 7.0 and Windows Vista. The principle of least privilege means that you should a user or a piece of code is given only the privileges it needs to do the job. For administrators, this means that they should have two accounts in an AD domain—one low-privilege account, and another high privilege account. UNIX is the best example that completely follows the principle of least privilege. The tools that administrators and users can use today in Windows 2000, Windows Server 2003, R2, and Windows XP to honor least privilege are RunAs, fast user switching, and third party tolls like dropmyrights and the privilege bar, The chapter also details the Windows XP Service Pack 2 security enhancements that include critical security enhancements for both users and developers. In Windows XP, Microsoft introduced a built-in personal firewall: the Internet Connection Firewall (ICF). XP SP2 offers more security resilience, that is, it increases the level of security and protection even on systems that don't have the latest security patches installed.’. The chapter also details the concept of browser security that highlights intelligent add-on management, pop-up blocking, and hidden IE security changes. Internet Explorer (IE) security zones are a powerful and often neglected security feature of Microsoft's Internet browser. Ensuring proper IE security require defining security zones, identifying security zone web site, customizing and administering the local computer security zone, and controlling security zone configuration settings. The chapter concludes with the malicious mobile code protection (MMC) that deals with protection against MMC annoyances like viruses, spyware, and rootkits.’",2007,2278
Newer Methods in the Rapid Development of Disease-Resistant Vegetables* *Published by permission of the Director of the Hawaii Agricultural Experiment Station as Technical Paper No.160.,W.A. FRAZIER,,1949,2279
Sommaire,,,2011,2280
The collaborative assay of the european pharmacopoeia biological reference preparation for insulin,D.R. Bangham and H. {de Jonge} and J. {van Noordwijk},"The European Pharmacopoeia Biological Reference Preparation for Insulin was assayed against the Fourth International Standard for Insulin by 12 European laboratories in collaboration. In addition to the Rabbit Blood Glucose (RBG) assay and the Mouse Convulsion Assay (MCA) of the European Pharmacopoeia, the Mouse Blood Glucose (MBG) assay of the Pharmacopoeia Nordica was used. The MBG was superior to the MCA and not inferior to the RBG with regard to precision and statistical weight per assay and per animal. The 111 valid MCA results were analysed with the full probit procedure, a simplified probit procedure, the logit transformation and the arc sine transformation. The potency estimates calculated with the full probit procedures were approached most closely by those obtained with the arc sine transformation; the simplified probit procedure and the logit transformation yielded results which agreed reasonably well with those of the full probit procedure in most cases, but there were some exceptions for which no explanation was found. The twin cross-over design contributed markedly to the precision of the RBG, but this was not the case for the MBG. A potency of 24·0 i.u. mg−1 was assigned to the European Pharmacopoeia Biological Reference Preparation for Insulin.",1978,2281
Chapter 4 Petrophysical NMR measurements,,"Publisher Summary
To understand and/or interpret the results of nuclear magnetic resonance (NMR) logs from borehole measurements, it is imperative to conduct NMR laboratory measurements on brine and/or oil saturated rock samples first. Such measurements can be carried out to almost any desired accuracy using various means not readily available in a logging situation. For example, the measured NMR signals can be conveniently stacked many times to obtain a very high signal to noise ratio. One can study how the level of noise tends to affect the characteristics of the T2 distributions. When the water and oil signals are both present in the measurements, one can soak the core plugs in D2O to remove the water signal, or use the Magic Angle Spinning (MAS) technique to differentiate the two phases in high magnetic field. This chapter discusses the determination of irreducible water saturation, the understanding of oil T2 relaxation times regarding its viscosity and diffusion coefficient, as well as the establishment of permeability estimators, that are important in the laboratory for the understanding, interpretation, and applications of NMR logs to formation evaluation.",2002,2282
5 - Kerberos,Jan De Clercq,"Publisher Summary
This chapter focuses on the Kerberos authentication protocol, the default authentication protocol of Windows Server 2003. Microsoft introduced Kerberos as the new default authentication protocol for enterprise environments in Windows 2000. Every Windows 2000, Windows XP, and Windows Server 2003 operating system platform includes a client Kerberos authentication provider. Over the past years, Microsoft has been actively involved in the Kerberos standardization process. Microsoft software engineers participated in the creation of several Kerberos-related Internet drafts. The basic Kerberos protocol only deals with authentication. Microsoft's implementation of the protocol also includes extensions for authorization. The Kerberos protocol always deals with three entities: two entities that want to authenticate to one another and one entity that mediates between these two entities, a trusted third party or the key distribution center (KDC). To make Kerberos more scalable, the Kerberos developers included the concept of a KDC that is a trusted third party with which every entity shares a secret key. This key is called the entity's master key. All entities trust the KDC to mediate in their mutual authentication. The KDC also maintains a centralized authentication database containing a copy of every user's master key.",2004,2283
Simulation of energy utilisation of bovine animals,H.M. Keener,Energy equations for partitioning energy utilisation in the bovine animal were written and utilised in a dynamic simulation model of beef animal growth. Mathematical models for the rates of anabolism and catabolism of stored products in the animal's body are presented. Estimates of the parameters used in the models were developed using experimental results and an experimental feeding trial. Computer simulations of a growing beef animal were compared with independent experimental data. Results indicated that the developed model can estimate bovine growth performance and the rates and modes of heat production under varying thermal and nutrient environments.,1979,2284
English patents,,,1830,2285
"PACKER—A FORTRAN 77 code for collection, analysis, and display of interval pressure injection test data",G.C. Pasquarell and D.G. Boyer and J.B. Urban,"A FORTRAN 77 code is presented which gives the user a systematic approach to the collection, storage, and graphical display of results from pressure injection testing of wells and boreholes. Transmissivities are computed using three different formulations: a simple, standard method; one which accounts for unsaturated conditions and the use of one or two packers; and a method which assumes that the majority of the transmissivity is the result of a single, horizontal fracture. Results may be displayed graphically with depth along with the matrix values to detect significant secondary porosity. A methodology is presented for calibrating for head loss in the piping system to maximize the accuracy of computed results. The code takes flow-head loss data either interactively, or from a database holding previous calibration data. The data then are fit with a best-fit polynomial, the order of which is selected by the user. The best fit of the head loss data may be viewed graphically. A set of field collected pressure injection data is presented and analyzed using the code. The results demonstrate the influence that both the computational formulation and the head loss calibration may have on computed results.",1995,2286
"Evolutionary changes over the last 1000 years of reduced sulfur phases and organic carbon in varved sediments of the Santa Barbara Basin, California",Arndt Schimmelmann and Miriam Kastner,"Sediments from the Santa Barbara Basin (SBB) of the last 1000 years were analyzed for elemental sulfur (Se), mineral sulfide sulfur (Sm), lipid-based organically bound sulfur (Sx), residual organically bound sulfur (So), total organic carbon (TOC) and total nitrogen (TN). The approximate annual resolution of the time-series data permitted us to evaluate the influence of oceanic variables on the benthic environment and the accumulating varved sediment in the SBB. A bacterial mat community at or near the sediment surface is influencing sediment porosity and the production and distribution of Se. Early diagenesis dampens the amplitude of environmental signals in the geochemical time series. The concentrations of Sm and So increase with burial depth, at the expense of decreasing concentrations of Sx and Se. At depths greater than 5.5 cm, corresponding to an age of 7 years, Sm becomes the largest pool of reduced sulfur. It requires approximately 500 years of primarily bacterially mediated diagenesis, equivalent to a burial depth of ~ 1.4 m, for SBB sediments to gradually approach and finally enter the region assumed to represent typical “normal” marine sediments on a plot of weight percent TOC vs. Sm.",1993,2287
"The production and trophic ecology of shallow-water fish assemblages in southern Australia III. General relationships between sediments, seagrasses, invertebrates and fishes",Graham J. Edgar and Craig Shaw,"Fishes and benthic invertebrates were sampled in seagrass and unvegetated habitats at 14 localities across southern Australia in order to determine any consistent relationships between animal production, fish consumption and environmental parameters over a large spatial scale. Most of the features identified in a related study at Western Port were confirmed over the extended geographic range; however, an exception was that the production of fishes in the smallest size-classes (i.e. <1 g wet weight) was not consistently greater in seagrass habitat than unvegetated habitat. The more important characteristics of seagrass and unvegetated habitat types identified in the extended study were (1) more fish species were associated with seagrass habitat than unvegetated habitat, (2) the majority of small-fish species in both habitat types fed on crustaceans, with relatively few species capable of utilising algae and virtually no species utilising seagrass, (3) the few species that did ingest algae often occurred in high abundance, (4) the size of prey eaten by fishes increased consistently with fish size, with prey length averaging ≈6% of fish length, (5) abundant large fish species generally consumed smaller prey than rarer large fish species at the same body size, (6) greater benthic invertebrate and demersal fish production occurred in seagrass habitat than in unvegetated habitat, (7) most of the production of crustaceans >1-mm sieve size was ingested by fish predators while only a small proportion of non-crustacean benthic production was consumed. Fish production was highly correlated with crustacean production and seagrass biomass, and was negatively correlated with wave exposure (measured as fetch) across the range of sites. The estimated production of crustaceans was highly correlated with the biomass of seagrass material and also with the proportion of particles <63 μm in the sediments. The overall relationships between macrofaunal production (M; mg · m−2 · day−1), macrocrustacean production (C; mg · m−2 · day−1), demersal fish production (D; mg · m−2 · day−1), fetch (F; km), seagrass biomass (L; g · m−2), proportion of particles <63 μm (S;%) and temperature (T; °C) were: ln M = 1.41 + 0.088 ln (L + 1) + 0.38 ln S + 0.89 ln T, ln C = −0.93 + 0.27 ln (L + 1) + 0.22 ln S + 0.89 ln T, ln D = −1.23 − 0.62 ln F + 0.36 ln (L + 1) + 1.04 ln T. These regression equations can be used as models to predict the production of macrofauna, crustaceans and small fishes at unexamined sites. When predictions were compared with estimates of annual production at the eight sites previously examined in Western Port, most predictions lay between 50 and 200% of measured values. Additional work in Australia and overseas should allow these models to be refined.",1995,2288
"Age, thermal evolution and history of the Black Sea Basin based on heat flow and multichannel reflection data",A.Ya Golmshtok and L.P Zonenshain and A.A Terekhov and R.V Shainurov,"Multichannel reflection data (Tugolessov et al., 1985) have revealed two deeps in the basement topography of the Black Sea which are filled with sediments from 12 to 15 km thick. The deeps lack the “granitic layer” and are underlain by oceanic-type crust which we assume to be generated by seafloor spreading processes. The age of the deeps was interpreted previously, in a highly controversial manner, as being from the Paleozoic — Early Mesozoic to the Recent. In the paper, age estimations were undertaken using surficial heat flow data, assuming that they are related to deep-seated age-dependent heat flow generated by the cooling oceanic lithosphere, but that they are strongly distorted by the heating of continuously accumulating sediments as well as by additional heat input from radiogenic production within sediments. Using reliable thermophysical parameters of compacting sediments, the distorted heat flow in the sediments was evaluated numerically. This allowed us to estimate the age of the Black Sea deeps floor. The results show that the West Black Sea deep is 130 to 95 m.y. old, and the East Black Sea deep is nearly 110 m.y. old. These figures support an interpretation of the Black Sea deeps as remnants of a Late Mesozoic back-arc basin that evolved behind the Lesser Caucasian — Pontide island arc. The inferred Middle Cretaceous age of the deeps is the first estimate obtained quantitatively, and corresponds well with available heat flow and multichannel reflection data.",1992,2289
A Comparative Survey of Open-Source Platforms for Chat-Based Generative AI,"Kurivella, Pavan Kumar
and Lauren, Paula","Generative AI for text-based applications have gained in popularity with the release of ChatGPT in late 2022 by OpenAI. Users engage with the application via a web-based user interface or programmatically via the API. The details of the model internals and specifics on the training have not been made public, which poses several challenges in better understanding the system. It would be advantageous to have insight into the system to better understand the occurrence of bias and issues pertaining to privacy, among other concerns. These aforementioned issues could be better addressed if the code base used for training these large language models were made publicly available. Since the release of ChatGPT and its underlying technologies, there have been various open-source platforms that have become publicly available. This survey presents an analysis of four open-source platforms for chat-based generative AI. These specific platforms include Open Assistant, Colossal AI, ChatLLaMA and OpenChatKit. This survey evaluates each of the four open-source platforms for chat-based Generative AI based on evaluative criteria consisting of understandability, setup and configuration, community and support, regularity of code updates, as well as system compatibility and efficiency.",2023,2290
A Human-friendly Verbal Communication Platform for Multi-Robot Systems: Design and Principles,"Carr, Christopher
and Wang, Peng
and Wang, Shengling","Aiming at the first steps toward de-coupling the application of multi-robot systems from reliance on network infrastructure, this paper proposes a human-friendly verbal communication platform for multi-robot systems, following the deliberately designed principles of being adaptable, transparent, and secure. The platform is network independent and is subsequently capable of functioning in network-deprived environments--from underwater scenarios to planet exploration. A series of experiments were conducted to demonstrate the platform's capability in multi-robot systems communication and task co-ordination, showcasing its potential in infrastructure-free applications. To benefit the community, we have made the code open source at https://github.com/jynxmagic/MSc{\_}AI{\_}projectGitHub.",2024,2291
Development of a Telegram Bot to Determine the Level of Technological Readiness,"Suntsova, Darya I.
and Pavlov, Viktor A.
and Makarenko, Zinaida V.
and Bakholdin, Petr P.
and Politsinsky, Alexander S.
and Kremlev, Artem S.
and Margun, Alexey A.","The article proposes a Telegram bot to automate the process of evaluating the technology readiness level (TRL). A brief overview of the solutions existing in this area was carried out, a flowchart reflecting the algorithm of the bot, and images demonstrating the appearance of its interface are presented. The bot has been successfully tested for performance: with equal input data, the conclusion about the TRL level obtained at the output of the algorithm coincides with the result of an expert assessment in accordance with GOST R 58048-2017. It is proposed to use the bot as a source of another additional opinion about the level of technological readiness of the project.",2022,2292
"Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories","Atkins, Conor
and Zhao, Benjamin Zi Hao
and Asghar, Hassan Jameel
and Wood, Ian
and Kaafar, Mohamed Ali","One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We generate 150 examples of misinformation, of which 114 (76{\%}) were remembered by BlenderBot 2 when combined with a personal statement. We further assessed the risk of this misinformation being recalled after intervening innocuous conversation and in response to multiple questions relevant to the injected memory. Our evaluation was performed on both the memory-only and the combination of memory and internet search modes of BlenderBot 2. From the combinations of these variables, we generated 12,890 conversations and analyzed recalled misinformation in the responses. We found that when the chat bot is questioned on the misinformation topic, it was 328{\%} more likely to respond with the misinformation as fact when the misinformation was in the long-term memory.",2023,2293
Research on the Collaborative Mechanism of Project Robots in Online Knowledge Production Communities: An Analysis of GitHub,"Wang, Ziyu
and Ma, Chengming
and Shen, Wenhao","Bots are becoming more widely adopted in GitHub, with implications for online knowledge collaboration. It is necessary to study the extent of the impact of bots in online knowledge collaboration. A total of 843 projects with around 63125 aggregate data were acquired by crawling projects through GitHub API and filtering out projects that do not contain code or do not have any issues. Preliminary research found no correlation between the adoption of bots and the popularity of the project, while most of the literature support a positive correlation. But further classified observation according to the attributes of the bot, we found that projects with dedicated bots were more popular than projects with template bots. The study found that the adoption of dedicated bots for projects led to an increase in commit, fork, and issue count, which affected the addition, reorganization, and integration of knowledge collaboration. The study also found that projects with dedicated bots received more attention, with a positive correlation between fork count and star count, but no significant correlation between commit, issue count and star count. The impact of bots in online knowledge collaboration is still lacking in terms of adding and integrating knowledge.",2024,2294
Fingerprint Surface-Based Detection of Web Bot Detectors,"Jonker, Hugo
and Krumnow, Benjamin
and Vlot, Gabry","Web bots are used to automate client interactions with websites, which facilitates large-scale web measurements. However, websites may employ web bot detection. When they do, their response to a bot may differ from responses to regular browsers. The discrimination can result in deviating content, restriction of resources or even the exclusion of a bot from a website. This places strict restrictions upon studies: the more bot detection takes place, the more results must be manually verified to confirm the bot's findings.",2019,2295
The Use of Large Language Model in Code Review Automation: An Examination of Enforcing SOLID Principles,"Martins, Gustavo F.
and Firmino, Emiliandro C. M.
and De Mello, Vinicius P.","Within the ever-evolving domain of software development, the practice of having teams located in different geographical locations presents distinct obstacles, including disparate time zones, language hurdles, and differing degrees of experience. This paper presents a novel approach to address these difficulties by using an automated GitHub bot that utilizes Large Language Models (LLMs) to enforce SOLID principles during code reviews. This bot, which incorporates advanced models such as OpenAI's GPT-4 and the locally deployable Mixtral, has the objective of delivering immediate and practical feedback. Its purpose is to improve the quality of code and make learning easier for developers, particularly those who are new to programming. The bot's structure enables effortless incorporation into GitHub, utilizing LLMs to examine code modifications and offer observations regarding adherence to SOLID principles. An important characteristic of this method is the incorporation of Mixtral, which may be operated on-site, providing advantages in terms of data confidentiality and operational adaptability, essential for global enterprises with strict privacy demands. Here, we explores the bot's architecture, its incorporation with LLMs, and its capacity to revolutionize code reviews by offering a secure, efficient, and instructive instrument for geographically dispersed software development teams.",2024,2296
CodeScan: A Supervised Machine Learning Approach to Open Source Code Bot Detection,"Gaurav, Vipul
and Singh, Shresth
and Srivastava, Avikant
and Shidnal, Sushila",Enhancing software productivity would help companies to cut their costs and increase profits. Software metrics rely heavily on the personal experiences and skills of managers in pattern recognition and rewards. Differentiating between actual human effort and machine-generated code can help drive an organization's decision-making process that is rewarding its employees and provide an assistive tool to the managers allowing effective monitoring without micromanagement that has a wide application in managing work from home and other virtual environments. The paper explores the insight into the quality of machine-generated bot code compared to actual human coding efforts. It uses machine learning techniques to identify patterns and gives intelligent insights that can be used as a performance metric for versioning systems and business intelligence. We successfully distinguished between a bot and human-written code with an F1-score of 0.945 using the Light Gradient Boosting Method.,2022,2297
Inside the Tool Set of Automation: Free Social Bot Code Revisited,"Assenmacher, Dennis
and Adam, Lena
and Frischlich, Lena
and Trautmann, Heike
and Grimme, Christian","Social bots have recently gained attention in the context of public opinion manipulation on social media platforms. While a lot of research effort has been put into the classification and detection of such automated programs, it is still unclear how technically sophisticated those bots are, which platforms they target, and where they originate from. To answer these questions, we gathered repository data from open source collaboration platforms to identify the status-quo of social bot development as well as first insights into the overall skills of publicly available bot code.",2020,2298
Integration of Wit API with Python Coded Terminal Bot,"Jain, Sanyam
and Sharma, Shivani
and Tomar, Ravi","This research paper consists of the development of a python chatbot integrated with specific packages and dependencies having capabilities to perform various real-life applications which can also be packaged with some integrated systems as a personal assistant. Various Web services are used using their APIs so as to make it more powerful and less dependent on python packages. It operates completely on Linux (Ubuntu) terminal. Python command with the {\_}{\_}main{\_}{\_}.py file has been used to run it. Terminal then becomes a live Web-based chatbot service with STT and TTS services. Natural language processing is further done by Wit.ai API which token key is set in the code. Each NLP is done by the API and then processed, and crunched text is sent from the wit engine direct to display terminal.",2019,2299
Bot Log Mining: Using Logs from Robotic Process Automation for Process Mining,"Egger, Andreas
and ter Hofstede, Arthur H. M.
and Kratsch, Wolfgang
and Leemans, Sander J. J.
and R{\""o}glinger, Maximilian
and Wynn, Moe Thandar","Robotic Process Automation (RPA) is an emerging technology for automating tasks using bots that can mimic human actions on computer systems. Most existing research focuses on the earlier phases of RPA implementations, e.g. the discovery of tasks that are suitable for automation. To detect exceptions and explore opportunities for bot and process redesign, historical data from RPA-enabled processes in the form of bot logs or process logs can be utilized. However, the isolated use of bot logs or process logs provides only limited insights and not a good understanding of an overall process. Therefore, we develop an approach that merges bot logs with process logs for process mining. A merged log enables an integrated view on the role and effects of bots in an RPA-enabled process. We first develop an integrated data model describing the structure and relation of bots and business processes. We then specify and instantiate a `bot log parser' translating bot logs of three leading RPA vendors into the XES format. Further, we develop the `log merger' functionality that merges bot logs with logs of the underlying business processes. We further introduce process mining measures allowing the analysis of a merged log.",2020,2300
On Twitter Bots Behaving Badly: Empirical Study of Code Patterns on GitHub,"Millimaggi, Andrea
and Daniel, Florian","Bots, i.e., algorithmically driven entities that behave like humans in online communications, are increasingly infiltrating social conversations on the Web. If not properly prevented, this presence of bots may cause harm to the humans they interact with. This paper aims to understand which types of abuse may lead to harm and whether these can be considered intentional or not. We manually review a dataset of 60 Twitter bot code repositories on GitHub, derive a set of potentially abusive actions, characterize them using a taxonomy of abstract code patterns, and assess the potential abusiveness of the patterns. The study does not only reveal the existence of 31 communication-specific code patterns -- which could be used to assess the harmfulness of bot code -- but also their presence throughout all studied repositories.",2019,2301
Designing Philobot: A Chatbot for Mental Health Support with CBT Techniques,"Ge, Qi
and Liu, Lu
and Zhang, Hewei
and Li, Linfang
and Li, Xiaonan
and Zhu, Xinyi
and Liao, Lejian
and Song, Dandan","Mental health issues are a major concern for teenagers, and access to affordable and accessible support is critical for promoting positive mental health outcomes. Philobot is a chatbot designed to provide personalized mental health support for teenagers with cognitive behavioral therapy (CBT) practices. It also offers a chatting moodule that facilitates building a rapport between the virtual assistant and its users. This paper presents the design, implementation, and validation of Philobot's key features, including its fine-grained intent classification system, FAQ retrieval model, and response generation model using top-k sampling. Evaluation results show that Philobot is a promising tool for promoting positive mental health outcomes in teenagers.",2023,2302
To Bot or Not to Bot?: Analysing Mental Health Data Disclosures,"Taylor, Deborah
and Melvin, Clare
and Aung, Hane
and Asif, Rameez","Disclosure of personal information about wellbeing and mental health is a nuanced situation requiring trust between agents. Current methods for initial mental health assessments are time and resource intensive. With increases in demand for mental health services and decreases in funding and staffing levels, this paper explores whether conversational agents can be sufficiently `trusted' to collect the sensitive data disclosed in initial mental health assessment, thereby reducing the workload for trained professionals.",2024,2303
I Am Bot the ``Fish Finder'': Detecting Malware that Targets Online Gaming Platform,"Ouellette, Nicholas
and Baseri, Yaser
and Kaur, Barjinder","Malware in the gaming industry presents many forms of risk for users, such as phishing, trojans, malware, and network attacks. Few studies have been published on gaming industry malware. The ones identified were found to be primarily focused mainly on in-game cheat detection, such as cheat clients and aimbots. This paper leverages related research drawn from the broader field of cybersecurity, including, URL-phishing detection and crytpojacking. To detect URL phishing attacks data, we used eight filter, wrapper, and embedded-based feature selection and five machine learning techniques, i.e., AdaBoost, extra trees (ET), random forest (RF), decision tree (DT), and gradient boosting (GB) where highest accuracy, precision, recall, and F1{\_}score are achieved with RF. We further scrutinize the feature selection and classifiers based on threshold that will help to provide an aggregate simplified recommendation to the user and alerting about the malicious URL. The outcome of whether the URL is benign or malicious can easily be seen on developed bot application named ``Fish Finder.'' For allowing Discord users to protect themselves from future phishing attacks, we have shared the built application on github: https://github.com/Dinnerspy/Discord-Bot-Phishing-Detection.",2023,2304
The GitHub Development Workflow Automation Ecosystems,"Wessel, Mairieli
and Mens, Tom
and Decan, Alexandre
and Mazrae, Pooya Rostami","Large-scale software development has become a highly collaborative and geographically distributed endeavor, especially in open-source software development ecosystems and their associated developer communities. It has given rise to modern development processes (e.g., pull-based development) that involve a wide range of activities such as issue and bug handling, code reviewing, coding, testing, and deployment. These often very effort-intensive activities are supported by a wide variety of tools such as version control systems, bug and issue trackers, code reviewing systems, code quality analysis tools, test automation, dependency management, and vulnerability detection tools. To reduce the complexity of the collaborative development process, many of the repetitive human activities that are part of the development workflow are being automated by CI/CD tools that help to increase the productivity and quality of software projects. Social coding platforms aim to integrate all this tooling and workflow automation in a single encompassing environment. These social coding platforms gave rise to the emergence of development bots, facilitating the integration with external CI/CD tools and enabling the automation of many other development-related tasks. GitHub, the most popular social coding platform, has introduced GitHub Actions to automate workflows in its hosted software development repositories since November 2019. This chapter explores the ecosystems of development bots and GitHub Actions and their interconnection. It provides an extensive survey of the state of the art in this domain, discusses the opportunities and threats that these ecosystems entail, and reports on the challenges and future perspectives for researchers as well as software practitioners.",2023,2305
Setting Up GitHub Actions,"Jackson, Simon","Originally, GitHub was just about source control, a safe and decentralized offering to host your code for free (if it was public and open source). When Microsoft bought GitHub, things started to evolve and grow; one of the biggest areas of growth was in automation. From that date, Microsoft saw an opportunity to learn from how they evolved their original Azure Pipelines automation and start afresh with those learnings, eventually growing GitHub Actions into a competing product with their own services. You might ask why they would do this, but ultimately it was in their own self-interest; Azure while powerful can be cumbersome; GitHub is new, fresh, easier to use, and more available.",2023,2306
