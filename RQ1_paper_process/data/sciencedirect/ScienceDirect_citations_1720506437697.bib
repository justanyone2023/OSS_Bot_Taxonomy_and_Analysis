@article{NOHRA2008125,
title = {FERTILITY IN MEN WITH CHRONIC RENAL FAILURE BEFORE AND AFTER KIDNEY TRANSPLANTATION},
journal = {European Urology Supplements},
volume = {7},
number = {3},
pages = {125},
year = {2008},
note = {23rd Annual Congress of the European Association of Urology},
issn = {1569-9056},
doi = {https://doi.org/10.1016/S1569-9056(08)60218-3},
url = {https://www.sciencedirect.com/science/article/pii/S1569905608602183},
author = {J. Nohra and A. Zairi and G. Ghazal and N. Kamar and L. Rostaing and P. Plante and E. Huyghe}
}
@incollection{HAMED2017109,
title = {Chapter 6 - Intrusion Detection in Contemporary Environments},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
address = {Boston},
pages = {109-130},
year = {2017},
isbn = {978-0-12-803843-7},
doi = {https://doi.org/10.1016/B978-0-12-803843-7.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038437000065},
author = {Tarfa Hamed and Rozita Dara and Stefan C. Kremer},
keywords = {Anomaly-based IDS, Attacks, Cloud computing, DoS, IDS, Intrusions, Malware, Mobile devices, Signature-based IDS, Smartphones, Trojans},
abstract = {This chapter discusses intrusion detection applications for two contemporary environments: mobile devices and cloud computing. The chapter starts by introducing the most well-known mobile device operating systems and cloud computing models. Next, the chapter discusses the risks to which these environments are exposed as a result of intrusions, and the sources and origins of attacks in both environments. Furthermore, classes of malware and types of attacks are explained. In addition, the chapter explores techniques employed by mobile malware as well as techniques employed by intrusions that infect cloud computing systems. The chapter also gives a variety of new examples of malware that infect mobile phones and intrusions into cloud computing systems. Moreover, the chapter discusses types of intrusion detection systems and explains performance metrics for evaluating intrusion detection systems in both environments.}
}
@article{LENG202395,
title = {Towards resilience in Industry 5.0: A decentralized autonomous manufacturing paradigm},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {95-114},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001723},
author = {Jiewu Leng and Yuanwei Zhong and Zisheng Lin and Kailin Xu and Dimitris Mourtzis and Xueliang Zhou and Pai Zheng and Qiang Liu and J. Leon Zhao and Weiming Shen},
keywords = {Decentralized autonomous manufacturing, Blockchain 3.0, Mass individualization, Industry 5.0, Resilient manufacturing systems},
abstract = {Manufacturers are increasingly aware of the importance of system resilience against unexpected disruptive occurrences, such as the recent global Covid-19 pandemic and geopolitical wars in Europe. Meanwhile, Decentralized Autonomous Organization (DAO) is recently envisioned as the Blockchain 3.0 stage, in which enabling DAO in the manufacturing domain could be a promising attempt and will lead to decentralized autonomous manufacturing. Blockchain-enabled smart contracts and decentralized applications have the characteristics of verifiability, decentralization, transparency, autonomy, and tamper-proofing, which can enable a mass individualization paradigm to realize the promising Industry 5.0 vision of resilience. DAO provides a probable manner to regulate cross-prosumer activities under the Industry 5.0 context. Inspired by this vision, our paper reviews the literature on Decentralized Manufacturing (DM) and Autonomous Manufacturing (AM). Then, these two streams of efforts are unified, and a manufacturing paradigm, named Decentralized Autonomous Manufacturing (DAM), is defined towards resilience in Industry 5.0. In this paper, a reference architecture of the DAM is given. Followed by a comprehensive investigation of the key enablers, challenges, and barriers in the implementation of DAM, based on the insights from this analysis, future research directions of DAM are highlighted. We believe that our effort can lay a foundation for positioning DAM in futuristic Industry 5.0 research and engineering practice.}
}
@article{DOGAN2022106737,
title = {Towards a taxonomy of code review smells},
journal = {Information and Software Technology},
volume = {142},
pages = {106737},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106737},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001877},
author = {Emre Doğan and Eray Tüzün},
keywords = {Modern code review, Bad practices, Conformance checking, Code review smell, Process smell, Process debt},
abstract = {Context:
Code review is a crucial step of the software development life cycle in order to detect possible problems in source code before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the code review process, many companies and open source software (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective code review process. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are code review (CR) smells.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight OSS projects.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the OSS projects are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.}
}
@article{ALI2019139,
title = {A proactive malicious software identification approach for digital forensic examiners},
journal = {Journal of Information Security and Applications},
volume = {47},
pages = {139-155},
year = {2019},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2214212618306367},
author = {Muhammad Ali and Stavros Shiaeles and Nathan Clarke and Dimitrios Kontogeorgis},
keywords = {Digital forensics, Malware, Machine learning, Registry hives, Windows Registry, Windows 7/8/10, Sandbox, Agentless sandbox, Cuckoo},
abstract = {Digital investigators often get involved with cases, which seemingly point the responsibility to the person to which the computer belongs, but after a thorough examination malware is proven to be the cause, causing loss of precious time. Whilst Anti-Virus (AV) software can assist the investigator in identifying the presence of malware, with the increase in zero-day attacks and errors that exist in AV tools, this is something that cannot be relied upon. The aim of this paper is to investigate the behaviour of malware upon various Windows operating system versions in order to determine and correlate the relationship between malicious software and OS artifacts. This will enable an investigator to be more efficient in identifying the presence of new malware and provide a starting point for further investigation. The study analysed several versions of the Windows operating systems (Windows 7, 8.1 and 10) and monitored the interaction of 90 samples of malware (across three categories of the most prevalent (Trojan, Worm, and Bot) and 90 benign samples through the Windows Registry. Analysis of the interactions has provided a rich source of knowledge about how various forms of malware interact with key areas of the Registry. Using this knowledge, the study sought to develop an approach to predict the presence and type of malware present through an analysis of the Registry. To this end, different classifiers such as Neural Network, Random forest, Decision tree, Boosted tree and Logistic regression were tested. It was observed that Boosted tree was resulting in a correct classification of over 72% – providing the investigator with a simple approach to determining which type of malware might be present independent and faster than an Antivirus. The modelling of these findings and their integration in an application or forensic analysis within an existing tool would be useful for digital forensic investigators.}
}
@article{NEVINS2001459,
title = {A Clinical Approach to Periodontal Regeneration},
journal = {Oral and Maxillofacial Surgery Clinics of North America},
volume = {13},
number = {3},
pages = {459-473},
year = {2001},
issn = {1042-3699},
doi = {https://doi.org/10.1016/S1042-3699(20)30131-X},
url = {https://www.sciencedirect.com/science/article/pii/S104236992030131X},
author = {Myron Nevins and Marc L. Nevins and Marcelo Camelo and James T. Mellonig}
}
@article{MCINTOSH2023103160,
title = {Applying staged event-driven access control to combat ransomware},
journal = {Computers & Security},
volume = {128},
pages = {103160},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103160},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000706},
author = {Timothy McIntosh and A.S.M. Kayes and Yi-Ping Phoebe Chen and Alex Ng and Paul Watters},
keywords = {Ransomware, Malware, Access control, Ransomware mitigation, Intrusion prevention},
abstract = {The advancement of modern Operating Systems (OSs), and the popularity of personal computing devices with Internet connectivity, have facilitated the proliferation of ransomware attacks. Ransomware has evolved from executable programs encrypting user files, to novel attack vectors including fileless command scripts, information exfiltration and human-operated ransomware. Many anti-ransomware studies have been published, but many of them assumed newer ransomware variants only performed file encryption, were similar to existing variants, and often did not consider those novel attack vectors. We have defined an updated ransomware threat model to include those novel attack vectors, and redefined false positives and false negatives in the context of ransomware mitigation. We proposed to apply both program-centric and user-centric access control to combat ransomware, but only delegate access control decisions that users are capable of making to users, while enforcing non-negotiable access control decisions by OS and software developers. We have designed a Staged Event-Driven Access Control (SEDAC) approach to incorporate both program-centric and user-centric access control measures, and demonstrated a prototype on Windows OS. Our prototype was able to intercept more types of ransomware attack vectors than existing proposals. We hope to convince OS and software architects to incorporate our design to better combat ransomware.}
}
@incollection{RAMSBROCK2013223,
title = {Chapter 12 - The Botnet Problem},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {223-238},
year = {2013},
isbn = {978-0-12-394397-2},
doi = {https://doi.org/10.1016/B978-0-12-394397-2.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012394397200012X},
author = {Daniel Ramsbrock and Xinyuan Wang},
keywords = {botnet, Internet service provider, best practices, botmaster, bot-header, denial of service, malware, zombie, Trojan horse, distributed access},
abstract = {This chapter addresses the issue of Internet service provider (ISP) network protection, with a focus on addressing bots and botnets, which are a serious and growing problem for end users and ISP networks. Botnets are formed by maliciously infecting end-user computers and other devices with bot (from the word robot) software through a variety of means, and surreptitiously controlling the devices remotely to transmit onto the Internet spam and other attacks (targeting both end users and the network itself). This chapter examines potentially relevant existing best practices (BPs) and identifies additional best practices to address this growing problem. The chapter also identifies best practices that address protection for end users as well as the network. The best practices are organized into the logical steps required to address botnets. The first step is prevention, followed by detection, notification, and then mitigation. In addition, the best practices on privacy considerations are also identified to address the handling of customer information in botnet response. The best practices identified are primarily for use by ISPs that provide service to consumer end users on residential broadband networks, but may apply to other end users and networks as well. It is critical to note that best practices in general are not applicable in every situation because of multiple factors. Therefore, the best practices are intended to be voluntary in nature for ISPs and may not apply in all contexts (and thus for a host of reasons should not be made mandatory). With this understanding, this chapter recommends that the best practices can be implemented by ISPs, where applicable, in order to address the growing botnet problem in consumer end-user devices and ISP networks.}
}
@article{SAITO2002141,
title = {Morphology of the decrementing expiratory neurons in the brainstem of the rat},
journal = {Neuroscience Research},
volume = {44},
number = {2},
pages = {141-153},
year = {2002},
issn = {0168-0102},
doi = {https://doi.org/10.1016/S0168-0102(02)00095-0},
url = {https://www.sciencedirect.com/science/article/pii/S0168010202000950},
author = {Yoshiaki Saito and Ikuko Tanaka and Kazuhisa Ezure},
keywords = {Axonal projection, Bötzinger complex, VRG, nucleus retroambiguus, Intracellular labeling, Neurobiotin},
abstract = {In anesthetized and artificially-ventilated rats, the morphological properties of decrementing expiratory (E-DEC) neurons were studied using intracellular recording and labeling with Neurobiotin. Sixteen E-DEC neurons were successfully labeled; ten of which were cranial motoneurons located in the facial (FN) and ambiguus (NA) nuclei. Two interneurons were labeled in the Bötzinger complex (BOT) and the ventral respiratory group (VRG) rostral to the obex, and the remaining four in the VRG caudal to the obex. All the interneurons had extensive intramedullary collaterals within the ventrolateral medulla. Terminal-like boutons were distributed ventral to the NA at the level of the BOT, both ventral to and within the NA at the level rostral to the obex and largely within the cell column tentatively designed as the ambiguous-retroambiguus complex (NA/NRA) caudal to the obex. The four interneurons in the NA/NRA had axons projecting to the spinal cord as well. The extensive intramedullary projections suggest that these E-DEC interneurons of the BOT and the VRG play a significant role in respiration. The simultaneous projections from the caudal E-DEC neurons to both the spinal cord and the NA suggest that these neurons also play integrative roles in non-respiratory behaviors including vocalization, swallowing and defecation.}
}
@article{BAPTISTA1998445,
title = {Congenital neuroblastoma in a bot born to a woman with bipolar disorder treated with carbamazepine during pregnancy},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {22},
number = {3},
pages = {445-454},
year = {1998},
issn = {0278-5846},
doi = {https://doi.org/10.1016/S0278-5846(98)00016-5},
url = {https://www.sciencedirect.com/science/article/pii/S0278584698000165},
author = {Trino Baptista and Hilarion Araujo and Pedro Rada and Luis Hernández},
keywords = {bipolar disorders, carbamazepine, carcinogenesis, neuroblastoma, pregnancy, teratogenesis}
}
@article{CALEFATO20191,
title = {A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem},
journal = {Information and Software Technology},
volume = {114},
pages = {1-20},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301216},
author = {Fabio Calefato and Filippo Lanubile and Bogdan Vasilescu},
keywords = {Personality traits, Large-scale distributed projects, Ecosystems, Apache, Big five, Five-Factor model, Open source software, Human aspects, Psychometric analysis, Computational personality detection},
abstract = {Context
Large-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.
Objective
We aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global software engineering — has been found to influence positively the result of code reviews in distributed projects.
Method
In this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.
Results
We find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.
Conclusion
Overall, our findings reinforce the need for future studies on human factors in software engineering to use psychometric tools to control for differences in developers’ personalities.}
}
@article{ASRI201937,
title = {An empirical study of sentiments in code reviews},
journal = {Information and Software Technology},
volume = {114},
pages = {37-54},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301387},
author = {Ikram El Asri and Noureddine Kerzazi and Gias Uddin and Foutse Khomh and M.A. {Janati Idrissi}},
keywords = {Empirical software engineering, Code review, Sentiment analysis, Opinion mining, Affective analysis, Propensity score matching},
abstract = {Context
Modern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.
Objective
In this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the code review process.
Method
Based on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.
Results
We found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (e.g., core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.
Conclusion
Through this work, we provide evidences that text-based sentiments have an impact on the duration of the code review process as well as the acceptance or rejection of the suggested changes.}
}
@article{DOLMANS2005897,
title = {Efficacy of in vitro fertilization after chemotherapy},
journal = {Fertility and Sterility},
volume = {83},
number = {4},
pages = {897-901},
year = {2005},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2004.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S001502820403170X},
author = {Marie-Madeleine Dolmans and Dominique Demylle and Belen Martinez-Madrid and Jacques Donnez},
keywords = {Chemotherapy, cancer, IVF, embryo quality, cryopreservation},
abstract = {Objective
To evaluate if in vitro fertilization (IVF) with embryo cryopreservation can be proposed to patients immediately after one or two regimens of chemotherapy.
Design
Retrospective study.
Setting
Academic research center and IVF unit.
Patient(s)
Eleven young patients diagnosed with cancer between September 1999 and April 2003 who wanted to preserve their fertility via IVF.
Intervention(s)
Stimulation and IVF before or soon after chemotherapy treatment.
Main outcome measure(s)
The number and quality of embryos obtained after stimulation in cancer patients undergoing IVF before or soon after chemotherapeutic treatment.
Result(s)
Four patients underwent IVF in the interval between two regimens of chemotherapy. Two of them had no follicular development; one underwent follicular puncture but no oocytes were retrieved; and, in one, six oocytes were harvested but only one good quality embryo was obtained. In the seven patients who underwent IVF before starting chemotherapy, between 4 and 11 embryos were obtained per patient, the majority being good quality embryos.
Conclusion(s)
Because the efficacy of IVF is dramatically reduced after even one round of chemotherapy, IVF should be performed before chemotherapy. For those who require immediate chemotherapy, ovarian tissue cryopreservation and/or oocyte cryopreservation could be used before treatment.}
}
@article{JIN2017S2210,
title = {P3.01-027 TET2 Mutation as a Novel Mechanism of Acquired Resistance to EGFR TKIs Identified by a Mutational Profiling Using NGS},
journal = {Journal of Thoracic Oncology},
volume = {12},
number = {11, Supplement 2},
pages = {S2210-S2211},
year = {2017},
note = {IASLC 18th World Conference on Lung Cancer},
issn = {1556-0864},
doi = {https://doi.org/10.1016/j.jtho.2017.09.1468},
url = {https://www.sciencedirect.com/science/article/pii/S1556086417322037},
author = {Y. Jin and X. Hu and M. Chen and X. Yu}
}
@article{FERHI2008125,
title = {AGE AS ONLY PREDICTIVE FACTOR FOR SUCCESSFUL SPERM RECOVERY IN PATIENTS WITH KLINEFELTER'S SYNDROME},
journal = {European Urology Supplements},
volume = {7},
number = {3},
pages = {125},
year = {2008},
note = {23rd Annual Congress of the European Association of Urology},
issn = {1569-9056},
doi = {https://doi.org/10.1016/S1569-9056(08)60217-1},
url = {https://www.sciencedirect.com/science/article/pii/S1569905608602171},
author = {K. Ferhi and R. Avakian and J.F. Griveau and D. Lelannou and J.J. Patard}
}
@article{SAHAR2021110852,
title = {How are issue reports discussed in Gitter chat rooms?},
journal = {Journal of Systems and Software},
volume = {172},
pages = {110852},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110852},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302429},
author = {Hareem Sahar and Abram Hindle and Cor-Paul Bezemer},
keywords = {Developer discussions, Gitter, Issue reports},
abstract = {Informal communication channels like mailing lists, IRC and instant messaging play a vital role in open source software development by facilitating communication within geographically diverse project teams e.g., to discuss issue reports to facilitate the bug-fixing process. More recently, chat systems like Slack and Gitter have gained a lot of popularity and developers are rapidly adopting them. Gitter is a chat system that is specifically designed to address the needs of GitHub users. Gitter hosts project-based asynchronous chats which foster frequent project discussions among participants. Developer discussions contain a wealth of information such as the rationale behind decisions made during the evolution of a project. In this study, we explore 24 open source project chat rooms that are hosted on Gitter, containing a total of 3,133,106 messages and 14,096 issue references. We manually analyze the contents of chat room discussions around 457 issue reports. The results of our study show the prevalence of issue discussions on Gitter, and that the discussed issue reports have a longer resolution time than the issue reports that are never brought on Gitter.}
}
@article{DONALDSON198215,
title = {Rate of oxygen effect reactions in irradiated barley seeds},
journal = {Environmental and Experimental Botany},
volume = {22},
number = {1},
pages = {15-21},
year = {1982},
issn = {0098-8472},
doi = {https://doi.org/10.1016/0098-8472(82)90004-1},
url = {https://www.sciencedirect.com/science/article/pii/0098847282900041},
author = {E. Donaldson and R.A. Nilan and C.F. Konzak},
abstract = {The influence of concentrations of oxygen and radiation-induced oxygen sensitive sites on their rates of reactions in barley seeds was investigated. Himalaya (C.I. 620) barley seeds were adjusted to 9.9% water content irradiated with 60Co gamma rays and soaked at 0°C in distilled water bubbled with oxygen and nitrogen gas mixtures containing 0.0, 12.5, 25, 50 and 100% oxygen. Treatment effects were measured as M1 seedling injury. In one experiment, irradiated seeds were initially soaked in oxygen-saturate water, then transferred to O2-free water (nitrogen soaking) at selected time intervals. An increase in oxygen enhancement (OE) as measured by seedling injury is obtained with increased O2-soaking duration. A measure of the reaction rate between O2 and O2-sensitive sites (OSS) is thus obtained. This reaction is radiation exposure dependent. A reverse experiment (initial nitrogen soaking with transfer to O2 soaking at selected intervals) gives a measure of the lifetime of the OSS in water (quenching reaction). The same experimental plan is followed in two other experiments where the oxygen concentration in the gas phase of the soaking solution (OC) is variable. These experiments provide a measure of the influence of OC on the OE and OSS quenching reactions. A single radiation exposure was used. These results are demonstrated in four figures and numerical results and “reaction rates” are in two tables. The quenching reactions were independent of radiation exposure and OC. At low to intermediate OC the quenching reactions terminated the O2-OSS reaction. At 100% OC the O2-OSS reaction was two to three times faster and started three times sooner than the quenching reaction.}
}
@article{CHEN2016170,
title = {Controlled volatile release of structured emulsions based on phytosterols crystallization},
journal = {Food Hydrocolloids},
volume = {56},
pages = {170-179},
year = {2016},
issn = {0268-005X},
doi = {https://doi.org/10.1016/j.foodhyd.2015.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S0268005X15301673},
author = {Xiao-Wei Chen and Jian Guo and Jin-Mei Wang and Shou-Wei Yin and Xiao-Quan Yang},
keywords = {Controlled volatile release, Phytosterols, Crystallization, Structured emulsion, -sitosterol, OSA starch},
abstract = {Flavor is one of the most important criteria for consumer acceptance of food products, especially for low-fat food emulsions. In this study, we prepared structured flavoring oil-in-water (O/W) emulsions based on the crystallization behavior of β-sitosterol (Sito), a functional phytosterol, in the presence of emulsifier (sodium caseinate, SC and octenylsuccinate starch, OSS), These structured emulsions improved colloidal stability during long-term storage and delayed volatiles release under real time dynamic condition. However, the equilibrium static headspace analysis did not show significant differences in the affinities of hydrophobic volatile compounds with pure constitutes of unstructured and structured emulsions. This highlighted the importance of structural properties of the O/W interface in volatile release modulation. A modified gel trapping technology (GTT) combined with polarized light microscopy (PLM) and confocal laser scanning microscope (CLSM) were applied to characterize the microstructure at the oil-water interface, and it clearly showed the formation of a novel Sito crystal/OSA starch complex interface for OSS stabilized structured emulsion. This unique interfacial microstructure might contribute to the strong retention of volatile compounds due to steric barrier and enhanced affinity to those lipophilic volatiles. The formulated flavor emulsion with controlled volatile release profile was successfully prepared by simply blending the unstructured and structured flavoring emulsions. This work provides indications for potential applications of the formulation design in flavor emulsions and phytosterols structured emulsion as novel aroma delivery systems to improve flavor perception.}
}
@incollection{VACCA2005217,
title = {13 - External Servers Protection},
editor = {John R. Vacca and Scott R. Ellis},
booktitle = {Firewalls},
publisher = {Digital Press},
address = {Burlington},
pages = {217-228},
year = {2005},
isbn = {978-1-55558-297-5},
doi = {https://doi.org/10.1016/B978-155558297-5/50015-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781555582975500153},
author = {John R. Vacca and Scott R. Ellis},
abstract = {Publisher Summary
This chapter focuses on traffic problem and data security. Being “aware” of the precautions that need to be taken does not serve anyone; security is less about understanding and all about taking action. Access lists should be treated with the same level of security as government threat codes. They should change frequently (daily), should be token driven, and the storage of critical information (including and especially e-mail addresses) should be encrypted at the data level. Merely protecting these items from access is not enough. Special encryption algorithms are required for even simple examination of the data. Only the full gamut of security measures can ensure the protection of critical data―data that, if compromised and released to the public, may cause the annihilation of public trust when the data fall into the wrong hands. Such flagrant heists as the AOL breach give spammers and hackers a veritable gold mine of information. AOL users are, typically, novice users and are the most likely to fall prey to spam scams, or “scam mail.” By carefully and thoroughly monitoring traffic and engaging the full capabilities of access control lists (ACLs), application firewalls, and inherent database security, much of the problematic traffic present on the Internet today can have been avoided.}
}
@article{NDIEGE201465,
title = {The impacts of financial linkage on sustainability of less-formal financial institutions: Experience of savings and credit co-operative societies in Tanzania},
journal = {Journal of Co-operative Organization and Management},
volume = {2},
number = {2},
pages = {65-71},
year = {2014},
issn = {2213-297X},
doi = {https://doi.org/10.1016/j.jcom.2014.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2213297X14000263},
author = {Benson Otieno Ndiege and Xuezhi Qin and Isaac Kazungu and John Moshi},
keywords = {Financial linkage, SACCOS, Sustainability, Tanzania},
abstract = {The developing economies are experiencing a growing trend of financial Linkage between formal and less-formal financial institutions. Normally, less-formal financial institutions receive loanable funds from formal financial institutions as an approach to meet their financing deficit, while formal financial institutions engage in linkage as a mean to expand business. The main concern of stakeholders regarding this practice is how such linkage can affect the performance of the less-formal financial institutions. In Tanzania, the Savings and Credit Co-operative Societies (SACCOS) are the most used less-formal financial institutions which are also highly involved in financial linkage. In this study therefore, we used Tanzania SACCOS’ financial statement data, for the period of 2004–2011, and panel data regression model to examine the relationship between financial linkage (measured as financial dependency ratio) and sustainability (measured as Operational Self Sufficiency) of less-formal financial institutions. The findings suggest that the higher the level of financial linkage the more the SACCOS become unsustainable. Implying that, to be sustainable institutions, the SACCOS should try keep away from the use of external funds in their loan portfolio.}
}
@article{MOMMERS2008125,
title = {EFFICACY AND SAFETY OF ETONOGESTREL AND TESTOSTERONE UNDECANOATE FOR MALE HORMONAL CONTRACEPTION},
journal = {European Urology Supplements},
volume = {7},
number = {3},
pages = {125},
year = {2008},
note = {23rd Annual Congress of the European Association of Urology},
issn = {1569-9056},
doi = {https://doi.org/10.1016/S1569-9056(08)60216-X},
url = {https://www.sciencedirect.com/science/article/pii/S156990560860216X},
author = {E. Mommers and W.M. Kersemaekers and J. Elliesen and E.J.H. Meuleman and M. Kepers and D. Apter and H.M. Behre and J. Beynon and P.M. Bouloux and A. Costantino and H.P. Gerbershagen and L. Grønlund and D. Heger-Mahn and I. Huhtaniemi and E. Koldewijn and C. Lange and S. Lindenberg and C. Meriggiola and P. Mulders and E. Nieschlag and A. Perheentupa and A. Solomon and L. Väisälä and F. Wu and M. Zitzmann}
}
@article{NEVES2017229,
title = {Future mode of operations for 5G – The SELFNET approach enabled by SDN/NFV},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {229-246},
year = {2017},
note = {SI: Standardization SDN&NFV},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916302434},
author = {Pedro Neves and Rui Calé and Mário Costa and Gonçalo Gaspar and Jose Alcaraz-Calero and Qi Wang and James Nightingale and Giacomo Bernini and Gino Carrozzo and Ángel Valdivieso and Luis Javier {García Villalba} and Maria Barros and Anastasius Gravas and José Santos and Ricardo Maia and Ricardo Preto},
keywords = {Network Functions Virtualization, Software Defined Networking, 5G, Self-Organizing Networks, Autonomic management},
abstract = {The 5G infrastructure initiative in Europe115G Infrastructure Public Private Partnership, [Online]. Available here: https://5g-ppp.eu/ has agreed a number of challenging key performance indicators (KPIs) to significantly enhance the user experience and support a number of use cases with very demanding requirements on the network infrastructure. At the same time there is high pressure on the reduction of the operational expenditure (OPEX). A contribution to meeting the KPIs and to reduce OPEX is to evolve the management of the network into a fully autonomic and intelligent framework. Based on advanced technologies, such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV), the EU H2020 project SELFNET (https://selfnet-5g.eu/) is proposing an advanced network management framework to achieve these objectives.}
}
@article{ZHONG20141,
title = {Constructing a meta-model for assembly tolerance types with a description logic based approach},
journal = {Computer-Aided Design},
volume = {48},
pages = {1-16},
year = {2014},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2013.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0010448513002078},
author = {Yanru Zhong and Yuchu Qin and Meifa Huang and Wenlong Lu and Liang Chang},
keywords = {Meta-model, Assembly tolerance types, Semantic interoperability, Description logics},
abstract = {There is a critical requirement for semantic interoperability among heterogeneous computer-aided tolerancing (CAT) systems with the sustainable growing demand of collaborative product design. But current data exchange standard for exchanging tolerance information among these systems can only exchange syntaxes and cannot exchange semantics. Semantic interoperability among heterogeneous CAT systems is difficult to be implemented only with this standard. To address this problem, some meta-models of tolerance information supporting semantic interoperability and an interoperability platform based on these meta-models should be constructed and developed, respectively. This paper mainly focuses on the construction of a meta-model for assembly tolerance types with a description logic ALC(D) based approach. Description logics, a family of knowledge representation languages for authoring ontologies, are well-known for having rigorous logic-based semantics which supports semantic interoperability. ALC(D) can provide a formal method to describe the research objects and the relations among them. In this formal method, constraint relations among parts, assembly feature surfaces and geometrical features are defined with some ALC(D) assertional axioms, and the meta-model of assembly tolerance types is constructed through describing the spatial relations between geometrical features with some ALC(D) terminological axioms. Besides, ALC(D) can also provide a highly efficient reasoning algorithm to automatically detect the inconsistency of the knowledge base, a finite set of assertional and terminological axioms. With this reasoning algorithm, assembly tolerance types for each pair of geometrical features are generated automatically through detecting the inconsistencies of the knowledge base. An application example is provided to illustrate the process of generating assembly tolerance types.}
}
@article{SAFAEIPOUR2020101707,
title = {On data-driven curation, learning, and analysis for inferring evolving internet-of-Things (IoT) botnets in the wild},
journal = {Computers & Security},
volume = {91},
pages = {101707},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101707},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819302445},
author = {Morteza {Safaei Pour} and Antonio Mangino and Kurt Friday and Matthias Rathbun and Elias Bou-Harb and Farkhund Iqbal and Sagar Samtani and Jorge Crichigno and Nasir Ghani},
keywords = {Data science, Cyber forensics, Internet-of-things, IoT Security, Internet measurements},
abstract = {The insecurity of the Internet-of-Things (IoT) paradigm continues to wreak havoc in consumer and critical infrastructures. The highly heterogeneous nature of IoT devices and their widespread deployments has led to the rise of several key security and measurement-based challenges, significantly crippling the process of collecting, analyzing and correlating IoT-centric data. To this end, this paper explores macroscopic, passive empirical data to shed light on this evolving threat phenomena. The proposed work aims to classify and infer Internet-scale compromised IoT devices by solely observing one-way network traffic, while also uncovering, reporting and thoroughly analyzing “in the wild” IoT botnets. To prepare a relevant dataset, a novel probabilistic model is developed to cleanse unrelated traffic by removing noise samples (i.e., misconfigured network traffic). Subsequently, several shallow and deep learning models are evaluated in an effort to train an effective multi-window convolutional neural network. By leveraging active and passing measurements when generating the training dataset, the neural network aims to accurately identify compromised IoT devices. Consequently, to infer orchestrated and unsolicited activities that have been generated by well-coordinated IoT botnets, hierarchical agglomerative clustering is employed by scrutinizing a set of innovative and efficient network feature sets. Analyzing 3.6 TB of recently captured darknet traffic revealed a momentous 440,000 compromised IoT devices and generated evidence-based artifacts related to 350 IoT botnets. Moreover, by conducting thorough analysis of such inferred campaigns, we reveal their scanning behaviors, packet inter-arrival times, employed rates and geo-distributions. Although several campaigns exhibit significant differences in these aspects, some are more distinguishable; by being limited to specific geo-locations or by executing scans on random ports besides their core targets. While many of the inferred botnets belong to previously documented campaigns such as Hide and Seek, Hajime and Fbot, newly discovered events portray the evolving nature of such IoT threat phenomena by demonstrating growing cryptojacking capabilities or by targeting industrial control services. To motivate empirical (and operational) IoT cyber security initiatives as well as aid in reproducibility of the obtained results, we make the source codes of all the developed methods and techniques available to the research community at large.}
}
@article{YANG2022108949,
title = {Center Prediction Loss for Re-identification},
journal = {Pattern Recognition},
volume = {132},
pages = {108949},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108949},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004290},
author = {Lu Yang and Yunlong Wang and Lingqiao Liu and Peng Wang and Yanning Zhang},
keywords = {Person re-identification, Loss, Deep metric learning},
abstract = {The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e., the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.}
}
@article{LARRANAGA2013356,
title = {Remarks on the analysis method for determining diffusion coefficient in ternary mixtures},
journal = {Comptes Rendus Mécanique},
volume = {341},
number = {4},
pages = {356-364},
year = {2013},
note = {10th International Meeting on Thermodiffusion},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2013.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1631072113000247},
author = {Miren Larrañaga and M. Mounir Bou-Ali and Daniel Soler and Manex Martinez-Agirre and Aliaksandr Mialdun and Valentina Shevtsova},
keywords = {Molecular diffusion, Ternary mixture, Sliding Symmetric Tubes, Fitting procedure},
abstract = {The objective of this work is the determination of diagonal and cross-diagonal molecular diffusion coefficients in a ternary mixture, using the ‘Sliding Symmetric Tubes’ (SST) technique. The analyzed mixture consists of two aromatics and one normal alkane (tetrahydronaphthalene–dodecane–isobutylbenzene) with an equal mass fraction for all components (1:1:1) at 25 °C. The analytical solution corresponding to the SST technique has been successfully derived. The different fitting procedures were utilized by two scientific teams to subtract diffusion coefficients from the experimentally measured time-dependent concentration field. None of the attempts provided reliable results for the data from a single experiment. The “simplex”-based methods display reasonable results assuming that cross-diagonal coefficients are close to zero, i.e. quasi-binary and diluted mixtures. The results obtained by “trust region method” are satisfactory if the initial guess is good. To achieve better results, it is necessary to increase the number of experimental data.}
}
@article{VERMA2022227,
title = {iNIDS: SWOT Analysis and TOWS Inferences of State-of-the-Art NIDS solutions for the development of Intelligent Network Intrusion Detection System},
journal = {Computer Communications},
volume = {195},
pages = {227-247},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422003371},
author = {Jyoti Verma and Abhinav Bhandari and Gurpreet Singh},
keywords = {Big data, Cloud computing, Deep learning, Intrusion Detection System, IDS, IoT, Machine learning, Network Intrusion Detection, NIDS, SWOT, TOWS},
abstract = {Introduction:
The growth of ubiquitous networked devices and the proliferation of geographically dispersed ‘Internet of Thing’ devices have exponentially increased network traffic. The socio-economical society is highly dependent on modern devices, and unavailability may lead to catastrophic results for even a short time. The less secure and heterogeneous devices in the public domain have shaped a cyber-attack surface in the cloud environment. Traditional approaches for Network Intrusion Detection Systems have proven ineffective and insufficient in defending against zero-day attacks.
Methods:
This article visited the advancements in the intrusion detection realm in the last five years and conducted a comprehensive retrospection of modern network intrusion detection systems. The authors have performed a comprehensive SWOT (Strength, Weakness, Opportunities, Threats) analysis of contemporary Network Intrusion Detection Systems in multiple technology dimensions, including big-data processing of high volume network traffic, machine learning, deep learning for self-learning machines, readiness for zero-day attacks, distributed processing, cost-effective solution, and ability to perform autonomous operations.
Results:
The paper turns SWOT analysis into TOWS inferences from the retrospective study for strategy formulation and features the attributes of a futuristic NIDS solution.
Discussion:
The article concludes with the discussion and future scope as the pinnacle of security solution development against zero-day attacks.}
}
@article{EZURE200341,
title = {Brainstem and spinal projections of augmenting expiratory neurons in the rat},
journal = {Neuroscience Research},
volume = {45},
number = {1},
pages = {41-51},
year = {2003},
issn = {0168-0102},
doi = {https://doi.org/10.1016/S0168-0102(02)00197-9},
url = {https://www.sciencedirect.com/science/article/pii/S0168010202001979},
author = {Kazuhisa Ezure and Ikuko Tanaka and Yoshiaki Saito},
keywords = {Bötzinger complex, Pons, Intracellular labeling, Neurobiotin, VRG, Axonal projection},
abstract = {There are two types of expiratory neurons with augmenting firing patterns (E-AUG neurons), those in the Bötzinger complex (BOT) and those in the caudal ventral respiratory group (cVRG). We studied their axonal projections morphologically using intracellular labeling of single E-AUG neurons with Neurobiotin, in anesthetized, paralyzed and artificially-ventilated rats. BOT E-AUG neurons (n=11) had extensive axonal projections to the brainstem, but E-AUG neurons (n=5) of the cVRG sent axons that descended the contralateral spinal cord without medullary collaterals. In addition to these somewhat expected characteristics, the present study revealed a number of new projection patterns of the BOT E-AUG neurons. First, as compared with the dense projections to the ipsilateral brainstem, those to the contralateral side were sparse. Second, several BOT E-AUG neurons sent long ascending collaterals to the pons, which included an axon that reached the ipsilateral parabrachial and Kölliker–Fuse nuclei and distributed boutons. Third, conspicuous projections from branches of these ascending collaterals to the area dorsolateral to the facial nucleus were found. Thus, the present study has shown an anatomical substrate for the extensive inhibitory projections of single BOT E-AUG neurons to the areas spanning the bilateral medulla and the pons.}
}
@article{BALAREZO2022101065,
title = {A survey on DoS/DDoS attacks mathematical modelling for traditional, SDN and virtual networks},
journal = {Engineering Science and Technology, an International Journal},
volume = {31},
pages = {101065},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001944},
author = {Juan Fernando Balarezo and Song Wang and Karina Gomez Chavez and Akram Al-Hourani and Sithamparanathan Kandeepan},
keywords = {Networks security, Attack modelling, Distributed Denial of Service (DDoS), Software Defined Networks (SDN), Virtual networks, Traditional networks},
abstract = {Denial of Service and Distributed Denial of Service (DoS/DDoS) attacks have been one of the biggest threats against communication networks and applications throughout the years. Modelling DoS/DDoS attacks is necessary to get a better understanding of their behaviour at each step of the attack process, from the Botnet recruitment up to the dynamics of the attack. A deeper understanding of DoS/DDoS attacks would lead to the development of more efficient solutions and countermeasures to mitigate their impact. In this survey, we present a classification approach for existing DoS/DDoS models in different kinds of networks; traditional networks, Software Defined Networks (SDN) and virtual networks. In addition, this article provides a thorough review and comparison of the existing attack models, in particular we explain, analyze and simulate different aspects of three prominent models; congestion window, queuing, and epidemic models (same model used for corona virus spread analysis). Furthermore, we quantify the damage of DoS/DDoS attacks at three different levels; protocol (Transmission Control Protocol-TCP), device’s resources (bandwidth, CPU, memory), and network (infection and recovery speed).}
}
@article{YAACOUB2023280,
title = {Ethical hacking for IoT: Security issues, challenges, solutions and recommendations},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {3},
pages = {280-308},
year = {2023},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667345223000238},
author = {Jean-Paul A. Yaacoub and Hassan N. Noura and Ola Salman and Ali Chehab},
keywords = {Internet-of-things (IoT), IoT ethical hacking, IoT penetration testing, Internet of ethical hacking things (IoEHT), IoT cyber-security},
abstract = {In recent years, attacks against various Internet-of-Things systems, networks, servers, devices, and applications witnessed a sharp increase, especially with the presence of 35.82 billion IoT devices since 2021; a number that could reach up to 75.44 billion by 2025. As a result, security-related attacks against the IoT domain are expected to increase further and their impact risks to seriously affect the underlying IoT systems, networks, devices, and applications. The adoption of standard security (counter) measures is not always effective, especially with the presence of resource-constrained IoT devices. Hence, there is a need to conduct penetration testing at the level of IoT systems. However, the main issue is the fact that IoT consists of a large variety of IoT devices, firmware, hardware, software, application/web-servers, networks, and communication protocols. Therefore, to reduce the effect of these attacks on IoT systems, periodic penetration testing and ethical hacking simulations are highly recommended at different levels (end-devices, infrastructure, and users) for IoT, and can be considered as a suitable solution. Therefore, the focus of this paper is to explain, analyze and assess both technical and non-technical aspects of security vulnerabilities within IoT systems via ethical hacking methods and tools. This would offer practical security solutions that can be adopted based on the assessed risks. This process can be considered as a simulated attack(s) with the goal of identifying any exploitable vulnerability or/and a security gap in any IoT entity (end devices, gateway, or servers) or firmware.}
}
@article{MJAAVATTEN1992263,
title = {Operator Support System for Fertilizer Plant},
journal = {IFAC Proceedings Volumes},
volume = {25},
number = {4},
pages = {263-268},
year = {1992},
note = {IFAC Symposium on On-line Fault Detection and Supervision in the Chemical Process Industries, Newark, Delaware, 22-24 April},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)50252-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017502529},
author = {A. Mjaavatten and S. Saelid},
keywords = {Operator support system, production control, expert systems, chemical industry, estimation, pollution, diagnosis},
abstract = {Norsk Hydro a.s. is designing an operator support system to be implemented in a compound fertilizer plant by the summer of 1992. The goal of the system is to help operators keep process upsets to a minimum, thus reducing the total pollution from the plant as well as ensuring more stable product quality. This will be done by giving early warnings of process conditions that may lead to undesirable effects at a later time and by assisting the operators in tracing process upsets to their root cause. A combination of qualitative (rule-based) and quantitative (model-based) methods are used. The diagnosis is performed by a topological search from the detection point to the probable cause. The search is directed by automatic checking of process streams for unacceptable deviations. With few exceptions, rules are independent of the specific configuration, thus minimising the work needed after process modifications.}
}
@incollection{FARMER2017347,
title = {Chapter 15 - Production Operations},
editor = {James Farmer and Brian Lane and Kevin Bourg and Weyl Wang},
booktitle = {FTTx Networks},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {347-376},
year = {2017},
isbn = {978-0-12-420137-8},
doi = {https://doi.org/10.1016/B978-0-12-420137-8.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124201378000159},
author = {James Farmer and Brian Lane and Kevin Bourg and Weyl Wang},
keywords = {Build-operate-transfer, Configuration management, Equipment vendor, Lifecycle, Network documentation, Operations team, Production engineering, Technical support, Troubleshooting},
abstract = {This chapter provides information for operating the production network in steady-state production with a focus on the lifecycle of the network and the role of the Operations team in maintaining and optimizing the network. Several models for building the network are detailed, and key processes used to operate the network are enumerated and discussed.}
}
@article{SULUN2021106455,
title = {RSTrace+: Reviewer suggestion using software artifact traceability graphs},
journal = {Information and Software Technology},
volume = {130},
pages = {106455},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106455},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300021},
author = {Emre Sülün and Eray Tüzün and Uğur Doğrusöz},
keywords = {Suggesting reviewers, Reviewer recommendation, Graph mining, Software traceability, Pull-request review, Modern code review},
abstract = {Context:
Various types of artifacts (requirements, source code, test cases, documents, etc.) are produced throughout the lifecycle of a software. These artifacts are connected with each other via traceability links that are stored in modern application lifecycle management repositories. Throughout the lifecycle of a software, various types of changes can arise in any one of these artifacts. It is important to review such changes to minimize their potential negative impacts. To make sure the review is conducted properly, the reviewer(s) should be chosen appropriately.
Objective:
We previously introduced a novel approach, named RSTrace, to automatically recommend reviewers that are best suited based on their familiarity with a given artifact. In this study, we introduce an advanced version of RSTrace, named RSTrace+ that accounts for recency information of traceability links including practical tool support for GitHub.
Methods:
In this study, we conducted a series of experiments on finding the appropriate code reviewer(s) using RSTrace+ and provided a comparison with the other code reviewer recommendation approaches.
Results:
We had initially tested RSTrace+ on an open source project (Qt 3D Studio) and achieved a top-3 accuracy of 0.89 with an MRR (mean reciprocal ranking) of 0.81. In a further empirical evaluation of 40 open source projects, we compared RSTrace+ with Naive-Bayes, RevFinder and Profile based approach, and observed higher accuracies on the average.
Conclusion:
We confirmed that the proposed reviewer recommendation approach yields promising top-k and MRR scores on the average compared to the existing reviewer recommendation approaches. Unlike other code reviewer recommendation approaches, RSTrace+ is not limited to recommending reviewers for source code artifacts and can potentially be used for recommending reviewers for other types of artifacts. Our approach can also visualize the affected artifacts and help the developer to make assessments of the potential impacts of change to the reviewed artifact.}
}
@article{WESCOTT1987503,
title = {Anthelmintics for horses},
journal = {International Journal for Parasitology},
volume = {17},
number = {2},
pages = {503-510},
year = {1987},
issn = {0020-7519},
doi = {https://doi.org/10.1016/0020-7519(87)90126-3},
url = {https://www.sciencedirect.com/science/article/pii/0020751987901263},
author = {Richard B. Wescott},
abstract = {Modern equine anthelmintics can be divided into at least seven principal groups based on mode of action, i.e., benzimidazoles, pro-benzimidazoles, imidothiazoles, tetrahydropyrimidines, organophosphates, piperazines, and avermectins. The spectrum of activity of these drugs varies and resistance of cyathostomes to benzimidazole and pro-benzimidazole drugs has been observed in many areas. Cross resistance with other groups has not been reported in equines, however. Control is dependent upon understanding the capabilities of anthelmintics and the epizootiology of the important parasites, e.g., large strongyles, cyathostomes, ascarids and bots. Development of effective programs in a given region should be based on routine fecal analysis to ensure that treatment schedule and products selected are adequate for local climatic conditions and management methods.}
}
@article{SHEN20143,
title = {Analysis on the acceptance of Global Trust Management for unwanted traffic control based on game theory},
journal = {Computers & Security},
volume = {47},
pages = {3-25},
year = {2014},
note = {Trust in Cyber, Physical and Social Computing},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2014.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167404814000492},
author = {Yue Shen and Zheng Yan and Raimo Kantola},
keywords = {Game theory, Social dilemma, Public goods game, Trust management, Unwanted traffic control, Equilibrium},
abstract = {The Internet has witnessed an incredible growth in its pervasive use and brought unprecedented convenience to its users. However, an increasing amount of unwanted traffic, such as spam and malware, severely burdens both users and Internet service providers (ISPs), which arouses wide public concern. A Global Trust Management (GTM) system was proposed and demonstrated to be accurate, robust and effective on unwanted traffic control in our previous work (Yan et al., 2011, Yan et al., 2013). But its acceptance by network entities (ISPs and hosts) is crucial to its practical deployment and final success. In this paper, we investigate the acceptance conditions of the GTM system using game theory. Considering the selfish nature of network entities, we address our problem as a social dilemma. To enhance cooperation among network entities, a public-goods-based GTM game is formulated with a trust-based punishment mechanism that can provide the incentives of behaving cooperatively for network entities. Meanwhile, the conditions of the adoption of GTM system are figured out. We also carry out a number of simulations to illustrate the acceptance conditions of the GTM system in practical deployment, and show the effectiveness of the trust-based punishment mechanism. Furthermore, suggestions for ISPs cooperating with antivirus vendors are put forward.}
}
@article{TANOH2022114125,
title = {The contribution of tipping fees to the operation, maintenance, and management of fecal sludge treatment plants: The case of Ghana},
journal = {Journal of Environmental Management},
volume = {303},
pages = {114125},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114125},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721021873},
author = {Rebecca Tanoh and Josiane Nikiema and Zipporah Asiedu and Nilanthi Jayathilake and Olufunke Cofie},
keywords = {Public private partnership, Co-treatment, Developing countries, Cost recovery, Waste stabilization ponds},
abstract = {Globally, collection of tipping fees is being promoted as a solution to sustain the operation of fecal sludge treatment plants (FSTPs). Currently, there are six large-scale FSTPs in Ghana, of which five were in operation in June 2017. In Kumasi, Sekondi-Takoradi and Tamale, fecal sludge (FS) is co-treated with landfill leachate using waste stabilization ponds (WSPs). In Tema and Accra, FS is treated using WSPs and a mechanical dewatering system coupled with an upflow anaerobic sludge blanket (UASB). The focus of this study is FSTPs and to assess how, and if, the tipping fees set by the municipalities could enable cost recovery to sustain their long-term operation. Using a questionnaire survey to interview plant managers from the public and private sectors, and directors of waste management departments, we found that the overall average operation, maintenance and management (OM&M) costs per 1000 m3 of treated waste (FS or FS + leachate) in 2017 were USD89 in Kumasi, USD150 in Tamale, USD179 in Tema, USD244 in Sekondi-Takoradi and USD1,743 in Accra. There were important disparities between FSTPs due to their scale, age, and level of treatment and monitoring. Currently, most FSTPs charge tipping fees that range between USD310 and USD530/1000 m3 of FS, averaging USD421 ± 98/1000 m3 of FS discharged at FSTPs. Our study also showed that the OM&M costs of large-scale intensive FSTPs cannot be sustained by relying solely on tipping fees. However, there could be potential to cover the routine expenditures associated with operating smaller FSTPs that relying on WSP technologies.}
}
@article{SAIDANI2021106618,
title = {On the impact of Continuous Integration on refactoring practice: An exploratory study on TravisTorrent},
journal = {Information and Software Technology},
volume = {138},
pages = {106618},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106618},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000914},
author = {Islem Saidani and Ali Ouni and Mohamed Wiem Mkaouer and Fabio Palomba},
keywords = {Continuous integration, Refactoring, Exploratory study, Mining software repositories, Multiple Regression Analysis},
abstract = {Context:
The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption.
Objective:
We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied.
Method:
We collect a corpus of 99,545 commits and 89,926 refactoring operations extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA).
Results:
Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context.
Conclusion:
Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems.}
}
@article{JANAMANCHI2009457,
title = {The State and Profile of Open Source Software Projects in health and medical informatics},
journal = {International Journal of Medical Informatics},
volume = {78},
number = {7},
pages = {457-472},
year = {2009},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2009.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505609000318},
author = {Balaji Janamanchi and Evangelos Katsamakas and Wullianallur Raghupathi and Wei Gao},
keywords = {Health and medical informatics, Open source software (OSS), Sponsorship, License type},
abstract = {Purpose
Little has been published about the application profiles and development patterns of open source software (OSS) in health and medical informatics. This study explores these issues with an analysis of health and medical informatics related OSS projects on SourceForge, a large repository of open source projects.
Methodology
A search was conducted on the SourceForge website during the period from May 1 to 15, 2007, to identify health and medical informatics OSS projects. This search resulted in a sample of 174 projects. A Java-based parser was written to extract data for several of the key variables of each project. Several visually descriptive statistics were generated to analyze the profiles of the OSS projects.
Results
Many of the projects have sponsors, implying a growing interest in OSS among organizations. Sponsorship, we discovered, has a significant impact on project success metrics. Nearly two-thirds of the projects have a restrictive license type. Restrictive licensing may indicate tighter control over the development process. Our sample includes a wide range of projects that are at various stages of development (status). Projects targeted towards the advanced end user are primarily focused on bio-informatics, data formats, database and medical science applications.
Conclusion
We conclude that there exists an active and thriving OSS development community that is focusing on health and medical informatics. A wide range of OSS applications are in development, from bio-informatics to hospital information systems. A profile of OSS in health and medical informatics emerges that is distinct and unique to the health care field. Future research can focus on OSS acceptance and diffusion and impact on cost, efficiency and quality of health care.}
}
@article{PARK2024105496,
title = {Participatory Framework for Urban Pluvial Flood Modeling in the Digital Twin Era},
journal = {Sustainable Cities and Society},
volume = {108},
pages = {105496},
year = {2024},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105496},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724003238},
author = {Samuel Park and Jaekyoung Kim and Yejin Kim and Junsuk Kang},
keywords = {Citizen participation, Crowdsourcing platform, Digitalization, Rapid flood spreading model (RFSM), 3D inundation mapping, Volunteered geographic information (VGI)},
abstract = {The recent advancement in digital twin technology, which creates virtual replicas of real-world processes, offers an interactive testbed for understanding and predicting environmental changes. As pluvial flood damage escalates globally in urban areas, there remains a gap in understanding the most effective collaboration between governments and local residents for sustainable flood risk management. To address this gap, we develop a participatory framework for urban pluvial flood modeling, incorporating open source software, virtual reality, minimum viable product, and gamification components. This framework engages citizens in every phase of the participatory modeling process, from input data preparation, through hydrological model construction, to model verification, and experiments. We present a case study on the recurring pluvial flood damages in South Korea's Gangnam region, demonstrating the practical implications of an interactive, web-based crowdsourcing platform to leverage community engagement and local knowledge. The results underscore the evidence of a proactive role for citizens, not merely as recipients of disaster information but as key contributors collaborating with a range of stakeholders in stormwater management and modeling. Combining digital twin technology with citizen participation can empower informed decision-making and collective actions in the evolving digital era, leading to a disaster-resilient community.}
}
@incollection{2011247,
editor = {Will Gragido and John Pirc},
booktitle = {Cybercrime and Espionage},
publisher = {Syngress},
address = {Boston},
pages = {247-254},
year = {2011},
isbn = {978-1-59749-613-1},
doi = {https://doi.org/10.1016/B978-1-59749-613-1.00019-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781597496131000194}
}
@article{JARCZYK201832,
title = {Surgical teams on GitHub: Modeling performance of GitHub project development processes},
journal = {Information and Software Technology},
volume = {100},
pages = {32-46},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S095058491730304X},
author = {Oskar Jarczyk and Szymon Jaroszewicz and Adam Wierzbicki and Kamil Pawlak and Michal Jankowski-Lorek},
keywords = {Open source software (OSS), Development process performance, Issue closure rate, Work centralization, Issue workflow, Surgical team},
abstract = {Context: Better methods of evaluating process performance of OSS projects can benefit decision makers who consider adoption of OSS software in a company. This article studies the closure of issues (bugs and features) in GitHub projects, which is an important measure of OSS development process performance and quality of support that project users receive from the developer team. Objective: The goal of this article is a better understanding of the factors that affect issue closure rates in OSS projects. Methodology: The GHTorrent repository is used to select a large sample of mature, active OSS projects. Using survival analysis, we calculate short-term, and long-term issue closure rates. We formulate several hypotheses regarding the impact of OSS project and team characteristics, such as measures of work centralization, measures that reflect internal project workflows, and developer social networks measures on issue closure rates. Based on the proposed features and several control features, a model is built that can predict issue closure rate. The model allows to test our hypotheses. Results: We find that large teams that have many project members have lower issue closure rates than smaller teams. Similarly, increased work centralization increases issue closure rates. While desirable social network characteristics have a positive impact on the amount of commits in a project, they do not have significant influence on issue closure. Conclusion: Overall, findings from empirical analysis support the classic notion of Brook’s – the “surgical team” – in the context of OSS project development process performance on GitHub. The model of issue closure rates proposed in this article is a first step towards an improved understanding and prediction of this important measure of OSS development process performance.}
}
@article{NGUYEN2022117267,
title = {DeepLib: Machine translation techniques to recommend upgrades for third-party libraries},
journal = {Expert Systems with Applications},
volume = {202},
pages = {117267},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117267},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422006388},
author = {Phuong T. Nguyen and Juri {Di Rocco} and Riccardo Rubei and Claudio {Di Sipio} and Davide {Di Ruscio}},
keywords = {Mining software repositories, Deep learning, Encoder–decoder neural network, Third-party libraries upgrade},
abstract = {To keep their code up-to-date with the newest functionalities as well as bug fixes offered by third-party libraries, developers often need to replace an old version of third-party libraries (TPLs) with a newer one. However, choosing a suitable version for a library to be upgraded is complex and susceptible to error. So far, Dependabot is the only tool that supports library upgrades; however, it targets only security fixes and singularly analyzes libraries without considering the whole set of related libraries. In this work, we propose DeepLib as a practical approach to learn upgrades for third-party libraries that have been performed by similar clients. Such upgrades are considered safe, i.e., they do not trigger any conflict, since, in the training clients, the libraries already co-exist without causing any compatibility or dependency issues. In this way, the upgrades provided by DeepLib allow developers to maintain a harmonious relationship with other libraries. By mining the development history of projects, we build migration matrices to train deep neural networks. Once being trained, the networks are then used to forecast the subsequent versions of the related libraries, exploiting the well-founded background related to the machine translation domain. As input, DeepLib accepts a set of library versions and returns a set of future versions to which developers should upgrade the libraries. The framework has been evaluated on two real-world datasets curated from the Maven Central Repository. The results show promising outcomes: DeepLib can recommend the next version for a library as well as a set of libraries under investigation. At its best performance, DeepLib gains a perfect match for several libraries, earning an accuracy of 1.0.}
}
@article{ZHANG2023105678,
title = {Nanoscale soil-water retention mechanism of unsaturated clay via MD and machine learning},
journal = {Computers and Geotechnics},
volume = {163},
pages = {105678},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105678},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X23004354},
author = {Zhe Zhang and Xiaoyu Song},
keywords = {Soil-water retention, Interfacial area, Unsaturated clay, Adsorption, Machine learning},
abstract = {In this article, we investigate the nanoscale soil-water retention mechanism of unsaturated clay through molecular dynamics and machine learning. Pyrophyllite was chosen due to its stable structure and as the precursor of other 2:1 clay minerals. A series of molecular dynamics simulations of clay at low degrees of saturation were conducted. Soil water was represented by a point cloud through the center-of-mass method. Water-air interface area was measured numerically by the alpha-shape method. The soil-water retention mechanism at the nanoscale was analyzed by distinguishing adsorptive pressure and capillary pressure at different mass water contents and considering the apparent capillary interface area (i.e., water-air interface area per unit water volume). The water number density profile was used to quantify the adsorption effect. A neural-network based machine learning technique was utilized to construct functional relationships among matric suction, the mass water content, and the apparent water-air interface area. Our numerical results have demonstrated from a nanoscale perspective that the adsorption effect is dominated by the van der Waals force and hydroxyl hydration between the clay surface and water. As the mass water content increases, the adsorption pressure decreases, and capillarity plays a prominent role in the soil-water retention mechanism at the nanoscale.}
}
@article{SLOBOGEAN2011248,
title = {Measuring shoulder injury function: Common scales and checklists},
journal = {Injury},
volume = {42},
number = {3},
pages = {248-252},
year = {2011},
note = {Assessing Patient Outcomes},
issn = {0020-1383},
doi = {https://doi.org/10.1016/j.injury.2010.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S0020138310007813},
author = {G.P. Slobogean and B.L. Slobogean},
keywords = {Shoulder, Fracture, Trauma, Outcome instrument, Psychometric, Validity, Reliability, Responsiveness},
abstract = {The increasing shift towards patient-centred healthcare has lead to an emergence of patient-reported outcome instruments to quantify functional outcomes in orthopaedic patients. Unfortunately, selecting an instrument for use in a shoulder trauma population is often problematic because most shoulder instruments were initially designed for use with chronic shoulder pathology patients. To ensure an instrument is valid, reliable, and sensitive to clinical changes, it is important to obtain psychometric evidence of its use in the target population. Four commonly used shoulder outcome instruments are reviewed in this paper: American Shoulder and Elbow Surgeons Standardized Shoulder Assessment Form (ASES); Constant–Murley shoulder score (CMS); Disabilities of Arm, Shoulder, and Hand (DASH); Oxford Shoulder Score (OSS). Each instrument was reviewed for floor or ceiling effects, validity, reliability, responsiveness, and interpretability. Additionally, evidence of each instrument's psychometric properties was sought in shoulder fracture populations. Based on the current literature, each instrument has limited amounts of evidence to support their use in shoulder trauma populations. Overall, psychometric evaluations in isolated shoulder fracture populations remain scarce, and clinicians must remember that an instrument's properties are defined for the population tested and not the instrument. Therefore, caution must always be exercised when using an instrument that has not been fully evaluated in trauma populations.}
}
@article{BABU2024100419,
title = {BERT-Based Medical Chatbot: Enhancing Healthcare Communication through Natural Language Understanding},
journal = {Exploratory Research in Clinical and Social Pharmacy},
volume = {13},
pages = {100419},
year = {2024},
issn = {2667-2766},
doi = {https://doi.org/10.1016/j.rcsop.2024.100419},
url = {https://www.sciencedirect.com/science/article/pii/S2667276624000143},
author = {Arun Babu and Sekhar Babu Boddu},
keywords = {Chatbot, IoT, Deep learning, BERT, NLP, AI},
abstract = {The advent of modern technologies like Artificial Intelligence(AI), Internet of Things(IoT) and Deep Learning(DL) has ushered in a transformative era in healthcare, offering innovative solutions towards personalized healthcare by enhancing the quality of various medical services. Our proposed methodology involves the development of a BERT-based medical chatbot, leveraging cutting-edge deep learning technology to significantly enhance healthcare communication and accessibility. The traditional challenges faced by medical chatbots, such as imprecise understanding of medical conversations, inaccurate responses to jargon, and the inability to offer personalized feedback, are addressed through the utilization of Bidirectional Encoder Representations from Transformers (BERT). The performance metrics of our chatbot underscore its effectiveness. With an accuracy of 98%, the chatbot ensures a high level of precision in handling medical queries. The precision score of 97% attests to the accuracy and reliability of its responses. The AUC-ROC score of 97% indicates the chatbot's exceptional ability to predict specific diseases based on user queries and symptoms, showcasing its robust predictive power. Furthermore, a recall of 96% demonstrates the chatbot's capability to avoid missing cases in medical diagnoses, ensuring comprehensive coverage of potential conditions. The F1 score of 98% showcases the chatbot's proficiency in delivering accurate and personalized healthcare information, striking a harmonious balance between precision and recall. Our BERT-based medical chatbot not only addresses the limitations of traditional approaches but also achieves a remarkable performance with high accuracy, precision, predictive power, and comprehensive coverage, making it a valuable tool for advancing the quality of healthcare services.}
}
@article{SIENKIEWICZ2024141524,
title = {Eco-friendly modification of bitumen: The effects of rubber wastes and castor oil on the microstructure, processability and properties},
journal = {Journal of Cleaner Production},
volume = {447},
pages = {141524},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.141524},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624009727},
author = {Maciej Sienkiewicz and Przemysław Gnatowski and Mateusz Malus and Anna Grzegórska and Hossein Ipakchi and Maryam Jouyandeh and Justyna Kucińska-Lipka and Francisco Javier Navarro and Mohammad Reza Saeb},
keywords = {Bitumen modification, Castor oil, Waste tires recycling, Reclaimed rubber, Rheological properties, Epoxy resin},
abstract = {The bitumen industry in the European Union is facing several difficulties, including rising demand, unstable oil supply, rising prices for synthetic polymer modifiers, and a focus on lowering carbon footprint. Bitumen modification with crumb rubber (CR) is one of the most promising solution to these challenges. However, CR-modified bitumen have poor processability and low storage stability. To overcome these flaws we are introducing a sustainable approach for ecological modification of bitumen taking advantage of renewable resources. For this reason, unmodified castor oil was selected as a green modifier of reclaimed rubber dust. The ecologically modified bitumen underwent visco-elastic behavior analysis based on rheological tests varying the temperature. The modification with rubber-oil improved the longevity of typical pavement, featured by an exceptional deformation resistance at elevated temperatures (well above 70 °C, the maximum pavement temperature reported in the region). The Cole-Cole graphs and black space diagrams unraveled the enhanced elasticity of bitumen. Technically, in comparison to plain bitumen, the compatibility ratio of modified bitumen to aggregates showed an uplift by 258%. The environmentally friendly bitumen modified ecologically herein revealed potential for performance window enlargement. Nevertheless, future investigations should focus on optimization of the bitumen formulation, along with examination of other sustainable moieties for the sake of commercialization of the developed binders in pavement construction.}
}
@article{CHOUCHEN2021106908,
title = {WhoReview: A multi-objective search-based approach for code reviewers recommendation in modern code review},
journal = {Applied Soft Computing},
volume = {100},
pages = {106908},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106908},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620308462},
author = {Moataz Chouchen and Ali Ouni and Mohamed Wiem Mkaouer and Raula Gaikovina Kula and Katsuro Inoue},
keywords = {Modern code review, Software quality, Code reviewers recommendation, Search-based software engineering},
abstract = {Contemporary software development is distributed and characterized by high dynamics with continuous and frequent changes to fix defects, add new user requirements or adapt to other environmental changes. To manage such changes and ensure software quality, modern code review is broadly adopted as a common and effective practice. Yet several open-source as well as commercial software projects have adopted peer code review as a crucial practice to ensure the quality of their software products using modern tool-based code review. Nevertheless, the selection of peer reviewers is still merely a manual and hard task especially with the growing size of distributed development teams. Indeed, it has been proven that inappropriate peer reviewers selection can consume more time and effort from both developers and reviewers and increase the development costs and time to market. To address this problem, we introduce a multi-objective search-based approach, named WhoReview, to find the optimal set of peer reviewers for code changes. We use the Indicator-Based Evolutionary Algorithm (IBEA) to find the best set of code reviewers that are (1) most experienced with the code change to be reviewed, while (2) considering their current workload, i.e., the number of open code reviews they are working on. We conduct an empirical study on 4 long-lived open source software projects to evaluate our approach. The obtained results show that WhoReview outperforms state-of-the-art approach by an average precision of 68% and recall of 77%. Moreover, we deployed our approach in an industrial context and evaluated it qualitatively from developers perspective. Results show the effectiveness of our approach with a high acceptance ratio in identifying relevant reviewers.}
}
@article{VANDEWATER2011333,
title = {Outcome measures in the management of proximal humeral fractures: a systematic review of their use and psychometric properties},
journal = {Journal of Shoulder and Elbow Surgery},
volume = {20},
number = {2},
pages = {333-343},
year = {2011},
issn = {1058-2746},
doi = {https://doi.org/10.1016/j.jse.2010.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S1058274610004441},
author = {Alexander T.M. {van de Water} and Nora Shields and Nicholas F. Taylor}
}
@article{MEIDAN2023103073,
title = {D-Score: An expert-based method for assessing the detectability of IoT-related cyber-attacks},
journal = {Computers & Security},
volume = {126},
pages = {103073},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.103073},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822004655},
author = {Yair Meidan and Daniel Benatar and Ron Bitton and Dan Avraham and Asaf Shabtai},
keywords = {Internet of things (IoT) security, Attack detection, Network traffic predictability, Multi-Criteria decision making, Analytical hierarchical process (AHP)},
abstract = {IoT devices are known to be vulnerable to various cyber-attacks, such as data exfiltration and the execution of flooding attacks as part of a DDoS attack. When it comes to detecting such attacks using network traffic analysis, it has been shown that some attack scenarios are not always equally easy to detect if they involve different IoT models. That is, when targeted at some IoT models, a given attack can be detected rather accurately, while when targeted at others the same attack may result in too many false alarms. In this research, we attempt to explain this variability of IoT attack detectability and devise a risk assessment method capable of addressing a key question: how easy is it for an anomaly-based network intrusion detection system to detect a given cyber-attack involving a specific IoT model? In the process of addressing this question we (a) investigate the predictability of IoT network traffic, (b) present a novel taxonomy for IoT attack detection which also encapsulates traffic predictability aspects, (c) propose an expert-based attack detectability estimation method which uses this taxonomy to derive a detectability score (termed ‘D-Score’) for a given combination of IoT model and attack scenario, and (d) empirically evaluate our method while comparing it with a data-driven method.}
}
@article{HARINI2023469,
title = {An effective technique for detecting minority attacks in NIDS using deep learning and sampling approach},
journal = {Alexandria Engineering Journal},
volume = {78},
pages = {469-482},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2023.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S1110016823006531},
author = {R. Harini and N. Maheswari and Sannasi Ganapathy and M. Sivagami},
keywords = {Network intrusion detection system, Triple layered hybrid approach, Weighted deep neural network, CNN+LSTM and XGBoost},
abstract = {Anomaly-based intrusion detection system have been consistently used in business organizations and military to detect a breach in network by identifying any activity that deviates from the baseline pattern. In this paper, we propose an effective intrusion detection technique to identify and predict the minority attacks with three layers. Here, the first layer utilizes a Weighted Deep Neural Network (WDNN) for identifying the suspicious traffic samples in network and it is passed to the second layer. Layer 2 classifies the traffic samples as normal or majority and minority attacks using Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM). Any traffic sample classified as minority attack is sent to Layer 3 that utilizes XGBoost algorithm. Layer 3 classifies the samples into their respective minority attack classes. To boost the detection rate of minority attacks, system employs a One-Sided Selection under-sampling algorithm to remove noisy samples from the majority attack classes. An Adaptive Synthetic (ADASYN) oversampling algorithm generates synthetic samples of minority attack classes. To evaluate the system, the datasets namely NSL KDD, CICIDS-2017 and CIDDS 001 dataset are used. The system attained an overall accuracy of 97.94% on NSL KDD dataset, 98.3% on CICIDS-2017 dataset and 97.9% on CIDDS 001 dataset.}
}
@article{FATEMI20111787.e15,
title = {Ex-vivo oocyte retrieval for fertility preservation},
journal = {Fertility and Sterility},
volume = {95},
number = {5},
pages = {1787.e15-1787.e17},
year = {2011},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2010.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0015028210028098},
author = {Human M. Fatemi and Dimitra Kyrou and Majedah Al-Azemi and Dominique Stoop and Philippe {De Sutter} and Claire Bourgain and Paul Devroey},
keywords = {Borderline ovarian tumor, ex-vivo oocyte retrieval, fertility preservation},
abstract = {Objective
To report a novel fertility preservation strategy in a woman with recurrent serous borderline ovarian tumor in the conserved ovary involving ex-vivo retrieval of in vivo matured oocytes and subsequent embryo cryopreservation.
Design
Case report.
Setting
Tertiary infertility care unit.
Patient(s)
A 27-year-old woman presented for follow-up visit with a history of borderline serous adenocarcinoma treated conservatively with left oophorectomy and fertility-sparing laparoscopic staging. Ultrasound scan revealed a recurrent disease in the right ovary.
Intervention(s)
Ex-vivo retrieval of mature oocytes after ovarian stimulation.
Main Outcome Measure(s)
Fertility preservation.
Result(s)
The patient underwent ovarian stimulation followed by a laparotomy and oophorectomy on the day of oocyte retrieval. A puncture of the follicles was performed in the operating theatre with a maximum ischemia time of 14 minutes. Eleven mature oocytes were aspirating, resulting in seven zygotes for cryopreservation.
Conclusion(s)
Mature oocytes can be successfully retrieved ex-vivo from the oophorectomy specimen after a controlled ovarian hyperstimulation (COH) protocol. This method provides a possible strategy for fertility preservation in patients with recurrent ovarian cancer without the risk of cancer cells spillage associated with the standard transvaginal oocyte retrieval.}
}
@article{KHAN201611,
title = {A survey of security issues for cloud computing},
journal = {Journal of Network and Computer Applications},
volume = {71},
pages = {11-29},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301060},
author = {Minhaj Ahmad Khan},
keywords = {Cloud security, Cloud computing, Denial-of-service, Security threats, Intrusion detection systems},
abstract = {High quality computing services with reduced cost and improved performance have made cloud computing a popular paradigm. Due to its flexible infrastructure, net centric approach and ease of access, the cloud computing has become prevalent. Its widespread usage is however being diminished by the fact that the cloud computing paradigm is yet unable to address security issues which may in turn aggravate the quality of service as well as the privacy of customers' data. In this paper, we present a survey of security issues in terms of security threats and their remediations. The contribution aims at the analysis and categorization of working mechanisms of the main security issues and the possible solutions that exist in the literature. We perform a parametric comparison of the threats being faced by cloud platforms. Moreover, we compare various intrusion detection and prevention frameworks being used to address security issues. The trusted cloud computing and mechanisms for regulating security compliance among cloud service providers are also analyzed. Since the security mechanisms continue to evolve, we also present the future orientation of cloud security issues and their possible countermeasures.}
}
@article{APAT2024100379,
title = {A hybrid meta-heuristic algorithm for multi-objective IoT service placement in fog computing environments},
journal = {Decision Analytics Journal},
volume = {10},
pages = {100379},
year = {2024},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2023.100379},
url = {https://www.sciencedirect.com/science/article/pii/S2772662223002199},
author = {Hemant Kumar Apat and Bibhudutta Sahoo and Veena Goswami and Rabindra K. Barik},
keywords = {Service placement, Internet of Things, Single objective, Multi-objective, Optimization, Meta-heuristic},
abstract = {The fog computing paradigm is promising for deploying various delay-sensitive Internet of Things (IoT) applications. The resource-constrained fog devices restrict the number of application deployments due to a lack of efficient resource estimation and discovery mechanisms for various emergent heterogeneous IoT applications. An efficient resource allocation strategy is one of the best choices to meet these application’s Quality of Service (QoS) requirements and improve system performance. However, finding the best allocation strategy for IoT applications with more than one QoS parameter is a challenge, and it has been proved as a non-deterministic polynomial time (NP)-complete problem. This article formulates a classical weighted multi-objective IoT service placement to optimize three parameters, i.e., makespan, cost, and energy. The non-convexity nature of the solution space motivates us to focus on the population-based meta-heuristic algorithm, i.e. Genetic Algorithm (GA), Simulated Annealing (SA) and Particle Swarm Optimization (PSO), along with their combination GA-SA, and GA-PSO. It implements the algorithm and compares it with the greedy-based random placement approach, varying the number of IoT applications with different parameters. The final results reveal that the hybrid method GA-SA outperforms other state-of-the-art algorithms.}
}
@incollection{MIMS2017265,
title = {Chapter 14 - The Botnet Problem},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
address = {Boston},
pages = {265-274},
year = {2017},
isbn = {978-0-12-803843-7},
doi = {https://doi.org/10.1016/B978-0-12-803843-7.00014-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038437000144},
author = {Nailah Mims},
keywords = {Bot, Botnet, Command and control or C2, Cyberattack, Internet relay chat (IRC), Malware, Trojan horse, Zombie},
abstract = {Automated programs that covertly infect vulnerable computers and turn them into hosts for unauthorized networks are called bots. These infected hosts are configured to report back to a central system(s) run by an attacker, or bot-herder, and collectively form a botnet. Botnets may contain thousands of hosts and can be used to execute a variety of cyber-based attacks, in particular flooding target's networks and devices with too much traffic and stealing data from hosts infected with the bots. Accordingly, the aim of attackers who deploy botnets is usually to interrupt a target's operations or achieve financial gain. This problem is compounded by the spread of new technologies that connect a variety of devices to the Internet, making them susceptible to being an unwitting bot host or else a target of the botnet's massive disruption campaign. Given the global implications of so many potential hosts, preventing the spread of botnets and tracing the malicious cyber activity are key challenges to overcome when addressing the botnet problem.}
}
@incollection{XU2006247,
title = {12 - Application of Social Network Analysis to the Study of Open Source Software},
editor = {Jürgen Bitzer and Philipp J.H. Schröder},
booktitle = {The Economics of Open Source Software Development},
publisher = {Elsevier},
address = {Amsterdam},
pages = {247-269},
year = {2006},
isbn = {978-0-444-52769-1},
doi = {https://doi.org/10.1016/B978-044452769-1/50012-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527691500123},
author = {Jin Xu and Scott Christley and Gregory Madey},
abstract = {Publisher Summary
This chapter constructs four social networks for the Open Source Software (OSS) development community at Source Forge. Social network analysis has been used in many research areas to discover the intrinsic mechanisms of social communities by examining the topological properties of the social network formed by relationships between the actors and the groups in those communities. For each social network, number of people are expanded in the network by including the next set of peripheral users as defined by their role in the community, moving from the core project leaders, to the core developers, to the co-developers, and finally out to active users. All the social networks have scale-free properties, and the inclusion of the co-developers and active users triggers the emergence of the small world phenomenon for the social network. The chapter examines how these topological network properties potentially explain the success and efficiency of OSS development practices.}
}
@article{JIANG201748,
title = {Who should comment on this pull request? Analyzing attributes for more accurate commenter recommendation in pull-based development},
journal = {Information and Software Technology},
volume = {84},
pages = {48-62},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S095058491630283X},
author = {Jing Jiang and Yun Yang and Jiahuan He and Xavier Blanc and Li Zhang},
keywords = {Commenter recommendation, Reviewer recommendation, Attribute selection, Pull-based software development},
abstract = {Context: The pull-based software development helps developers make contributions flexibly and efficiently. Commenters freely discuss code changes and provide suggestions. Core members make decision of pull requests. Both commenters and core members are reviewers in the evaluation of pull requests. Since some popular projects receive many pull requests, commenters may not notice new pull requests in time, and even ignore appropriate pull requests. Objective: Our objective in this paper is to analyze attributes that affect the precision and recall of commenter prediction, and choose appropriate attributes to build commenter recommendation approach. Method: We collect 19,543 pull requests, 206,664 comments and 4817 commenters from 8 popular projects in GitHub. We build approaches based on different attributes, including activeness, text similarity, file similarity and social relation. We also build composite approaches, including time-based text similarity, time-based file similarity and time-based social relation. The time-based social relation approach is the state-of-the-art approach proposed by Yu et al. Then we compare precision and recall of different approaches. Results: We find that for 8 projects, the activeness based approach achieves the top-3 precision of 0.276, 0.386, 0.389, 0.516, 0.322, 0.572, 0.428, 0.402, and achieves the top-3 recall of 0.475, 0.593, 0.613, 0.66, 0.644, 0.791, 0.714, 0.65, which outperforms approaches based on text similarity, file similarity or social relation by a substantial margin. Moreover, the activeness based approach achieves better precision and recall than composite approaches. In comparison with the state-of-the-art approach, the activeness based approach improves the top-3 precision by 178.788%, 30.41%, 25.08%, 41.76%, 49.07%, 32.71%, 25.15%, 78.67%, and improves the top-3 recall by 196.875%, 36.32%, 29.05%, 46.02%, 43.43%, 27.79%, 25.483%, 79.06% for 8 projects. Conclusion: The activeness is the most important attribute in the commenter prediction. The activeness based approach can be used to improve the commenter recommendation in code review.}
}
@article{GUPTA2024108434,
title = {Quality of life outcomes comparing primary Transoral Robotic Surgery (TORS) with primary radiotherapy for early-stage oropharyngeal squamous cell carcinoma: A systematic review and meta-analysis},
journal = {European Journal of Surgical Oncology},
volume = {50},
number = {7},
pages = {108434},
year = {2024},
issn = {0748-7983},
doi = {https://doi.org/10.1016/j.ejso.2024.108434},
url = {https://www.sciencedirect.com/science/article/pii/S0748798324004864},
author = {Keshav Kumar Gupta and Mriganka De and Thanos Athanasiou and Christos Georgalas and George Garas},
keywords = {Oropharyngeal carcinoma, Transoral robotic surgery, TORS, Radiotherapy, Quality of life},
abstract = {Background
Transoral Robotic Surgery (TORS) and radiotherapy are considered oncologically equivalent primary treatment options for early-stage HPV-positive oropharyngeal squamous cell carcinoma (OPSCC). Quality of Life (QoL) and Patient Reported Outcome Measures (PROMs) are therefore imperative in supporting clinical decision-making and optimising patient-centred care. The aim of this article is to evaluate how these primary treatment modalities compare in terms of QoL.
Materials and methods
Systematic review and meta-analysis of studies comparing primary TORS and primary radiotherapy for OPSCC using validated QoL tools. Swallowing and global QoL were the primary endpoints with secondary endpoints including all other QoL domains. An inverse variance random-effects model was employed to calculate the weighted estimate of the treatment effects across trials.
Results
A total of six studies collectively reporting on 555 patients were included (n = 236 TORS and n = 319 radiotherapy). Meta-analysis showed no significant difference for swallowing (mean difference = −0.24, p = 0.89) and global QoL (mean difference = 4.55, p = 0.14). For the remaining QoL domains (neck/shoulder impairment, neurotoxicity, voice, xerostomia, speech, and distress), the scarcity of data did not permit meta-analysis. However, the existing data showed no significant difference for any except for xerostomia where TORS appears favourable in the sole study reporting on this.
Conclusions
TORS and radiotherapy appear to be comparable primary treatment options for early stage OPSCC when it comes to QoL. However, a substantial proportion of patients in the TORS group received adjuvant (chemo)radiotherapy rendering it difficult to establish the ‘true’ QoL outcomes following surgery alone. There are also minimal studies reporting QoL outcomes beyond swallowing and global QoL. Further research is therefore needed, including more randomised trials adequately powered to detect differences in QoL outcomes.}
}
@article{FORTIN2007591,
title = {Impact of infertility drugs after treatment of borderline ovarian tumors: results of a retrospective multicenter study},
journal = {Fertility and Sterility},
volume = {87},
number = {3},
pages = {591-596},
year = {2007},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2006.07.1503},
url = {https://www.sciencedirect.com/science/article/pii/S001502820604009X},
author = {Anne Fortin and Philippe Morice and Anne Thoury and Sophie Camatte and Caroline Dhainaut and Patrick Madelenat},
keywords = {Borderline tumor, conservative treatment, infertility, in vitro fertilization, pregnancy, recurrence},
abstract = {Objective
To evaluate safety and fertility outcome after the use of infertility drugs in patients who were treated conservatively for a borderline ovarian tumor (BOT).
Design
A retrospective multicenter study.
Setting
Centers participating in the French National Register on In Vitro Fertilization registry.
Patient(s)
Thirty patients who were treated for BOT who underwent ovarian induction (OI).
Intervention(s)
Ovarian induction was performed in 25 patients for infertility after conservative surgery and before surgery for recurrent disease in 5 patients with a single ovary (emergency cases).
Main Outcomes Measure(s)
Fertility and recurrences rates.
Result(s)
The mean number of cycles of OI per patient was 2.6 (range, 1–10 cycles). The median follow-up time after treatment of the BOT was 93 months (range, 26–276 months). After a median follow-up time of 42 months after OI, 4 recurrences were observed (initial management was simple cystectomy in 3 of them). All recurrences were borderline tumors on a remaining ovary that had been treated by surgery alone. All patients are currently disease-free. Thirteen pregnancies were observed (10 pregnancies (40%) in the group of 25 patients who were treated for infertility).
Conclusion(s)
These results suggest that infertility drugs could be used safely in patients who experience infertility after conservative management of an early-stage BOT.}
}
@article{KUHN2023103286,
title = {Common vulnerability scoring system prediction based on open source intelligence information sources},
journal = {Computers & Security},
volume = {131},
pages = {103286},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103286},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823001967},
author = {Philipp Kühn and David N. Relke and Christian Reuter},
keywords = {IT Security, Common vulnerability scoring system, Classification, National vulnerability database, Security management, Deep learning},
abstract = {The number of newly published vulnerabilities is constantly increasing. Until now, the information available when a new vulnerability is published is manually assessed by experts using a Common Vulnerability Scoring System (CVSS) vector and score. This assessment is time consuming and requires expertise. Various works already try to predict CVSS vectors or scores using machine learning based on the textual descriptions of the vulnerability to enable faster assessment. However, for this purpose, previous works only use the texts available in databases such as National Vulnerability Database. With this work, the publicly available web pages referenced in the National Vulnerability Database are analyzed and made available as sources of texts through web scraping. A Deep Learning based method for predicting the CVSS vector is implemented and evaluated. The present work provides a classification of the National Vulnerability Database’s reference texts based on the suitability and crawlability of their texts. While we identified the overall influence of the additional texts is negligible, we outperformed the state-of-the-art with our Deep Learning prediction models.}
}
@article{BIAZOTTO2024107375,
title = {Technical debt management automation: State of the art and future perspectives},
journal = {Information and Software Technology},
volume = {167},
pages = {107375},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107375},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923002306},
author = {João Paulo Biazotto and Daniel Feitosa and Paris Avgeriou and Elisa Yumi Nakagawa},
keywords = {Systematic mapping study, Technical debt, Technical debt management, Tools, Automation},
abstract = {Context:
Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system’s maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most.
Objectives:
The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM.
Methods:
We conducted a systematic mapping study (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation.
Results:
We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges.
Conclusion:
The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts.}
}
@article{LI2024112019,
title = {Bug priority change: An empirical study on Apache projects},
journal = {Journal of Systems and Software},
volume = {212},
pages = {112019},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112019},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000621},
author = {Zengyang Li and Guangzong Cai and Qinyi Yu and Peng Liang and Ran Mo and Hui Liu},
keywords = {Bug priority change, Open source software, Empirical study},
abstract = {In issue tracking systems, each bug is assigned a priority level (e.g., Blocker, Critical, Major, Minor, or Trivial in JIRA from highest to lowest), which indicates the urgency level of the bug. In this sense, understanding bug priority changes helps to arrange the work schedule of participants reasonably, and facilitates a better analysis and resolution of bugs. According to the data extracted from JIRA deployed by Apache, a proportion of bugs in each project underwent priority changes after such bugs were reported, which brings uncertainty to the bug fixing process. However, there is a lack of in-depth investigation on the phenomenon of bug priority changes, which may negatively impact the bug fixing process. Thus, we conducted a quantitative empirical study on bugs with priority changes through analyzing 32 non-trivial Apache open source software projects. The results show that: (1) 8.3% of the bugs in the selected projects underwent priority changes; (2) the median priority change time interval is merely a few days for most (28 out of 32) projects, and half (50. 7%) of bug priority changes occurred before bugs were handled; (3) for all selected projects, 87.9% of the bugs with priority changes underwent only one priority change, most priority changes tend to shift the priority to its adjacent priority, and a higher priority has a greater probability to undergo priority change; (4) bugs that require bug-fixing changes of higher complexity or that have more comments are likely to undergo priority changes; and (5) priorities of bugs reported or allocated by a few specific participants are more likely to be modified, and maximally only one participant in each project tends to modify priorities.}
}
@article{KOKINDA2023111630,
title = {Streaming software development: Accountability, community, and learning},
journal = {Journal of Systems and Software},
volume = {199},
pages = {111630},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111630},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000250},
author = {Ella Kokinda and Paige Rodeghero},
keywords = {Live streaming, Developer communities, Gaming communities, Live coding, Online education, Collaborative learning},
abstract = {People use the Internet to learn new skills, stay connected with friends, and find new communities to engage with. Live streaming platforms like Twitch.tv, YouTube Live, and Facebook Gaming provide a place where all three of these activities intersect and enable users to live-stream themselves playing a video game or live-coding software and game development, as well as the ability to participate in chat while watching someone else engage in an activity. Through fifteen interviews with software and game development streamers, we investigate why people choose to stream themselves programming and if they perceive themselves improving their programming skills by live streaming. We found that the motivations to stream included accountability, self-education, community, and visibility of the streamers’ work, and streamers perceived a positive influence on their ability to write source code. Our findings implicate that alternative learning methods like live streaming programming are a beneficial tool in the age of the virtual classroom. This work also contributes to and extends research efforts surrounding educational live streaming and collaboration in developer communities.}
}
@article{VANDOOREN2006S75,
title = {F.70. Melanoma Inhibitory Activity (MIA) Reflects Chondrocyte Anabolism in Chronic Inflammatory Arthritis: Suppression By Proinflammatory Cytokines Is Reversed By Targeted Therapy},
journal = {Clinical Immunology},
volume = {119},
pages = {S75},
year = {2006},
note = {FOCIS 2006 Abstract Supplement},
issn = {1521-6616},
doi = {https://doi.org/10.1016/j.clim.2006.04.110},
url = {https://www.sciencedirect.com/science/article/pii/S1521661606002324},
author = {Bernard Vandooren and Marie-Jose {van Lierop} and Tineke Cantaert and Leen {De Rycke} and Elli Kruithof and Eric Veys and Ebo Bos and Annemieke Boots and Dominique Baeten}
}
@article{PARADIS2024111967,
title = {Analyzing the Tower of Babel with Kaiaulu},
journal = {Journal of Systems and Software},
volume = {210},
pages = {111967},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.111967},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000104},
author = {Carlos Paradis and Rick Kazman and Damian Tamburri},
keywords = {Socio-smells, Socio-technical smells, Mining-software-repositories, Gitlog, Mailing-list, Issue-tracker, Identity-matching, Networks, Tools},
abstract = {Context:
An extensive body of work has examined socio-technical activities in software development; however, the availability of tools to enable these studies is limited.
Aim:
We extend Kaiaulu, a software package for Mining Software Repositories to enable a broad spectrum analysis of Social Smells and Motifs.
Methods:
We perform a literature review to identify what tools are available which implement graph construction methods and social smell metrics, contextualizing the contributions of our tool.
Results:
The few tools identified in the literature either leverage fewer parts of the software ecosystem, have been archived, or depend on components no longer maintained.
Conclusion:
The socio-technical features in Kaiaulu complement existing tools and related literature, while providing a simple architecture to facilitate ease or use, and ease of learning, benefitting reproducibility.
Tool Repository:
github.com/sailuh/kaiaulu}
}
@article{CITARTAN2021122436,
title = {Aptamers as the powerhouse of dot blot assays},
journal = {Talanta},
volume = {232},
pages = {122436},
year = {2021},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2021.122436},
url = {https://www.sciencedirect.com/science/article/pii/S003991402100357X},
author = {Marimuthu Citartan},
keywords = {Dot blot assay, Aptamer, SELEX, Diagnostics},
abstract = {Dot blot assays have always been associated with antibodies as the main molecular recognition element, which are widely employed in a myriad of diagnostic applications. With the rising of aptamers as the equivalent molecular recognition elements of antibodies, dot blot assays are also one of the diagnostic avenues that should be scrutinized for their amenability with aptamers as the potential surrogates of antibodies. In this review, the stepwise procedures of an aptamer-based dot blot assays are underscored before reviewing the existing aptamer-based dot blot assays developed so far. Most of the applications center on monitoring the progress of SELEX and as the validatory assays to assess the potency of aptamer candidates. For the purpose of diagnostics, the current effort is still languid and as such possible suggestions to galvanize the move to spur the aptamer-based dot blot assays to a point-of-care arena are discussed.}
}
@incollection{2017429,
title = {Index},
editor = {James Farmer and Brian Lane and Kevin Bourg and Weyl Wang},
booktitle = {FTTx Networks},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {429-436},
year = {2017},
isbn = {978-0-12-420137-8},
doi = {https://doi.org/10.1016/B978-0-12-420137-8.18001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124201378180011}
}
@article{WANG20132623,
title = {Floatability of polymer materials modulated by frothers},
journal = {Waste Management},
volume = {33},
number = {12},
pages = {2623-2631},
year = {2013},
issn = {0956-053X},
doi = {https://doi.org/10.1016/j.wasman.2013.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0956053X13004066},
author = {Hui Wang and Chong-qing Wang and Jian-gang Fu},
keywords = {Adsorption, Floatability, Flotation, Frother, Low-energy surface, Plastic},
abstract = {Flotation tests of 35 polymer materials were carried out to investigate their floatability modulated by frothers. Results of flotation tests demonstrated that polymer resins and soft PVC showed high floatability, floatability of hard PVC plastics was relatively low and was related to the frothers, and there exists significant difference in the floatability of different post-consumer plastics. Flotation rate of post-consumer plastics varies from 0% to 100%. Furthermore, three-category low-energy surface (LES) was defined based on the hydrophile index of the materials involved in this paper, and an adsorption model was proposed to explain the results of flotation and to discuss the floatability of polymer materials modulated by frothers. Frother molecules are prone to adsorb on the surface of bubble rather than LES at relatively low concentration, bubble adsorbed by frother molecules is prone to approach first-category LES rather than third-category LES, and the structure of liquid film is formed on the first-category LES at large concentration. Floatability of polymer materials modulated by frothers is further discussed: frothers increase the floatability of the first-category LES but decrease the floatability of the third-category LES, while the floatability of the second-category LES is related to the type of frothers.}
}
@article{HADJAIDJIBENSEGHIER2017852,
title = {Did the global warming confirm in central northern Sahara (case of the Ouargla region )?},
journal = {Energy Procedia},
volume = {119},
pages = {852-862},
year = {2017},
note = {International Conference on Technologies and Materials for Renewable Energy, Environment and Sustainability, TMREES17, 21-24 April 2017, Beirut Lebanon},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.138},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217326954},
author = {Fatiha Hadjaidji-Benseghier and Talbi Nadjib and et Derridj Arrezki},
keywords = {Regional climate, central northern Sahara, NAO, Warming, Ouargla},
abstract = {Climate change shows itself in various scales in the Mediterranean and Sahara region. The study aims at characterizing climate of the Sahara in a bigger scale otherwise-said, precision as for the current climate reigning (evolution), at the level of the region of Ouargla. For that purpose, we adopted a complementary, dynamic and static approach. The dynamic approach was approached by compilations of the previous works. Followed by a static analysis, leaning on climatological data, spread over a period going from 1978 till 2015. It emerges from it that the dynamic character is characterized by the frequency of the regime NAO + with regard to that of the NAO-. This regime expresses himself daily, in terms, of temperature and haste, explaining the importing lived reheating these last decades. So, the results show that the region is characterized by a "hot" thermoclimate, expressed by all the energy parameters (rise of the fraction of sunstroke and the temperature) and a "dry"ombroclimate.}
}
@article{BLACKMAN201481,
title = {Our referees – An appreciation},
journal = {World Patent Information},
volume = {36},
pages = {81-83},
year = {2014},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2013.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0172219013001221},
author = {Michael Blackman and David Newton}
}
@article{PROCHAZKOVA20121778,
title = {Surface Modification of Chlorella Vulgaris Cells Using Magnetite Particles},
journal = {Procedia Engineering},
volume = {42},
pages = {1778-1787},
year = {2012},
note = {CHISA 2012},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2012.07.572},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812029797},
author = {G. Procházková and I. Šafařík and T. Brányik},
keywords = {icroalgae, physicochemical approaches, surface interactions, magnetite, XDLVO theory, harvesting},
abstract = {Expensive cell concentration procedures represent one of the bottlenecks of large-scale microalgal biotechnological processes as many industrially attractive species have a small cell size and sustain in suspension. An economically effective solution is to alter the process conditions for the cells to form aggregates, which sediment faster. The use of magnetic agents binding to the cell surface and forming larger complexes, that sediment very fast upon application of an external magnetic field, is a rarely explored possibility in this area. We used commercially available, finely pulverized magnetite (Sigma Aldrich) as a potential harvesting agent and studied its surface interactions with an industrially important microalgal strain (Chlorella vulgaris). Firstly, we characterized the interacting surfaces in model environments by zeta potential and contact angle measurements, which were followed by particle size determination. Secondly, we applied the XDLVO theory to predict favorable experimental conditions for a successful magnetic cell modification, which would lead to an effective biomass separation. The hypotheses were then tested by using various ratios of magnetic agent and microalgal biomass under different environmental conditions. Obtained results were in good accordance with the predictions and we achieved an excellent separation efficiency of over 90% within a few minutes at a ratio of microalgae to magnetite 1:26 (w/w). We can conclude that magnetite successfully modifies the microalgal surface under certain conditions and is a promising agent for harvesting C. vulgaris, enabling high separation efficiencies in a very short period of time, but further research is necessary to optimize the process.}
}
@article{NAKHAIPOUR20071228.e1,
title = {Oral testosterone supplementation and chronic low-grade inflammation in elderly men: A 26-week randomized, placebo-controlled trial},
journal = {American Heart Journal},
volume = {154},
number = {6},
pages = {1228.e1-1228.e7},
year = {2007},
issn = {0002-8703},
doi = {https://doi.org/10.1016/j.ahj.2007.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0002870307007594},
author = {Hamid Reza Nakhai-Pour and Diederick E. Grobbee and Marielle H. Emmelot-Vonk and Michiel L. Bots and Harald J.J. Verhaar and Yvonne T. {van der Schouw}},
abstract = {Background
To determine the effect of oral testosterone supplementation on systemic low-grade inflammation measured by high-sensitive C-reactive protein (hs-CRP) in aging men with low testosterone levels.
Methods
Two hundred thirty-seven men aged 60 to 80 years with a testosterone level of <13.7 nmol/L (below the 50th percentile of the population distribution) were recruited into a double-blind randomized placebo-controlled trial. Participants were randomized to either 4 capsules of 40 mg testosterone undecanoate (Andriol Testocaps, NV Organon, Oss, The Netherlands) or placebo daily for 26 weeks. Serum levels of hs-CRP were measured at baseline and at 26 weeks using a near-infrared particle immunoassay of the Synchron LX System (Beckman Coulter, Fullteron, CA).
Results
The median baseline hs-CRP level was 1.95 mg/L (0.30-6.43) in the testosterone group compared with 1.90 mg/L (0.40-5.91) in the placebo group. After 26 weeks of testosterone supplementation therapy, the 2 intervention groups were not statistically significantly different (median hs-CRP 2.20 vs 2.00 mg/L, interquartile range 0.40-6.54 vs 0.50-5.70, P = .36). In subgroup analysis, neither baseline testosterone level, nor age, nor baseline CRP-level modified the effect of testosterone supplementation on CRP levels.
Conclusion
Oral testosterone undecanoate supplementation, in dosage of 160 mg daily for 26 weeks, does not increase hs-CRP levels in elderly men.}
}
@article{TSYGANOV2022366,
title = {Comprehensive Mechanism for Four-Level Energy Cost Control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {9},
pages = {366-371},
year = {2022},
note = {11th IFAC Symposium on Control of Power and Energy Systems CPES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.064},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322004499},
author = {Vladimir V. Tsyganov},
keywords = {Company, energy, costs, hierarchy, control, stochastic, learning, mechanism},
abstract = {The paper examines the model of energy cost management in a four-level control system of company, includes its boss with the advisor on top level, administrator on the middle level, and managing director of the plant at the bottom level. Neither the boss nor the advisor knows minimal administrator and plant stochastic energy costs, and need to learn to control them. The administrator knows minimal stochastic costs better than the boss and the advisor. So the administrator can manipulate costs in order to influence the results of learning the advisor and the boss in own favor. But the administrator itself does not know the minimum stochastic costs in plant. This can be used by its director to achieve own goal. So the administrator also needs to learn to control the managing director. All of the interests of such active elements are reflected in the model by introducing goal functions. On the basis of this model, sufficient conditions have been found for the synthesis of a hierarchy of mechanisms for managing energy costs, ensuring the use of stochastic possibilities of reducing these costs. In these conditions, the boss needs training with the help of a self-learning consultant. Through this training, the boss can rate the administrator's costs. This mechanism encourages the administrator, firstly, to minimize overhead energy costs and, secondly, to implement an adaptive cost savings mechanism on the plant. Such an adaptive mechanism includes standardization and stimulation procedures that encourage the managing director to minimize the energy costs of the plant. The application of this approach is illustrated by the example of managing energy costs in a wagon repair company.}
}
@article{PIO2023106923,
title = {Ovarian-sparing surgery for ovarian tumors in children: A systematic review and meta-analysis},
journal = {European Journal of Surgical Oncology},
volume = {49},
number = {10},
pages = {106923},
year = {2023},
issn = {0748-7983},
doi = {https://doi.org/10.1016/j.ejso.2023.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0748798323004821},
author = {Luca Pio and Ahmed Abu-Zaid and Tarek Zaghloul and Huma F. Halepota and Andrew M. Davidoff and Paul D. Losty and Hafeez H. Abdelhafeez},
keywords = {Ovarian sparing surgery, Ovariectomy, Oophorectomy, Ovarian tumors, Pediatrics, Fertility, Long term outcomes},
abstract = {Introduction
An increased number of children and adolescents with ovarian tumors have been managed with ovarian-sparing surgery in the last few years. However, comprehensive data on fertility outcomes and local relapse are scarce. In this study, we systematically describe the contemporary outcomes of ovarian-sparing surgery, as reported in the literature.
Materials and methods
Using PRISMA guidelines, we analyzed studies reporting ovarian-sparing techniques for ovarian tumors in children and adolescents. from 1980 to 2022. Reports with fewer than three patients, narrative reviews, and opinion articles were excluded. Statistical analysis was performed for dichotomous and continuous variables.
Results
Of 283 articles screened, 16 papers (3057 patients) met inclusion criteria (15 retrospective/1 prospective) and were analyzed. The vast majority of studies had no long-term fertility follow-up data and direct comparison between ovarian-sparing surgery vs oophorectomy was reported in only a few studies. Ovarian sparing surgery was not associated with worse oncologic outcomes in terms of (i) tumour spillage or (ii) recurrence rates, and of key importance allowed a higher ovarian reserve at long term follow-up.
Conclusions
Ovarian-sparing surgery is a safe and feasible technique for benign tumors. Long-term outcome studies are needed to show efficacy and fertility preservation.}
}
@article{MOUSTAFA2021102994,
title = {A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets},
journal = {Sustainable Cities and Society},
volume = {72},
pages = {102994},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102994},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721002808},
author = {Nour Moustafa},
keywords = {Smart cities, Network datasets, Cybersecurity applications, Machine learning, Edge, Software-Defined Network (SDN), Network Function Virtualization (NFV), Service Orchestration (SO)},
abstract = {While there has been a significant interest in understanding the cyber threat landscape of Internet of Things (IoT) networks, and the design of Artificial Intelligence (AI)-based security approaches, there is a lack of distributed architecture led to generating heterogeneous datasets that contain the actual behaviors of real-world IoT networks and complex cyber threat scenarios to evaluate the credibility of the new systems. This paper presents a novel testbed architecture of IoT network which can be used to evaluate Artificial Intelligence (AI)-based security applications. The platform NSX vCloud NFV was employed to facilitate the execution of Software-Defined Network (SDN), Network Function Virtualization (NFV) and Service Orchestration (SO) to offer dynamic testbed networks, which allow the interaction of edge, fog and cloud tiers. While deploying the architecture, real-world normal and attack scenarios are executed to collect labeled datasets. The generated datasets are named ‘TON_IoT’, as they comprise heterogeneous data sources collected from telemetry datasets of IoT services, Windows and Linux-based datasets, and datasets of network traffic. The TON_IoT network dataset is validated using four machine learning-based intrusion detection algorithms of Gradient Boosting Machine, Random Forest, Naive Bayes, and Deep Neural Networks, revealing a high performance of detection accuracy using the set of training and testing. A comparative summary of the TON_IoT network dataset and other competing network datasets demonstrates its diverse legitimate and anomalous patterns that can be used to better validate new AI-based security solutions. The architecture and datasets can be publicly accessed from TON_IOT Datasets (2020).}
}
@article{MCINTOSH2021568,
title = {Enforcing situation-aware access control to build malware-resilient file systems},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {568-582},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20305641},
author = {Timothy McIntosh and Paul Watters and A.S.M. Kayes and Alex Ng and Yi-Ping Phoebe Chen},
keywords = {Access control, Ransomware, Malware, File systems, Software defects, Attacks, Cybersecurity},
abstract = {Traditional non-semantic file systems are not sufficient in protecting file systems against attacks, either caused by ransomware attacks or software-related defects. Furthermore, outbreaks of new malware often cannot provide a large quantity of training samples for machine-learning-based approaches to counter malware campaigns. The malware defense system should aim to achieve the best balance between early detection and detection accuracy. In this paper, we present a situation-aware access control framework to work with existing file systems as a stackable add-on. Our framework enables the access control decision making to be deferred when required, to observe the consequence of such an access request to the file system and to roll back changes if required. As an application against ransomware attacks, it can be applied to preserve file content integrity, by enforcing that all binary files written to the file system have consistent internal file structures with the declared file types, and rolling back changes that violate such constraints. We envision our access control framework to complement existing operating system access control frameworks, to significantly reduce the dimension of data required for machine learning, and to build extra resilience into the operating systems against damages caused by either malware or software defects. We demonstrate the practicality of our framework through a prototype testing, capturing relevant ransomware situations. The experimental results along with a large ransomware dataset show that our framework can be effectively applied in practice.}
}
@article{ILG2023103737,
title = {A survey of contemporary open-source honeypots, frameworks, and tools},
journal = {Journal of Network and Computer Applications},
volume = {220},
pages = {103737},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103737},
url = {https://www.sciencedirect.com/science/article/pii/S108480452300156X},
author = {Niclas Ilg and Paul Duplys and Dominik Sisejkovic and Michael Menth},
keywords = {Honeypot, Honeypot framework, Cybersecurity, Threat intelligence},
abstract = {Automated attacks allow adversaries to exploit vulnerabilities in enterprise IT systems at short notice. To identify such attacks as well as new cybersecurity threats, defenders use honeypot systems; these monitored decoy resources mimic legitimate devices to entice adversaries. The domain of enterprise IT honeypots has been an active area of development and research, especially in the open-source community. In this work, we survey open-source honeypots, honeypot frameworks, and tools that help to develop or discover honeypot deployments. In contrast to existing surveys, our work provides a detailed discussion of the honeypots’ system architecture, software architecture, and cloud-native deployment options. In addition, we cover the most recent academic research in honeypot detection and evasion techniques, and discuss how these advances impact current open-source honeypots. This work helps the reader to make an educated choice when selecting a honeypot for deployment or further development.}
}
@article{WANG2021106408,
title = {Large-scale intent analysis for identifying large-review-effort code changes},
journal = {Information and Software Technology},
volume = {130},
pages = {106408},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106408},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300033},
author = {Song Wang and Chetan Bansal and Nachiappan Nagappan},
keywords = {Change intent analysis, Review effort, Machine learning},
abstract = {Context: Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. Change intents have been studied for years to help developers understand the rationale behind code commits. However, in most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis. Objective: In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes—changes with large review effort. Method: Specifically, we first propose a feedback-driven and heuristics-based approach to identify change intents of code changes. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on four large-scale projects, one from Microsoft and three are open source projects, i.e., Qt, Android, and OpenStack. Results: Our results show that, (i) code changes with some intents (i.e., Feature and Refactor) are more likely to be LRE changes, (ii) machine learning based prediction models are applicable for identifying LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. Conclusion: The change intent analysis and its application on LRE identification proposed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process. To show how to deploy our approaches in real-world practice, we report a case study of developing and deploying the intent analysis system in Microsoft. Moreover, we also evaluate the usefulness of our approaches by using a questionnaire survey. The feedback from developers demonstrate its practical value.}
}
@article{KUK2024114406,
title = {All of the same breed? A networking perspective of private-collective innovation},
journal = {Journal of Business Research},
volume = {172},
pages = {114406},
year = {2024},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2023.114406},
url = {https://www.sciencedirect.com/science/article/pii/S0148296323007658},
author = {George Kuk and Mario Schaarschmidt and Dirk Homscheid},
keywords = {Network theory, Open source software, Private-collective innovation, Propensity score matching},
abstract = {Much research on open source software development has highlighted the best-of-both-worlds benefits generated by private and collective contributions from a broad base of developers. However, these studies have tended to overlook the heterogeneous nature of various developer groups including firm-sponsored developers. To unveil the behavioural differences between developer groups, we expand upon the private-collective innovation model using a networking approach, linking network closure and positional embeddedness with technical contribution. We tested our predictions using a lagged analysis based on communication and networking behaviours on the Linux kernel mailing-list in the production of the Linux operation system over a three-year period. Our findings support our predictions, showing that network closure has an adverse impact on technical contribution. Additionally, the relationship between positional embeddedness and technical contribution follows an inverted U-shape. In addition, high positional embeddedness counteracts the negative influence of extensive network closure on technical contribution. These effects are partly moderated by respective developer groups. Our model and results offer important theoretical and practical implications for community management within the framework of private-collective innovation.}
}
@article{CHAN2019138,
title = {Web-based experimental economics software: How do they compare to desirable features?},
journal = {Journal of Behavioral and Experimental Finance},
volume = {23},
pages = {138-160},
year = {2019},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S221463501830090X},
author = {Shu Wing Chan and Steven Schilizzi and Md Sayed Iftekhar and Raymond {Da Silva Rosa}},
keywords = {Experimental economics, Web-based, Software, Online experiments, Web-based experiments, Economic experiments},
abstract = {Web-based experiments that cut across the lab vs. field distinction are increasingly popular with economists. However, non-standardized software features and services hinder comparability and replication. This study reviews a wide selection of experimental economics software packages and evaluates them against criteria based on the logistics and operational requirements of economic experiments. We find that oTree and SoPHIE rank highest across criteria, but Veconlab and classEx might be suitable for those with a dominant need for a large library of ready-made experiments. We find a portability gap: no presently available software allows portability of experiments across platforms because of technical complexity and the challenging coordination needs of experimental economists. As a result, experiments may be replicated only on the same platform or with the same software, but general replicability is slow and costly. This constrains the development of experimental economics as a replicable science.}
}
@article{VANOSS200287,
title = {Hans Visser 1936–2001},
journal = {Colloids and Surfaces B: Biointerfaces},
volume = {24},
number = {2},
pages = {87-89},
year = {2002},
issn = {0927-7765},
doi = {https://doi.org/10.1016/S0927-7765(01)00295-8},
url = {https://www.sciencedirect.com/science/article/pii/S0927776501002958},
author = {Carel Jan {van Oss}}
}
@article{WATTANAKRIENGKRAI2022111117,
title = {GitHub repositories with links to academic papers: Public access, traceability, and evolution},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111117},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111117},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002144},
author = {Supatsara Wattanakriengkrai and Bodin Chinthanet and Hideaki Hata and Raula Gaikovina Kula and Christoph Treude and Jin Guo and Kenichi Matsumoto},
keywords = {Software documentation, Open science, Open access, Traceability},
abstract = {Traceability between published scientific breakthroughs and their implementation is essential, especially in the case of open-source scientific software which implements bleeding-edge science in its code. However, aligning the link between GitHub repositories and academic papers can prove difficult, and the current practice of establishing and maintaining such links remains unknown. This paper investigates the role of academic paper references contained in these repositories. We conduct a large-scale study of 20 thousand GitHub repositories that make references to academic papers. We use a mixed-methods approach to identify public access, traceability and evolutionary aspects of the links. Although referencing a paper is not typical, we find that a vast majority of referenced academic papers are public access. These repositories tend to be affiliated with academic communities. More than half of the papers do not link back to any repository. We find that academic papers from top-tier SE venues are not likely to reference a repository, but when they do, they usually link to a GitHub software repository. In a network of arXiv papers and referenced repositories, we find that the most referenced papers are (i) highly-cited in academia and (ii) are referenced by repositories written in different programming languages.}
}
@article{BLACKMAN201296,
title = {Our Referees – An Appreciation},
journal = {World Patent Information},
volume = {34},
number = {1},
pages = {96-98},
year = {2012},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2011.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0172219011001876},
author = {Michael Blackman and David Newton}
}
@article{ANDREE2022113978,
title = {Deep brain stimulation electrode modeling in rats},
journal = {Experimental Neurology},
volume = {350},
pages = {113978},
year = {2022},
issn = {0014-4886},
doi = {https://doi.org/10.1016/j.expneurol.2022.113978},
url = {https://www.sciencedirect.com/science/article/pii/S0014488622000036},
author = {Andrea Andree and Ningfei Li and Konstantin Butenko and Maria Kober and Jia Zhi Chen and Takahiro Higuchi and Mareike Fauser and Alexander Storch and Chi Wang Ip and Andrea A. Kühn and Andreas Horn and Ursula {van Rienen}},
keywords = {Deep brain stimulation, Parkinson's disease, Animal models, Rat, Rodent, Neuroimaging, Open-source, Research software},
abstract = {Deep Brain Stimulation (DBS) is an efficacious treatment option for an increasing range of brain disorders. To enhance our knowledge about the mechanisms of action of DBS and to probe novel targets, basic research in animal models with DBS is an essential research base. Beyond nonhuman primate, pig, and mouse models, the rat is a widely used animal model for probing DBS effects in basic research. Reconstructing DBS electrode placement after surgery is crucial to associate observed effects with modulating a specific target structure. Post-mortem histology is a commonly used method for reconstructing the electrode location. In humans, however, neuroimaging-based electrode localizations have become established. For this reason, we adapt the open-source software pipeline Lead-DBS for DBS electrode localizations from humans to the rat model. We validate our localization results by inter-rater concordance and a comparison with the conventional histological method. Finally, using the open-source software pipeline OSS-DBS, we demonstrate the subject-specific simulation of the VTA and the activation of axon models aligned to pathways representing neuronal fibers, also known as the pathway activation model. Both activation models yield a characterization of the impact of DBS on the target area. Our results suggest that the proposed neuroimaging-based method can precisely localize DBS electrode placements that are essentially rater-independent and yield results comparable to the histological gold standard. The advantages of neuroimaging-based electrode localizations are the possibility of acquiring them in vivo and combining electrode reconstructions with advanced imaging metrics, such as those obtained from diffusion or functional magnetic resonance imaging (MRI). This paper introduces a freely available open-source pipeline for DBS electrode reconstructions in rats. The presented initial validation results are promising.}
}
@article{BARCELLINI2008141,
title = {A socio-cognitive analysis of online design discussions in an Open Source Software community},
journal = {Interacting with Computers},
volume = {20},
number = {1},
pages = {141-165},
year = {2008},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2007.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0953543807000793},
author = {Flore Barcellini and Françoise Détienne and Jean-Marie Burkhardt and Warren Sack},
keywords = {Open Source Software community, Distributed and asynchronous design, Online discussions, Quoting, Role},
abstract = {This paper is an analysis of online discussions in an Open Source Software (OSS) design community, the Python project. Developers of Python are geographically distributed and work online asynchronously. The objective of our study is to understand and to model the dynamics of the OSS design process that takes place in mailing list exchanges. We develop a method to study distant and asynchronous collaborative design activity based on an analysis of quoting practices. We analyze and visualize three aspects of the online dynamics: social, thematic temporal, and design. We show that roles emerge during discussions according to the involvement and the position of the participants in the discussions and how they influence participation in the design discussions. In our analysis of the thematic temporal dynamics of discussion, we examine how themes of discussion emerge, diverge, and are refined over time. To understand the design dynamics, we perform a content analysis of messages exchanged between developers to reveal how the online discussions reflect the “work flow” of the project: it provides us with a picture of the collaborative design process in the OSS community. These combined results clarify how knowledge and artefacts are elaborated in this epistemic, exploration-oriented, OSS community. Finally, we outline the need to automate of our method to extend our results. The proposed automation could have implications for both researchers and participants in OSS communities.}
}
@article{DAVILA2021110951,
title = {A systematic literature review and taxonomy of modern code review},
journal = {Journal of Systems and Software},
volume = {177},
pages = {110951},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110951},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000480},
author = {Nicole Davila and Ingrid Nunes},
keywords = {Modern code review, Software verification, Software quality, Systematic literature review},
abstract = {Context:
Modern Code Review (MCR) is a widely known practice of software quality assurance. However, the existing body of knowledge of MCR is currently not understood as a whole.
Objective:
Our goal is to identify the state of the art on MCR, providing a structured overview and an in-depth analysis of the research done in this field.
Methods:
We performed a systematic literature review, selecting publications from four digital libraries.
Results:
A total of 139 papers were selected and analyzed in three main categories. Foundational studies are those that analyze existing or collected data from the adoption of MCR. Proposals consist of techniques and tools to support MCR, while evaluations are studies to assess an approach or compare a set of them.
Conclusion:
The most represented category is foundational studies, mainly aiming to understand the motivations for adopting MCR, its challenges and benefits, and which influence factors lead to which MCR outcomes. The most common types of proposals are code reviewer recommender and support to code checking. Evaluations of MCR-supporting approaches have been done mostly offline, without involving human subjects. Five main research gaps have been identified, which point out directions for future work in the area.}
}
@article{JOOKEN2023106765,
title = {Mining Recency–Frequency–Monetary enriched insights into resources’ collaboration behavior from event data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106765},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106765},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623009491},
author = {Leen Jooken and Benoît Depaire and Mieke Jans},
keywords = {Event data behavioral analytics, Collaboration behavior, Mining resource behavior, Project mining, RFM, Social network analysis},
abstract = {Organizations increasingly rely on teamwork to achieve their goals. Therefore they continuously strive to improve their teams as their performance is interwoven with that of the organization. To implement beneficial changes, accurate insights into the working of the team are necessary. However, team leaders tend to have an understanding of the team’s collaboration that is subjective and seldom completely accurate. Recently there has been an increase in the adoption of digital support systems for collaborative work that capture objective data on how the work took place in reality. This creates the opportunity for data-driven extraction of insights into the collaboration behavior of a team. This data however, does not explicitly record the collaboration relationships, which many existing techniques expect as input. Therefore, these relationships first have to be discovered. Existing techniques that apply discovery are not generally applicable because their notion of collaboration is tailored to the application domain. Moreover, the information that these techniques extract from the data about the nature of the relationships is often limited to the network level. Therefore, this research proposes a generic algorithm that can discover collaboration relationships between resources from event data on any collaborative project. The algorithm adopts an established framework to provide insights into collaboration on a fine-grained level. To this end, three properties are calculated for both the resources and their collaboration relationships: a recency, frequency, and monetary value. The technique’s ability to provide valuable insights into the team structure and characteristics is empirically validated on two use cases.}
}
@article{MOHAMMADZAD2023103606,
title = {Using rootkits hiding techniques to conceal honeypot functionality},
journal = {Journal of Network and Computer Applications},
volume = {214},
pages = {103606},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103606},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523000255},
author = {Maryam Mohammadzad and Jaber Karimpour},
keywords = {Computer network, Cyber-attacks, Honeypot, Intrusion detection system, Rootkit, Security},
abstract = {Honeypot is one of the existing technologies in the area of computer network security. The goal of Honeypot is to create a tempting target for the attacker. The system that is considered as a Honeypot in the network includes the services and functions of a real system that the attacker sees as a normal system and enters for exploitation. In this way, Honeypot can monitor the behaviors, patterns, and tools used in various attacks. Certainly, if the intelligent attacker realizes the existence of this trap in the target network, he can design ways to bypass it. In this case, Honeypot practically loses its effectiveness. Therefore, the issue of hiding Honeypot is one of the primary challenges in this field. In this paper, a solution to this problem is presented. In the present paper, Honeypot is concealed using the concealment techniques used in Rootkits. We use application examples and theoretical analysis results to show that the proposed Honeypots concealment approach is strong against existing kernel-based Honeypots detection methods. Sebek, VMScope, and Qebek are three Honeypots that we choose for comparison purposes. The proposed Honeypot has been compared with them in virtualization, memory usage, and kernel modification. The experimental results show that the proposed hidden Honeypot in addition to low kernel modification has no track in the memory. Also, the proposed Honeypot does not use virtualization. It can successfully be concealed in the kernel of the target system without any effect on the target system. We also implemented the proposed algorithm on the example network and test all Sebek’s detection methods on it. The experimental results show that the proposed kernel-based approach can bypass these detection methods.}
}
@article{ZHANG2007243,
title = {A fast infrared radiative transfer model based on the adding–doubling method for hyperspectral remote-sensing applications},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {105},
number = {2},
pages = {243-263},
year = {2007},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2007.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S002240730700009X},
author = {Zhibo Zhang and Ping Yang and George Kattawar and Hung-Lung {(Allen) Huang} and Thomas Greenwald and Jun Li and Bryan A. Baum and Daniel K. Zhou and Yongxiang Hu},
keywords = {Hyperspectral, Radiative transfer, Clouds, Adding–doubling, Remote sensing, Fast model},
abstract = {A fast infrared radiative transfer (RT) model is developed on the basis of the adding–doubling principle, hereafter referred to as FIRTM-AD, to facilitate the forward RT simulations involved in hyperspectral remote-sensing applications under cloudy-sky conditions. A pre-computed look-up table (LUT) of the bidirectional reflection and transmission functions and emissivities of ice clouds in conjunction with efficient interpolation schemes is used in FIRTM-AD to alleviate the computational burden of the doubling process. FIRTM-AD is applicable to a variety of cloud conditions, including vertically inhomogeneous or multilayered clouds. In particular, this RT model is suitable for the computation of high-spectral-resolution radiance and brightness temperature (BT) spectra at both the top-of-atmosphere and surface, and thus is useful for satellite and ground-based hyperspectral sensors. In terms of computer CPU time, FIRTM-AD is approximately 100–250 times faster than the well-known discrete-ordinate (DISORT) RT model for the same conditions. The errors of FIRTM-AD, specified as root-mean-square (RMS) BT differences with respect to their DISORT counterparts, are generally smaller than 0.1K.}
}
@article{LI2023107319,
title = {Warnings: Violation symptoms indicating architecture erosion},
journal = {Information and Software Technology},
volume = {164},
pages = {107319},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107319},
url = {https://www.sciencedirect.com/science/article/pii/S095058492300174X},
author = {Ruiyin Li and Peng Liang and Paris Avgeriou},
keywords = {Architecture erosion symptom, Architecture violation, Code review, Code commit},
abstract = {Context:
As a software system evolves, its architecture tends to degrade, and gradually impedes software maintenance and evolution activities and negatively impacts the quality attributes of the system. The main root cause behind architecture erosion phenomenon derives from violation symptoms (i.e., various architecturally-relevant violations, such as violations of architecture pattern). Previous studies focus on detecting violations in software systems using architecture conformance checking approaches. However, code review comments are also rich sources that may contain extensive discussions regarding architecture violations, while there is a limited understanding of violation symptoms from the viewpoint of developers.
Objective:
In this work, we investigated the characteristics of architecture violation symptoms in code review comments from the developers’ perspective.
Methods:
We employed a set of keywords Related to violation symptoms to collect 606 (out of 21,583) code review comments from four popular OSS projects in the openStack and qt communities. We manually analyzed the collected 606 review comments to provide the categories and linguistic patterns of violation symptoms, as well as the reactions how developers addressed them.
Results:
Our findings show that: (1) three main categories of violation symptoms are discussed by developers during the code review process; (2) The frequently-used terms of expressing violation symptoms are “inconsistent” and “violate”, and the most common linguistic pattern is Problem Discovery; (3) Refactoring and removing code are the major measures (90%) to tackle violation symptoms, while a few violation symptoms were ignored by developers.
Conclusions:
Our findings suggest that the investigation of violation symptoms can help researchers better understand the characteristics of architecture erosion and facilitate the development and maintenance activities, and developers should explicitly manage violation symptoms, not only for addressing the existing architecture violations but also preventing future violations.}
}
@article{VANROIJEN2008125,
title = {ULTRASONOGRAPHIC EVALUATION OF THE EPIDIDYMIS IN 139 SUB FERTILE MALES AND COMPARISON WITH CLINICAL FINDINGS},
journal = {European Urology Supplements},
volume = {7},
number = {3},
pages = {125},
year = {2008},
note = {23rd Annual Congress of the European Association of Urology},
issn = {1569-9056},
doi = {https://doi.org/10.1016/S1569-9056(08)60219-5},
url = {https://www.sciencedirect.com/science/article/pii/S1569905608602195},
author = {J.H. {Van Roijen} and R.S.G.M. Bots and M.C. Schoemaker}
}
@article{MAO2024281,
title = {Simulation and evaluation study of atmospheric aerosol nonsphericity as a function of particle size},
journal = {Particuology},
volume = {90},
pages = {281-291},
year = {2024},
issn = {1674-2001},
doi = {https://doi.org/10.1016/j.partic.2023.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1674200124000014},
author = {Qianjun Mao and Xin Nie},
keywords = {Aerosol nonsphericity, Inversion, Aspect ratio, Radiative forcing, Dry deposition velocity},
abstract = {Aerosol nonsphericity causes great uncertainty in radiative forcing assessments and climate simulations. Although considerable studies have attempted to quantify this uncertainty, the relationship between aerosol nonsphericity and particle size is usually not considered, thus reducing the accuracy of the results. In this study, a coupled inversion algorithm combining an improved stochastic particle swarm optimization algorithm and angular light scattering is used for the nonparametric estimation of aerosol nonsphericity variation with particle size, and the optimal sample selection method is employed to screen the data. Based on the verification of inversion accuracy, the variation of aerosol aspect ratio with particle size based on the ellipsoidal model in global regions has been obtained from Aerosol Robotic Network (AERONET) data, and the effect of nonsphericity on radiative forcing and dry deposition has been studied. The results show that the aspect ratio increases with particle size in all regions, with the maximum ranging from 1.4 to 1.8 in the desert, reflecting the differences in aerosol composition at different particle sizes. In radiation calculations, considering aerosol nonsphericity makes the aerosol cooling effect weaker and surface radiative fluxes increase, but hardly changes the aerosol absorption, with maximum differences of 9.22% and 22.12% at the bottom and top of the atmosphere, respectively. Meanwhile, the differences in radiative forcing between aspect ratios as a function of particle size and not varying with particle size are not significant, averaging less than 2%. Besides, the aspect ratio not varying with particle size underestimates the deposition velocity of small particles and overestimates that of large particles compared to that as a function of particle size, with maximum differences of 7% and 4%, respectively.}
}
@article{GUARDIOLA1995289,
title = {Comparison of three methods for the determination of oxysterols in spray-dried egg},
journal = {Journal of Chromatography A},
volume = {705},
number = {2},
pages = {289-304},
year = {1995},
issn = {0021-9673},
doi = {https://doi.org/10.1016/0021-9673(95)00034-K},
url = {https://www.sciencedirect.com/science/article/pii/002196739500034K},
author = {Francesc Guardiola and Rafael Codony and Magda Rafecas and Josep Boatella},
abstract = {Three methods for the GC determination of oxysterols (OSs) in spray-dried egg, which combine different steps of purification, are compared. In addition, the efficiency of silica cartridges in the purification of OSs using four different systems of elution with increasing polarities is studied. The absence of cholesterol oxidation during the application of the analytical procedures is checked, and the linearity of the response and the chromatographic limits of detection and quantification are established. The methods are characterized by the calculation of precision and recovery for the different OSs. The method based on saponification alone is rejected, since it shows much lower precision. The method that includes saponification and silica cartridge purification offers higher reliability than the method based on cartridge purification alone, because it shows a higher precision and larger samples can be processed, which improves the limits of detection and quantification.}
}
@article{DECAN2020110573,
title = {GAP: Forecasting commit activity in git projects},
journal = {Journal of Systems and Software},
volume = {165},
pages = {110573},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110573},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300546},
author = {Alexandre Decan and Eleni Constantinou and Tom Mens and Henrique Rocha},
keywords = {Git, Commit activity, Developer abandonment, Distributed software development, Prediction model},
abstract = {Abandonment of active developers poses a significant risk for many open source software projects. This risk can be reduced by forecasting the future activity of contributors involved in such projects. Focusing on the commit activity of individuals involved in git repositories, this paper proposes a practicable probabilistic forecasting model based on the statistical technique of survival analysis. The model is empirically validated on a wide variety of projects accounting for 7528 git repositories and 5947 active contributors. We found that a model based on the last 20 observed days of commit activity per contributor provides the best concordance. We also found that the predictions provided by the model are generally close to actual observations, with slight underestimations for low probability predictions and slight overestimations for higher probability predictions. This model is implemented as part of an open source tool, called GAP, that predicts future commit activity.}
}
@article{BERHE2024618,
title = {Triage Software Update Impact via Release Notes Classification},
journal = {Procedia Computer Science},
volume = {238},
pages = {618-622},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.069},
url = {https://www.sciencedirect.com/science/article/pii/S187705092401305X},
author = {Solomon Berhe and Vanessa Kan and Omhier Khan and Nathan Pader and Ali Zain Farooqui and Marc Maynard and Foutse Khomh},
keywords = {Triage, Software, Update, Release Notes, Classifier, Evaluation},
abstract = {In the rapidly evolving domain of Industry 4.0, effective management of software updates is crucial for maintaining system continuity and security. This paper presents a novel machine learning-based approach for a prompt and effective triage of software updates, leveraging an evaluation of six release note classifiers to categorize updates by component type, release type, and security risk. Our methodology, tested on a dataset of 1,000 release notes commonly encountered in Industry 4.0 ecosystems, demonstrates Logistic Regression as the most accurate classifier. The findings not only highlight the practical applicability of our approach in real-world data but also set the foundation for future enhancements to streamline the machine learning triage process further.}
}
@article{KIBBY201175,
title = {Fungal Portraits: No. 47: Volvariella aethiops, the first British collection},
journal = {Field Mycology},
volume = {12},
number = {3},
pages = {75-76},
year = {2011},
issn = {1468-1641},
doi = {https://doi.org/10.1016/j.fldmyc.2011.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1468164111000168},
author = {Geoffrey Kibby}
}
@article{MACAS2024122223,
title = {Adversarial examples: A survey of attacks and defenses in deep learning-enabled cybersecurity systems},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122223},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122223},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027252},
author = {Mayra Macas and Chunming Wu and Walter Fuertes},
keywords = {Cybersecurity, Deep learning, Adversarial machine learning, Cyber threats, Adversarial examples},
abstract = {Over the last few years, the adoption of machine learning in a wide range of domains has been remarkable. Deep learning, in particular, has been extensively used to drive applications and services in specializations such as computer vision, natural language processing, machine translation, and cybersecurity, producing results that are comparable to or even surpass the performance of human experts. Nevertheless, machine learning systems are vulnerable to adversarial attacks, especially in nonstationary environments where actual adversaries exist, such as the cybersecurity domain. In this work, we comprehensively survey and present the latest research on attacks based on adversarial examples against deep learning-based cybersecurity systems, highlighting the risks they pose and promoting efficient countermeasures. To that end, adversarial attack methods are first categorized according to where they occur and the attacker’s goals and capabilities. Then, specific attacks based on adversarial examples and the respective defensive methods are reviewed in detail within the framework of eight principal cybersecurity application categories. Finally, the main trends in recent research are outlined, and the impact of recent advancements in adversarial machine learning is explored to provide guidelines and directions for future research in cybersecurity. In summary, this work is the first to systematically analyze adversarial example-based attacks in the cybersecurity field, discuss possible defenses, and highlight promising directions for future research.}
}
@article{ESCALEIRA2023100916,
title = {Moving Target Defense for the cloud/edge Telco environments},
journal = {Internet of Things},
volume = {24},
pages = {100916},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100916},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002391},
author = {Pedro Escaleira and Vitor A. Cunha and Diogo Gomes and João P. Barraca and Rui L. Aguiar},
keywords = {Moving Target Defense (MTD), MTD as a Service (MTDaaS), Network Functions Virtualization (NFV), Cloud security, Security as a Service (SECaaS), Zero-day vulnerabilities},
abstract = {The Internet of Things (IoT) paradigm has been one of the main contributors, in recent years, to the growth in the number of connected equipment. This fact has predominantly contributed to IoT being constrained by the 5th Generation Mobile Network (5G) progress and the promises this technology brings. However, this can be a double-edged sword. On the one hand, it will benefit from those progresses, but on the other, it will also be impacted by any security risk associated with 5G. One of the more serious security problems associated with it is the new wave of virtualization and softwarization of networks and analogous appliances, brought to light by paradigms such as Network Functions Virtualization (NFV) and Multi-access Edge Computing (MEC). Considering these predicaments, we propose a state-of-the-art Moving Target Defense (MTD) approach that defends Cloud-based Network Functions (CNFs) launched within MEC and NFV environments. Furthermore, our mechanism follows the famous Everything as a Service (XaaS) ideology, allowing any CNF provider to use this protection system, working agonistically. In the end, we created a Proof of Concept (PoC) of our proposed methodology, which we then used to conduct an extensive practical security analysis against the multiple phases of the Intrusion Kill Chain. Our final results have proven that our MTD as a Service (MTDaaS) approach can effectively delay and, in some cases, stop an attacker from achieving its objectives when trying to attack a CNF, even if the related vulnerability is a zero-day.}
}
@article{ARVANITOU2016110,
title = {Software metrics fluctuation: a property for assisting the metric selection process},
journal = {Information and Software Technology},
volume = {72},
pages = {110-124},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915002190},
author = {Elvira-Maria Arvanitou and Apostolos Ampatzoglou and Alexander Chatzigeorgiou and Paris Avgeriou},
keywords = {Object-oriented metrics, Fluctuation, Case study, Software evolution},
abstract = {Context
Software quality attributes are assessed by employing appropriate metrics. However, the choice of such metrics is not always obvious and is further complicated by the multitude of available metrics. To assist metrics selection, several properties have been proposed. However, although metrics are often used to assess successive software versions, there is no property that assesses their ability to capture structural changes along evolution.
Objective
We introduce a property, Software Metric Fluctuation (SMF), which quantifies the degree to which a metric score varies, due to changes occurring between successive system's versions. Regarding SMF, metrics can be characterized as sensitive (changes induce high variation on the metric score) or stable (changes induce low variation on the metric score).
Method
SMF property has been evaluated by: (a) a case study on 20 OSS projects to assess the ability of SMF to differently characterize different metrics, and (b) a case study on 10 software engineers to assess SMF's usefulness in the metric selection process.
Results
The results of the first case study suggest that different metrics that quantify the same quality attributes present differences in their fluctuation. We also provide evidence that an additional factor that is related to metrics’ fluctuation is the function that is used for aggregating metric from the micro to the macro level. In addition, the outcome of the second case study suggested that SMF is capable of helping practitioners in metric selection, since: (a) different practitioners have different perception of metric fluctuation, and (b) this perception is less accurate than the systematic approach that SMF offers.
Conclusions
SMF is a useful metric property that can improve the accuracy of metrics selection. Based on SMF, we can differentiate metrics, based on their degree of fluctuation. Such results can provide input to researchers and practitioners in their metric selection processes.}
}
@article{ROBERTS20188,
title = {Learning lessons from data breaches},
journal = {Network Security},
volume = {2018},
number = {11},
pages = {8-11},
year = {2018},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(18)30111-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485818301119},
author = {Steve Roberts},
abstract = {Bricks-and-mortar retail sales are rapidly being replaced by online shopping. And where money is spent, bad actors will follow. Data breaches involving e-commerce have now surpassed breaches at the point of sale and they're becoming very expensive. In spite of this, there remains an alarming lack of corporate self-awareness when it comes to cyber security. Where money is spent, bad actors will follow. Data breaches involving e-commerce have now surpassed breaches at the point of sale and they're becoming very expensive. In spite of this, there remains an alarming lack of corporate self-awareness when it comes to cyber security. This has led to major incidents that could have been avoided. Steve Roberts of OSS Technology analyses some major breaches of the past few years and asks what went wrong, how could they have been prevented and what lessons can we learn?}
}
@article{LIU2009277,
title = {Characteristics and Evolution of Sedimentary Microfacies of Chang 6–4+5 Layer in the Northern Area of Western Mahuang Mountain},
journal = {Earth Science Frontiers},
volume = {16},
number = {4},
pages = {277-286},
year = {2009},
issn = {1872-5791},
doi = {https://doi.org/10.1016/S1872-5791(08)60100-1},
url = {https://www.sciencedirect.com/science/article/pii/S1872579108601001},
author = {Yumei LIU and Xinghe YU and Shengli LI and Weiwei DU and Mei LI and Shunli LI},
keywords = {northern area of Western Mahuang Mountain, sedimentary microfacies, law of evolution, Chang6–4+5 layer},
abstract = {Although there are widespread indications of oil and gas in the Chang 6–4+5 layer of western Mahuang Mountian, well test proves that there is more water and less oil with the distribution and continuity of sand being not yet clear. To understand the distribution and evolution of sedimentary microfacies in the area, six kinds of sedimentary microfacies and ten kinds of sedimentary structures were identified, and ten patterns of contact between microfacies and five kinds of sedimentary sequence model were summarized through detailed observation and description of the core of rock and logging data. Each model represents different sedimentary facies. Reversed cycle is dominant and normal short-term cycle also exists in the vertical direction; Mouth bar and subaqueous distributary channel developed alternately in the lateral direction. The lower channel sand is “large in quantity but small in size with the layer narrow and thin”, whereas the upper sand is “few in quantity but large in size with a wide and thick layer”. The “single-factor analysis and multifactor comprehensive” research method was used to divide this area into three phase belts—delta inner front, delta outer front, and predelta. Subaqueous distributary channel, distributary mouth bar, and interdistributary deposits are prevailing in the delta inner front. Distal bar and sand sheet are dominant in the delta outer front, and predelta mud is dominant in the predelta. According to modern sedimentary facies model, the area is a constructive braided river-lake delta. Three stages appeared in its evolution: the initial development period (Chang 63), in which the predelta and the delta outer front are the main facies belts; the early-middle development period (Chang 62–Chang 61), in which the inner delta front and the delta outer front dominates and each micro-facies is a single phase and is deposited alternately; the middle-late development period (Chang 4 +52–Chang 4 +51), in which the delta inner front dominates and subaqueous distributary channel shows multiphase stacking pattern.}
}