@article{LONGO2021104269,
title = {Caspar: Towards decision making helpers agents for IoT, based on natural language and first order logic reasoning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104269},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104269},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001160},
author = {Carmelo Fabio Longo and Francesco Longo and Corrado Santoro},
keywords = {Internet of Things, Computational linguistic, Artificial Intelligence, First order logic, Cognitive architectures, Meta-reasoning},
abstract = {In the last decade, the market of Internet of Things has become quite disruptive, together with commercial clouds providing connection between every sort of devices and the global network, supported by vocal assistants. On the other hands, such commercial products are limited to work on limited domains, although easily scalable, without aspiring to higher level of reasoning in the field of Decisions Making. In this work, we show a way towards the design of an architecture for building cognitive agents leveraging Natural Language Processing. Such agents will be not based on clouds and do not require any semantic training, plus they will be able of deduction on facts and rules in First Order Logic inferred directly from Natural Language. After the description of the architecture and its underlying components, a case-study is provided to show the effectiveness in cases of direct commands and routines, subordinated also by a Meta-Reasoning in a conceptual space, parsing the utterances with promising real-time performances.}
}
@article{DWIVEDI2023102642,
title = {Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
journal = {International Journal of Information Management},
volume = {71},
pages = {102642},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102642},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000233},
author = {Yogesh K. Dwivedi and Nir Kshetri and Laurie Hughes and Emma Louise Slade and Anand Jeyaraj and Arpan Kumar Kar and Abdullah M. Baabdullah and Alex Koohang and Vishnupriya Raghavan and Manju Ahuja and Hanaa Albanna and Mousa Ahmad Albashrawi and Adil S. Al-Busaidi and Janarthanan Balakrishnan and Yves Barlette and Sriparna Basu and Indranil Bose and Laurence Brooks and Dimitrios Buhalis and Lemuria Carter and Soumyadeb Chowdhury and Tom Crick and Scott W. Cunningham and Gareth H. Davies and Robert M. Davison and Rahul Dé and Denis Dennehy and Yanqing Duan and Rameshwar Dubey and Rohita Dwivedi and John S. Edwards and Carlos Flavián and Robin Gauld and Varun Grover and Mei-Chih Hu and Marijn Janssen and Paul Jones and Iris Junglas and Sangeeta Khorana and Sascha Kraus and Kai R. Larsen and Paul Latreille and Sven Laumer and F. Tegwen Malik and Abbas Mardani and Marcello Mariani and Sunil Mithas and Emmanuel Mogaji and Jeretta Horn Nord and Siobhan O’Connor and Fevzi Okumus and Margherita Pagani and Neeraj Pandey and Savvas Papagiannidis and Ilias O. Pappas and Nishith Pathak and Jan Pries-Heje and Ramakrishnan Raman and Nripendra P. Rana and Sven-Volker Rehm and Samuel Ribeiro-Navarrete and Alexander Richter and Frantz Rowe and Suprateek Sarker and Bernd Carsten Stahl and Manoj Kumar Tiwari and Wil {van der Aalst} and Viswanath Venkatesh and Giampaolo Viglia and Michael Wade and Paul Walton and Jochen Wirtz and Ryan Wright},
keywords = {Conversational agent, Generative artificial intelligence, Generative AI, ChatGPT, Large language models},
abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.}
}
@article{ROY2023200283,
title = {MalHyStack: A hybrid stacked ensemble learning framework with feature engineering schemes for obfuscated malware analysis},
journal = {Intelligent Systems with Applications},
volume = {20},
pages = {200283},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200283},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001084},
author = {Kowshik Sankar Roy and Tanim Ahmed and Pritom Biswas Udas and Md. Ebtidaul Karim and Sourav Majumdar},
keywords = {Malware detection, Obfuscated malware, Ensemble learning, Stacking, Deep learning, Pearson correlation coefficient},
abstract = {Since the advent of malware, it has reached a toll in this world that exchanges billions of data daily. Millions of people are victims of it, and the numbers are not decreasing as the year goes by. Malware is of various types in which obfuscation is a special kind. Obfuscated malware detection is necessary as it is not usually detectable and is prevalent in the real world. Although numerous works have already been done in this field so far, most of these works still need to catch up at some points, considering the scope of exploration through recent extensions. In addition to that, the application of a hybrid classification model is yet to be popularized in this field. Thus, in this paper, a novel hybrid classification model named, MalHyStack, has been proposed for detecting such obfuscated malware within the network. This proposed working model is built incorporating a stacked ensemble learning scheme, where conventional machine learning algorithms namely, Extremely Randomized Trees Classifier (ExtraTrees), Extreme Gradient Boosting (XgBoost) Classifier, and Random Forest are used in the first layer which is then followed by a deep learning layer in the second stage. Before utilizing the classification model for malware detection, an optimum subset of features has been selected using Pearson correlation analysis which improved the accuracy of the model by more than 2 % for multiclass classification. It also reduces time complexity by approximately two and three times for binary and multiclass classification, respectively. For evaluating the performance of the proposed model, a recently published balanced dataset named CIC-MalMem-2022 has been used. Utilizing this dataset, the overall experimental results of the proposed model represent a superior performance when compared to the existing classification models.}
}
@article{FLANAGAN2024105814,
title = {School-age children are more skeptical of inaccurate robots than adults},
journal = {Cognition},
volume = {249},
pages = {105814},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105814},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724001008},
author = {Teresa Flanagan and Nicholas C. Georgiou and Brian Scassellati and Tamar Kushnir},
keywords = {Cognitive development, Educational robotics, Selective trust, Human-robot interaction},
abstract = {We expect children to learn new words, skills, and ideas from various technologies. When learning from humans, children prefer people who are reliable and trustworthy, yet children also forgive people's occasional mistakes. Are the dynamics of children learning from technologies, which can also be unreliable, similar to learning from humans? We tackle this question by focusing on early childhood, an age at which children are expected to master foundational academic skills. In this project, 168 4–7-year-old children (Study 1) and 168 adults (Study 2) played a word-guessing game with either a human or robot. The partner first gave a sequence of correct answers, but then followed this with a sequence of wrong answers, with a reaction following each one. Reactions varied by condition, either expressing an accident, an accident marked with an apology, or an unhelpful intention. We found that older children were less trusting than both younger children and adults and were even more skeptical after errors. Trust decreased most rapidly when errors were intentional, but only children (and especially older children) outright rejected help from intentionally unhelpful partners. As an exception to this general trend, older children maintained their trust for longer when a robot (but not a human) apologized for its mistake. Our work suggests that educational technology design cannot be one size fits all but rather must account for developmental changes in children's learning goals.}
}
@article{MCINTOSH2023103160,
title = {Applying staged event-driven access control to combat ransomware},
journal = {Computers & Security},
volume = {128},
pages = {103160},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103160},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000706},
author = {Timothy McIntosh and A.S.M. Kayes and Yi-Ping Phoebe Chen and Alex Ng and Paul Watters},
keywords = {Ransomware, Malware, Access control, Ransomware mitigation, Intrusion prevention},
abstract = {The advancement of modern Operating Systems (OSs), and the popularity of personal computing devices with Internet connectivity, have facilitated the proliferation of ransomware attacks. Ransomware has evolved from executable programs encrypting user files, to novel attack vectors including fileless command scripts, information exfiltration and human-operated ransomware. Many anti-ransomware studies have been published, but many of them assumed newer ransomware variants only performed file encryption, were similar to existing variants, and often did not consider those novel attack vectors. We have defined an updated ransomware threat model to include those novel attack vectors, and redefined false positives and false negatives in the context of ransomware mitigation. We proposed to apply both program-centric and user-centric access control to combat ransomware, but only delegate access control decisions that users are capable of making to users, while enforcing non-negotiable access control decisions by OS and software developers. We have designed a Staged Event-Driven Access Control (SEDAC) approach to incorporate both program-centric and user-centric access control measures, and demonstrated a prototype on Windows OS. Our prototype was able to intercept more types of ransomware attack vectors than existing proposals. We hope to convince OS and software architects to incorporate our design to better combat ransomware.}
}
@incollection{CONRAD2016429,
title = {Chapter 9 - Domain 8: Software Development Security (Understanding, Applying, and Enforcing Software Security)},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
address = {Boston},
pages = {429-477},
year = {2016},
isbn = {978-0-12-802437-9},
doi = {https://doi.org/10.1016/B978-0-12-802437-9.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128024379000096},
author = {Eric Conrad and Seth Misenar and Joshua Feldman},
keywords = {Extreme Programming, Object, Object-Oriented Programming, Procedural languages, Spiral Model, Systems Development Life Cycle, Waterfall Model},
abstract = {Chapter 9 introduces Domain 8 of the CISSP, Software Development Security. The most important aspects of this domain are related to managing the development of software and applications. Approaches to software development that attempt to reduce the likelihood of defects or flaws are a key topic in this domain. In particular, the Waterfall, Spiral, and Rapid Application Development (RAD) models of the software development are considered. Another significant portion of this chapter is dedicated to understanding the principles of Object Oriented programming and design. A basic discussion of several types of software vulnerabilities and the issues surrounding disclosure of the vulnerabilities are also a topic for this domain. Finally, databases, being a key component of many applications, are considered.}
}
@article{BAUER2023107299,
title = {Code review guidelines for GUI-based testing artifacts},
journal = {Information and Software Technology},
volume = {163},
pages = {107299},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107299},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001532},
author = {Andreas Bauer and Riccardo Coppola and Emil Alégroth and Tony Gorschek},
keywords = {GUI testing, GUI-based testing, Software testing, Code review, Modern code review, Guidelines, Practices},
abstract = {Context:
Review of software artifacts, such as source or test code, is a common practice in industrial practice. However, although review guidelines are available for source and low-level test code, for GUI-based testing artifacts, such guidelines are missing.
Objective:
The goal of this work is to define a set of guidelines from literature about production and test code, that can be mapped to GUI-based testing artifacts.
Method:
A systematic literature review is conducted, using white and gray literature to identify guidelines for source and test code. These synthesized guidelines are then mapped, through examples, to create actionable, and applicable, guidelines for GUI-based testing artifacts.
Results:
The results of the study are 33 guidelines, summarized in nine guideline categories, that are successfully mapped as applicable to GUI-based testing artifacts. Of the collected literature, only 10 sources contained test-specific code review guidelines. These guideline categories are: perform automated checks, use checklists, provide context information, utilize metrics, ensure readability, visualize changes, reduce complexity, check conformity with the requirements and follow design principles and patterns.
Conclusion:
This pivotal set of guidelines provides an industrial contribution in filling the gap of general guidelines for review of GUI-based testing artifacts. Additionally, this work highlights, from an academic perspective, the need for future research in this area to also develop guidelines for other specific aspects of GUI-based testing practice, and to take into account other facets of the review process not covered by this work, such as reviewer selection.}
}
@article{SAHAY201989,
title = {The application of Software Defined Networking on securing computer networks: A survey},
journal = {Journal of Network and Computer Applications},
volume = {131},
pages = {89-108},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S108480451930027X},
author = {Rishikesh Sahay and Weizhi Meng and Christian D. Jensen},
keywords = {Software Defined Networking, Attack detection and mitigation, Network security, Middlebox management, Traffic management, Policy management, Traffic engineering, Smart grid security},
abstract = {Software Defined Networking (SDN) has emerged as a new networking paradigm for managing different kinds of networks ranging from enterprise to home network through software enabled control. The logically centralized control plane and programmability offers a great opportunity to improve network security, like implementing new mechanisms to detect and mitigate various threats, as well as enables deploying security as a service on the SDN controller. Due to the increasing and fast development of SDN, this paper provides an extensive survey on the application of SDN on enhancing the security of computer networks. In particular, we survey recent research studies that focus on applying SDN for network security including attack detection and mitigation, traffic monitoring and engineering, configuration and policy management, service chaining, and middlebox deployment, in addition to smart grid security. We further identify some challenges and promising future directions on SDN security, compatibility and scalability issues that should be addressed in this field.}
}
@article{SELIVERSTOV2020626,
title = {Traffic safety evaluation in Northwestern Federal District using sentiment analysis of Internet users’ reviews},
journal = {Transportation Research Procedia},
volume = {50},
pages = {626-635},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.074},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520308255},
author = {Yaroslav Seliverstov and Svyatoslav Seliverstov and Igor Malygin and Oleg Korolev},
keywords = {automatic text mining, web crawlers, text classification, intelligent transportation systems, machine learning, tf-idf, n-gram, naïve Bayes algorithm, linear classifier, sentiment analysis},
abstract = {The paper addresses the task of analyzing traffic safety in the Northwestern Federal District according to the reviews published in the Web. To accomplish the task, the authors developed a system of automatic review classification based on a sentiment classifier. They analyzed open source libraries for data mining, developed a web crawler using Scrapy framework, written in Python 3, and collected reviews. They also considered the methods of text vectorization and lemmatization and their application in the Scikit-Learn library: Bag-of-Words, N-gram, CountVectorizer, and TF-IDF Vectorizer. For the purpose of classification, the authors used the naïve Bayes algorithm and a linear classifier model with stochastic gradient descent optimization. A base of tagged Twitter reviews was used as a training set. The classifier was trained using cross-validation and ShuffleSplit strategies. The authors also tested and compared the classification results for different classifiers. As a result of validation, the best model was determined. The developed system was applied to analyze the quality of roads in the Northwestern Federal District. Based on the outcome, the roads were marked-up in color to illustrate the results of the research.}
}
@incollection{PANCHAL2023295,
title = {Chapter 11 - Challenges and future work directions in artificial intelligence with human-computer interaction},
editor = {Surbhi {Bhatia Khan} and Suyel Namasudra and Swati Chandna and Arwa Mashat and Fatos Xhafa},
booktitle = {Innovations in Artificial Intelligence and Human-Computer Interaction in the Digital Era},
publisher = {Academic Press},
pages = {295-310},
year = {2023},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-99891-8},
doi = {https://doi.org/10.1016/B978-0-323-99891-8.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998918000061},
author = {Mahesh H. Panchal and Shaileshkumar D. Panchal},
keywords = {Explainable artificial intelligence, Interpretable machine learning, Machine learning model, User interface},
abstract = {Artificial intelligence–based systems are developed and successfully used for applications like home appliances, defense systems, virtual assistance, robotics, self-driving vehicles, and many more. Their success lies in accurate and timely decision-making ability. But the other side of these systems is a lack of transparency that can be described as black box. Due to the opaque nature of existing artificial intelligence systems, researchers are not able to interpret the decisions that have been derived from given input situations. The lack of openness not only causes the end users to resist trusting the system but also tends to make it difficult for machine learning engineers to detect and mitigate the fault in case of failure in deriving desired output. The solution is to open the black box working nature of the system and provide required explanations as well interpretations to making the whole processes humanly understandable and meaningful. This chapter focuses on the need for explainable artificially intelligent systems, present paradigms that exist to achieve it, along with various forms of explanations expected by different stakeholders and challenges in the field of making transparent systems in the direction of trustworthy human–computer interaction.}
}
@article{SUBRAMANIAN2024103989,
title = {Do cryptocurrency rewards improve platform valuations?},
journal = {Information & Management},
volume = {61},
number = {6},
pages = {103989},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.103989},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624000715},
author = {Hemang Subramanian and Florent Rouxelin},
keywords = {Blockchain platforms, Cryptocurrency, User-generated content provisioning, Token supply mechanism, Coarsened exact matching},
abstract = {This study investigates the impact of cryptocurrency rewards and token prices on user-generated content (UGC) provision by content creators on a blockchain-based platform. Analyzing data from the Steemit platform, we find that although an increase in total reward value incentivizes UGC contributions, the rise in token prices alone does not lead to a surge in UGC. Instead, token prices have a mediating role in the relationship between total rewards earned by content creators and the volume of UGC they produce. Furthermore, we observe that an increase in UGC does not lead to a corresponding rise in the platform's market capitalization, as increased website traffic intensifies competition for rewards from a constant pool, suggesting that heightened user engagement does not translate to enhanced market capitalization. These findings imply that carefully designed reward mechanisms are crucial for sustaining user engagement and content creation amidst market fluctuations. Our study underscores the importance of a comprehensive approach to incentivizing user participation and ensuring platform growth, as a mere increase in token prices may not guarantee sustained engagement or an associated increase in market capitalization.}
}
@incollection{2024481,
title = {Glossary},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {481-501},
year = {2024},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.17001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905343170015}
}
@article{ARAYA2023100792,
title = {Anomaly-based cyberattacks detection for smart homes: A systematic literature review},
journal = {Internet of Things},
volume = {22},
pages = {100792},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100792},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001154},
author = {Juan Ignacio Iturbe Araya and Helena Rifà-Pous},
keywords = {Anomaly detection, Machine Learning, Internet of Things (IoT), Smart home, Cybersecurity, Cyber attacks, Systematic literature review (SLR)},
abstract = {Smart homes, leveraging IoT technology to interconnect various devices and appliances to the internet, enable remote monitoring, automation, and control. However, collecting sensitive personal and business data assets renders smart homes a target for cyberattacks. Anomaly detection is a promising approach for identifying malicious behavior in smart homes. Yet, the current literature primarily discusses IoT-related cyberattacks and gives limited attention to detecting anomalies specific to the smart home context. Furthermore, there is a lack of datasets that accurately represent the complexity inherent in a smart home environment in terms of users with varying levels of expertise and diverse, evolving types of devices. Therefore, this paper presents a systematic literature review (SLR) that focuses on using anomaly detection to identify cyberattacks in smart home environments. The SLR includes an adapted taxonomy that classifies existing anomaly detection methods and a critical analysis of the current state of knowledge and future research challenges. Our findings show a growing interest in detecting cyberattacks with anomaly-based models in smart homes using centralized and network-based features. Ensemble and deep learning techniques are popular methods for detecting these anomalies. However, the limited diversity of cyberattacks in existing datasets and the absence of comprehensive datasets representing the complexity of smart home environments call for further research to improve the generalizability of detection models.}
}
@article{DAVID202179,
title = {Discriminating flash crowds from DDoS attacks using efficient thresholding algorithm},
journal = {Journal of Parallel and Distributed Computing},
volume = {152},
pages = {79-87},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S074373152100040X},
author = {Jisa David and Ciza Thomas},
keywords = {Network security, Tsallis entropy, DDoS attack},
abstract = {Distributed Denial-of-Service attacks have been a challenge to cyberspace, as the attackers send a large number of attack packets similar to the normal traffic, to throttle legitimate flows. These attacks intentionally disrupt the services offered by the systems resulting in heavy cost. A flash crowd or flash event is an unexpected surge in the number of visitors to a particular website resulting in a sudden increase in server load. Flash crowds, which are legitimate flows, are difficult to be discriminated from Distributed Denial-of-Service attacks that are illicit flows. Effective and accurate detection of Distributed Denial of Service attacks still remains a challenge due to the difficulty in its detection and the false alerts generated in the case of flash crowds. There is a trade off between detection rate and false positive rate. This work deals with an efficient and early detection of distributed denial of service attacks and discriminates flash crowd by considering two network traffic parameters such as packet size and destination IP address. Using these traffic features two attributes are computed and its generalized entropies are calculated. The threshold is computed using the mean value of network attributes to detect the attacks. Threshold updater can automatically adjust the threshold values according to the changes in the channel conditions. The data sets used to evaluate the performance of the proposed approach are the MIT Lincoln Laboratory DARPA data set and a data set generated in a University network. Experimental results show this research approach achieves higher detection rate and lower false positives in a much reduced processing time as compared to the existing methods.}
}
@article{ZHANG2020107315,
title = {An effective convolutional neural network based on SMOTE and Gaussian mixture model for intrusion detection in imbalanced dataset},
journal = {Computer Networks},
volume = {177},
pages = {107315},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107315},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620300712},
author = {Hongpo Zhang and Lulu Huang and Chase Q. Wu and Zhanbo Li},
keywords = {Network intrusion detection, Deep learning, Class imbalance, Gaussian mixture model, Convolutional neural network},
abstract = {Network Intrusion Detection System (NIDS) is a key security device in modern networks to detect malicious activities. However, the problem of imbalanced class associated with intrusion detection dataset limits the classifier’s performance for minority classes. To improve the detection rate of minority classes while ensuring efficiency, we propose a novel class imbalance processing technology for large-scale dataset, referred to as SGM, which combines Synthetic Minority Over-Sampling Technique (SMOTE) and under-sampling for clustering based on Gaussian Mixture Model (GMM). We then design a flow-based intrusion detection model, SGM-CNN, which integrates imbalanced class processing with convolutional neural network, and investigate the impact of different numbers of convolution kernels and different learning rates on model performance. The advantages of the proposed model are verified using the UNSW-NB15 and CICIDS2017 datasets. The experimental results show that i) for binary classification and multiclass classification on the UNSW-NB15 dataset, SGM-CNN achieves a detection rate of 99.74% and 96.54%, respectively; ii) for 15-class classification on the CICIDS2017 dataset, it achieves a detection rate of 99.85%. We compare five imbalanced processing methods and two classification algorithms, and conclude that SGM-CNN provides an effective solution to imbalanced intrusion detection and outperforms the state-of-the-art intrusion detection methods.}
}
@article{MONTENEGRO2021104189,
title = {Analysis of the sensitivity of the End-Of-Turn Detection task to errors generated by the Automatic Speech Recognition process},
journal = {Engineering Applications of Artificial Intelligence},
volume = {100},
pages = {104189},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104189},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621000361},
author = {César Montenegro and Roberto Santana and Jose A. Lozano},
keywords = {Spoken dialogue systems, Automatic speech recognition, End of turn detection, Natural language processing, Neural networks},
abstract = {An End-Of-Turn Detection Module (EOTD-M) is an essential component of automatic Spoken Dialogue Systems. The capability of correctly detecting whether a user’s utterance has ended or not improves the accuracy in interpreting the meaning of the message and decreases the latency in the answer. Usually, in dialogue systems, an EOTD-M is coupled with an Automatic Speech Recognition Module (ASR-M) to transmit complete utterances to the Natural Language Understanding unit. Mistakes in the ASR-M transcription can have a strong effect on the performance of the EOTD-M. The actual extent of this effect depends on the particular combination of ASR-M transcription errors and the sentence featurization techniques implemented as part of the EOTD-M. In this paper we investigate this important relationship for an EOTD-M based on semantic information and particular characteristics of the speakers (speech profiles). We introduce an Automatic Speech Recognition Simulator (ASR-SIM) that models different types of semantic mistakes in the ASR-M transcription as well as different speech profiles. We use the simulator to evaluate the sensitivity to ASR-M mistakes of a Long Short-Term Memory network classifier trained in EOTD with different featurization techniques. Our experiments reveal the different ways in which the performance of the model is influenced by the ASR-M errors. We corroborate that not only is the ASR-SIM useful to estimate the performance of an EOTD-M in customized noisy scenarios, but it can also be used to generate training datasets with the expected error rates of real working conditions, which leads to better performance.}
}
@article{FARHI2019240,
title = {Malboard: A novel user keystroke impersonation attack and trusted detection framework based on side-channel analysis},
journal = {Computers & Security},
volume = {85},
pages = {240-269},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818309957},
author = {Nitzan Farhi and Nir Nissim and Yuval Elovici},
keywords = {Keystroke dynamics, Keyboard, USB, Malicious, Malware, Concealment, Authentication, Supply chain attack},
abstract = {Concealing malicious components within widely used USB peripherals has become a popular attack vector utilizing social engineering techniques and exploiting users’ trust in USB devices. This vector enables the attacker to easily penetrate an organization's computers even when the target is secured or in an air-gapped network. Such malicious concealment can be done as part of a supply chain attack or during the device manufacturing process. In cases where the device allows the user to update its firmware, a supply chain attack may involve changing just the device's firmware, thus compromising the device without the need for concealment. A compromised device can impersonate other devices like keyboards in order to send malicious keystrokes to the computer. However, the keystrokes generated maliciously do not match human keystroke characteristics, and therefore they can be easily detected by security tools that are designed to continuously verify the user's identity based on his/her keystroke dynamics. In this paper, we present Malboard, a sophisticated attack based on designated hardware concealment, which automatically generates keystrokes that have the attacked user's behavioral characteristics; in this attack these keystrokes are injected into the computer in the form of malicious commands and thus can evade existing detection mechanisms designed to continuously verify the user's identity based on keystroke dynamics. We implemented this novel attack and evaluated its performance on 30 subjects performing three different keystroke tasks; we evaluated the attack against three existing detection mechanisms, and the results show that our attack managed to evade detection in 83–100% of the cases, depending on the detection tools in place. Malboard was proven to be effective in two scenarios: either by a remote attacker using wireless communication to communicate with Malboard or by an inside attacker (malicious employee) that physically operates and uses Malboard. In addition, in order to address the evasion gap, we developed three different modules aimed at detecting keystroke injection attacks in general, and particularly, the more sophisticated Malboard attack. Our proposed detection modules are trusted and secured, because they are based on three side-channel resources which originate from the interaction between the keyboard, user, and attacked host. These side-channel resources include (1) the keyboard's power consumption, (2) the keystrokes’ sound, and (3) the user's behavior associated with his/her ability to respond to displayed textual typographical errors. Our results showed that each of the proposed detection modules is capable of detecting the Malboard attack in 100% of the cases, with no misses and no false positives; using them together as an ensemble detection framework will assure that an organization is immune to the Malboard attack in particular and other keystroke injection attacks in general.}
}
@article{LU2020105760,
title = {A survey of public datasets for computer vision tasks in precision agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {178},
pages = {105760},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105760},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920312709},
author = {Yuzhen Lu and Sierra Young},
keywords = {Dataset, Crop, Computer vision, Precision agriculture, Robotics, Data sharing, Images},
abstract = {Computer vision technologies have attracted significant interest in precision agriculture in recent years. At the core of robotics and artificial intelligence, computer vision enables various tasks from planting to harvesting in the crop production cycle to be performed automatically and efficiently. However, the scarcity of public image datasets remains a crucial bottleneck for fast prototyping and evaluation of computer vision and machine learning algorithms for the targeted tasks. Since 2015, a number of image datasets have been established and made publicly available to alleviate this bottleneck. Despite this progress, a dedicated survey on these datasets is still lacking. To fill this gap, this paper makes the first comprehensive but not exhaustive review of the public image datasets collected under field conditions for facilitating precision agriculture, which include 15 datasets on weed control, 10 datasets on fruit detection, and 9 datasets on miscellaneous applications. We survey the main characteristics and applications of these datasets, and discuss the key considerations for creating high-quality public image datasets. This survey paper will be valuable for the research community on the selection of suitable image datasets for algorithm development and identification of where creation of new image datasets is needed to support precision agriculture.}
}
@article{MARIJAN2022100492,
title = {Blockchain verification and validation: Techniques, challenges, and research directions},
journal = {Computer Science Review},
volume = {45},
pages = {100492},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100492},
url = {https://www.sciencedirect.com/science/article/pii/S1574013722000314},
author = {Dusica Marijan and Chhagan Lal},
keywords = {Blockchain, Smart contracts, P2P, Consensus, Ledger, Testing, Verification, Validation, Simulation, Benchmarking, Software testing, Security testing, Performance testing, System under test, Formal verification, Platform testing},
abstract = {As blockchain technology is gaining popularity in industry and society, solutions for Verification and Validation (V&V) of blockchain-based software applications (BC-Apps) have started gaining equal attention. To ensure that BC-Apps are properly developed before deployment, it is paramount to apply systematic V&V to verify their functional and non-functional requirements. While existing research aims at addressing the challenges of engineering BC-Apps by providing testing techniques and tools, blockchain-based software development is still an emerging research discipline, and therefore, best practices and tools for the V&V of BC-Apps are not yet sufficiently developed. In this paper, we provide a comprehensive survey on V&V solutions for BC-Apps. Specifically, using a layered approach, we synthesize V&V tools and techniques addressing different components at various layers of the BC-App stack, as well as across the whole stack. Next, we provide a discussion on the challenges associated with BC-App V&V, and summarize a set of future research directions based on the challenges and gaps identified in existing research work. Our study aims to highlight the importance of BC-App V&V and pave the way for a disciplined, testable, and verifiable BC development.}
}
@article{HERNANDEZCASTRO2017744,
title = {Using machine learning to identify common flaws in CAPTCHA design: FunCAPTCHA case analysis},
journal = {Computers & Security},
volume = {70},
pages = {744-756},
year = {2017},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817301128},
author = {Carlos Javier Hernández-Castro and María D. R-Moreno and David F. Barrero and Stuart Gibson},
keywords = {HIP, CAPTCHA, Machine learning, Gender classification, Side-channel attack},
abstract = {Human Interactive Proofs (HIPs 11Human Interaction Proof, or also Human Interactive Proof. or CAPTCHAs 22Completely Automated Public Turing test to tell Computers and Humans Apart.) have become a first-level security measure on the Internet to avoid automatic attacks or minimize their effects. All the most widespread, successful or interesting CAPTCHA designs put to scrutiny have been successfully broken. Many of these attacks have been side-channel attacks. New designs are proposed to tackle these security problems while improving the human interface. FunCAPTCHA is the first commercial implementation of a gender classification CAPTCHA, with reported improvements in conversion rates. This article finds weaknesses in the security of FunCAPTCHA and uses simple machine learning (ML) analysis to test them. It shows a side-channel attack that leverages these flaws and successfully solves FunCAPTCHA on 90% of occasions without using meaningful image analysis. This simple yet effective security analysis can be applied with minor modifications to other HIPs proposals, allowing to check whether they leak enough information that would in turn allow for simple side-channel attacks.}
}
@article{SILVA202010000,
title = {Embedded Architecture Composed of Cognitive Agents and ROS for Programming Intelligent Robots},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {10000-10005},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2718},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334819},
author = {Gustavo R. Silva and Leandro B. Becker and Jomi F. Hübner},
keywords = {BDI Agents, Robotics, UAVs, ROS, Jason},
abstract = {This paper proposes and evaluates an embedded architecture aimed to promote the utilization of cognitive agents in cooperation with the Robotic Operating System (ROS), serving as an alternative for programming intelligent robots. It promotes the programming abstraction level in two directions. The first direction regards using cognitive agents facilities for programming the robots intelligence, consisting of its perceptions and related actions. The second direction exploits the facilities of using ROS layers for programming the robot interaction with its sensors and actuators. The paper reports experiments of using agents to command simulated UAVs while measuring performance metrics that allowed us to evaluate the benefits of the proposed architecture.}
}
@article{VANMIL2022105689,
title = {A Matter of (Joint) control? Virtual assistants and the general data protection regulation},
journal = {Computer Law & Security Review},
volume = {45},
pages = {105689},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2022.105689},
url = {https://www.sciencedirect.com/science/article/pii/S026736492200036X},
author = {Jurriaan {van Mil} and João Pedro Quintais},
keywords = {General data protection regulation, Controller, Joint controller, Household exception, Virtual assistant, Google assistant},
abstract = {This article provides an overview and critical examination of the rules for determining who qualifies as controller or joint controller under the General Data Protection Regulation. Using Google Assistant – an artificial intelligence-driven virtual assistant – as a case study, we argue that these rules are overreaching and difficult to apply in the present-day information society and Internet of Things environments. First, as a consequence of recent developments in case law and supervisory guidance, these rules lead to a complex and ambiguous test to determine (joint) control. Second, due to advances in technological applications and business models, it is increasingly challenging to apply such rules to contemporary processing operations. In particular, as illustrated by the Google Assistant, individuals will likely be qualified as joint controllers, together with Google and also third-party developers, for at least the collection and possible transmission of other individuals’ personal data via the virtual assistant. Third, we identify follow-on issues relating to the apportionment of responsibilities between joint controllers and the effective and complete protection of data subjects. We conclude by questioning whether the framework for determining who qualifies as controller or joint controller is future-proof and normatively desirable.}
}
@article{WU2023120694,
title = {Characterization of halogenated organic compounds by the Fourier transform ion cyclotron resonance mass spectrometry: A critical review},
journal = {Water Research},
volume = {246},
pages = {120694},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120694},
url = {https://www.sciencedirect.com/science/article/pii/S004313542301134X},
author = {Shixi Wu and Manabu Fujii and Xin Yang and Qing-Long Fu},
keywords = {Nontarget analysis, Disinfection byproducts, Ultrahigh-resolution mass spectrometry, Dissolved organic matter, Compounds of emerging concerns},
abstract = {Halogenated organic compounds (HOCs), widely present in various environments, are generally formed by natural processes (e.g., photochemical halogenation) and anthropogenic activities (e.g., water disinfection and anthropogenic discharge of HOCs), posing health and environmental risks. Therefore, in-depth knowledge of the molecular composition, transformation, and fate of HOCs is crucial to regulate and reduce their formation. Because of the extremely complex nature of HOCs and their precursors, the molecular composition of HOCs remains largely unknown. The Fourier transform ion cyclotron resonance mass spectrometry (FT-ICR MS) offers the most powerful resolution and mass accuracy for the simultaneous molecular-level characterization of HOCs and their precursors. However, there is still a paucity of reviews regarding the comprehensive characterization of HOCs by FT-ICR MS. Based on the FT-ICR MS, the formation mechanism, sample pretreatment, and analysis methods were summarized for two typical HOCs classes, namely halogenated disinfection byproducts and per- and polyfluoroalkyl substances in this review. Moreover, we have highlighted data analysis methods and some typical applications of HOCs using FT-ICR MS and proposed suggestions for current issues. This review will deepen our understanding of the chemical characterization of HOCs and their formation mechanisms and transformation at the molecular level in aquatic systems, facilitating the application of the state-of-the-art FT-ICR MS in environmental and geochemical research.}
}
@article{SACKS2022101711,
title = {Toward artificially intelligent cloud-based building information modelling for collaborative multidisciplinary design},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101711},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101711},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001690},
author = {Rafael Sacks and Zijian Wang and Boyuan Ouyang and Duygu Utkucu and Siyu Chen},
keywords = {Building information modelling, Concurrent engineering, Design collaboration, Knowledge graphs, Semantic enrichment},
abstract = {The technological tools people use for designing buildings have progressed from drawings to descriptive geometry, and from computer-aided drafting and design (CAD) to building information modelling (BIM). Yet despite their use of state-of-the-art BIM technology, the multidisciplinary teams that design modern buildings still face numerous challenges. Building models lack sufficient semantic content to properly express design intent, concurrent design is difficult due to the need for operators to maintain model consistency and integrity manually, managing design variations is cumbersome due to the packaging of information in files, and collaboration requires making-do with imperfect interoperability between application software. In response, we propose a ‘Cloud BIM’ (CBIM) approach to building modelling that seeks to automate maintenance of consistency across federated discipline-specific models by enriching models with semantic information that encapsulates design intent. The approach requires a new ontology to represent knowledge about the relationships between building model objects within and across disciplines. Discipline-specific building models are stored together with their data schema in knowledge graphs, and linked using objects and relationships from the CBIM ontology. The links are established using artificially intelligent semantic enrichment methods that recognize patterns of location, geometry, topology and more. Software methods that operate along CBIM relationship chains can detect inconsistencies that arise across disciplines and act to inform users, propose meaningful corrections, and apply them if approved. Future CBIM systems may provide designers with the functionality for collaborative multidisciplinary design by maintaining model consistency and managing versioning at the object level.}
}
@article{FORTUNATO2021e00139,
title = {Robotic platform and path planning algorithm for in situ bioprinting},
journal = {Bioprinting},
volume = {22},
pages = {e00139},
year = {2021},
issn = {2405-8866},
doi = {https://doi.org/10.1016/j.bprint.2021.e00139},
url = {https://www.sciencedirect.com/science/article/pii/S2405886621000129},
author = {Gabriele Maria Fortunato and Gabriele Rossi and Amedeo Franco Bonatti and Aurora {De Acutis} and Christian Mendoza-Buenrostro and Giovanni Vozzi and Carmelo {De Maria}},
keywords = { bioprinting, Robotic bioprinting platform, Path planning algorithm},
abstract = {The aim of this work is to design a robotic bioprinting platform able to fabricate a three-dimensional structure onto irregular surfaces. With respect to the limitations of current in vitro bioprinting approach, widely used in scaffold-based tissue engineering – handling difficulty, risk of contamination, shape not matching with the defect site – this robotic bioprinter can offer an innovative solution allowing in situ bioprinting, a direct dispensing of biological materials onto and into the damaged site. The robotic platform was developed starting from the 5 degrees-of-freedom open source MOVEO robot from BCN3D. The hardware and the software of the original project were re-engineered to control the robot using LinuxCNC, a path planning algorithm was developed in Matlab®, and the end-effector was equipped with a pneumatic extruder. The algorithm automatically projects any generic printing pattern on the surface on which the scaffold will be 3D bioprinted. For each point, the algorithm calculates the joint angles to keep the end effector always perpendicular to the surface. A g-code file is then exported to Linux CNC adding parameters to control the air pressure and the printing speed. The robotic platform was tested to evaluate its performances. Resolution (~200 ​μm) and repeatability were estimated and preliminary in situ bioprinting tests were performed onto different irregular surfaces, including a physiologically relevant bone model.}
}
@article{ZHOU2021103217,
title = {Progresses and challenges in link prediction},
journal = {iScience},
volume = {24},
number = {11},
pages = {103217},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.103217},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221011858},
author = {Tao Zhou},
keywords = {Computer science, Network, Network topology},
abstract = {Summary
Link prediction is a paradigmatic problem in network science, which aims at estimating the existence likelihoods of nonobserved links, based on known topology. After a brief introduction of the standard problem and evaluation metrics of link prediction, this review will summarize representative progresses about local similarity indices, link predictability, network embedding, matrix completion, ensemble learning, and some others, mainly extracted from related publications in the last decade. Finally, this review will outline some long-standing challenges for future studies.}
}
@article{ZHOU201951,
title = {A fog computing based approach to DDoS mitigation in IIoT systems},
journal = {Computers & Security},
volume = {85},
pages = {51-62},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818311349},
author = {Luying Zhou and Huaqun Guo and Gelei Deng},
keywords = {DDoS, Attack detection, Mitigation, Fog computing, VNF},
abstract = {Distributed denial of service (DDoS) cyber-attack poses a severe threat to the industrial Internet of Things (IIoT) operation due to the security vulnerabilities resulted from increased connectivity and openness, and the large number of deployed low computation power devices. This paper applies Fog computing concept in DDoS mitigation by allocating traffic monitoring and analysis work close to local devices, and, on the other hand, coordinating and consolidating work to cloud central servers so as to achieve fast response while at low false alarm rate. The mitigation scheme consists of real-time traffic filtering via field firewall devices, which are able to reversely filter the signature botnet attack packets; offline specification based traffic analysis via virtualized network functions (VNFs) in the local servers; and centralized coordination via cloud server, which consolidates and correlates the information from the distributed local servers to make a more accurate decision. The proposed scheme is tested in an industrial control system testbed and the experiments evaluate the detection time and rate for two types of DDoS attacks and demonstrate the effectiveness of the scheme.}
}
@article{DEANDRES2022101966,
title = {Challenges of the market for initial coin offerings},
journal = {International Review of Financial Analysis},
volume = {79},
pages = {101966},
year = {2022},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2021.101966},
url = {https://www.sciencedirect.com/science/article/pii/S1057521921002842},
author = {Pablo {de Andrés} and David Arroyo and Ricardo Correia and Alvaro Rezola},
keywords = {Blockchain, Alternative financing solutions, Initial coin offerings, Asymmetrical information},
abstract = {This article analyzes the main problems and the solutions adopted in the market for Initial Coin Offerings (ICO), to anticipate the future of this market and determine implications for issuers, investors and regulators. ICOs represent an alternative and innovative financing solution that has experienced spectacular growth and notoriety in recent years. ICOs rely on Blockchain protocols and the ICO market is, therefore, characterized as decentralized, disintermediated and unregulated. Our results show that although the ICO market is innovative, it already displays many of the problems of traditional financial markets, and that these problems were at the genesis of the last financial crisis. Our analysis of the problems and solutions adopted shows a tension between what the Blockchain technology offers, and the problems associated with the financing of innovation. Considering the problems and solutions adopted, we no longer expect the ICO market to be characterized as disintermediated, unregulated or even decentralized in the near future. Furthermore, it is a real possibility that ICOs may end up being a progressor model eventually replaced by similar but more specialized financing models, some of which may already exist. With respect to the particular solutions of the ICO market, while some represent the realization of the potential of Blockchain, others such as forks have important Governance implications with the potential to create as many problems as the ones they address.}
}
@article{GOK2024111503,
title = {Dynamic path planning via Dueling Double Deep Q-Network (D3QN) with prioritized experience replay},
journal = {Applied Soft Computing},
volume = {158},
pages = {111503},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111503},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624002771},
author = {Mehmet Gök},
keywords = {Mobile robots, Path planning, Deep reinforcement learning, Deep Q-Learning, Dueling neural network, ROS},
abstract = {Path planning is a key requirement for mobile robots employed for different tasks such as rescue or transport missions. Conventional methods such as A* or Dijkstra to tackle path planning problem need a premise map of the robot's environment. Nowadays, dynamic path planning is a popular research topic, which drives mobile robots without prior static requirements. Deep reinforcement learning (DRL), which is another popular research area, is being harnessed to solve dynamic path planning problem by the researchers. In this study, Deep Q-Networks, which is a subdomain of DRL are opted to solve dynamic path planning problem. We first employ well known techniques Double Deep Q-Networks (D2QN) and Dueling Double Deep Q-Networks (D3QN) to train a model which can drive a mobile robot in environments with static and dynamic obstacles within 3 different configurations. Then we propose D3QN with Prioritized Experience Replay (PER) extension in order to further optimize the DRL model. We created a test bed to measure the performance of the DRL models against 99 randomly generated goal locations. According to our experiments, D3QN-PER method performs better than D2QN and D3QN in terms of path length and travel time to the goal without any collisions. Robot Operating System and Gazebo simulation environment is utilized to realize the training and testing environments, thus, the trained DRL models can be deployed to any ROS compatible robot seamlessly.}
}
@incollection{JINCHENG2024553,
title = {Chapter 28 - Understanding the Evolution of the Internet: Web 1.0 to Web3.0, Web3, and Web 3+∗∗The article is written with the help of ChatGPT to enhance the writing style. ChatGPT also assisted the author in verification of some information.},
editor = {David {Lee Kuo Chuen}},
booktitle = {Handbook of Digital Currency (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {553-581},
year = {2024},
isbn = {978-0-323-98973-2},
doi = {https://doi.org/10.1016/B978-0-323-98973-2.00052-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323989732000526},
author = {Zheng JinCheng and David Lee Kuo Chuen},
keywords = {Decentralization, Immutable, Independence, Interoperability, Privacy, Protection, Trustless, Web3, Web3+, Web3.0},
abstract = {The Internet has undergone numerous changes since its emergence in 1969 and has now become an indispensable aspect of modern life. With the introduction of the World Wide Web by Tim Berners-Lee, the Internet has transformed into a tool for sharing and accessing vast amounts of information. As the Internet evolves toward its third iteration, Web3+ offers a decentralized solution that empowers users and returns control over the Internet to them. With the rise of cryptocurrency and blockchain, Web3+ focuses on data ownership and protection, making the Internet more secure and fair for everyone. In this article, we will explore the differences between Web 1.0, Web 2.0, Web 3.0, Web3, and Web3+ and how they shape the future of the Internet.}
}
@article{NAVEED2024107423,
title = {Model driven engineering for machine learning components: A systematic literature review},
journal = {Information and Software Technology},
volume = {169},
pages = {107423},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107423},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000284},
author = {Hira Naveed and Chetan Arora and Hourieh Khalajzadeh and John Grundy and Omar Haggag},
keywords = {Model driven engineering, Software engineering, Artificial intelligence, Machine learning, Systematic literature review},
abstract = {Context:
Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components.
Objective:
The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations.
Method:
Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting.
Results:
We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research.
Conclusion:
This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners.}
}
@article{BATTISTON2021100721,
title = {Reliance on scientists and experts during an epidemic: Evidence from the COVID-19 outbreak in Italy},
journal = {SSM - Population Health},
volume = {13},
pages = {100721},
year = {2021},
issn = {2352-8273},
doi = {https://doi.org/10.1016/j.ssmph.2020.100721},
url = {https://www.sciencedirect.com/science/article/pii/S235282732030358X},
author = {Pietro Battiston and Ridhi Kashyap and Valentina Rotondi},
keywords = {Public health, Trust in science, Trust in health authorities, Information-seeking responses and reliance, Epidemics, Infectious disease outbreaks, Pandemics},
abstract = {Research suggests trust in experts and authorities are important correlates of compliance with public health measures during infectious disease outbreaks. Empirical evidence on the dynamics of reliance on scientists and public health authorities during the early phases of an epidemic outbreak is limited. We examine these processes during the COVID-19 outbreak in Italy by leveraging data from Twitter and two online surveys, including a survey experiment. We find that reliance on experts followed a curvilinear path. Both Twitter and survey data showed initial increases in information-seeking from expert sources in the three weeks after the detection of the first case. Consistent with these increases, knowledge about health information linked to COVID-19 and support for containment measures was widespread, and better knowledge was associated with stronger support for containment policies. Both knowledge and containment support were positively associated with trust in science and public health authorities. However, in the third week after the outbreak, we detected a slowdown in responsiveness to experts. These processes were corroborated with a survey experiment, which showed that those holding incorrect beliefs about COVID-19 gave no greater – or even lower – importance to information when its source was stated as coming from experts than when the source was unstated. Our results suggest weakened trust in public health authorities with prolonged exposure to the epidemic as a potential mechanism for this effect. Weakened responsiveness to expert sources may increase susceptibility to misinformation and our results call for efforts to sustain trust in adapting public health response.}
}
@article{PARSSINEN2018177,
title = {Environmental impact assessment of online advertising},
journal = {Environmental Impact Assessment Review},
volume = {73},
pages = {177-200},
year = {2018},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0195925517303505},
author = {M. Pärssinen and M. Kotila and R. Cuevas and A. Phansalkar and J. Manner},
keywords = {Internet energy consumption, CO emission, Online advertising, Invalid traffic},
abstract = {There are no commonly agreed ways to assess the total energy consumption of the Internet. Estimating the Internet's energy footprint is challenging because of the interconnectedness associated with even seemingly simple aspects of energy consumption. The first contribution of this paper is a common modular and layered framework, which allows researchers to assess both energy consumption and CO2e emissions of any Internet service. The framework allows assessing the energy consumption depending on the research scope and specific system boundaries. Further, the proposed framework allows researchers without domain expertise to make such an assessment by using intermediate results as data sources, while analyzing the related uncertainties. The second contribution is an estimate of the energy consumption and CO2e emissions of online advertising by utilizing our proposed framework. The third contribution is an assessment of the energy consumption of invalid traffic associated with online advertising. The second and third contributions are used to validate the first. The online advertising ecosystem resides in the core of the Internet, and it is the sole source of funding for many online services. Therefore, it is an essential factor in the analysis of the Internet's energy footprint. As a result, in 2016, online advertising consumed 20–282 TWh of energy. In the same year, the total infrastructure consumption ranged from 791 to 1334 TWh. With extrapolated 2016 input factor values without uncertainties, online advertising consumed 106 TWh of energy and the infrastructure 1059 TWh. With the emission factor of 0.5656 kg CO2e/kWh, we calculated the carbon emissions of online advertising, and found it produces 60 Mt CO2e (between 12 and 159 Mt of CO2e when considering uncertainty). The share of fraudulent online advertising traffic was 13.87 Mt of CO2e emissions (between 2.65 and 36.78 Mt of CO2e when considering uncertainty). The global impact of online advertising is multidimensional. Online advertising affects the environment by consuming significant amounts of energy, leading to the production CO2e emissions. Hundreds of billions of ad dollars are exchanged yearly, placing online advertising in a significant role economically. It has become an important and acknowledged component of the online-bound society, largely due to its integration with the Internet and the amount of revenue generated through it.}
}
@article{SHEN2024103846,
title = {GHGDroid: Global heterogeneous graph-based android malware detection},
journal = {Computers & Security},
volume = {141},
pages = {103846},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103846},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824001470},
author = {Lina Shen and Mengqi Fang and Jian Xu},
keywords = {Malware detection, Semantic information, Heterogeneous subgraph, Deep learning, Graph convolutional network},
abstract = {As the most popular mobile platform, Android has become the major attack target of malware, and thus there is an urgent need to effectively thwart them. Recently, the graph-based technique has been a promising solution for malware detection, which highly depends on graph structures to capture behaviors separating the malware from the benign apps. However, existing graph-based malware detection approaches still suffer from high computation cost in constructing or updating a graph for APK under detection, high false negative and false positive. To cope with these issues, we propose a novel global heterogeneous graph-based Android malware detection approach, named GHGDroid. A global heterogeneous graph (GHG) with a good updatability is first built on large-scale Android applications to characterize complex relationships among APKs and sensitive APIs. And then, using the GHG, a multi-layer graph convolutional network based embedding method is proposed to learn APK embeddings for well capturing behaviors that can separate malware from benign. Finally, using APK embeddings as well their labels, a malware classifier is trained. Experiments on real-world Android applications show that GHGDroid achieves 99.17 % F1-score, which outperforms the state-of-the-art approaches. Moreover, GHGDroid spends about 8 s on detecting an APK, which shows that it has a good potential as a practical tool for the Android malware detection task.}
}
@article{SHAO202184,
title = {Adaptive online learning for IoT botnet detection},
journal = {Information Sciences},
volume = {574},
pages = {84-95},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.05.076},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521005697},
author = {Zhou Shao and Sha Yuan and Yongli Wang},
keywords = {IoT botnet, Adaptive learning, Ensemble learning, Online learning},
abstract = {With the number of Internet of Things (IoT) devices proliferating, the traffic volume of IoT-based attacks has shown a gradually increasing trend. The IoT botnet attack, which aims to commit real, efficient, and profitable cybercrimes, has become one of the most severe IoT threats. Applying traditional techniques to IoT is difficult due to its particular characteristics, such as resource-constrained devices, massive volumes of data, and real-time requirements. In this paper, we explore an adaptive online learning strategy for real-time IoT botnet attack detection. Furthermore, we operate the proposed adaptive strategy in conjunction with online ensemble learning. To evaluate the proposed strategy, we use real IoT traffic data, including benign traffic data and botnet traffic data infected by Mirai. In real-time IoT botnet attack detection, our experimental results demonstrate that the proposed adaptive online learning strategy achieves remarkable performance.}
}
@article{DESOUZA2022109154,
title = {Intrusion detection and prevention in fog based IoT environments: A systematic literature review},
journal = {Computer Networks},
volume = {214},
pages = {109154},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109154},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002651},
author = {Cristiano Antonio {de Souza} and Carlos Becker Westphall and Renato Bobsin Machado and Leandro Loffi and Carla Merkle Westphall and Guilherme Arthur Geronimo},
keywords = {Internet of Things, Fog computing, Intrusion detection and prevention},
abstract = {Currently, the Internet of Things is spreading in all areas that apply computing resources. An important ally of the IoT is fog computing. It extends cloud computing and services to the edge of the network. Smart environments are becoming real and possible through IoT and fog computing. However, they are not free from security threats and vulnerabilities. This makes special security techniques indispensable. Security is one of the biggest challenges to ensuring an optimal IoT and Fog environment. Combined with the significant damage generated by application attacks, this fact creates the need to focus efforts in this area. This need can be proven through existing reviews of the state-of-the-art that pointed out several open aspects that need greater research effort. In this way, this article presents a Systematic Literature Review (SLR) considering the context of intrusion detection and prevention in environments based on fog computing and IoT. This review addresses more than 100 studies that were included after undergoing an extensive inclusion/exclusion process with well-defined criteria. From these studies, information was extracted to build a view of the current state-of-the-art and answer the research questions of this study. In this way, we identify the state-of-the-art, open questions and possibilities for future research.}
}
@article{DAGAN2023100668,
title = {What is “big data” and how should we use it? The role of large datasets, secondary data, and associated analysis techniques in outdoor recreation research},
journal = {Journal of Outdoor Recreation and Tourism},
volume = {44},
pages = {100668},
year = {2023},
note = {Social media and other user created content for outdoor recreation and nature-based tourism research},
issn = {2213-0780},
doi = {https://doi.org/10.1016/j.jort.2023.100668},
url = {https://www.sciencedirect.com/science/article/pii/S2213078023000658},
author = {Dani T. Dagan and Emily J. Wilkins},
keywords = {Big data, Outdoor recreation, Social media, Mobile device data, Machine learning, Novel data},
abstract = {With researchers increasingly interested in big data research, this conceptual paper describes how large datasets, secondary data, and associated analysis techniques can be used to understand outdoor recreation. Some types of large, secondary datasets that have been increasingly used in outdoor recreation research include social media, mobile device data, and trip reports or online reviews. First, we give a brief overview of big data terms and outline the steps involved in conducting big data research. In doing so, we describe data sources and analysis techniques relevant for outdoor recreation, and review how they have been applied in previous published works. We then describe opportunities, limitations, and considerations of using big data. Finally, we outline several questions researchers may consider when designing, conducting, reporting, and reviewing outdoor recreation research using big data. Overall, big data approaches can expand our understanding of outdoor recreation and, by addressing key questions, may help researchers harness the strengths of big data while ensuring quality and integrity.}
}
@article{DEBNATH2023106166,
title = {Conspiracy spillovers and geoengineering},
journal = {iScience},
volume = {26},
number = {3},
pages = {106166},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.106166},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223002432},
author = {Ramit Debnath and David M. Reiner and Benjamin K. Sovacool and Finn Müller-Hansen and Tim Repke and R. Michael Alvarez and Shaun D. Fitzgerald},
keywords = {Engineering, Energy engineering, Social sciences, Research methodology social sciences},
abstract = {Summary
Geoengineering techniques such as solar radiation management (SRM) could be part of a future technology portfolio to limit global temperature change. However, there is public opposition to research and deployment of SRM technologies. We use 814,924 English-language tweets containing #geoengineering globally over 13 years (2009–2021) to explore public emotions, perceptions, and attitudes toward SRM using natural language processing, deep learning, and network analysis. We find that specific conspiracy theories influence public reactions toward geoengineering, especially regarding “chemtrails” (whereby airplanes allegedly spray poison or modify weather through contrails). Furthermore, conspiracies tend to spillover, shaping regional debates in the UK, USA, India, and Sweden and connecting with broader political considerations. We also find that positive emotions rise on both the global and country scales following events related to SRM governance, and negative and neutral emotions increase following SRM projects and announcements of experiments. Finally, we also find that online toxicity shapes the breadth of spillover effects, further influencing anti-SRM views.}
}
@article{ZHANG2021116452,
title = {A review of machine learning in building load prediction},
journal = {Applied Energy},
volume = {285},
pages = {116452},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116452},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921000209},
author = {Liang Zhang and Jin Wen and Yanfei Li and Jianli Chen and Yunyang Ye and Yangyang Fu and William Livingood},
keywords = {Building energy system, Building load prediction, Building energy forecasting, Machine learning, Feature engineering, Data engineering},
abstract = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.}
}
@article{ZHANG201892,
title = {Utilizing Twitter data for analysis of chemotherapy},
journal = {International Journal of Medical Informatics},
volume = {120},
pages = {92-100},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618304325},
author = {Ling Zhang and Magie Hall and Dhundy Bastola},
keywords = {Cancer, Chemotherapy, Social media, Twitter, Side effect, Deep learning},
abstract = {Objective
Twitter has become one of the most popular social media platforms that offers real-world insights to healthy behaviors. The purpose of this study was to assess and compare perceptions about chemotherapy of patients and health-care providers through analysis of chemo-related tweets.
Materials and methods
Cancer-related Twitter accounts and their tweets were obtained through using Tweepy (Python library). Multiple text classification algorithms were tested to identify the models with best performance in classifying the accounts into individual and organization. Chemotherapy-specific tweets were extracted from historical tweetset, and the content of these tweets was analyzed using topic model, sentiment analysis and word co-occurrence network.
Results
Using the description in Twitter users’ profiles, the accounts related with cancer were collected and coded as individual or organization. We employed Long Short Term Memory (LSTM) network with GloVe word embeddings to identify the user into individuals and organizations with accuracy of 85.2%. 13, 273 and 14,051 publicly available chemotherapy-related tweets were retrieved from individuals and organizations, respectively. The content of the chemo-related tweets was analyzed by text mining approaches. The tweets from individual accounts pertained to personal chemotherapy experience and emotions. In contrast with the personal users, professional accounts had a higher proportion of neutral tweets about side effects. The information about the assessment of response to chemotherapy was deficient from organizations on Twitter.
Discussion
Examining chemotherapy discussions on Twitter provide new lens into content and behavioral patterns associated with treatments for cancer patients. The methodology described herein allowed us to collect relatively large number of health-related tweets over a greater time period and exploit the potential power of social media, which provide comprehensive view on patients’ perceptions of chemotherapy.
Conclusion
This study sheds light on using Twitter data as a valuable healthcare data source for helping oncologists (organizations) in understanding patients’ experiences while undergoing chemotherapy, in developing personalize therapy plans, and a supplement to the clinical electronic medical records (EMRs).}
}
@article{NEDUCHAL2019323,
title = {Environment detection system for localization and mapping purposes},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {27},
pages = {323-328},
year = {2019},
note = {16th IFAC Conference on Programmable Devices and Embedded Systems PDES 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.681},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319326308},
author = {P. Neduchal and L. Bureš and M. Železný},
keywords = {Mobile Robots, Multisensor Integration, Simultaneous Localization, Mapping, Robotics, Sensors, Cameras, Encoders, Embedded systems},
abstract = {The goal of this paper is to present a concept and implementation of an Environment Detection System. The system is supposed to collect data from sensors attached to the mobile robot and then enhance the map of the environment by this information. Moreover, the data can be processed to get valuable information about the environment or its change. The open-source implementation of the system is written for Robot Operating System. Thus, the system can handle data from different sensors using a unified way. It is possible by employing the messaging mechanism implemented in the Robot Operating System. Another contribution of this paper are records of our testing runs with a 6WD mobile robot equipped by multiple sensors, which can be used as a dataset for SLAM and Environment Detection System implementations.}
}
@article{VONDRACEK2023102923,
title = {Rise of the Metaverse’s Immersive Virtual Reality Malware and the Man-in-the-Room Attack & Defenses},
journal = {Computers & Security},
volume = {127},
pages = {102923},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102923},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822003157},
author = {Martin Vondráček and Ibrahim Baggili and Peter Casey and Mehdi Mekni},
keywords = {Emerging technologies, Network-level security and protection, Network communications, Network Protocols, Protection mechanisms, Quality analysis and evaluation, System issues, Security and Privacy Protection, Authentication, Communications Applications, Virtual reality, Security and Protection, Artificial, augmented, and virtual realities, Invasive software (viruses, worms, Trojan horses), Unauthorized access (hacking, phreaking)},
abstract = {The allure of the metaverse along with Virtual Reality (VR) technologies and speed at which they are deployed may shift focus away from security and privacy fundamentals. In this work we employ classic exploitation techniques against cutting edge devices to obtain equally novel results. The unique features of the Virtual Reality landscape set the stage for our primary account of a new attack, the Man-in-the-Room (MitR). This attack, realized from a vulnerable social networking application led to both worming and botnet capabilities being adapted for VR with potential critical impacts affecting millions of users. Our work improves the state-of-the-art in Virtual Reality (VR) security and socio-technical research in VR. It shares several analytical and attacking tools, example exploits, evaluation dataset, and vulnerability signatures with the scientific and professional communities to ensure secure VR software development. The presented results demonstrate the detection and prevention of VR vulnerabilities, and raise questions in the law and policy domains pertaining to VR security and privacy.}
}
@article{WU2020453,
title = {Discovering differential features: Adversarial learning for information credibility evaluation},
journal = {Information Sciences},
volume = {516},
pages = {453-473},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.12.040},
url = {https://www.sciencedirect.com/science/article/pii/S002002551931151X},
author = {Lianwei Wu and Yuan Rao and Ambreen Nazir and Haolin Jin},
keywords = {Information credibility evaluation, Fake news detection, Adversarial networks, Reinforcement learning},
abstract = {A series of deep learning approaches extract a large number of credibility features to detect fake news on the Internet. However, these extracted features still suffer from many irrelevant and noisy features that restrict severely the performance of the approaches. In this paper, we propose a novel model based on Adversarial Networks and inspirited by the Shared-Private model (ANSP), which aims at reducing common, irrelevant features from the extracted features for information credibility evaluation. Specifically, ANSP involves two tasks: one is to prevent the binary classification of true and false information for capturing common features relying on adversarial networks guided by reinforcement learning. Another extracts credibility features (henceforth, private features) from multiple types of credibility information and compares with the common features through two strategies, i.e., orthogonality constraints and KL-divergence for making the private features more differential. Experiments first on two six-label LIAR and Weibo datasets demonstrate that ANSP achieves state-of-the-art performance, boosting the accuracy by 2.1%, 3.1%, respectively and then on four-label Twitter16 validate the robustness of the model with 1.8% performance improvements.}
}
@article{NEU201938,
title = {Twitter and social accountability: Reactions to the Panama Papers},
journal = {Critical Perspectives on Accounting},
volume = {61},
pages = {38-53},
year = {2019},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1045235418302314},
author = {Dean Neu and Greg Saxton and Abu Rahaman and Jeffery Everett},
keywords = {Social accountability, Social media, Panama papers, Big data and quantitative methods, Accounting interventions},
abstract = {The potential of social media to disseminate, aggregate, channel and democratize social accountability processes has encouraged a variety of organizations to actively promote and champion such initiatives. These initiatives typically envision a three step social accountability process where, for example, the publication of previously-private financial information about the inappropriate wealth accumulation activities of politicians and their business allies (step #1), combined with social media dissemination and discussion of these activities (step #2), can result in an accountability conversation that spills out of the medium and that sometimes results in positive social change (step #3). The current study examines Twitter reactions to the International Consortium of Investigative Journalist’s (ICIJ) publication of the Panama Papers. The analysis illustrates that there was a Twitter reaction: furthermore, that there were different styles of response and that certain styles were more likely to elicit an audience reaction, especially if the tweeter was a journalist or organization. While the provided analysis focuses on step #2 within the social accountability process, the results imply that publicly-interested accounting academics qua activists can facilitate social accountability by helping to make previously-private financial information public and by cultivating sympathetic individuals within the traditional media as well as within organizations that are active on social media.}
}
@article{LAWSON202134,
title = {Machine learning for metabolic engineering: A review},
journal = {Metabolic Engineering},
volume = {63},
pages = {34-60},
year = {2021},
note = {Tools and Strategies of Metabolic Engineering},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2020.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S109671762030166X},
author = {Christopher E. Lawson and Jose Manuel Martí and Tijana Radivojevic and Sai Vamshi R. Jonnalagadda and Reinhard Gentz and Nathan J. Hillson and Sean Peisert and Joonhoon Kim and Blake A. Simmons and Christopher J. Petzold and Steven W. Singer and Aindrila Mukhopadhyay and Deepti Tanjore and Joshua G. Dunn and Hector {Garcia Martin}},
keywords = {Machine Learning, Metabolic Engineering, Synthetic Biology, Deep Learning},
abstract = {Machine learning provides researchers a unique opportunity to make metabolic engineering more predictable. In this review, we offer an introduction to this discipline in terms that are relatable to metabolic engineers, as well as providing in-depth illustrative examples leveraging omics data and improving production. We also include practical advice for the practitioner in terms of data management, algorithm libraries, computational resources, and important non-technical issues. A variety of applications ranging from pathway construction and optimization, to genetic editing optimization, cell factory testing, and production scale-up are discussed. Moreover, the promising relationship between machine learning and mechanistic models is thoroughly reviewed. Finally, the future perspectives and most promising directions for this combination of disciplines are examined.}
}
@incollection{2017207,
title = {Index},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {Eleventh Hour CISSP® (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
pages = {207-221},
year = {2017},
isbn = {978-0-12-811248-9},
doi = {https://doi.org/10.1016/B978-0-12-811248-9.09992-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128112489099927}
}
@article{DELATORREPARRA2020102662,
title = {Detecting Internet of Things attacks using distributed deep learning},
journal = {Journal of Network and Computer Applications},
volume = {163},
pages = {102662},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102662},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520301363},
author = {Gonzalo {De La Torre Parra} and Paul Rad and Kim-Kwang Raymond Choo and Nicole Beebe},
keywords = {Cyber security, Cloud computing, Machine learning, Deep learning, Recurrent neural network},
abstract = {The reliability of Internet of Things (IoT) connected devices is heavily dependent on the security model employed to protect user data and prevent devices from engaging in malicious activity. Existing approaches for detecting phishing, distributed denial of service (DDoS), and Botnet attacks often focus on either the device or the back-end. In this paper, we propose a cloud-based distributed deep learning framework for phishing and Botnet attack detection and mitigation. The model comprises two key security mechanisms working cooperatively, namely: (1) a Distributed Convolutional Neural Network (DCNN) model embedded as an IoT device micro-security add-on for detecting phishing and application layer DDoS attacks; and (2) a cloud-based temporal Long-Short Term Memory (LSTM) network model hosted on the back-end for detecting Botnet attacks, and ingest CNN embeddings to detect distributed phishing attacks across multiple IoT devices. The distributed CNN model, embedded into a ML engine in the client's IoT device, allows us to detect and defend the IoT device from phishing attacks at the point of origin. We create a dataset consisting of both phishing and non-phishing URLs to train the proposed CNN add-on security model, and select the N_BaIoT dataset for training the back-end LSTM model. The joint training method minimizes communication and resource requirements for attack detection, and maximizes the usefulness of extracted features. In addition, an aggregation of schemes allows the automatic fusion of multiple requests to improve the overall performance of the system. Our experiments show that the IoT micro-security add-on running the proposed CNN model is capable of detecting phishing attacks with an accuracy of 94.3% and a F-1 score of 93.58%. Using the back-end LSTM model, the model detects Botnet attacks with an accuracy of 94.80% using all malicious data points in the used dataset. Thus, the findings demonstrate that the proposed approach is capable of detecting attacks, both at device and at the back-end level, in a distributed fashion.}
}
@article{PARSHEERA2024105947,
title = {Stack is the New Black?: Evolution and Outcomes of the ‘India-Stackification’ Process},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105947},
year = {2024},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2024.105947},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000141},
author = {Smriti Parsheera},
keywords = {India stack, Digital public infrastructure, Digital transformation, Aadhaar, Unified payments interface},
abstract = {India is going through a transformative phase in its digital journey. A large part of this is enfolding in the field of digital public infrastructures as the ‘India Stack’ branded suite of technological solutions permeates through areas like digital identity, instant payments, digital commerce, and consent management. The paper traces the socio-technical imaginaries that have fueled India's digital transformation strategy and how India Stack acquired its central place in that scheme. Drawing upon India's performance on global ICT-related indices and the OECD's Good Practice Principles for Public Service Design and Delivery, the paper also examines how the country is faring in translating its visions of digital transformation into outcomes. It identifies reliance on coercive digital adoption strategies, lack of participative decision-making, and insufficient accountability safeguards as some of the fault lines in India's path to fair and equitable digital transformation.}
}
@article{PICCOLI2020103263,
title = {Designing scalability in required in-class introductory college courses},
journal = {Information & Management},
volume = {57},
number = {8},
pages = {103263},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103263},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619300394},
author = {Gabriele Piccoli and Marcin Łukasz Bartosiak and Biagio Palese and Joaquin Rodriguez},
keywords = {Digital innovation, Socio-technical artifact, Design science research, Information systems scalability},
abstract = {We posit that design science enables the creation of in-class introductory college courses that can scale to large numbers of students, under resource constraints. We build on the centrality of human interactions in learning environments and conceptualize a college course as a socio-technical (ST) artifact. Grounded in the intervention theory, we draw meta-requirements guiding the design of college courses that leverage IT to scale, while maintaining the centrality of the professor’s role. We use the design-build-evaluate cycle to instantiate the ST artifact and demonstrate its feasibility using evaluation episodes as prescribed by the Framework for Evaluation in Design Science Research.}
}
@incollection{MONKEN2023371,
title = {11 - Assuring AI methods for economic policymaking},
editor = {Feras A. Batarseh and Laura J. Freeman},
booktitle = {AI Assurance},
publisher = {Academic Press},
pages = {371-427},
year = {2023},
isbn = {978-0-323-91919-7},
doi = {https://doi.org/10.1016/B978-0-32-391919-7.00025-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919197000251},
author = {Anderson Monken and William Ampeh and Flora Haberkorn and Uma Krishnaswamy and Feras A. Batarseh},
keywords = {AI assurance, AI explainability, detecting bias, natural language processing, large language models},
abstract = {AI methods are becoming more common in the field of economics, but these models must be bias-free, fair, and explainable. In other words, we need AI assurance. Economic forecasting has benefited from machine learning techniques, such as neural networks, to increase model performance, but these AI techniques must be audited, accountable, and interpretable to be useful for economic policymaking. The rise of natural language processing and large language models has created new challenges for economic policymaking institutions, which need to be aware of AI assurance and how to harness them safely.}
}
@article{SUN2022108376,
title = {THINK: A novel conversation model for generating grammatically correct and coherent responses},
journal = {Knowledge-Based Systems},
volume = {242},
pages = {108376},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108376},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122001423},
author = {Bin Sun and Shaoxiong Feng and Yiwei Li and Jiamou Liu and Kan Li},
keywords = {Open-domain dialogue generation, Teamwork generation framework, Semantics extractor, Conversation model},
abstract = {Many existing conversation models that are based on the encoder–decoder framework incorporate complex encoders. These powerful encoders serve to enrich the context vectors, so that the generated responses are more diverse and informative. However, these approaches face two potential challenges. First, the high complexity of the encoder means relative simplicity of the decoder. There is a danger that the decoder becomes too simple to effectively capture previously generated information. As a result, the decoder may produce duplicated and self-contradicting responses. Second, by having a complex encoder, the model may generate incoherent responses because the complex context vectors may deviate from the true semantics of context. In this work, we propose a conversation model named “THINK” (Teamwork generation Hover around Impressive Noticeable Keywords) that is equipped with a complex decoder to avoid generating duplicated and self-contradicting responses. The model also simplifies the context vectors and increases the coherence of generated responses in a reasonable way. For this model, we propose Teamwork generation framework and Semantics extractor. Compared with other baselines, both automatic and human evaluation showed the advantages of our model.}
}
@article{MOUBARAK2020183,
title = {On distributed ledgers security and illegal uses},
journal = {Future Generation Computer Systems},
volume = {113},
pages = {183-195},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17330650},
author = {Joanna Moubarak and Maroun Chamoun and Eric Filiol},
keywords = {Blockchain, Bitcoin, Ethereum, Hyperledger, Algorand, Ripple, IOTA, Tangle, Smart contracts, Hashing, Security, Cryptography, Trust, Attacks, Vulnerabilities, Consensus},
abstract = {Distributed ledgers stimulate innovative services and enabled new applications in several domains, creating new concepts for trust and regulation. However, this backbone that is enabling novelties and abridging businesses comes with drawbacks and security flaws. In this paper, we evaluate several Distributed Ledger Technologies (DLTs) features depicting the Bitcoin, Ripple, Ethereum, Hyperledger, Algorand and IOTA networks. We focus on their security challenges and expose numerous threats and vulnerabilities. For instance, we have simulated a few of their possible attacks proving them non-immune. In the other hand, we show a few of their malicious use cases. Meticulously presenting DLTs menaces and flaws, we are not involved in preferring any specific DLT network.}
}
@article{ZHAO20181,
title = {Tweets or nighttime lights: Comparison for preeminence in estimating socioeconomic factors},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {146},
pages = {1-10},
year = {2018},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618302375},
author = {Naizhuo Zhao and Guofeng Cao and Wei Zhang and Eric L. Samson},
keywords = {Nighttime lights imagery, Twitter, Socioeconomic factors, Location-based social media, The United States},
abstract = {Nighttime lights (NTL) imagery is one of the most commonly used tools to quantitatively study socioeconomic systems over large areas. In this study we aim to use location-based social media big data to challenge the primacy of NTL imagery on estimating socioeconomic factors. Geo-tagged tweets posted in the contiguous United States in 2013 were retrieved to produce a tweet image with the same spatial resolution of the NTL imagery (i.e., 0.00833° × 0.00833°). Sum tweet (the total number of tweets) and sum light (summed DN value of the NTL image) of each state or county were obtained from the tweets and the NTL images, respectively, to estimate three important socioeconomic factors: personal income, electric power consumption, and fossil fuel carbon dioxide emissions. Results show that sum tweet is a better measure of personal income and electric power consumption while carbon dioxide emissions can be more accurately estimated by sum light. We further exploited that African-Americans adults are more likely than White seniors to post geotagged tweets in the US, yet did not find any significant correlations between proportions of the subpopulations and the estimation accuracy of the socioeconomic factors. Existence of saturated pixels and blooming effects and failure to remove gas flaring reduce quality of NTL imagery in estimating socioeconomic factors, however, such problems are nonexistent in the tweet images. This study reveals that the number of geo-tagged tweets has great potential to be deemed as a substitute of brightness of NTL to assess socioeconomic factors over large geographic areas.}
}
@article{SEIFERMANN2022111138,
title = {Detecting violations of access control and information flow policies in data flow diagrams},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111138},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111138},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002351},
author = {Stephan Seifermann and Robert Heinrich and Dominik Werle and Ralf Reussner},
keywords = {Data flow diagram, Access control, Information flow},
abstract = {The security of software-intensive systems is frequently attacked. High fines or loss in reputation are potential consequences of not maintaining confidentiality, which is an important security objective. Detecting confidentiality issues in early software designs enables cost-efficient fixes. A Data Flow Diagram (DFD) is a modeling notation, which focuses on essential, functional aspects of such early software designs. Existing confidentiality analyses on DFDs support either information flow control or access control, which are the most common confidentiality mechanisms. Combining both mechanisms can be beneficial but existing DFD analyses do not support this. This lack of expressiveness requires designers to switch modeling languages to consider both mechanisms, which can lead to inconsistencies. In this article, we present an extended DFD syntax that supports modeling both, information flow and access control, in the same language. This improves expressiveness compared to related work and avoids inconsistencies. We define the semantics of extended DFDs by clauses in first-order logic. A logic program made of these clauses enables the automated detection of confidentiality violations by querying it. We evaluate the expressiveness of the syntax in a case study. We attempt to model nine information flow cases and six access control cases. We successfully modeled fourteen out of these fifteen cases, which indicates good expressiveness. We evaluate the reusability of models when switching confidentiality mechanisms by comparing the cases that share the same system design, which are three pairs of cases. We successfully show improved reusability compared to the state of the art. We evaluated the accuracy of confidentiality analyses by executing them for the fourteen cases that we could model. We experienced good accuracy.}
}
@article{JAVANMARDI2024103778,
title = {M-RL: A mobility and impersonation-aware IDS for DDoS UDP flooding attacks in IoT-Fog networks},
journal = {Computers & Security},
volume = {140},
pages = {103778},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103778},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824000798},
author = {Saeed Javanmardi and Meysam Ghahramani and Mohammad Shojafar and Mamoun Alazab and Antonio M. Caruso},
keywords = {IoT-Fog networks, Data security, Intrusion Detection System (IDS), Received Signal Strength (RSS), Rate Limiting (RL)},
abstract = {The Internet of Things (IoT) has recently received a lot of attention from the information and communication technology community. It has turned out to be a crucial development for harnessing the incredible power of wireless media in the real world. The nature of IoT-Fog networks requires the use of defense techniques who are light and mobile-aware. The edge resources in such a distributed environment are open to various safety hazards. DDoS UDP flooding attacks are the most frequent threats to edge resources in IoT-Fog networks. It is crucial for sabotaging fog gateways and can overcome traditional data filtering techniques. This paper introduces M-RL, a lightweight intrusion detection system with mobility awareness that can detect DDoS UDP flooding attacks while taking into account adversarial IoT devices that engage in IP spoofing. To this end, this paper analyzes the malicious behaviors that result in anonymity against Rate Limiting and Received Signal Strength (RSS)-based approaches, combines their advantages, and addresses their vulnerabilities. We test our method in different contexts to achieve that goal, and we find that it may decrease the accuracy of the RL, RSS, and RSS-RL methods to 70%, 48.9%, and 64.3%, respectively. The outcomes demonstrate the proposed approach's resistance to software-based source address forgery, impersonation, and signal modification. It offers more than 99% accuracy and supports node mobility. In this case, the best possible accuracy of the previous methods is 77%.}
}
@article{CASE2020101872,
title = {Hooktracer: Automatic Detection and Analysis of Keystroke Loggers Using Memory Forensics},
journal = {Computers & Security},
volume = {96},
pages = {101872},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101872},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820301450},
author = {Andrew Case and Ryan D. Maggio and Md Firoz-Ul-Amin and Mohammad M. Jalalzai and Aisha Ali-Gombe and Mingxuan Sun and Golden G. Richard},
keywords = {Memory forensics, Keystroke loggers, Malware detection, Emulation, Incident response, Reverse engineering},
abstract = {Advances in malware development have led to the widespread use of attacker toolkits that do not leave any trace in the local filesystem. This negatively impacts traditional investigative procedures that rely on filesystem analysis to reconstruct attacker activities. As a solution, memory forensics has replaced filesystem analysis in these scenarios. Unfortunately, existing memory forensics tools leave many capabilities inaccessible to all but the most experienced investigators, who are well versed in operating systems internals and reverse engineering. The goal of the research described in this paper is to make investigation of one of the greatest threats that organizations face, userland keyloggers, less error-prone and less dependent on manual reverse engineering. To accomplish this, we have added significant new capabilities to HookTracer, which is an engine capable of emulating code discovered in a physical memory captures and recording all actions taken by the emulated code. Based on this work, we present new memory forensics capabilities, embodied in a new Volatility plugin, hooktracer_messagehooks, that uses Hooktracer to automatically decide whether a hook in memory is associated with a malicious keylogger or benign software. We also include a detailed case study that illustrates our technique’s ability to successfully analyze very sophisticated keyloggers, such as Turla.}
}
@article{KALYAN2024100048,
title = {A survey of GPT-3 family large language models including ChatGPT and GPT-4},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100048},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000456},
author = {Katikapalli Subramanyam Kalyan},
keywords = {Large language models, LLMs, GPT-3, ChatGPT, GPT-4, Transformers, LLM survey},
abstract = {Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI’s GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.}
}
@article{ASTUDILLO202178,
title = {Mixed Use of Analytical Derivatives and Algorithmic Differentiation for NMPC of Robot Manipulators⁎⁎The authors would like to thank Flanders Make SBO MULTIROB: “Rigorous approach for programming and optimal control of multi-robot systems”, FWO project G0A6917N of the Research Foundation - Flanders (FWO - Flanders), and KU Leuven-BOF PFV/10/002 Centre of Excellence: Optimization in Engineering (OPTEC) for supporting this research.},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {20},
pages = {78-83},
year = {2021},
note = {Modeling, Estimation and Control Conference MECC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.156},
url = {https://www.sciencedirect.com/science/article/pii/S240589632102200X},
author = {Alejandro Astudillo and Justin Carpentier and Joris Gillis and Goele Pipeleers and Jan Swevers},
keywords = {Analytical derivatives, algorithmic differentiation, robot manipulators, optimal control, predictive control},
abstract = {In the context of nonlinear model predictive control (NMPC) for robot manipulators, we address the problem of enabling the mixed and transparent use of algorithmic differentiation (AD) and efficient analytical derivatives of rigid-body dynamics (RBD) to decrease the solution time of the subjacent optimal control problem (OCP). Efficient functions for RBD and their analytical derivatives are made available to the numerical optimization framework CasADi by overloading the operators in the implementations made by the RBD library Pinocchio and adding a derivative-overloading feature to CasADi. A comparison between analytical derivatives and AD is made based on their influence on the solution time of the OCP, showing the benefits of using analytical derivatives for RBD in optimal control of robot manipulators.}
}
@incollection{2020455,
title = {Glossary},
editor = {Anthony C. Chang},
booktitle = {Intelligence-Based Medicine},
publisher = {Academic Press},
pages = {455-479},
year = {2020},
isbn = {978-0-12-823337-5},
doi = {https://doi.org/10.1016/B978-0-12-823337-5.00024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823337500024X}
}
@article{CHOUCHEN2021106908,
title = {WhoReview: A multi-objective search-based approach for code reviewers recommendation in modern code review},
journal = {Applied Soft Computing},
volume = {100},
pages = {106908},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106908},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620308462},
author = {Moataz Chouchen and Ali Ouni and Mohamed Wiem Mkaouer and Raula Gaikovina Kula and Katsuro Inoue},
keywords = {Modern code review, Software quality, Code reviewers recommendation, Search-based software engineering},
abstract = {Contemporary software development is distributed and characterized by high dynamics with continuous and frequent changes to fix defects, add new user requirements or adapt to other environmental changes. To manage such changes and ensure software quality, modern code review is broadly adopted as a common and effective practice. Yet several open-source as well as commercial software projects have adopted peer code review as a crucial practice to ensure the quality of their software products using modern tool-based code review. Nevertheless, the selection of peer reviewers is still merely a manual and hard task especially with the growing size of distributed development teams. Indeed, it has been proven that inappropriate peer reviewers selection can consume more time and effort from both developers and reviewers and increase the development costs and time to market. To address this problem, we introduce a multi-objective search-based approach, named WhoReview, to find the optimal set of peer reviewers for code changes. We use the Indicator-Based Evolutionary Algorithm (IBEA) to find the best set of code reviewers that are (1) most experienced with the code change to be reviewed, while (2) considering their current workload, i.e., the number of open code reviews they are working on. We conduct an empirical study on 4 long-lived open source software projects to evaluate our approach. The obtained results show that WhoReview outperforms state-of-the-art approach by an average precision of 68% and recall of 77%. Moreover, we deployed our approach in an industrial context and evaluated it qualitatively from developers perspective. Results show the effectiveness of our approach with a high acceptance ratio in identifying relevant reviewers.}
}
@incollection{TAN2024259,
title = {Chapter 13 - Artificial intelligence and basic human needs: the shadow aspects of emerging technology},
editor = {Santi Caballé and Joan Casas-Roma and Jordi Conesa},
booktitle = {Ethics in Online AI-based Systems},
publisher = {Academic Press},
pages = {259-278},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-18851-0},
doi = {https://doi.org/10.1016/B978-0-443-18851-0.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443188510000044},
author = {Tay Keong Tan},
keywords = {Artificial intelligence, autonomous vehicles, facial recognition, AI writing assistant, AI image generator, human needs},
abstract = {While advancing artificial intelligence (AI) applications have brought ease and benefit to human life in meeting our physical needs, it is less obvious how they would impact psychological needs. This study analyzes three emerging technologies—autonomous vehicles; facial recognition systems; and AI writing or image generators—from the perspective of six fundamental human needs; certainty, variety, significance, connection, growth, and contribution. Our core human needs can greatly influence the acceptability, feasibility, and utility of these technologies. A prognosis of the human needs implications of AI can help algorithm designers, policymakers, regulators, and end users mitigate the risks and accentuate its benefits.}
}
@article{TAHIR2023111837,
title = {Test flakiness’ causes, detection, impact and responses: A multivocal review},
journal = {Journal of Systems and Software},
volume = {206},
pages = {111837},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111837},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223002327},
author = {Amjed Tahir and Shawn Rasheed and Jens Dietrich and Negar Hashemi and Lu Zhang},
keywords = {Flaky tests, Non-deterministic tests, Test bugs, Software testing, Multivocal review},
abstract = {Flaky tests (tests with non-deterministic outcomes) pose a major challenge for software testing. They are known to cause significant issues, such as reducing the effectiveness and efficiency of testing and delaying software releases. In recent years, there has been an increased interest in flaky tests, with research focusing on different aspects of flakiness, such as identifying causes, detection methods and mitigation strategies. Test flakiness has also become a key discussion point for practitioners (in blog posts, technical magazines, etc.) as the impact of flaky tests is felt across the industry. This paper presents a multivocal review that investigates how flaky tests, as a topic, have been addressed in both research and practice. Out of 560 articles we reviewed, we identified and analysed a total of 200 articles that are focused on flaky tests (composed of 109 academic and 91 grey literature articles/posts) and structured the body of relevant research and knowledge using four different dimensions: causes, detection, impact and responses. For each of those dimensions, we provide categorization and classify existing research, discussions, methods and tools With this, we provide a comprehensive and current snapshot of existing thinking on test flakiness, covering both academic views and industrial practices, and identify limitations and opportunities for future research.}
}
@article{CATILLO2022103363,
title = {No more DoS? An empirical study on defense techniques for web server Denial of Service mitigation},
journal = {Journal of Network and Computer Applications},
volume = {202},
pages = {103363},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103363},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000303},
author = {Marta Catillo and Antonio Pecchia and Umberto Villano},
keywords = {Denial of service, Web server, Defense, Enlargement, Availability},
abstract = {Denial-of-Service (DoS) attacks are becoming increasingly common and undermine the availability of widely used web servers. Even if DoS attacks cannot be rendered completely harmless, ready-to-use defense modules and solutions to mitigate their effect are highly beneficial for site administrators. Unfortunately, there is a lack of measurement studies that explore the pros and cons of common DoS web server defense modules in order to understand their limitations and to drive practitioners’ choices. This paper presents an empirical study of the ubiquitous Apache web server, with an assessment of two well-known pluggable defense modules and an enlargement technique that provides the server with additional resources. Measurements are based on a mixture of flooding and slow DoS attacks. The experimentation shows that, in spite of the large availability of pluggable security modules that can be usefully deployed in practice, there is not a bulletproof defense solution to mitigate the DoS attacks in hand. The findings of our analysis can be useful to support the deployment of proper defense mechanisms, as well as the development of robust and effective solutions for DoS protection.}
}
@article{SMITS20233536,
title = {Maintaining symmetry during body axis elongation},
journal = {Current Biology},
volume = {33},
number = {16},
pages = {3536-3543.e6},
year = {2023},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2023.07.050},
url = {https://www.sciencedirect.com/science/article/pii/S0960982223009892},
author = {Celia M. Smits and Sayantan Dutta and Vishank Jain-Sharma and Sebastian J. Streichan and Stanislav Y. Shvartsman},
abstract = {Summary
Bilateral symmetry defines much of the animal kingdom and is crucial for numerous functions of bilaterian organisms. Genetic approaches have discovered highly conserved patterning networks that establish bilateral symmetry in early embryos,1 but how this symmetry is maintained throughout subsequent morphogenetic events remains largely unknown.2 Here we show that the terminal patterning system—which relies on Ras/ERK signaling through activation of the Torso receptor by its ligand Trunk3—is critical for preserving bilateral symmetry during Drosophila body axis elongation, a process driven by cell rearrangements in the two identical lateral regions of the embryo and specified by the dorsal-ventral and anterior-posterior patterning systems.4 We demonstrate that fluctuating asymmetries in this rapid convergent-extension process are attenuated in normal embryos over time, possibly through noise-dissipating forces from the posterior midgut invagination and movement. However, when Torso signaling is attenuated via mutation of Trunk or RNAi directed against downstream Ras/ERK pathway components, body axis elongation results in a characteristic corkscrew phenotype,5 which reflects dramatic reorganization of global tissue flow and is incompatible with viability. Our results reveal a new function downstream of the Drosophila terminal patterning system in potentially active control of bilateral symmetry and should motivate systematic search for similar symmetry-preserving regulatory mechanisms in other bilaterians.}
}
@article{XU2022101985,
title = {FaNDS: Fake News Detection System using energy flow},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {101985},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.101985},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000040},
author = {Jiawei Xu and Vladimir Zadorozhny and Danchen Zhang and John Grant},
keywords = {Fake news, Inconsistency graph, Energy flow},
abstract = {Recently, the term “fake news” has been broadly and extensively utilized for disinformation, misinformation, hoaxes, propaganda, satire, rumors, click-bait, and junk news. It has become a serious problem around the world. We present a new system, FaNDS, that detects fake news efficiently. The system is based on several concepts used in some previous works but in a different context. There are two main concepts: an Inconsistency Graph and Energy Flow. The Inconsistency Graph contains news items as nodes and inconsistent opinions between them for edges. Energy Flow assigns each node an initial energy and then some energy is propagated along the edges until the energy distribution on all nodes converges. To illustrate FaNDS we use the original data from the Fake News Challenge (FNC-1). First, the data has to be reconstructed in order to generate the Inconsistency Graph. The graph contains various subgraphs with well-defined shapes that represent different types of connections between the news items. Then the Energy Flow method is applied. The nodes with high energy are the candidates for being fake news. In our experiments, all these were indeed fake news as we checked each using several reliable web sites. We compared FaNDS to several other fake news detection methods and found it to be more sensitive in discovering fake news items.}
}
@article{HOU2022103033,
title = {Identification of Chinese dark jargons in Telegram underground markets using context-oriented and linguistic features},
journal = {Information Processing & Management},
volume = {59},
number = {5},
pages = {103033},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103033},
url = {https://www.sciencedirect.com/science/article/pii/S030645732200142X},
author = {Yiwei Hou and Hailin Wang and Haizhou Wang},
keywords = {Jargons identification, Information security, Feature engineering, Word embedding, Transfer learning, Vectors projection},
abstract = {When cybercriminals communicate with their customers in underground markets, they tend to use secure and customizable instant messaging (IM) software, i.e. Telegram. It is a popular IM software with over 700 million monthly active users (MAU) up to June 2022. In recent years, more and more dark jargons (i.e. an innocent-looking replacement of sensitive terms) appear frequently on Telegram. Therefore, jargons identification is one of the most significant research perspectives to track online underground markets and cybercrimes. This paper proposes a novel Chinese Jargons Identification Framework (CJI-Framework) to identify dark jargons. Firstly, we collect chat history from Telegram groups that are related to the underground market and construct the corpus TUMCC (Telegram Underground Market Chinese Corpus), which is the first Chinese corpus in jargons identification research field. Secondly, we extract seven brand-new features which can be classified into three categories: Vectors-based Features (VF), Lexical analysis-based Features (LF), and Dictionary analysis-based Features (DF), to identify Chinese dark jargons from commonly-used words. Based on these features, we then run a statistical outlier detection to decide whether a word is a jargon. Furthermore, we employ a word vector projection method and a transfer learning method to improve the effect of the framework. Experimental results show that CJI-Framework achieves a remarkable performance with an F1-score of 89.66%. After adaptation for English, it performs better than state-of-the-art English jargons identification method as well. Our built corpus and code have been publicly released to facilitate the reproduction and extension of our work.}
}
@article{DIVYA2022403,
title = {A model to detect domain names generated by DGA malware},
journal = {Procedia Computer Science},
volume = {215},
pages = {403-412},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922021135},
author = {T Divya and P.P Amritha and Sangeetha Viswanathan},
keywords = {Domain Generation Algorithm, Command, Control server, Network Security, Machine Learning},
abstract = {Command and control(C&C) servers are being more frequently used in cyberattacks in recent years. A malware-infected machine is controlled and directed by an attacker using a command-and-control server in order to steal data from the network. To hide their servers, attackers commonly employ a domain generation algorithm that generates domain names for them by concatenating words from word lists. Some of the algorithmically-generated domain names are used to connect to the C&C server. With the emergence of sophisticated domain generation algorithms, detecting such domains has become a challenge, which in turn poses a severe danger to computer networks. In this paper, we are proposing a concept called centrality, which is used as one of the features to analyze the words in the domain names generated by the domain generation algorithm malware. For classification, we are using Naïve Bayes, KNN, SVM, Decision Trees, Random Forest and logistic regression. Experimental results showed that Random Forest gave the highest classification accuracy rate of 88.64% and Naive Bayes gave the lowest accuracy of 44.32%.}
}
@article{DESHPANDE2022108289,
title = {AI-Based human audio processing for COVID-19: A comprehensive overview},
journal = {Pattern Recognition},
volume = {122},
pages = {108289},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108289},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004696},
author = {Gauri Deshpande and Anton Batliner and Björn W. Schuller},
keywords = {COVID-19, Digital health, Audio processing, Computational paralinguistics},
abstract = {The Coronavirus (COVID-19) pandemic impelled several research efforts, from collecting COVID-19 patients’ data to screening them for virus detection. Some COVID-19 symptoms are related to the functioning of the respiratory system that influences speech production; this suggests research on identifying markers of COVID-19 in speech and other human generated audio signals. In this article, we give an overview of research on human audio signals using ‘Artificial Intelligence’ techniques to screen, diagnose, monitor, and spread the awareness about COVID-19. This overview will be useful for developing automated systems that can help in the context of COVID-19, using non-obtrusive and easy to use bio-signals conveyed in human non-speech and speech audio productions.}
}
@article{REUTER20213,
title = {FESTUNG 1.0: Overview, usage, and example applications of the MATLAB/GNU Octave toolbox for discontinuous Galerkin methods},
journal = {Computers & Mathematics with Applications},
volume = {81},
pages = {3-41},
year = {2021},
note = {Development and Application of Open-source Software for Problems with Numerical PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2020.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0898122120303254},
author = {Balthasar Reuter and Hennes Hajduk and Andreas Rupp and Florian Frank and Vadym Aizinger and Peter Knabner},
keywords = {Open source MATLAB/GNU Octave, Discontinuous Galerkin method, Vectorization, Coupled modeling, Shallow–water equations, Cahn–Hilliard equation},
abstract = {The present work documents the current state of development for our MATLAB/GNU Octave-based open source toolbox FESTUNG (Finite Element Simulation Toolbox for UNstructured Grids). The goal of this project is to design a user-friendly, research-oriented, yet computationally efficient software tool for solving partial differential equations (PDEs). Since the release of its first version, FESTUNG has been actively used for research and teaching purposes such as the design of novel algorithms and discretization schemes, benchmark studies, or just providing students with an easy-to-learn software package to study advanced numerical techniques and good programming practices. For spatial discretization, the package employs various discontinuous Galerkin (DG) methods, while different explicit, implicit, or semi-implicit Runge–Kutta schemes can be used for time stepping. The current publication discusses the most important aspects of our toolbox such as the code design concepts and various discretization procedures illustrated in some detail using a standard advection–diffusion–reaction equation. Moreover, we present selected applications already supported in FESTUNG including solvers for the two-dimensional shallow-water equations, the Cahn–Hilliard equation, and a coupled multi-physics model of free surface/subsurface flow.}
}
@article{ANAND20201129,
title = {An Ensemble Approach For Algorithmically Generated Domain Name Detection Using Statistical And Lexical Analysis},
journal = {Procedia Computer Science},
volume = {171},
pages = {1129-1136},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.121},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920310991},
author = {P. Mohan Anand and T. Gireesh Kumar and P.V. Sai Charan},
keywords = {DGA, Ensemble Learning, PRNG, Malware},
abstract = {Domain Generation Algorithms are the new source of mediators which will provide the attackers an intelligent way of avoiding detection at the host level. Typically, before the existence of DGA, the malware was having a hardcoded command and control (C&C) IP address. That hardcoded mechanism is prone to detection and thus how DGA came into existence. Domain Generation Algorithms use the traditional cryptographic principles of Pseudo-random number generators (PRNGs) to generate a list of domain names to which malware communicates. In this paper, we constructed a list of 44 features (lexical+statistical) from domain names and used the ensemble approaches like C5.0, Random Forest, Gradient Boosting and CART to classify DGA domain names. C5.0 stands out as the best one with an accuracy value of 0.9704.}
}
@article{MATTHEWS2022101580,
title = {Tweet, like, subscribe! Understanding leadership through social media use},
journal = {The Leadership Quarterly},
volume = {33},
number = {1},
pages = {101580},
year = {2022},
note = {The Leadership Quarterly Yearly Review (LQYR) for 2022},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2021.101580},
url = {https://www.sciencedirect.com/science/article/pii/S1048984321000850},
author = {Michael J. Matthews and Samuel H. Matthews and Dawei(David) Wang and Thomas K. Kelemen},
keywords = {leader social media, Social media, Leadership, Twitter, Facebook},
abstract = {The proliferation of digital data has opened the door for a 21st-century social science that explores human relationships on an unprecedented scale. A particular area of interest is that of leader social media (SM) usage. As studies on leader SM usage have grown dramatically in the past several years, we take stock of the extant literature across various research disciplines. Within this manuscript, we contextualize leader SM usage and demonstrate how it compares to analogous concepts. We subsequently abridge relevant findings and reflect on methodological and theoretical components of the research studies identified in this review. Further, we outline the nature of SM data and provide practical recommendations for leadership scholars to capitalize on this rich data source in their investigations. We also offer a theoretical framework and summary of how scholars have studied leader SM usage. Specifically, this review article synthesizes the current literature while also elevating the academic rigor of leader SM research.}
}
@article{MACAS2022109032,
title = {A survey on deep learning for cybersecurity: Progress, challenges, and opportunities},
journal = {Computer Networks},
volume = {212},
pages = {109032},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109032},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622001864},
author = {Mayra Macas and Chunming Wu and Walter Fuertes},
keywords = {Cybersecurity, Artificial intelligence, Machine learning, Deep learning, Cyber-threat, Botnets, Intrusion detection, Spam filtering, Encrypted traffic analysis},
abstract = {As the number of Internet-connected systems rises, cyber analysts find it increasingly difficult to effectively monitor the produced volume of data, its velocity and diversity. Signature-based cybersecurity strategies are unlikely to achieve the required performance for detecting new attack vectors. Moreover, technological advances enable attackers to develop sophisticated attack strategies that can avoid detection by current security systems. As the cyber-threat landscape worsens, we need advanced tools and technologies to detect, investigate, and make quick decisions regarding emerging attacks and threats. Applications of artificial intelligence (AI) have the potential to analyze and automatically classify vast amounts of Internet traffic. AI-based solutions that automate the detection of attacks and tackle complex cybersecurity problems are gaining increasing attention. This paper comprehensively presents the promising applications of deep learning, a subfield of AI based on multiple layers of artificial neural networks, in a wide variety of security tasks. Before critically and comparatively surveying state-of-the-art solutions from the literature, we discuss the key characteristics of representative deep learning architectures employed in cybersecurity applications, we introduce the emerging trends in deep learning, and we provide an overview of necessary resources like a generic framework and suitable datasets. We identify the limitations of the reviewed works, and we bring forth a vision of the current challenges of the area, providing valuable insights and good practices for researchers and developers working on related problems. Finally, we uncover current pain points and outline directions for future research to address them.}
}
@article{HAZAN2021106982,
title = {Supporting unknown number of users in keystroke dynamics models},
journal = {Knowledge-Based Systems},
volume = {221},
pages = {106982},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106982},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002458},
author = {Itay Hazan and Oded Margalit and Lior Rokach},
keywords = {Behavioral biometrics, Keystroke dynamics, Multi-user model, X-means},
abstract = {In recent years, keystroke dynamics has gained popularity as a reliable means of verifying user identity in remote systems. Due to its high performance in verification and the fact that it does not require additional effort from the user, keystroke dynamics has become one of the most preferred second factor of authentication. Despite its prominence, it has one major limitation: keystroke dynamics algorithms are good at fitting a model to one user and one user only. When such algorithms try to fit a model to more than one user, the verification accuracy decreases dramatically. However, in real-world applications it is common practice for two or more users to use the same credentials, such as in shared bank accounts, shared social media profiles, and shared streaming licenses which allow multiple users in one account. In these cases, keystroke dynamics solutions become unreliable. To address this limitation, we propose a method that can leverage existing keystroke dynamics algorithms to automatically determine the number of users sharing the account and accurately support accounts that are shared with multiple users. We evaluate our method using eight state-of-the-art keystroke dynamics algorithms and three public datasets, with up to five different users in one model, achieving an average improvement in verification of 9.2% for the AUC and 8.6% for the EER in the multi-user cases, with just a negligible reduction of 0.2% for the AUC and 0.3% for the EER in the one-user cases.}
}
@article{DAHLIN201825,
title = {Sparse Bayesian ARX models with flexible noise distributions⁎⁎This work was supported by the Australian Research Council Discovery Project DP140104350. The EEG data was kindly provided by Eline Borch Petersen and Thomas Lunner at Eriksholm Research Centre, Oticon A/S, Denmark.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {15},
pages = {25-30},
year = {2018},
note = {18th IFAC Symposium on System Identification SYSID 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.085},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318317452},
author = {Johan Dahlin and Adrian Wills and Brett Ninness},
keywords = {Bayesian inference, Hamiltonian Monte Carlo, Gaussian mixture models},
abstract = {This paper considers the problem of estimating linear dynamic system models when the observations are corrupted by random disturbances with nonstandard distributions. The paper is particularly motivated by applications where sensor imperfections involve significant contribution of outliers or wrap-around issues resulting in multi-modal distributions such as commonly encountered in robotics applications. As will be illustrated, these nonstandard measurement errors can dramatically compromise the effectiveness of standard estimation methods, while a computational Bayesian approach developed here is demonstrated to be equally effective as standard methods in standard measurement noise scenarios, but dramatically more effective in nonstandard measurement noise distribution scenarios.}
}
@article{SAMBANGI2022107955,
title = {A Feature Similarity Machine Learning Model for DDoS Attack Detection in Modern Network Environments for Industry 4.0},
journal = {Computers and Electrical Engineering},
volume = {100},
pages = {107955},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107955},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002324},
author = {Swathi Sambangi and Lakshmeeswari Gondi and Shadi Aljawarneh},
keywords = {Machine learning, Cloud security, DDoS Attack, DoS attack, Industry 4.0, Early detection, Botnet, Feature transformation, Classification},
abstract = {ABSTRACT
Recent advancements in artificial intelligence and machine learning technologies have laid the flagstone for the fourth industrial revolution, Industry 4.0. The industry 4.0 is at a very high momentum when compared to previous revolutions witnessed by humans in a way which was never anticipated. Cyber Physical Systems and Cloud computing are the basis for Industry 4.0. An ongoing research challenge in cloud computing is the immediate need to address security and data availability challenges coined in modern networking environments. For instance, DDoS attacks in cloud are continuously throwing new challenges to network community which makes detection of these attacks, an ongoing research challenge with respect to cloud security. At the outset, the research reported in this work has addressed three important contributions (i) A new gaussian based traffic attribute-pattern similarity function for evolutionary feature clustering to achieve feature transformation-based dimensionality reduction, (ii) A Gaussian based network traffic similarity function for similarity computation between network traffic instances and (iii) A machine learning model SWASTHIKA which uses feature transformation traffic for detection of low rate and high-rate network attacks. For experimental study, the most recent benchmark dataset namely IoT DoS and DDoS attack dataset available at IEEE Dataport is considered as this dataset has highly non-linear traffic instances which are like the real-world traffic. The performance evaluation of the proposed machine learning model SWASTHIKA is done by considering various classifier evaluation parameters such as accuracy, precision, detection rate, and F-Score. The experiment results proved that the attack detection rate of SWASTHIKA is significantly better compared to state of art machine learning classifiers.}
}
@incollection{FAIRCLOTH201731,
title = {Chapter 2 - Reconnaissance},
editor = {Jeremy Faircloth},
booktitle = {Penetration Tester's Open Source Toolkit (Fourth Edition)},
publisher = {Syngress},
edition = {Fourth Edition},
address = {Boston},
pages = {31-106},
year = {2017},
isbn = {978-0-12-802149-1},
doi = {https://doi.org/10.1016/B978-0-12-802149-1.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021491000026},
author = {Jeremy Faircloth},
keywords = {Reconnaissance, OSINT, footprinting, DNS, human recon, whois},
abstract = {This chapter covers information gathering by focusing on reconnaissance and learning as much about a target as possible before you actually interact with it. This is typically a very stealthy part of penetration testing and is the first step in gathering the information that you need to move forward with testing.}
}
@article{MIRSKY202075,
title = {Lightweight collaborative anomaly detection for the IoT using blockchain},
journal = {Journal of Parallel and Distributed Computing},
volume = {145},
pages = {75-97},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520303154},
author = {Yisroel Mirsky and Tomer Golomb and Yuval Elovici},
keywords = {IoT security, Markov-chain, Anomaly detection, Blockchain, Collaborative security},
abstract = {Due to their rapid growth and deployment, the Internet of things (IoT) have become a central aspect of our daily lives. Unfortunately, IoT devices tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques, such as anomaly detection, can be used to secure these devices in a plug-and-protect manner. However, anomaly detection models must be trained for a long time in order to capture all benign behaviors. Furthermore, the anomaly detection model is vulnerable to adversarial attacks since, during the training phase, all observations are assumed to be benign. In this paper, we propose (1) a novel approach for anomaly detection and (2) a lightweight framework that utilizes the blockchain to ensemble an anomaly detection model in a distributed environment. Blockchain framework incrementally updates a trusted anomaly detection model via self-attestation and consensus among the IoT devices. We evaluate our method on a distributed IoT simulation platform, which consists of 48 Raspberry Pis. The simulation demonstrates how the approach can enhance the security of each device and the security of the network as a whole.}
}
@article{MORALESMARIN2021125874,
title = {Simulating seasonal to multi-decadal variation in lake thermal response to meteorological forcing using the UCLAKE 1-dimensional model code},
journal = {Limnologica},
volume = {88},
pages = {125874},
year = {2021},
issn = {0075-9511},
doi = {https://doi.org/10.1016/j.limno.2021.125874},
url = {https://www.sciencedirect.com/science/article/pii/S0075951121000268},
author = {Luis A. Morales-Marín and Jon R. French and Helene Burningham and Chris Evans and Annette Burden},
keywords = {Lake temperature, Lake warming, Stratification, Mixing, Numerical model, Open source},
abstract = {Lake temperature responses to climate forcing are of interest on account of the important linkages between water temperature and ecosystem processes. This paper describes a new 1-dimensional (1D) numerical model code and its application to investigations of multi-scale linkages between the vertical temperature structure and meteorological forcing. UCLAKE is implemented as highly portable open-source software, based on computationally efficient algorithms, and able to resolve sub-daily (e.g., hourly) dynamics while retaining the efficiency to simulate multi-decadal time scales. A UCLAKE model is calibrated and validated against thermistor profile time series for a small upland lake in North Wales, UK. Some of the challenges in 1D model calibration are explored and a sensitivity analysis reveals a dependence of optimal parameter set values on water column depth and time. An exploratory 52-year hindcast simulation demonstrates the computational efficiency of UCLAKE for multi-decadal studies of trends in lake temperature that vary with depth. A supplementary application of UCLAKE to Windermere, in the English Lake District, demonstrates its performance for larger and deeper lakes.}
}
@article{LIU2024111883,
title = {DG Embeddings: The unsupervised definition embeddings learned from dictionary and glossary to gloss context words of Cloze task},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111883},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111883},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005173},
author = {Xiaodong Liu and Rafal Rzepka and Kenji Araki},
keywords = {Unsupervised definition embeddings, Semantic features of glosses, Context words, Auto-encoding models, Natural language processing},
abstract = {For both humans and machines to acquire vocabulary, it is effective to learn words from context while using dictionaries as an auxiliary tool. It has been shown in previous linguistic studies that for humans, glossing either target words to be learned or words comprising context is an effective approach. For machines, however, previous NLP studies are mainly focused on the former. In this paper, we investigate the potentiality of context words-glossed setting. During pre-training BERT, to infuse context words with semantic features of glosses, we propose DG embeddings — the unsupervised definition embeddings learned from dictionaries and glossaries. To employ unsupervised learning is inspired by a real-world scenario of dictionary use called headword search. This can also prevent a technical duplicate from happening, as learning words from context is already based on auto-encoding models with self-supervised learning. BERT-base is used for evaluation, and we refer to BERT-base with DG embeddings as DG-BERT. According to our experimental results, compared to the vanilla BERT, DG-BERT shows the following strengths: faster pre-training convergence, noticeable improvements on various downstream tasks, a better grasp of figurative semantics, more accurate self-attention for collocation of phrases, and higher sensitivity to context words for target-word predictions in psycholinguistic diagnostics.}
}
@article{SILVA2017314,
title = {Towards filtering undesired short text messages using an online learning approach with semantic indexing},
journal = {Expert Systems with Applications},
volume = {83},
pages = {314-325},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417303056},
author = {Renato M. Silva and Tulio C. Alberto and Tiago A. Almeida and Akebo Yamakami},
keywords = {Minimum description length, Short text messages, Semantic indexing, Text categorization, Machine learning},
abstract = {The popularity and reach of short text messages commonly used in electronic communication have led spammers to use them to propagate undesired content. This is often composed by misleading information, advertisements, viruses, and malwares that can be harmful and annoying to users. The dynamic nature of spam messages demands for knowledge-based systems with online learning and, therefore, the most traditional text categorization techniques can not be used. In this study, we introduce the MDLText, a text classifier based on the minimum description length principle, to the context of filtering undesired short text messages. The proposed approach supports incremental learning and, therefore, its predictive model is scalable and can adapt to continuously evolving spamming techniques. It is also fast, with computational cost increasing linearly with the number of samples and features, which is very desirable for expert systems applied to real-time electronic communication. In addition to the dynamic nature of these messages, they are also short and usually poorly written, rife with slangs, symbols, and abbreviations that difficult text representation, learning, and filtering. In this scenario, we also investigated the benefits of using text normalization and semantic indexing techniques. We showed these techniques can improve the text content quality and, consequently, enhance the performance of the expert systems for spamming detection. Based on these findings, we propose a new hybrid ensemble approach that combines the predictions obtained by the classifiers using the original text samples along with their variations created by applying text normalization and semantic indexing techniques. It has the advantages of being independent of the classification method and the results indicated it is efficient to filter undesired short text messages.}
}
@article{ZANGENEH2020101164,
title = {Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101164},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101164},
url = {https://www.sciencedirect.com/science/article/pii/S147403462030135X},
author = {Pouya Zangeneh and Brenda McCabe},
keywords = {Industrial megaprojects, Knowledge representation, Project analytics, Risk analysis, Ontology, Semantic web},
abstract = {The fourth industrial revolution has affected most industries, including construction and those within the delivery chain of megaprojects. These major paradigm shifts, however, did not considerably improve the track record in predicting project outcomes and estimating required resources. One reason is the lack of unified data definitions and expandable knowledge representation across project lifecycle to represent megaprojects for analytics. This paper proposes and evaluates a unified ontology for project knowledge representation that facilitates data collection, processing, and utilization for industrial megaprojects through their lifecycle. The proposed Uniform Project Ontology, or UPonto, provides a data infrastructure for project analytics by enabling logical deductions and inferences, and flexible expansion and partitioning of the data utilizing linked data and the semantic web. The ontology facilitates cost normalization processes, temporal queries, and graph queries using SPARQL, while defining universal semantics for a wide range of project risk factors and characteristics based on comprehensive research of the empirical project risk and success literature augmented by practical considerations gained through expert consultations. UPonto forms the basis for a project knowledge graph to utilize unstructured data; it as well provides semantic definitions for smart IoT agents to consume project risk data and knowledge.}
}
@article{CHIBA2019291,
title = {Intelligent approach to build a Deep Neural Network based IDS for cloud environment using combination of machine learning algorithms},
journal = {Computers & Security},
volume = {86},
pages = {291-317},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819301221},
author = {Zouhair Chiba and Noreddine Abghour and Khalid Moussaid and Amina {El omri} and Mohamed Rida},
keywords = {Cloud computing, Network intrusion detection system, Deep Neural Network, Genetic algorithm, Simulated Annealing Algorithm, CICIDS dataset 2017, NSL-KDD dataset, CIDDS-001 dataset},
abstract = {The appealing features of Cloud Computing continue to fuel its adoption and its integration in many sectors such industry, governments, education and entertainment. Nevertheless, uploading sensitive data to public cloud storage services poses security risks such as integrity, availability and confidentiality to organizations. Moreover, the open and distributed (decentralized) structure of the cloud has resulted this class of computing, prone to cyber attackers and intruders. Thereby, it is imperative to develop an anomaly network intrusion system to detect and prevent both inside and outside assaults in cloud environment with high detection precision and low false warnings. In this work, we propose an intelligent approach to build automatically an efficient and effective Deep Neural Network (DNN) based anomaly Network IDS using a hybrid optimization framework (IGASAA) based on Improved Genetic Algorithm (IGA) and Simulated Annealing Algorithm (SAA). The IDS resulted is called “MLIDS” (Machine Learning based Intrusion Detection System). Genetic Algorithm (GA) is improved through optimization strategies, namely Parallel Processing and Fitness Value Hashing, which reduce execution time, convergence time and save processing power. Moreover, SAA was incorporated to IGA with the aim to optimize its heuristic search. Our approach consists of using IGASAA in order to search the optimal or near-optimal combination of most relevant values of the parameters included in construction of DNN based IDS or impacting its performance, like feature selection, data normalization, architecture of DNN, activation function, learning rate and Momentum term, which ensure high detection rate, high accuracy and low false alarm rate. For simulation and validation of the proposed method, CloudSim 4.0 simulator platform and three benchmark IDS datasets were used, namely CICIDS2017, NSL-KDD version 2015 and CIDDS-001. The implementation results of our model demonstrate its ability to detect intrusions with high detection accuracy and low false alarm rate, and indicate its superiority in comparison with state-of-the-art methods.}
}
@article{CERISARA2018175,
title = {On the effects of using word2vec representations in neural networks for dialogue act recognition},
journal = {Computer Speech & Language},
volume = {47},
pages = {175-193},
year = {2018},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2017.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885230816300456},
author = {Christophe Cerisara and Pavel Král and Ladislav Lenc},
keywords = {Dialogue act, Deep learning, LSTM, Word embeddings, Word2vec},
abstract = {Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.}
}
@article{REEDY2023100313,
title = {Interpol review of digital evidence for 2019–2022},
journal = {Forensic Science International: Synergy},
volume = {6},
pages = {100313},
year = {2023},
issn = {2589-871X},
doi = {https://doi.org/10.1016/j.fsisyn.2022.100313},
url = {https://www.sciencedirect.com/science/article/pii/S2589871X22000985},
author = {Paul Reedy}
}
@article{ALECSA2020178,
title = {New optimization algorithms for neural network training using operator splitting techniques},
journal = {Neural Networks},
volume = {126},
pages = {178-190},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300952},
author = {Cristian Daniel Alecsa and Titus Pinţa and Imre Boros},
keywords = {Neural network, MNIST, CIFAR10, Splitting, Nesterov, Dynamical system},
abstract = {In the following paper we present a new type of optimization algorithms adapted for neural network training. These algorithms are based upon sequential operator splitting technique for some associated dynamical systems. Furthermore, we investigate through numerical simulations the empirical rate of convergence of these iterative schemes toward a local minimum of the loss function, with some suitable choices of the underlying hyper-parameters. We validate the convergence of these optimizers using the results of the accuracy and of the loss function on the MNIST, MNIST-Fashion and CIFAR 10 classification datasets.}
}
@article{HONG2024103004,
title = {Fake news virality: Relational niches and the diffusion of COVID-19 vaccine misinformation},
journal = {Social Science Research},
volume = {120},
pages = {103004},
year = {2024},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2024.103004},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X24000267},
author = {Chen-Shuo Hong},
keywords = {Digital media, Misinformation, Diffusion, Networks, Relational niche, COVID-19},
abstract = {This study explores why some fake news publishers are able to propagate misinformation while others receive little attention on social media. Using COVID-19 vaccine tweets as a case study, this study combined the relational niche framework with pooled and multilevel models that address the unobserved heterogeneity. The results showed that, as expected, ties to accounts with more followers were associated with more fake news tweets, retweets, and likes. However, more surprisingly, embedding with fake news publishers had an inverted U-shaped association with diffusion, whereas social proximity to mainstream media was positively associated. Although the effect of influential users is in line with opinion leader theory, the newly-identified effects of social proximity to reliable sources and embeddedness suggest that the key to fake news virality is to earn greater organizational status and modest, not overly, echo chambers. This study highlights the potential of dynamic media networks to shape the misinformation market.}
}
@article{MARINI2024101946,
title = {A toolkit for localisation queries},
journal = {Pervasive and Mobile Computing},
volume = {103},
pages = {101946},
year = {2024},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2024.101946},
url = {https://www.sciencedirect.com/science/article/pii/S1574119224000725},
author = {Gabriele Marini and Jorge Goncalves and Eduardo Velloso and Raja Jurdak and Vassilis Kostakos},
keywords = {Semantic localisation, Indoor localisation, Patterns, Movements, Time-series, Pattern syntax, Mobility},
abstract = {While UbiComp research has steadily improved the performance of localisation systems, the analysis of such datasets remains largely unaddressed. In this paper, we present a tool to facilitate querying and analysis of localisation time-series with a focus on semantic localisation. Drawing on well-established models to represent movement and mobility, we first develop a query language for localisation datasets. We then develop a software library in R that implements this querying. We use case studies to demonstrate how our programming tool can be used to query localisation datasets. Our work addresses an important gap in localisation research, by providing a flexible tool that can model and analyse localisation data programmatically and in real time.}
}
@incollection{SCHMIDT20243,
title = {Chapter 1 - Rigor and reproducibility in genetic research and the effects on scientific reporting and public discourse},
editor = {Douglas F. Dluzen and Monika H.M. Schmidt},
booktitle = {Rigor and Reproducibility in Genetics and Genomics},
publisher = {Academic Press},
pages = {3-22},
year = {2024},
series = {Translational and Applied Genomics},
isbn = {978-0-12-817218-6},
doi = {https://doi.org/10.1016/B978-0-12-817218-6.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128172186000127},
author = {Monika H.M. Schmidt and Douglas F. Dluzen},
keywords = {Reproducibility, Replication, Rigor, Genomic research, Experimental methodology, Ethical legal social implications, ELSI, Data availability, Open science},
abstract = {The scientific method is the fundamental framework used to make observations, identify and address unanswered questions, and interpret outcomes against the context of existing knowledge and predictions. As scientific investigations become increasingly complex and are conducted with rapidly evolving technologies, a high degree of rigor is necessary to develop and conduct experiments while also ensuring the ability to reliably reproduce results. This introductory chapter provides a brief overview of the scientific method and highlights the challenges of rigor and reproducibility in present-day genetic and biomedical research. Furthermore, this chapter demonstrates how these challenges have impacted public discourse and trust in scientific—particularly biomedical—research. Finally, suggestions for addressing these challenges are presented, including the use of “open science” to redefine research parameters and encourage collaboration.}
}
@article{GITTINS202088,
title = {Malware Persistence Mechanisms},
journal = {Procedia Computer Science},
volume = {176},
pages = {88-97},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318342},
author = {Zane Gittins and Michael Soltys},
keywords = {Malware Analysis, Malware Persistence, Incident Response},
abstract = {In the public imagination Cybersecurity is very much about malware, even though malware constitutes only part of all the threats faced by Cybersecurity experts. However, malware is still one of the best methods to gain persistent access and control of a target system. Malware is often combined with a well socially-engineered phishing attack that deceives a user to gain a foothold on a system. Once the attakcer gains a beachhead in the victim’s network, it may be used to download additional payloads and exploit vulnerabilities, to gain more control and access within a network. Using malware as their foothold, attackers are able to to conduct reconnaissance, gather intelligence (e.g., exfiltration of intellectual property) or simply inflict damage or extortion (e.g., ransomware). All of this has to be done in a way that allows an attacker to retain access for as long as possible; the ability to do so is called persistence, and this paper examines the different techniques used by malware to accomplish persistence in an ever evolving landscape.}
}
@article{JANG2023102360,
title = {Introducing the Co-oriented Scansis (CoS) model: A case of chatbot, Lee-Luda},
journal = {Public Relations Review},
volume = {49},
number = {4},
pages = {102360},
year = {2023},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102360},
url = {https://www.sciencedirect.com/science/article/pii/S0363811123000759},
author = {Heesoo Jang and Suman Lee},
keywords = {Co-oriented Scansis (CoS) model, Crisis communication, Situational Crisis Communication Theory (SCCT), AI crisis, Scansis, Moral outrage},
abstract = {This study presents the Co-oriented Scansis (CoS) model, which provides a comprehensive understanding of scansis—a recently identified crisis type integrated into the Situational Crisis Communication Theory (SCCT). Using a crisis case of Scatter Lab, a South Korean AI company, as a model case, the study applies the CoS model to analyze the perceptions and meta-perceptions of both the organization and the public regarding the crisis. The data collection involved three official statements released by Scatter Lab and an analysis of 365 reviews from the Google Play users' reviews page of Science of Love—the app used by Scatter Lab to collect intimate conversations between romantic partners. The findings highlight the utility of the CoS model in explaining how Scatter Lab's AI crisis evolved into a scansis. Specifically, the organization's failure to accurately comprehend the public's perception of the crisis (second level co-orientation) and the resulting discrepancy between the organization and the public's perceptions (third level co-orientation) contributed to moral outrage, ultimately leading to a scansis. The study concludes by discussing the theoretical contributions of the CoS model and its practical implications for crisis management.}
}
@article{LAADHAR2022997,
title = {Web of Things Semantic Interoperability in Smart Buildings},
journal = {Procedia Computer Science},
volume = {207},
pages = {997-1006},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010377},
author = {Amir Laadhar and Junior Dongo and Søren Enevoldsen and Frédéric Revaz and Dominique Gabioud and Torben Bach Pedersen and Martin Meyer and Brian Nielsen and Christian Thomsen},
keywords = {Web of Things, Internet of Things, Web of Things Discovery, Semantic Interoperability, Semantic Web, Ontology},
abstract = {Buildings are the largest energy consumers in Europe and are responsible for approximately 40% of EU energy consumption and 36% of the greenhouse gas emissions in Europe. Two-thirds of the building consumption is for residential buildings. To achieve energy efficiency, buildings are being integrated with IoT devices through the use of smart IoT services. For instance, a smart space heating service reduces energy consumption by dynamically heating apartments based on indoor and outdoor temperatures. The W3C recommends the use of the Web of Things (WoT) standard to enable IoT interoperability on the Web. However, in the context of a smart building, the ability to search and discover building metadata and IoT devices available in the WoT ecosystems remains a challenge due to the limitation of the current WoT Discovery, which only includes a directory containing only IoT devices metadata without including building metadata. Integrating the IoT device's metadata with building metadata in the same directory can provide better discovery capabilities to the IoT services providers. In this paper, we integrate building metadata into the W3C WoT Discovery through the construction of a Building Description JSON-LD file. This Building Description is integrated into the W3C WoT Discovery and based on the domOS Common Ontology (dCO) to achieve semantic interoperability in smart residential buildings for the WoT IoT ecosystem within the Horizon 2020 domOS project. This integration results in a Thing and Building Description Directory. dCO integrates the SAREF core ontology with the Thing Description ontology, devices, and building metadata. We have implemented and validated the WoT discovery on top of a WoT Thing and Building Description Directory. The WoT Discovery implementation is also made available for the WoT community.}
}
@article{BOJARAJULU2023103064,
title = {Intelligent IoT-BOTNET attack detection model with optimized hybrid classification model},
journal = {Computers & Security},
volume = {126},
pages = {103064},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.103064},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822004564},
author = {Balaganesh Bojarajulu and Sarvesh Tanwar and Thipendra Pal Singh},
keywords = {IoT, BOTNET, RNN, Bi-GRU, SMIE},
abstract = {The botnet have developed into a severe risk to Internet of Things (IoT) systems as a result of manufacturers ‘insufficient security policies and end users’ lack of security awareness. By default, several ports are open and user credentials are left unmodified. ML and DL strategies have been suggested in numerous latest research for identifying and categorising botnet assaults in the IoT context, but still, it has a few issues like high error susceptibility, working only with a large amount of data, poor quality, and data acquisition. This research provided use of a brand-new IoT botnet detector built on an improved hybrid classifier. The proposed work's main components are "pre-processing, feature extraction, feature selection, and attack detection." Following that, the improved Information Gain (IIG) model is used to choose the most reliable characteristics from the received information. To detect an attack, a hybrid classifier is utilized which can be constructed by integrating the optimized Bi-GRU with the Recurrent Neural Network (RNN). To increase the detection accuracy of IoT-BOTNETS, a novel hybrid optimization approach called SMIE (Slime Mould with Immunity Evolution) is created by conceptually integrating two conventional optimization modes: Coronavirus herd immunity optimizer (CHIO) and the Slime mould algorithm. The final output of the hybrid classifier displays the presence or absence of IoT-BOTNET attacks. The projected model's accuracy is 97%, which is 22.6%, 18.5%, 27.8%, 22.6%, and 24.8% higher than the previous models like GWO+ HC, SSO+ HC, WOA+ HC, SMA+ HC, and CHIO+ HC, respectively.}
}
@article{VANDERLEE2021101151,
title = {Human evaluation of automatically generated text: Current trends and best practice guidelines},
journal = {Computer Speech & Language},
volume = {67},
pages = {101151},
year = {2021},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2020.101151},
url = {https://www.sciencedirect.com/science/article/pii/S088523082030084X},
author = {Chris {van der Lee} and Albert Gatt and Emiel {van Miltenburg} and Emiel Krahmer},
keywords = {Natural Language Generation, Human evaluation, Recommendations, Literature review, Open science, Ethics},
abstract = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated, with a particularly high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how (mostly intrinsic) human evaluation is currently conducted and presents a set of best practices, grounded in the literature. These best practices are also linked to the stages that researchers go through when conducting an evaluation research (planning stage; execution and release stage), and the specific steps in these stages. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.}
}
@article{YANG202313,
title = {Multi-stage hybrid algorithm-enabled optimization of sequence-dependent assembly line configuration for automotive engine},
journal = {Journal of Manufacturing Systems},
volume = {66},
pages = {13-26},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522002059},
author = {Miao Yang and Congbo Li and Ying Tang and Wei Wu and Yan Lv},
keywords = {Automotive engine, Multi-stage hybrid algorithm, Assembly line configuration, Process sequence},
abstract = {Engines are the most expensive and technology-intensive components in automobiles, so an optimized configuration of the automotive engine assembly line (AEAL) is anticipated to improve efficiency and reduce cost. The traditional methods for assembly line configuration can mainly work out a proper machine number, but they generally ignore process sequences that could also influence the buffer cost derived from the assignment of divergence and confluence buffers. Simultaneously, how to reduce the number of variables in the algorithm iteration process to improve computational efficiency is rarely considered in the existing studies. To bridge the gaps, this study proposes a multi-stage hybrid algorithm based on a backtracking searching algorithm (BSA) to realize an effective configuration that can further improve production efficiency and reduce equipment cost for sequence-dependent AEALs. First, an AEAL configuration model is developed to involve machine number and process sequence as decision variables and aims to satisfy multiple objectives concerned with equipment cost and cycle time. Then, a multi-stage hybrid algorithm is proposed to efficiently acquire the optimal solutions to machine number and process sequence in multiple stages that can improve computational efficiency. Finally, the effectiveness and superiority of the proposed method are validated via a case study. The numerical results show that the proposed method can effectively improve production efficiency and reduce equipment cost for sequence-dependent AEALs with a better convergence and diversity performance.}
}
@article{MACAS2024122223,
title = {Adversarial examples: A survey of attacks and defenses in deep learning-enabled cybersecurity systems},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122223},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122223},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027252},
author = {Mayra Macas and Chunming Wu and Walter Fuertes},
keywords = {Cybersecurity, Deep learning, Adversarial machine learning, Cyber threats, Adversarial examples},
abstract = {Over the last few years, the adoption of machine learning in a wide range of domains has been remarkable. Deep learning, in particular, has been extensively used to drive applications and services in specializations such as computer vision, natural language processing, machine translation, and cybersecurity, producing results that are comparable to or even surpass the performance of human experts. Nevertheless, machine learning systems are vulnerable to adversarial attacks, especially in nonstationary environments where actual adversaries exist, such as the cybersecurity domain. In this work, we comprehensively survey and present the latest research on attacks based on adversarial examples against deep learning-based cybersecurity systems, highlighting the risks they pose and promoting efficient countermeasures. To that end, adversarial attack methods are first categorized according to where they occur and the attacker’s goals and capabilities. Then, specific attacks based on adversarial examples and the respective defensive methods are reviewed in detail within the framework of eight principal cybersecurity application categories. Finally, the main trends in recent research are outlined, and the impact of recent advancements in adversarial machine learning is explored to provide guidelines and directions for future research in cybersecurity. In summary, this work is the first to systematically analyze adversarial example-based attacks in the cybersecurity field, discuss possible defenses, and highlight promising directions for future research.}
}
@article{PHAN201812,
title = {Automatically classifying source code using tree-based approaches},
journal = {Data & Knowledge Engineering},
volume = {114},
pages = {12-25},
year = {2018},
note = {Special Issue on Knowledge and Systems Engineering (KSE 2016)},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300344},
author = {Anh Viet Phan and Phuong Ngoc Chau and Minh Le Nguyen and Lam Thu Bui},
keywords = {Abtract Syntax Tree (AST), Tree-based convolutional neural networks(TBCNN), Support Vector Machines (SVMs), K-Nearest Neighbors (kNN)},
abstract = {Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. To overcome the limitations, this paper aims to solve software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. We propose two combination models between a tree-based convolutional neural network (TBCNN) and k-Nearest Neighbors (kNN), support vector machines (SVMs) to exploit both structural and semantic ASTs' information. In addition, to deal with high-dimensional data of ASTs, we present several pruning tree techniques which not only reduce the complexity of data but also enhance the performance of classifiers in terms of computational time and accuracy. We survey many machine learning algorithms on different types of program representations including software metrics, sequences, and tree structures. The approaches are evaluated based on classifying 52000 programs written in C language into 104 target labels. The experiments show that the tree-based classifiers dramatically achieve high performance in comparison with those of metrics-based or sequences-based; and two proposed models TBCNN + SVM and TBCNN + kNN rank as the top and the second classifiers. Pruning redundant AST branches leads to not only a substantial reduction in execution time but also an increase in accuracy.}
}
@article{ORTEGA2018579,
title = {The life cycle of altmetric impact: A longitudinal study of six metrics from PlumX},
journal = {Journal of Informetrics},
volume = {12},
number = {3},
pages = {579-589},
year = {2018},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1751157717302870},
author = {José Luis Ortega},
keywords = {Altmetrics, PlumX, Citations, Readers, Tweets, Longitudinal study},
abstract = {The main objective of this study is to describe the life cycle of altmetric and bibliometric indicators in a sample of publications. Altmetrics (Downloads, Views, Readers, Tweets, and Blog mentions) and bibliometric counts (Citations) (in this study, the indicators will be capitalized to differentiate them from the general language) of 5185 publications (19,186 observations) were extracted from PlumX to observe their distribution according to the publication age. Correlations between these metrics were calculated from month to month to observe the evolution of these relationships. The results showed that mention metrics (Tweets and Blog mentions) are the earliest metrics that become available most quickly and have the shortest life cycle. Next, Readers are the metrics with the highest prevalence and with the second fastest growth. Views and Downloads show a continuous growth, being the indicators with the longest life cycles. Finally, Citations are the slowest indicators and have a low prevalence. Correlations show a strong relationship between mention metrics and Readers and Downloads, and between Readers and Citations. These results enable us to create a schematic diagram of the relationships between these metrics from a longitudinal view.}
}
@article{CHAMBERLAIN2022102725,
title = {“Our world is worth fighting for”: Gas mask agency, copypasta sit-ins, and the material-discursive practices of the Blitzchung controversy},
journal = {Computers and Composition},
volume = {65},
pages = {102725},
year = {2022},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2022.102725},
url = {https://www.sciencedirect.com/science/article/pii/S8755461522000330},
author = {Elizabeth F. Chamberlain},
keywords = {New materialism, Distant reading, Computerized text analysis, Video games, Blizzard, Copypasta, Memes},
abstract = {In 2019, video game giant Blizzard banned a competitive e-sports player who made a pro-Hong Kong statement during a post-game interview. The international game community responded with outrage, organizing both on- and offline actions to provoke change within the organization. This article examines the #BoycottBlizzard gaming counterpublic via deceptively discrete mixed methods: a new materialist investigation of protest gear and a distant reading of a Reddit dataset of 3500 posts between October 7 and 10, 2019. The investigation concludes that gas masks demonstrate nonhuman aleatory agency in the #BoycottBlizzard protest movement, by inserting subversive subtext into costumes and gameplay. Online, protestors relied heavily on other resistance tactics, including using Twitch copypasta spam; this article suggests this form of resistance functions similarly to a sit-in. Finally, the article iconographically tracks the rise and dissemination of a particular meme image representing the movement's appointed mascot, a Chinese climatologist named Mei. Ultimately, the Blitzchung counterpublic achieved only modest success; the player's ban was reversed and his prize money reinstated, but many protestors considered Blizzard's response milquetoast. However, this analysis proposes that the Blitzchung counterpublic likely emboldened the 2021 #BoycottBlizzard movement and may in some measure be responsible for its success.}
}
@article{MCGAHAGAN2021102374,
title = {Discovering features for detecting malicious websites: An empirical study},
journal = {Computers & Security},
volume = {109},
pages = {102374},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102374},
url = {https://www.sciencedirect.com/science/article/pii/S016740482100198X},
author = {John McGahagan and Darshan Bhansali and Ciro Pinto-Coelho and Michel Cukier},
keywords = {Feature-based malicious website detection, Web security, Supervised learning, Feature selection},
abstract = {Website features and characteristics have shown the ability to detect various web threats – phishing, drive-by downloads, and command and control (C2). Prior research has thoroughly explored the practice of choosing features ahead of time (a priori) and building detection models. However, there is an opportunity to investigate new techniques and features for detection. We perform a comprehensive evaluation of discovering features for malicious website detection versus selecting features a priori. We gather 46,580 features derived from a response to a web request and, through a series of feature selection techniques, discover features for detection and compare their performance to those used in prior research. We build several detection models using unsupervised and supervised learning algorithms over various sampling and feature transformation scenarios. Our approach is evaluated on a diverse dataset composed of common threats on the internet. Overall, we find that discovered features can achieve more efficient and comparable detection performance to a priori features with 66% fewer features and can achieve a Matthews Correlation Coefficient (MCC) of up to 0.9008.}
}
@incollection{2023551,
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP® Study Guide (Fourth Edition)},
publisher = {Syngress},
edition = {Fourth Edition},
pages = {551-595},
year = {2023},
isbn = {978-0-443-18734-6},
doi = {https://doi.org/10.1016/B978-0-443-18734-6.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443187346000106}
}