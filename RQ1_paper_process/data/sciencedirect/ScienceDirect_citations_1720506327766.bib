@article{JIANG2023100456,
title = {MoreThanSentiments: A text analysis package},
journal = {Software Impacts},
volume = {15},
pages = {100456},
year = {2023},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100456},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001403},
author = {Jinhang Jiang and Karthik Srinivasan},
keywords = {Text mining, Natural language processing, Information extraction, Text complexity measures, Business analytics, Accounting},
abstract = {Text mining on a large corpus of data has gained utility and popularity over recent years owing to advancements in information retrieval and machine learning methods. However, popular text mining software packages mainly focus on either sentiment analysis or semantic meaning extraction, requiring pretraining on a large corpus of text data. In comparison, MoreThanSentiments provides computation of newer text attribution measures, including boiler score, specificity, redundancy, and hard info, which have been proposed in accounting analytics literature. Our software package, available in Python, is flexible in terms of parameter setting and is adaptable to different applications. Through this package, we seek to simplify the process of deploying nontrivial information extraction techniques published in domain-specific text analysis research into domain-agnostic analytics applications.}
}
@article{SUN2021543,
title = {Reactions on Twitter towards Australia's proposed import restriction on nicotine vaping products: a thematic analysis},
journal = {Australian and New Zealand Journal of Public Health},
volume = {45},
number = {6},
pages = {543-545},
year = {2021},
issn = {1326-0200},
doi = {https://doi.org/10.1111/1753-6405.13143},
url = {https://www.sciencedirect.com/science/article/pii/S1326020023003710},
author = {Tianze Sun and Carmen C.W. Lim and Coral Gartner and Jason P. Connor and Wayne D. Hall and Janni Leung and Daniel Stjepanović and Gary C.K. Chan},
keywords = {e‐cigarette, vaping, nicotine, Australia, attitudes, regulation, policy},
abstract = {Objective
In June 2020, the Australian Government announced that personal importation of nicotine vaping products (NVP) would be prohibited, pending a 12‐month classification and regulation review by the Therapeutic Goods Administration. This brief report examines the themes of responses on Twitter to this announcement.
Methods
Simple random sampling was used to retrieve tweets containing keywords from 19 to 26 June 2020. Tweets were manually coded and descriptive statistics calculated for themes and policy position.
Results
The vast majority of the 1,168 tweets were anti‐policy. Themes included: criticism towards government (59.8%), activism against NVP restriction (38%), potential adverse consequences (30.8%) and support for NVP restriction (1.4%). Tweets that identified potential adverse consequences of NVP restriction cited: smoking relapse for individuals currently using NVPs (75.6%); the impact of policy enforcement (8.6%); illicit market (8.3%); panic buying (3.6%); difficulty obtaining prescriptions (2.8%); and impacts on NVP businesses (2.8%).
Conclusion
Tweets predominately objected to the policy announcement. Approximately three‐quarters of tweets that cited potential adverse consequences of the policy mentioned smoking relapse as their primary concern.
Implications for public health
User‐generated content on Twitter was primarily used to lobby against the proposed policy, which was subsequently amended.}
}
@article{CRUZ2022100194,
title = {Selecting and combining complementary feature representations and classifiers for hate speech detection},
journal = {Online Social Networks and Media},
volume = {28},
pages = {100194},
year = {2022},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2021.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2468696421000719},
author = {Rafael M.O. Cruz and Woshington V. {de Sousa} and George D.C. Cavalcanti},
keywords = {Hate speech, Text classification, Multiple classifiers system, Natural language processing, Machine learning},
abstract = {Hate speech is a major issue in social networks due to the high volume of data generated daily. Recent works demonstrate the usefulness of machine learning (ML) in dealing with the nuances required to distinguish between hateful posts from just sarcasm or offensive language. Many ML solutions for hate speech detection have been proposed by either changing how features are extracted from the text or the classification algorithm employed. However, most works consider only one type of feature extraction and classification algorithm. This work argues that a combination of multiple feature extraction techniques and different classification models is needed. We propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme. Source code, figures and dataset splits can be found in the GitHub repository: https://github.com/Menelau/Hate-Speech-MCS.}
}
@article{DOGAN2022106737,
title = {Towards a taxonomy of code review smells},
journal = {Information and Software Technology},
volume = {142},
pages = {106737},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106737},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001877},
author = {Emre Doğan and Eray Tüzün},
keywords = {Modern code review, Bad practices, Conformance checking, Code review smell, Process smell, Process debt},
abstract = {Context:
Code review is a crucial step of the software development life cycle in order to detect possible problems in source code before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the code review process, many companies and open source software (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective code review process. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are code review (CR) smells.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight OSS projects.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the OSS projects are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.}
}
@article{MALAVOLTA2021110969,
title = {Mining guidelines for architecting robotics software},
journal = {Journal of Systems and Software},
volume = {178},
pages = {110969},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110969},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000662},
author = {Ivano Malavolta and Grace A. Lewis and Bradley Schmerl and Patricia Lago and David Garlan},
keywords = {Software architecture, Robotics, ROS},
abstract = {Context:
The Robot Operating System (ROS) is the de-facto standard for robotics software. However, ROS-based systems are getting larger and more complex and could benefit from good software architecture practices.
Goal:
We aim at (i) unveiling the state-of-the-practice in terms of targeted quality attributes and architecture documentation in ROS-based systems, and (ii) providing empirically-grounded guidance to roboticists about how to properly architect ROS-based systems.
Methods:
We designed and conducted an observational study where we (i) built a dataset of 335 GitHub repositories containing real open-source ROS-based systems, and (ii) mined the repositories to extract and synthesize quantitative and qualitative findings about how roboticists are architecting ROS-based systems.
Results:
First, we extracted an empirically-grounded overview of the state of the practice for architecting and documenting ROS-based systems. Second, we synthesized a catalog of 47 architecting guidelines for ROS-based systems. Third, the extracted guidelines were validated by 119 roboticists working on real-world open-source ROS-based systems.
Conclusion:
Roboticists can use our architecting guidelines for applying good design principles to develop robots that meet quality requirements, and researchers can use our results as evidence-based indications about how real-world ROS systems are architected today, thus inspiring future research contributions.}
}
@article{SCHMUTZ2024301769,
title = {Forensic analysis of hook Android malware},
journal = {Forensic Science International: Digital Investigation},
volume = {49},
pages = {301769},
year = {2024},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301769},
url = {https://www.sciencedirect.com/science/article/pii/S266628172400088X},
author = {Dominic Schmutz and Robin Rapp and Benjamin Fehrensen},
keywords = {Android malware, Hook, MaaS, RAT, Accessibility permission},
abstract = {This publication presents a thorough forensic investigation of the banking malware known as Hook, shedding light on its intricate functionalities and providing valuable insights into the broader realm of banking malware. Given the persistent evolution of Android malware, particularly in the context of banking threats, this research explores the ongoing development of these malicious entities. In particular, it emphasizes the prevalent “malware as a service” (MaaS) model, which engenders a competitive environment where malware developers continually strive to enhance their capabilities. Consequently, this investigation serves as a vital benchmark for evaluating the current state of banking MaaS capabilities in July 2023, enabling researchers and practitioners to gauge the advancements and trends within the field.}
}
@article{SINGH2024100543,
title = {A comprehensive survey on DDoS attacks detection & mitigation in SDN-IoT network},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {8},
pages = {100543},
year = {2024},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2024.100543},
url = {https://www.sciencedirect.com/science/article/pii/S2772671124001256},
author = {Chandrapal Singh and Ankit Kumar Jain},
keywords = {Internet of Things, Software-defined networks, Distributed denial of service, Network security},
abstract = {The Internet of Things (IoT) has transformed our lives by introducing new services and enhancing productivity. However, the widespread adoption of IoT devices and communication units has posed challenges in network management. In response, there is a growing necessity to rethink and redesign IoT network control. Software-defined networking (SDN) has emerged as a promising solution, leveraging its programmability and centralized management capabilities. SDN can simplify network management, offer network abstraction, facilitate development, and efficiently handle the complexities of IoT networks. Despite these advantages, security concerns, particularly the threat of Distributed Denial of Service (DDoS) attacks, persist in the IoT landscape. This survey focuses on exploring the collaboration between SDN and IoT. It investigates various types of DDoS attacks and highlights different types of defense, detection, and mitigation methods employed to address DDoS threats in SDN-based IoT (SDN-IoT) networks.}
}
@article{KOUTSIANA2023100799,
title = {An analysis of discussions in collaborative knowledge engineering through the lens of Wikidata},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100799},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100799},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000288},
author = {Elisavet Koutsiana and Gabriel Maia Rocha Amaral and Neal Reeves and Albert Meroño-Peñuela and Elena Simperl},
keywords = {Collaborative knowledge engineering, Knowledge graph, Discussion analysis, Wikidata},
abstract = {We study discussions in Wikidata, the world’s largest open-source collaborative knowledge graph (KG). This is important because it helps KG community managers understand how discussions are used and inform the design of collaborative practices and support tools. We follow a mixed-methods approach with descriptive statistics, thematic analysis, and statistical tests to investigate how much discussions in Wikidata are used, what they are used for, and how they support knowledge engineering (KE) activities. The study covers three core sources of discussion, the talk pages that accompany Wikidata items and properties, and a general-purpose communication page. Our findings show low use of discussion capabilities and a power-law distribution similar to other KE projects such as Schema.org. When discussions are used, they are mostly about KE activities, including activities that span across the entire KE lifecycle from conceptualisation and implementation to maintenance and taxonomy building. We hope that the findings will help Wikidata devise improved practices and capabilities to encourage the use of discussions as a tool to collaborate, improve editor engagement, and engineer better KGs.}
}
@article{NUZZI2021102085,
title = {MEGURU: a gesture-based robot program builder for Meta-Collaborative workstations},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {68},
pages = {102085},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102085},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302957},
author = {Cristina Nuzzi and Simone Pasinetti and Roberto Pagani and Stefano Ghidini and Manuel Beschi and Gabriele Coffetti and Giovanna Sansoni},
keywords = {Meta-Collaborative Workstations, Smart Manufacturing, Human-Machine Interaction, Industry 4.0, Deep Learning},
abstract = {This paper presents the Meta-Collaborative Workstation concept and a gesture-based robot program builder software named MEGURU. The software is ROS-based, and it is publicly available on GitHub. A hand-gestures language has been developed to create a fast and easy to use communication method, where single-hand gestures are combined to create composed Commands, allowing the user to create a customized, powerful, and flexible Gestures Dictionary. Gestures are recognized using an R-FCN Object Detector model fine-tuned on a custom dataset developed for this work. The system has been tested in two experiments. The first one was aimed at evaluating the user experience of people of different age, sex, and professional background concerning the proposed communication method. The second one was aimed at comparing the traditional teach pendant programming method and MEGURU in the task of assembling a small Moka coffee maker. The results of both experiments highlight that MEGURU is a promising robot programming method, especially for non-expert users.}
}
@article{VIGNAU2021102143,
title = {The evolution of IoT Malwares, from 2008 to 2019: Survey, taxonomy, process simulator and perspectives},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {102143},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102143},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001053},
author = {Benjamin Vignau and Raphaël Khoury and Sylvain Hallé and Abdelwahab Hamou-Lhadj},
keywords = {IoT security, Evolution of IoT malware, IoT botnet, VPNFilter, IoTReaper, Hide’n Seek, Echobot},
abstract = {The past decade has seen a rapidly growing interest in IoT-connected devices. But as is usually the case with computer systems and networks, malicious individuals soon realized that these objects could be exploited for criminal purposes. The problem is particularly salient since the firmware used in many Internet connected devices was developed without taking into consideration the expertise and best security practices gained over the past several years by programmers in other areas. Consequently, multiple attacks on IoT devices took place over the last decade, culminating in the largest ever recorded DDoS attack, the Mirai botnet, which took advantage of weaknesses in the security of the IoT. In this survey, we seek to shed light on the evolution of the IoT malware. We compare the characteristic features of 28 of the most widespread IoT malware programs of the last decade and propose a novel methodology for classifying malware based on its behavioral features. Our study also highlights the common practice of feature reuse across multiple malware programs.}
}
@article{ROBINSON2024100087,
title = {Infrastructural justice for responsible software engineering,},
journal = {Journal of Responsible Technology},
volume = {19},
pages = {100087},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100087},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000131},
author = {Sarah Robinson and Jim Buckley and Luigina Ciolfi and Conor Linehan and Clare McInerney and Bashar Nuseibeh and John Twomey and Irum Rauf and John McCarthy},
keywords = {Responsible software engineering, Infrastructure, Social connection model of responsibility, Installed base, Deepfake technology},
abstract = {In recent years, we have seen many examples of software products unintentionally causing demonstrable harm. Many guidelines for ethical and responsible computing have been developed in response. Dominant approaches typically attribute liability and blame to individual companies or actors, rather than understanding how the working practices, norms, and cultural understandings in the software industry contribute to such outcomes. In this paper, we propose an understanding of responsibility that is infrastructural, relational, and cultural; thus, providing a foundation to better enable responsible software engineering into the future. Our approach draws on Young's (2006) social connection model of responsibility and Star and Ruhleder's (1994) concept of infrastructure. By bringing these theories together we introduce a concept called infrastructural injustice, which offers a new way for software engineers to consider their opportunities for responsible action with respect to society and the planet. We illustrate the utility of this approach by applying it to an Open-Source software communities’ development of Deepfake technology, to find key leverage points of responsibility that are relevant to both Deepfake technology and software engineering more broadly.}
}
@article{ACIEN2022108643,
title = {BeCAPTCHA-Mouse: Synthetic mouse trajectories and improved bot detection},
journal = {Pattern Recognition},
volume = {127},
pages = {108643},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108643},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001248},
author = {Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez},
keywords = {CAPTCHA, Bot detection, Behavior, Biometrics, Mouse, Neuromotor},
abstract = {We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions, and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36%, proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.}
}
@article{BLOCK2022434,
title = {Image-Bot: Generating Synthetic Object Detection Datasets for Small and Medium-Sized Manufacturing Companies},
journal = {Procedia CIRP},
volume = {107},
pages = {434-439},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002876},
author = {Lukas Block and Adrian Raiser and Lena Schön and Franziska Braun and Oliver Riedel},
keywords = {Image Recognition, SME, Object Detection, Synthetic Training Data, Chroma Keying, Low-Effort},
abstract = {Training datasets for image recognition are poorly available for small and medium-sized manufacturing companies, due to the specialized products they work with, and the disproportionate investment to generate their own ones. Thus, we investigate a new approach: The Image-Bot consists of a physical apparatus and a processing pipeline to generate training datasets from real-world objects easily. It takes pictures of the objects in front of a green screen and blends them with random backgrounds. The approach was tested with 23 objects and a YOLOv5 algorithm. It creates a state-of-the-art training dataset with about 2,000 images per object in under 45 min.}
}
@article{NGUYEN2023110966,
title = {TS-IDS: Traffic-aware self-supervised learning for IoT Network Intrusion Detection},
journal = {Knowledge-Based Systems},
volume = {279},
pages = {110966},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110966},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123007165},
author = {Hoang Nguyen and Rasha Kashef},
keywords = {Intrusion detection, Internet of Things, Graph neural networks, Artificial intelligence},
abstract = {With recent advances in the Internet of Things (IoT) technology, more people can have instant and easy access to the IoT network of vast and diverse interconnected devices (e.g., surveillance cameras, motion sensors, or smart watches). This trend leads to a significant increase in the frequency and complexity of cyber attacks in the IoT network. Further, these attacks inflict severe financial and privacy damages to individuals and evince the need to develop a more effective and robust network intrusion detection system (NIDS). Network Intrusion Detection (NID) aims to identify the attacks in the networked devices, which is an essential task to protect and maintain Cyber Security. Although recent Machine Learning-based methods have developed and provided more efficient non-human intervention solutions to this problem, these methods still have some unsolved issues. One of the main limitations of existing solutions is that most focus on extracting the features at the flow level independently and ignore their interactions in the network, which impacts the detection performance. To address this problem, in this paper, we propose a Traffic-aware Self-supervised learning for IoT Network Intrusion Detection System, namely TS-IDS, which aims to capture the flow relationships between the network entities. Our approach leverages both node and edge features for improved performance. Additionally, we incorporate auxiliary property-based self-supervised learning (SSL) to enhance the graph representation, even in the absence of labelled data. We conducted experiments on two real-world datasets, NF-ToN-IoT and NF-BoT-IoT. We compared the proposed model with state-of-the-art baseline models to demonstrate the potential of our proposed framework.}
}
@article{VIGLIANISI2020110647,
title = {A federated society of bots for smart contract testing},
journal = {Journal of Systems and Software},
volume = {168},
pages = {110647},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110647},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301163},
author = {Emanuele Viglianisi and Mariano Ceccato and Paolo Tonella},
keywords = {Software testing, Blockchain, Smart contracts},
abstract = {Smart contracts are a new type of software that allows its users to perform irreversible transactions on a distributed persistent data storage called the blockchain. The nature of such contracts and the technical details of the blockchain architecture give raise to new kinds of faults, which require specific test behaviours to be exposed. In this paper we present SoCRATES, a generic and extensible framework to test smart contracts running in a blockchain. The key properties of SoCRATES are: (1) it comprises bots that interact with the blockchain according to a set of composable behaviours; (2) it can instantiate a society of bots, which can trigger faults due to multi-user interactions that are impossible to expose with a single bot. Our experimental results show that SoCRATES can expose known faults and detect previously unknown faults in contracts currently published in the Ethereum blockchain. They also show that a society of bots is often more effective than a single bot in fault exposure.}
}
@article{HU2018108,
title = {User influence analysis for Github developer social networks},
journal = {Expert Systems with Applications},
volume = {108},
pages = {108-118},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418302793},
author = {Yan Hu and Shanshan Wang and Yizhi Ren and Kim-Kwang Raymond Choo},
keywords = {Distributed social coding, Developer social network, Github, Github social influence analysis, User influence, Following-Star-Fork-Activity},
abstract = {Github, one of the largest social coding platforms, offers software developers the opportunity to engage in social activities relating to software development and to store or share their codes/projects with the wider community using the repositories. Analysis of data representing the social interactions of Github users can reveal a number of interesting features. In this paper, we analyze the data to understand user social influence on the platform. Specifically, we propose a Following-Star-Fork-Activity based approach to measure user influence in the Github developer social network. We first preprocess the Github data, and construct the social network. Then, we analyze user influence in the social network, in terms of popularity, centrality, content value, contribution and activity. Finally, we analyze the correlation of different user influence measures, and use Borda Count to comprehensively quantify user influence and verify the results.}
}
@article{SINGH20228852,
title = {A survey on near-human conversational agents},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part A},
pages = {8852-8866},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821003001},
author = {Satwinder Singh and Himanshu Beniwal},
keywords = {Conversational agents, Chatbots, Personal assistants, Conversational AI, Natural Language Processing},
abstract = {Conversational AI intends for machine-human interactions to appear and feel more natural and inclined to communicate in a near-human context. Chatbots, also known as conversational agents, are typically divided into two use-cases: task-oriented bots and social friend-bots. Task-oriented bots are often used to do activities such as answering questions or solving basic queries. Furthermore, social-friend-bots are designed to communicate like humans, where the user can speak freely and the bot answers organically while maintaining the conversation’s ambience. This paper analyses recent works in the conversational AI domain examining the exclusive methodologies, existing frameworks or tools, evaluation metrics, and available datasets for building robust conversational agents. Finally, a mind-map encompassing all the stated elements and qualities of chatbots is created.}
}
@article{ACARALI20161,
title = {Survey of approaches and features for the identification of HTTP-based botnet traffic},
journal = {Journal of Network and Computer Applications},
volume = {76},
pages = {1-15},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516302363},
author = {Dilara Acarali and Muttukrishnan Rajarajan and Nikos Komninos and Ian Herwono},
keywords = {Bot, Botnet traffic, Network analysis, Feature analysis, Network-based detection},
abstract = {Botnet use is on the rise, with a growing number of botmasters now switching to the HTTP-based C&C infrastructure. This offers them more stealth by allowing them to blend in with benign web traffic. Several works have been carried out aimed at characterising or detecting HTTP-based bots, many of which use network communication features as identifiers of botnet behaviour. In this paper, we present a survey of these approaches and the network features they use in order to highlight how botnet traffic is currently differentiated from normal traffic. We classify papers by traffic types, and provide a breakdown of features by protocol. In doing so, we hope to highlight the relationships between features at the application, transport and network layers.}
}
@article{VO2022100042,
title = {Domain-specific NLP system to support learning path and curriculum design at tech universities},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100042},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2021.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X21000369},
author = {Nhi N.Y. Vo and Quang T. Vu and Nam H. Vu and Tu A. Vu and Bang D. Mach and Guandong Xu},
keywords = {Data science applications in education, Architectures for educational technology system, Teaching/learning strategies},
abstract = {The tech sector has been growing at a rapid speed, demanding a higher level of expertise from its labor force. New skills and programming languages are introduced and required by the industry every day, while the university courses are not updated adequately. Finding the high-demand skills and relevant courses to study has become essential to both students and faculty members at tech universities, which leads to a growing research interest in building an intelligence system to support decision making. Leveraging recent development in Natural Language Processing, we built an NLP-based course recommendation system specifically for the computer science (CS) and information technology (IT) fields. In particular, we built (1) a Named Entity Recognition (CSIT-NER) model to extract tech-related skills and entities, then used these skills to build (2) a personalized multi-level course recommendation system using a hybrid model (hybrid CSIT-CRS). Our CSIT-NER model, trained and fine-tuned on a large corpus of text extracted from StackOverflow and GitHub, can accurately extract the relevant skills and entities, outperforming state-of-the-art models across all evaluation metrics. Our hybrid CSIT-CRS can provide recommendations on multiple individualized levels of university courses, career paths with job listings, and industry-required with suitable online courses. The whole system received good ratings and feedback from users from our survey with 201 volunteers who are students and faculty members of tech universities in Australia and Vietnam. This research is beneficial to students, faculty members, universities in CS/IT higher education sector, and stakeholders in tech-related industries.}
}
@article{SAHAR2021110852,
title = {How are issue reports discussed in Gitter chat rooms?},
journal = {Journal of Systems and Software},
volume = {172},
pages = {110852},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110852},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302429},
author = {Hareem Sahar and Abram Hindle and Cor-Paul Bezemer},
keywords = {Developer discussions, Gitter, Issue reports},
abstract = {Informal communication channels like mailing lists, IRC and instant messaging play a vital role in open source software development by facilitating communication within geographically diverse project teams e.g., to discuss issue reports to facilitate the bug-fixing process. More recently, chat systems like Slack and Gitter have gained a lot of popularity and developers are rapidly adopting them. Gitter is a chat system that is specifically designed to address the needs of GitHub users. Gitter hosts project-based asynchronous chats which foster frequent project discussions among participants. Developer discussions contain a wealth of information such as the rationale behind decisions made during the evolution of a project. In this study, we explore 24 open source project chat rooms that are hosted on Gitter, containing a total of 3,133,106 messages and 14,096 issue references. We manually analyze the contents of chat room discussions around 457 issue reports. The results of our study show the prevalence of issue discussions on Gitter, and that the discussed issue reports have a longer resolution time than the issue reports that are never brought on Gitter.}
}
@article{ZHANG2020300926,
title = {IoT Botnet Forensics: A Comprehensive Digital Forensic Case Study on Mirai Botnet Servers},
journal = {Forensic Science International: Digital Investigation},
volume = {32},
pages = {300926},
year = {2020},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2020.300926},
url = {https://www.sciencedirect.com/science/article/pii/S2666281720300214},
author = {Xiaolu Zhang and Oren Upton and Nicole Lang Beebe and Kim-Kwang Raymond Choo},
keywords = {Mirai, IoT malware, Forensics, Botnet server},
abstract = {Internet of Things (IoT) bot malware is relatively new and not yet well understood forensically, despite its potential role in a broad range of malicious cyber activities. For example, it was abused to facilitate the distributed denial of service (DDoS) attack that took down a significant portion of the Internet on October 21, 2016, keeping millions of people from accessing over 1200 websites, including Twitter and NetFlix for nearly an entire day. The widespread adoption of an estimated 50 billion IoT devices, as well as the increasing interconnectivity of those devices to traditional networks, not to mention to one another with the advent of fifth generation (5G) networks, underscore the need for IoT botnet forensics. This study is the first published, comprehensive digital forensic case study on one of the most well known families of IoT bot malware - Mirai. Past research has largely studied the botnet architecture and analyzed the Mirai source code (and that of its variants) through traditional static and dynamic malware analysis means, but has not fully and forensically analyzed infected devices or Mirai network devices. In this paper, we set up a fully functioning Mirai botnet network architecture and conduct a comprehensive forensic analysis on the Mirai botnet server. We discuss forensic artifacts left on the attacker's terminal, command and control (CNC) server, database server, scan receiver and loader, as well as the network packets therefrom. We discuss how a forensic investigator might acquire some of these artifacts remotely, without direct physical access to the botnet server itself. This research provides findings tactically useful to forensic investigators, not only from the perspective of what data can be obtained (e.g., IP addresses of bot members), but also important information about which device they should target for acquisition and investigation to obtain the most investigatively useful information.}
}
@article{URMAN2023102477,
title = {How transparent are transparency reports? Comparative analysis of transparency reporting across online platforms},
journal = {Telecommunications Policy},
volume = {47},
number = {3},
pages = {102477},
year = {2023},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2022.102477},
url = {https://www.sciencedirect.com/science/article/pii/S0308596122001793},
author = {Aleksandra Urman and Mykola Makhortykh},
keywords = {Transparency reports, Transparency, Online platforms, Santa Clara Principles, Compliance, Comparative analysis, Regulation, Digital Services Act},
abstract = {Over the last decade, transparency reports have been adopted by most large information technology companies. These reports provide important information on the requests tech companies receive from state actors around the world and the ways they respond to these requests, including what content the companies remove from platforms they own. In theory, such reports shall make inner workings of companies more transparent, in particular with respect to their collaboration with state actors. They shall also allow users and external entities (e.g., researchers or watchdogs) to assess to what extent companies adhere to their own policies on user privacy and content moderation as well as to the principles formulated by global entities that advocate for the freedom of expression and privacy online such as the Global Network Initiative or Santa Clara Principles. However, whether the current state of transparency reports actually is conducive to meaningful transparency remains an open question. In this paper, we aim to address this through a critical comparative analysis of transparency reports using Santa Clara Principles 2.0 (SCP 2.0) as the main analytical framework. Specifically, we aim to make three contributions: first, we conduct a comparative analysis of the types of data disclosed by major tech companies and social media platforms in their transparency reports. The companies and platforms analyzed include Google (incl. YouTube), Microsoft (incl. its subsidiaries Github and LinkedIn), Apple, Meta (prev. Facebook), TikTok, Twitter, Snapchat, Pinterest, Reddit and Amazon (incl. subsidiary Twitch). Second, we evaluate to what degree the released information complies with SCP 2.0 and how it aligns with different purposes of transparency. Finally, we outline recommendations that could improve the level of transparency within the reports and beyond, and contextualize our recommendations with regard to the Digital Services Act (DSA) that received the final approval of the European Council in October 2022.}
}
@article{YU2016204,
title = {Reviewer recommendation for pull-requests in GitHub: What can we learn from code review and bug assignment?},
journal = {Information and Software Technology},
volume = {74},
pages = {204-218},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916000069},
author = {Yue Yu and Huaimin Wang and Gang Yin and Tao Wang},
keywords = {Pull-request, Reviewer recommendation, Social network analysis},
abstract = {Context: The pull-based model, widely used in distributed software development, offers an extremely low barrier to entry for potential contributors (anyone can submit of contributions to any project, through pull-requests). Meanwhile, the project’s core team must act as guardians of code quality, ensuring that pull-requests are carefully inspected before being merged into the main development line. However, with pull-requests becoming increasingly popular, the need for qualified reviewers also increases. GitHub facilitates this, by enabling the crowd-sourcing of pull-request reviews to a larger community of coders than just the project’s core team, as a part of their social coding philosophy. However, having access to more potential reviewers does not necessarily mean that it’s easier to find the right ones (the “needle in a haystack” problem). If left unsupervised, this process may result in communication overhead and delayed pull-request processing. Objective: This study aims to investigate whether and how previous approaches used in bug triaging and code review can be adapted to recommending reviewers for pull-requests, and how to improve the recommendation performance. Method: First, we extend three typical approaches used in bug triaging and code review for the new challenge of assigning reviewers to pull-requests. Second, we analyze social relations between contributors and reviewers, and propose a novel approach by mining each project’s comment networks (CNs). Finally, we combine the CNs with traditional approaches, and evaluate the effectiveness of all these methods on 84 GitHub projects through both quantitative and qualitative analysis. Results: We find that CN-based recommendation can achieve, by itself, similar performance as the traditional approaches. However, the mixed approaches can achieve significant improvements compared to using either of them independently. Conclusion: Our study confirms that traditional approaches to bug triaging and code review are feasible for pull-request reviewer recommendations on GitHub. Furthermore, their performance can be improved significantly by combining them with information extracted from prior social interactions between developers on GitHub. These results prompt for novel tools to support process automation in social coding platforms, that combine social (e.g., common interests among developers) and technical factors (e.g., developers’ expertise).}
}
@article{WANG2021102948,
title = {Neural networks based domain name generation},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102948},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102948},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001629},
author = {Zheng Wang and Yang Guo},
keywords = {Domain generation algorithm, Malicious domain name, Classification, Variational autoencoder, Deep learning},
abstract = {Domain generation algorithm (DGA) is used by botnets to build a stealthy command and control (C&C) communication channel between the C&C server and the bots. A DGA can periodically produce a large number of pseudo-random algorithmically generated domains (AGDs), a few of which direct the bots to the C&C server. AGD detection algorithms provide a lightweight, promising solution in response to the existing DGA techniques. In the constantly evolving attacker–defender game, attackers may seek more advanced DGA techniques to gain a better chance of evading detection by defenders. In this paper, we propose a new DGA, namely a neural networks-based domain name generation (NDG) architecture. NDG is based on a variational autoencoder (VAE), where the encoder and decoder networks use stacked gated convolutional neural networks (GCNNs) to learn the contextual structure hierarchically. NDG is experimentally validated using a set of state-of-the-art AGD detection algorithms. The existing DGAs of different classes following a DGA taxonomy are used to benchmark NDG. NDG shows the best overall anti-detection performance among all tested DGAs. We also demonstrate that NDG is effective in benchmarking AGD detection algorithms.}
}
@article{RIBEIRO2023103462,
title = {Detecting and mitigating DDoS attacks with moving target defense approach based on automated flow classification in SDN networks},
journal = {Computers & Security},
volume = {134},
pages = {103462},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103462},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823003723},
author = {Marcos Aurélio Ribeiro and Mauro Sergio {Pereira Fonseca} and Juliana {de Santi}},
keywords = {DDoS, Moving target defense, SDN, Machine learning, Cyber security},
abstract = {The Distributed Denial of Service (DDoS) coordinates synchronized attacks on systems on the Internet using a set of infected hosts (bots). Bots are programmed to attack a determined target by firing a lot of synchronized requests, causing slowness or unavailability of the service. This type of attack has recently grown in magnitude, diversity, and economic cost. Thus, this paper presents a DDoS detection and mitigation architecture based on Software Defined Networking (SDN). It considers the Moving Target Defense (MTD) approach, redirecting malicious floods to expendable low-capacity servers to protect the main server while discouraging the attacker. The redirecting decision is based on a sensor, that employs Machine Learning (ML) algorithms for flow classification. When malicious flows are detected, the sensor notifies the SDN controller to include them in the malicious hosts lists and to realize the redirection. The validation and evaluation of the proposed architecture are conducted by simulation. Results considering different classification models (probabilistic, linear model, neural networks, and trees) and attack types indicate that the proposed architecture is efficient in detecting and mitigating DDoS attacks in approximately 3 seconds.}
}
@article{ALOBAID2019100472,
title = {Automating ontology engineering support activities with OnToology},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100472},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300465},
author = {Ahmad Alobaid and Daniel Garijo and María Poveda-Villalón and Idafen Santana-Perez and Alba Fernández-Izquierdo and Oscar Corcho},
keywords = {Ontology engineering, Ontology evaluation, Ontology documentation, Ontology publication},
abstract = {Due to the increasing uptake of semantic technologies, ontologies are now part of a good number of information systems. As a result, software development teams that have to combine ontology engineering activities with software development practices are facing several challenges, since these two areas have evolved, in general, separately. In this paper we present OnToology, an approach to manage ontology engineering support activities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based application that builds on top of Git-based environments and integrates existing semantic web technologies. We have validated OnToology against a set of representative requirements for ontology development support activities in distributed environments, and report on a survey of the system to assess its usefulness and usability.}
}
@article{SPARKES202312,
title = {The war against AI web scraping},
journal = {New Scientist},
volume = {258},
number = {3438},
pages = {12},
year = {2023},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(23)00836-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407923008369},
author = {Matthew Sparkes},
abstract = {Elon Musk and Reddit are leading a new wave of objections to the long-accepted practice of scraping content from websites, discovers Matthew Sparkes}
}
@article{JOSHI2024124283,
title = {Saliency infused dialogue response generation: Improving task oriented text generation using feature attribution},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124283},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124283},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011497},
author = {Ratnesh Kumar Joshi and Arindam Chatterjee and Asif Ekbal},
keywords = {Natural language generation, Saliency, Dialogue Systems, End-to-End generation},
abstract = {Challenges persist in dialogue scenarios, particularly in multi-turn dialogues where response generation often disregards contextual information beyond the last user utterance, resulting in fluent yet inadequate responses. This paper addresses these issues by identifying and resolving common shortcomings in base model responses during response generation and proposes methods to enhance response quality in unannotated dialogue settings. Our approach involves augmenting information from multiple sources, including keywords, salient features, and knowledge graph triples. We compare the effectiveness of these methods against both the base model and human annotation, which includes dialogue acts and entities. Our findings demonstrate that appending extracted tokens significantly enhances response quality compared to annotated information. In task-oriented dialogue, models perform best when infused with saliency and knowledge graph triples, as shown in the MultiWOZ dataset. Conversely, focusing solely on saliency yields better results for open-domain dialogue, as demonstrated with the DailyDialog dataset. For contextual relevance, the information infusion could also approach the performance of the LLama2 model with only a tenth of the available parameters.}
}
@article{SAMPSON20188,
title = {Secret digital coin mining and trading is a threat to your business},
journal = {Computer Fraud & Security},
volume = {2018},
number = {4},
pages = {8-10},
year = {2018},
issn = {1361-3723},
doi = {https://doi.org/10.1016/S1361-3723(18)30032-0},
url = {https://www.sciencedirect.com/science/article/pii/S1361372318300320},
author = {Jesse Sampson},
abstract = {Digital coin software could be infecting your desktops and servers with malware, opening the doors to hackers. They could be after your customer lists, your passwords, your databases. Or they could be looking to turn your computers and devices into bots. Jesse Sampson of Ziften explains the nature of the threat and what to do about it. Bitcoin? Monero? Ethereum? It doesn't matter. Coin mining and trading activities by employees – or by hackers – is a huge security problem that every organisation needs to address.}
}
@article{KEIJZER2021100106,
title = {The strength of weak bots},
journal = {Online Social Networks and Media},
volume = {21},
pages = {100106},
year = {2021},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2020.100106},
url = {https://www.sciencedirect.com/science/article/pii/S2468696420300471},
author = {Marijn A. Keijzer and Michael Mäs},
keywords = {Social bots, Misinformation, Fake news, Social influence, Agent-based model},
abstract = {Some fear that social bots, automated accounts on online social networks, propagate falsehoods that can harm public opinion formation and democratic decision-making. Empirical research, however, resulted in puzzling findings. On the one hand, the content emitted by bots tends to spread very quickly in the networks. On the other hand, it turned out that bots’ ability to contact human users tends to be very limited. Here we analyze an agent-based model of social influence in networks explaining this inconsistency. We show that bots may be successful in spreading falsehoods not despite their limited direct impact on human users, but because of this limitation. Our model suggests that bots with limited direct impact on humans may be more and not less effective in spreading their views in the social network, because their direct contacts keep exerting influence on users that the bot does not reach directly. Highly active and well-connected bots, in contrast, may have a strong impact on their direct contacts, but these contacts grow too dissimilar from their network neighbors to further spread the bot’s content. To demonstrate this effect, we included bots in Axelrod’s seminal model of the dissemination of cultures and conducted simulation experiments demonstrating the strength of weak bots. A series of sensitivity analyses show that the finding is robust, in particular when the model is tailored to the context of online social networks. We discuss implications for future empirical research and developers of approaches to detect bots and misinformation.}
}
@article{ABBAS2021301224,
title = {Generic signature development for IoT Botnet families},
journal = {Forensic Science International: Digital Investigation},
volume = {38},
pages = {301224},
year = {2021},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2021.301224},
url = {https://www.sciencedirect.com/science/article/pii/S2666281721001323},
author = {Syed Ghazanfar Abbas and Fabiha Hashmat and Ghalib A. Shah and Kashif Zafar},
keywords = {Mirai, Qbot, Generic IoT botnet detection, IoT botnet},
abstract = {As the source code of various IoT botnet families including Mirai has been made publicly available, the adversaries are drastically introducing new variants of these IoT Botnet families. However, there is a lack of generic mechanism for the detection of these emerging variants. As a consequence, it is infeasible for security solution providers to effectively identify new variants of IoT botnets. In this paper, we have done static code analysis of 17 IoT botnet variants of family Mirai and Qbot in order to dig out the attacker's perspective, generic behavior, employed technologies and implemented techniques. With the help of this analysis, we have identified generic behavioral patterns of IoT botnets and have developed generic signatures for the identification of IoT botnets. These signatures includes identification on the basis of CPU architectures, Bot control commands, Bot scanning commands, obfuscation methods, botnet specific exploits and attacks. A comparative analysis of analyzed IoT-Botnet families has been presented. For the evaluation of identified signatures, we first tested them on unknown Mirai and Qbot variants and gained a detection rate of 100% for both the variants. Secondly, we tested those signatures on other IoT-Botnet families: IRC-Bot, Perl ShellBot, Trick-Bot and gained a detection rate of 98%, 96.79% and 98.2% respectively. Further, we have presented open research challenges in the field of IoT-Botnet detection. This research will enhance IoT botnets understanding and pave the way for generic detection and prevention methods of IoT botnets.}
}
@article{UNNISA2022104503,
title = {Security provision for protecting intelligent sensors and zero touch devices by using blockchain method for the smart cities},
journal = {Microprocessors and Microsystems},
volume = {90},
pages = {104503},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104503},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122000631},
author = {Khaleeq {Un Nisa} and Adi Alhudhaif and Kashif Naseer Qureshi and Hassan Jalil Hadi and Gwanggil Jeon},
keywords = {Authentication, Botnet, Ddos, Brute force, Port scanning, Corda virtual machine, Mirai, Manufacturer usage description, Owasp, Vulnerability},
abstract = {Internet of Things (IoT) networks has gained popularity due to their amazing and cost-effective services and one of the main areas in smart cities. The stability of these networks is based on stable and secure data transmission without any vulnerabilities present used devices. Distributed Denial of Services (DDoS) attacks have brought critical interruptions in IoT services and significantly damage the network. In DDoS attacks, attackers utilize botnets, with the capability of frequently exploiting the millions of IoT devices around the globe. After the source code of Mirai malware is loaded on GitHub, the threats are significantly increased. Manufacturer Usage Description (MUD) is an embedded software standard for IoT device makers to advertise device specifications, including the intended communication patterns when it connects to the network. Even though the MUD mechanism is promising exertion, still there is a need for evaluating its viability, recognize its limits, and upgrade its architecture to reduce shortcomings in its architecture as well as to increase its effectiveness. This standard neither identifies the vulnerability path before the creation of the MUD profile. Thus, it is possible to exploit an IoT device even after the MUD profile is issued to the device by manipulating the vulnerabilities in the device. By keeping in mind this situation, this paper discusses the limitations of MUD in detail and proposed a framework to identify the patch and default vulnerabilities by using blockchain method before the generation/creation of MUD profiles. The proposed framework can also mitigate open ports, DDoS attacks, and Brute force attacks. The experiment results show the identification, elimination, and sharing of vulnerability report with vendors and significantly minimized the risk of IoT device exploitation.}
}
@article{SULUN2021106455,
title = {RSTrace+: Reviewer suggestion using software artifact traceability graphs},
journal = {Information and Software Technology},
volume = {130},
pages = {106455},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106455},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300021},
author = {Emre Sülün and Eray Tüzün and Uğur Doğrusöz},
keywords = {Suggesting reviewers, Reviewer recommendation, Graph mining, Software traceability, Pull-request review, Modern code review},
abstract = {Context:
Various types of artifacts (requirements, source code, test cases, documents, etc.) are produced throughout the lifecycle of a software. These artifacts are connected with each other via traceability links that are stored in modern application lifecycle management repositories. Throughout the lifecycle of a software, various types of changes can arise in any one of these artifacts. It is important to review such changes to minimize their potential negative impacts. To make sure the review is conducted properly, the reviewer(s) should be chosen appropriately.
Objective:
We previously introduced a novel approach, named RSTrace, to automatically recommend reviewers that are best suited based on their familiarity with a given artifact. In this study, we introduce an advanced version of RSTrace, named RSTrace+ that accounts for recency information of traceability links including practical tool support for GitHub.
Methods:
In this study, we conducted a series of experiments on finding the appropriate code reviewer(s) using RSTrace+ and provided a comparison with the other code reviewer recommendation approaches.
Results:
We had initially tested RSTrace+ on an open source project (Qt 3D Studio) and achieved a top-3 accuracy of 0.89 with an MRR (mean reciprocal ranking) of 0.81. In a further empirical evaluation of 40 open source projects, we compared RSTrace+ with Naive-Bayes, RevFinder and Profile based approach, and observed higher accuracies on the average.
Conclusion:
We confirmed that the proposed reviewer recommendation approach yields promising top-k and MRR scores on the average compared to the existing reviewer recommendation approaches. Unlike other code reviewer recommendation approaches, RSTrace+ is not limited to recommending reviewers for source code artifacts and can potentially be used for recommending reviewers for other types of artifacts. Our approach can also visualize the affected artifacts and help the developer to make assessments of the potential impacts of change to the reviewed artifact.}
}
@article{ORTEGACANDEL2024109921,
title = {Generation of a dataset for DoW attack detection in serverless architectures},
journal = {Data in Brief},
volume = {52},
pages = {109921},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109921},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923009605},
author = {José Manuel {Ortega Candel} and Francisco José {Mora Gimeno} and Higinio {Mora Mora}},
keywords = {Functions as a service, Serverless, Denial of wallet, Botnet},
abstract = {Denial of Wallet (DoW) attacks refers to a type of cyberattack that aims to exploit and exhaust the financial resources of an organization by triggering excessive costs or charges within their cloud or serverless computing environment. These attacks are particularly relevant in the context of serverless architectures due to characteristics like pay-as-you-go model, auto-scaling, limited control and cost amplification. Serverless computing, often referred to as Function-as-a-Service (FaaS), is a cloud computing model that allows developers to build and run applications without the need to manage traditional server infrastructure. Serverless architectures have gained popularity in cloud computing due to their flexibility and ability to scale automatically based on demand. These architectures are based on executing functions without the need to manage the underlying infrastructure. However, the lack of realistic and representative datasets that simulate function invocations in serverless environments has been a challenge for research and development of solutions in this field. The aim is to create a dataset for simulating function invocations in serverless architectures, that is a valuable practice for ensuring the reliability, efficiency, and security of serverless applications. Furthermore, we propose a methodology for the generation of the dataset, which involves the generation of synthetic data from traffic generated on cloud platforms and the identification of the main characteristics of function invocations. These characteristics include SubmitTime, Invocation Delay, Response Delay, Function Duration, Active Functions at Request, Active Functions at Response. By generating this dataset, we expect to facilitate the detection of Denial of Wallet (DoW) attacks using machine learning techniques and neural networks. In this way, this dataset available in Mendeley data repository could provide other researchers and developers with a dataset to test and evaluate machine learning algorithms or use other techniques based on the detection of attacks and anomalies in serverless environments.}
}
@article{SCHLACHTER2022104258,
title = {Using Linked Building Data for managing temporary construction items},
journal = {Automation in Construction},
volume = {139},
pages = {104258},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104258},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522001315},
author = {Alexander Schlachter and Mads Holten Rasmussen and Jan Karlshøj},
keywords = {Linked Building Data, Temporary construction items, BIM},
abstract = {For decades, the construction industry has experienced poor productivity due to challenges such as increasing project complexity and a fragmented project environment. Even though some technological innovations around Building Information Modeling (BIM) might have the potential to overcome these challenges, data integration across disciplines, companies and software solutions is yet to be solved entirely. Trending advancements try to enrich existing BIM data using Linked Data technologies to semantically describe the building information and facilitate data integration. By that, project data from different data sources is made available in an accessible format, so project participants can use it for their planning efforts. In this paper we explore the use of Linked Building Data (LBD) on a specific use case to answer the question of how the planning of Temporary Construction Items (TCIs) can be improved by integrating data and automating the demand calculation. A literature review concludes that TCIs only experience little attention in the current planning of construction projects but have a critical impact on the outcome of a project. Thus, the objective of this paper is to develop standard ontologies to provide a semantically rich terminology of the data and to propose a framework for TCI consideration within a BIM based project delivery system. A prototype solution is developed, taking formwork as a TCI representative. The result is a process for automatically creating a TCI utilization plan that quantifies the precise time- and location-based on-site TCI demand by integrating data from BIM, Location-Based Scheduling (LBS) and TCI information. Based on the results of prototyping and findings from expert interviews, this research integrates the solution into the process of construction and finally proposes two implementation scenarios for the solution – one being based on the current industry situation and one exploring the future vision of a more integrated and decentralized project delivery in the construction industry.}
}
@article{BUCAIONI2024100526,
title = {Programming with ChatGPT: How far can we go?},
journal = {Machine Learning with Applications},
volume = {15},
pages = {100526},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2024.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666827024000021},
author = {Alessio Bucaioni and Hampus Ekedahl and Vilma Helander and Phuong T. Nguyen},
keywords = {ChatGPT, Large language models, Programming},
abstract = {Artificial intelligence (AI) has made remarkable strides, giving rise to the development of large language models such as ChatGPT. The chatbot has garnered significant attention from academia, industry, and the general public, marking the beginning of a new era in AI applications. This work explores how well ChatGPT can write source code. To this end, we performed a series of experiments to assess the extent to which ChatGPT is capable of solving general programming problems. Our objective is to assess ChatGPT’s capabilities in two different programming languages, namely C++ and Java, by providing it with a set of programming problem, encompassing various types and difficulty levels. We focus on evaluating ChatGPT’s performance in terms of code correctness, run-time efficiency, and memory usage. The experimental results show that, while ChatGPT is good at solving easy and medium programming problems written in C++ and Java, it encounters some difficulties with more complicated tasks in the two languages. Compared to code written by humans, the one generated by ChatGPT is of lower quality, with respect to runtime and memory usage.}
}
@article{AFFINITO2023103629,
title = {The evolution of Mirai botnet scans over a six-year period},
journal = {Journal of Information Security and Applications},
volume = {79},
pages = {103629},
year = {2023},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2023.103629},
url = {https://www.sciencedirect.com/science/article/pii/S2214212623002132},
author = {Antonia Affinito and Stefania Zinno and Giovanni Stanco and Alessio Botta and Giorgio Ventre},
keywords = {Mirai, Botnet, IoT malware, Command & control server, Mirai signature},
abstract = {The proliferation of Internet of Things devices has resulted in an increase in security vulnerabilities and network attacks. The Mirai botnet is a well-known example of a network used for malicious activities, detected for the first time by the white-hat research group in August 2016. Since then, Mirai initiated massive DDoS attacks by scanning for and exploiting vulnerabilities in network devices. In this paper, we investigate the evolution of the Mirai botnet over a six-year period, analyzing the TCP SYN packets using Mirai signature, i.e. with TCP sequence number equal to the destination IP address. Our analysis stands out as we extensively investigate the evolution of Mirai scans over a prolonged six-year period (2016–2022). Our findings reveal that the Mirai signature is still implemented by malicious actors today, in contrast with previous works. Moreover, we observe that the number of hijacked devices and TCP SYN packets involved in the scanning phase have increased over time. We also confirm that cybercriminals generally target Telnet port 23, followed by fewer requests on Telnet port 2323. Conversely, the number of probes on the SSH ports decreases over time, followed by a subsequent increase in 2022. Lastly, we identify several ports that had not been contacted until 2018 but have since received a large number of TCP SYN packets that verify the Mirai’s signature. These ports are linked with the emergence of new variants of the Mirai botnet.}
}
@article{SAIDANI2021106618,
title = {On the impact of Continuous Integration on refactoring practice: An exploratory study on TravisTorrent},
journal = {Information and Software Technology},
volume = {138},
pages = {106618},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106618},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000914},
author = {Islem Saidani and Ali Ouni and Mohamed Wiem Mkaouer and Fabio Palomba},
keywords = {Continuous integration, Refactoring, Exploratory study, Mining software repositories, Multiple Regression Analysis},
abstract = {Context:
The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption.
Objective:
We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied.
Method:
We collect a corpus of 99,545 commits and 89,926 refactoring operations extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA).
Results:
Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context.
Conclusion:
Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems.}
}
@article{HUANG202310893,
title = {StarCraft adversary-agent challenge for pursuit–evasion game},
journal = {Journal of the Franklin Institute},
volume = {360},
number = {15},
pages = {10893-10916},
year = {2023},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2023.08.032},
url = {https://www.sciencedirect.com/science/article/pii/S0016003223005203},
author = {Xun Huang},
abstract = {A reinforcement learning environment with adversary agents is proposed in this work for pursuit–evasion game in the presence of fog of war, which is of both scientific significance and practical importance in aerospace applications. One of the most popular learning environments, StarCraft, is adopted here and the associated mini-games are analyzed to identify its potential applications and limitations for training adversary agents. The key contribution includes the analysis of the best performance that an intelligent agent could be achieved by incorporating control and differential game theory into the specific reinforcement learning environment, and the development of a StarCraft adversary-agents challenge (SAAC) environment by extending the current StarCraft mini-games. The subsequent study showcases the use of this learning environment and the effectiveness of an adversary agent for evaders. Overall, along with rapidly-emerging reinforcement learning technologies, the proposed SAAC environment should benefit pursuit–evasion studies in particular and aerospace applications in general. Last but not least, the corresponding code is available at GitHub.}
}
@article{JARCZYK201832,
title = {Surgical teams on GitHub: Modeling performance of GitHub project development processes},
journal = {Information and Software Technology},
volume = {100},
pages = {32-46},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S095058491730304X},
author = {Oskar Jarczyk and Szymon Jaroszewicz and Adam Wierzbicki and Kamil Pawlak and Michal Jankowski-Lorek},
keywords = {Open source software (OSS), Development process performance, Issue closure rate, Work centralization, Issue workflow, Surgical team},
abstract = {Context: Better methods of evaluating process performance of OSS projects can benefit decision makers who consider adoption of OSS software in a company. This article studies the closure of issues (bugs and features) in GitHub projects, which is an important measure of OSS development process performance and quality of support that project users receive from the developer team. Objective: The goal of this article is a better understanding of the factors that affect issue closure rates in OSS projects. Methodology: The GHTorrent repository is used to select a large sample of mature, active OSS projects. Using survival analysis, we calculate short-term, and long-term issue closure rates. We formulate several hypotheses regarding the impact of OSS project and team characteristics, such as measures of work centralization, measures that reflect internal project workflows, and developer social networks measures on issue closure rates. Based on the proposed features and several control features, a model is built that can predict issue closure rate. The model allows to test our hypotheses. Results: We find that large teams that have many project members have lower issue closure rates than smaller teams. Similarly, increased work centralization increases issue closure rates. While desirable social network characteristics have a positive impact on the amount of commits in a project, they do not have significant influence on issue closure. Conclusion: Overall, findings from empirical analysis support the classic notion of Brook’s – the “surgical team” – in the context of OSS project development process performance on GitHub. The model of issue closure rates proposed in this article is a first step towards an improved understanding and prediction of this important measure of OSS development process performance.}
}
@article{LONGO202364,
title = {A framework for cognitive chatbots based on abductive–deductive inference},
journal = {Cognitive Systems Research},
volume = {81},
pages = {64-79},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000359},
author = {Carmelo Fabio Longo and Paolo Marco Riela and Daniele Francesco Santamaria and Corrado Santoro and Antonio Lieto},
keywords = {Chatbot, Question answering, Artificial intelligence, First-order logic, Cognitive architectures, Meta-reasoning},
abstract = {This paper presents a framework based on natural language processing and first-order logic aiming at instantiating cognitive chatbots. The proposed framework leverages two types of knowledge bases interacting with each other in a meta-reasoning process. The first one is devoted to the reactive interactions within the environment, while the second one to conceptual reasoning. The latter exploits a combination of axioms represented with rich semantics and abduction as pre-stage of deduction, dealing also with some of the state-of-the-art issues in the natural language ontology domain. As a case study, a Telegram chatbot system has been implemented, supported by a module which automatically transforms polar and wh-questions into one or more likely assertions, so as to infer Boolean values or snippets with variable length as factoid answer. The conceptual knowledge base is organized in two layers, representing both long- and short-term memory. The knowledge transition between the two layers is achieved by leveraging both a greedy algorithm and the engine’s features of a NoSQL database, with promising timing performance if compared with the adoption of a single layer. Furthermore, the implemented chatbot only requires the knowledge base in natural language sentences, avoiding any script updates or code refactoring when new knowledge has to income. The framework has been also evaluated as cognitive system by taking into account the state-of-the art criteria: the results show that AD-Caspar is an interesting starting point for the design of psychologically inspired cognitive systems, endowed of functional features and integrating different types of perception.}
}
@article{LING2022109873,
title = {Hard-style Selective Context Utilization for dialogue generation based on what user just said},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109873},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109873},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009662},
author = {Yanxiang Ling and Zheng Liang and Tianqi Wang and Fei Cai and Honghui Chen},
keywords = {Context utilization, Dialogue generation, Chit-chat-bots, Co-reference},
abstract = {Dialogue is a process of information exchanging, where global background is stable while local focuses are transiting. Thus, at the ongoing dialogue turn, there are both relevant and irrelevant semantics existing in dialogue contexts. How to filter out noises and selectively utilize context can pave the way to successful dialogue generation. Current work on dialogue context utilization either processes contexts as vanilla monologue text ignoring dynamic conversation flows, or depends on weighted strategies to fuse all contexts where irrelevant utterances cannot be filter out even may overwhelm relevant ones. To deal with this, this paper proposes a Hard-style Selective Context Utilization method (HardSCU). We first define and measure the information density of the last utterance (query) of a dialogue, marking it as “strong” or “weak”. For a dialogue with strong query, HardSCU directly inputs the query into a RNN-based or T5-based encoder–decoder framework to generate a response; for a dialogue with weak query, HardSCU conducts a selective context utilization for dialogue generation, where a semantic interaction module introduces relevant semantics of context to enrich the query and the co-reference relations existing in dialogue are extracted to promote the learning process of response decoder. Extensive experiments on two benchmark conversation corpora verify that our HardSCU method can outperform competitive baselines on generating appropriate responses for chit-chat-bots with yielding strong robustness to the variations of dialogue lengths.}
}
@article{JIANG2021106394,
title = {Recommending tags for pull requests in GitHub},
journal = {Information and Software Technology},
volume = {129},
pages = {106394},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106394},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301580},
author = {Jing Jiang and Qiudi Wu and Jin Cao and Xin Xia and Li Zhang},
keywords = {Tag recommendation, Pull request, Open-source project, Github},
abstract = {Context
In GitHub, contributors make code changes, then create and submit pull requests to projects. Tags are a simple and effective way to attach additional information to pull requests and facilitate their organization. However, little effort has been devoted to study pull requests’ tags in GitHub.
Objective
Our objective in this paper is to propose an approach which automatically recommends tags for pull requests in GitHub.
Method
We make a survey on the usage of tags in pull requests. Survey results show that tags are useful for developers to track, search or classify pull requests. But some respondents think that it is difficult to choose right tags and keep consistency of tags. 60.61% of respondents think that a tag recommendation tool is useful. In order to help developers choose tags, we propose a method FNNRec which uses feed-forward neural network to analyze titles, description, file paths and contributors.
Results
We evaluate the effectiveness of FNNRec on 10 projects containing 68,497 tagged pull requests. The experimental results show that on average, FNNRec outperforms approach TagDeepRec and TagMulRec by 62.985% and 24.953% in terms of F1−score@3, respectively.
Conclusion
FNNRec is useful to find appropriate tags and improve tag setting process in GitHub.}
}
@article{SUN2022102470,
title = {ompTG: From OpenMP Programs to Task Graphs},
journal = {Journal of Systems Architecture},
volume = {126},
pages = {102470},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102470},
url = {https://www.sciencedirect.com/science/article/pii/S138376212200056X},
author = {Jinghao Sun and Tao Jin and Yekai Xue and Liwei Zhang and Jinrong Liu and Nan Guan and Quan Zhou},
keywords = {OpenMP, ompTG, Parallel task graph, Control flow analysis},
abstract = {Real-time systems are shifting them from single-core to multi-core processors. Software must be parallelized to fully utilize the computation power of multi-core architectures. OpenMP is a promising framework to develop parallel real-time software on multi-cores. OpenMP programs keep certain similarity to real-time task graph models, and this motivates much recent work done on real-time scheduling of OpenMP tasks. However, these studies conduct evaluations with randomly generated task graphs, which cannot well capture the structure features of realistic OpenMP programs. To fill the gap between theoretical real-time scheduling research and the OpenMP software reality, we develop an ompTG tool for transforming OpenMP programs into parallel task graphs. ompTG prepares a way to exhibit OpenMP such that the researchers in real-time community can easily understand: An OpenMP system consists of a set of tasks. There are interdependencies among tasks, and each task has an intra structure of the control-flow graph. Besides the topology of OpenMP tasks, we also provide a safe WCET for each vertex of OpenMP task graphs by using static WCET analysis techniques. Moreover, we derive the flow facts, e.g, infeasible path and loop bounds for the task graph, which is necessary information for real-time scheduling and analysis. As a case study, we collect 12 OpenMP programs from the BOTS benchmark, and transform them into task graphs, demonstrating the usage of ompTG.}
}
@article{YU2023102108,
title = {U-YOLOv7: A network for underwater organism detection},
journal = {Ecological Informatics},
volume = {75},
pages = {102108},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102108},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123001371},
author = {Guoyan Yu and Ruilin Cai and Jinping Su and Mingxin Hou and Ruoling Deng},
keywords = {Underwater organism detection, Deep learning, U-YOLOv7, Quantity estimation},
abstract = {Detecting and monitoring underwater organisms is very important for sea aquaculture. The human eye struggles to quickly distinguish between aquatic species due to their variety and dense dispersion. In this paper, a deep learning object detection algorithm based on YOLOv7 is used to design a new network, called Underwater-YOLOv7 (U-YOLOv7), for underwater organism detection. This model satisfies the requirements with regards to both speed and accuracy. First, a network combining CrossConv and an efficient squeeze-excitation module is created. This network increases the extraction of channel information while reducing parameters and enhancing the feature fusion of the network. Second, a lightweight Content-Aware ReAssembly of FEatures (CARAFE) operator is used to obtain more semantic information about underwater images before feature fusion. A 3D attention mechanism is incorporated to improve the anti-interference ability of the model in underwater recognition. Finally, a decoupling head using hybrid convolution is designed to accelerate convergence and improve the accuracy of underwater detection. The results show that the network proposed in this paper obtains an improvement of 3.2% in accuracy, 2.3% in recall, and 2.8% in the mean average precision value and runs at up to 179 fps, far outperforming other advanced networks. Moreover, it has a higher estimation accuracy than the YOLOv7 network.}
}
@article{PATSAKIS2020101614,
title = {Encrypted and covert DNS queries for botnets: Challenges and countermeasures},
journal = {Computers & Security},
volume = {88},
pages = {101614},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101614},
url = {https://www.sciencedirect.com/science/article/pii/S016740481831321X},
author = {Constantinos Patsakis and Fran Casino and Vasilios Katos},
keywords = {Malware, Botnets, Domain Generation Algorithm, DNS, Covert communication},
abstract = {There is a continuous increase in the sophistication that modern malware exercise in order to bypass the deployed security mechanisms. A typical approach to evade the identification and potential take down of a botnet command and control server is domain fluxing through the use of Domain Generation Algorithms (DGAs). These algorithms produce a vast amount of domain names that the infected device tries to communicate with to find the C&C server, yet only a small fragment of them is actually registered. This allows the botmaster to pivot the control and make the work of seizing the botnet control rather difficult. Current state of the art and practice considers that the DNS queries performed by a compromised device are transparent to the network administrator and therefore can be monitored, analysed, and blocked. In this work, we showcase that the latter is a strong assumption as malware could efficiently hide its DNS queries using covert and/or encrypted channels bypassing the detection mechanisms. To this end, we discuss possible mitigation measures based on traffic analysis to address the new challenges that arise from this approach.}
}
@article{BROCKBANK2024101654,
title = {Repeated rock, paper, scissors play reveals limits in adaptive sequential behavior},
journal = {Cognitive Psychology},
volume = {151},
pages = {101654},
year = {2024},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2024.101654},
url = {https://www.sciencedirect.com/science/article/pii/S0010028524000252},
author = {Erik Brockbank and Edward Vul},
keywords = {Adaptive reasoning, Adversarial reasoning, Opponent modeling, Rock–paper–scissors},
abstract = {How do people adapt to others in adversarial settings? Prior work has shown that people often violate rational models of adversarial decision-making in repeated interactions. In particular, in mixed strategy equilibrium (MSE) games, where optimal action selection entails choosing moves randomly, people often do not play randomly, but instead try to outwit their opponents. However, little is known about the adaptive reasoning that underlies these deviations from random behavior. Here, we examine strategic decision-making across repeated rounds of rock, paper, scissors, a well-known MSE game. In experiment 1, participants were paired with bot opponents that exhibited distinct stable move patterns, allowing us to identify the bounds of the complexity of opponent behavior that people can detect and adapt to. In experiment 2, bot opponents instead exploited stable patterns in the human participants’ moves, providing a symmetrical bound on the complexity of patterns people can revise in their own behavior. Across both experiments, people exhibited a robust and flexible attention to transition patterns from one move to the next, exploiting these patterns in opponents and modifying them strategically in their own moves. However, their adaptive reasoning showed strong limitations with respect to more sophisticated patterns. Together, results provide a precise and consistent account of the surprisingly limited scope of people’s adaptive decision-making in this setting.}
}
@article{CHAKRABORTY2024114235,
title = {Freedom of speech or freedom of reach? Strategies for mitigating malicious content in social networks},
journal = {Decision Support Systems},
volume = {182},
pages = {114235},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114235},
url = {https://www.sciencedirect.com/science/article/pii/S016792362400068X},
author = {Saurav Chakraborty and Sandeep Goyal and Annamina Rieder and Agnieszka Onuchowska and Donald J. Berndt},
keywords = {Malicious content, Content propagation, Network intervention, Feed management, Agent-based model, Preferential attachment, Cognitive load},
abstract = {Malicious content threatens the integrity and quality of content in social networks. Research and practice have experimented with network intervention strategies to curb malicious content propagation. These strategies lack efficiency, target malicious content propagators, and abridge freedom of speech. We draw upon the preferential attachment literature and cognitive load theory to employ the mechanisms of network formation, information sharing, and limited human cognitive capacities to propose an alternative feed management strategy—Preferentiality Dampened Feed Management. We compare and contrast this strategy against other established strategies using an agent-based model that utilizes empirical data from Twitter and findings from the prior literature. The results from our two experiments suggest that our proposed strategy is more effective in curbing malicious content propagation than other established strategies. Our work has important implications for the network interventions literature and practical implications for platform providers, social media users, and society.}
}
@article{SHEVTSOV2022100333,
title = {Explainable machine learning pipeline for Twitter bot detection during the 2020 US Presidential Elections},
journal = {Software Impacts},
volume = {13},
pages = {100333},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100333},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822000598},
author = {Alexander Shevtsov and Christos Tzagkarakis and Despoina Antonakaki and Sotiris Ioannidis},
keywords = {Machine learning, Twitter bot detection, Model explainability},
abstract = {This study introduces a novel, reproducible and reusable Twitter bot identification system. The system uses a machine learning (ML) pipeline, fed with hundreds of features extracted from a Twitter corpus. The main objective of the proposed ML pipeline is to train and validate different state-of-the-art machine learning models, where the eXtreme Gradient Boosting (XGBoost) model is selected since it achieves the highest detection performance. The Twitter dataset was collected during the 2020 US Presidential Elections, and additional experimental evaluation on distinct Twitter datasets demonstrates the superiority of our approach, in terms of high bot detection accuracy.}
}
@article{NGUYEN2023100851,
title = {Robust detection of unknown DoS/DDoS attacks in IoT networks using a hybrid learning model},
journal = {Internet of Things},
volume = {23},
pages = {100851},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100851},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001749},
author = {Xuan-Ha Nguyen and Kim-Hung Le},
keywords = {Unknown attack, Adversarial attack, Intrusion detection system, DoS/DDoS attack, Machine learning, Deep learning},
abstract = {The fourth industrial revolution is marked by the rapid growth of Internet of Things (IoT) technology, leading to an increase in the number of IoT devices. Unfortunately, this also makes these devices more susceptible to cyber threats, especially DoS/DDoS attacks. While supervised learning models have been adopted to detect and mitigate these threats, they have limitations in detecting unknown attacks that can cause severe consequences. This research aims to address those limitations and provide better protection for IoT networks against DoS/DDoS attacks. We propose a new approach that combines a soft-ordering convolutional neural network (SOCNN) model with local outlier factor (LOF) and isolation-based anomaly detection using nearest-neighbor ensembles (iNNE) models that use both supervised and unsupervised learning methods. We evaluated our approach on three benchmark datasets with varying unknown attack scenarios, and our hybrid model achieved high accuracy in detecting unknown attacks with an average F1-score of 98.94%, 91.68%, and 96.07%, respectively, on BoT-IoT, CIC-IDS-2017, and CIC-IDS-2018 datasets, outperforming state-of-the-art competitors. Our model also showed resilience against adversarial attacks such as the fast gradient sign method (FGSM) and Carlini Wagner (CW) adversarial attacks, highlighting the potential of our approach to enhance IoT network security against DoS/DDoS attacks in unknown attack scenarios.}
}
@article{RIBEIRO202151,
title = {Robotic Process Automation and Artificial Intelligence in Industry 4.0 – A Literature review},
journal = {Procedia Computer Science},
volume = {181},
pages = {51-58},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.104},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001393},
author = {Jorge Ribeiro and Rui Lima and Tiago Eckhardt and Sara Paiva},
keywords = {Robotic Process Automation, Artificial Intelligence, Industry 4.0},
abstract = {Taking into account the technological evolution of the last decades and the proliferation of information systems in society, today we see the vast majority of services provided by companies and institutions as digital services. Industry 4.0 is the fourth industrial revolution where technologies and automation are asserting themselves as major changes. Robotic Process Automation (RPA) has numerous advantages in terms of automating organizational and business processes. Allied to these advantages, the complementary use of Artificial Intelligence (AI) algorithms and techniques allows to improve the accuracy and execution of RPA processes in the extraction of information, in the recognition, classification, forecasting and optimization of processes. In this context, this paper aims to present a study of the RPA tools associated with AI that can contribute to the improvement of the organizational processes associated with Industry 4.0. It appears that the RPA tools enhance their functionality with the objectives of AI being extended with the use of Artificial Neural Network algorithms, Text Mining techniques and Natural Language Processing techniques for the extraction of information and consequent process of optimization and of forecasting scenarios in improving the operational and business processes of organizations.}
}
@article{JIANG201748,
title = {Who should comment on this pull request? Analyzing attributes for more accurate commenter recommendation in pull-based development},
journal = {Information and Software Technology},
volume = {84},
pages = {48-62},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S095058491630283X},
author = {Jing Jiang and Yun Yang and Jiahuan He and Xavier Blanc and Li Zhang},
keywords = {Commenter recommendation, Reviewer recommendation, Attribute selection, Pull-based software development},
abstract = {Context: The pull-based software development helps developers make contributions flexibly and efficiently. Commenters freely discuss code changes and provide suggestions. Core members make decision of pull requests. Both commenters and core members are reviewers in the evaluation of pull requests. Since some popular projects receive many pull requests, commenters may not notice new pull requests in time, and even ignore appropriate pull requests. Objective: Our objective in this paper is to analyze attributes that affect the precision and recall of commenter prediction, and choose appropriate attributes to build commenter recommendation approach. Method: We collect 19,543 pull requests, 206,664 comments and 4817 commenters from 8 popular projects in GitHub. We build approaches based on different attributes, including activeness, text similarity, file similarity and social relation. We also build composite approaches, including time-based text similarity, time-based file similarity and time-based social relation. The time-based social relation approach is the state-of-the-art approach proposed by Yu et al. Then we compare precision and recall of different approaches. Results: We find that for 8 projects, the activeness based approach achieves the top-3 precision of 0.276, 0.386, 0.389, 0.516, 0.322, 0.572, 0.428, 0.402, and achieves the top-3 recall of 0.475, 0.593, 0.613, 0.66, 0.644, 0.791, 0.714, 0.65, which outperforms approaches based on text similarity, file similarity or social relation by a substantial margin. Moreover, the activeness based approach achieves better precision and recall than composite approaches. In comparison with the state-of-the-art approach, the activeness based approach improves the top-3 precision by 178.788%, 30.41%, 25.08%, 41.76%, 49.07%, 32.71%, 25.15%, 78.67%, and improves the top-3 recall by 196.875%, 36.32%, 29.05%, 46.02%, 43.43%, 27.79%, 25.483%, 79.06% for 8 projects. Conclusion: The activeness is the most important attribute in the commenter prediction. The activeness based approach can be used to improve the commenter recommendation in code review.}
}
@article{ATRI2022107593,
title = {De-CAPTCHA: A novel DFS based approach to solve CAPTCHA schemes},
journal = {Computers & Electrical Engineering},
volume = {97},
pages = {107593},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107593},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005292},
author = {Aditya Atri and Ankita Bansal and Manju Khari and S. Vimal},
keywords = {CAPTCHA, Island Problem, Depth-first Search, Security, Convolutional Neural Network},
abstract = {CAPTCHA stands for Completely Automated Public Turing Test to Tell Computers and Humans Apart. CAPTCHAs are used as security mechanism in web applications to differentiate between real users and automated users, also known as bots. Text-based CAPTCHAs are the popularly used CAPTCHA schemes due to their simplicity and thus, they are still being used despite the proposal of several attack mechanisms. In this work, the authors have proposed a novel approach to solve CAPTCHA schemes. In this approach, the authors have used Depth First Search algorithm for the extraction of characters from CAPTCHAs and Convolutional Neural Network for recognizing these extracted characters. The proposed approach was validated on 3000+ CAPTCHA schemes and proved to be efficient by providing an average accuracy of more than 92.0% in detecting CAPTCHA schemes.}
}
@article{QUEZADA2023109725,
title = {Real-time bot infection detection system using DNS fingerprinting and machine-learning},
journal = {Computer Networks},
volume = {228},
pages = {109725},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109725},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623001706},
author = {Vicente Quezada and Fabian Astudillo-Salinas and Luis Tello-Oquendo and Paul Bernal},
keywords = {Botnet, Bot detection, DNS-based bot detection, Anomaly detection, ELK stack, Machine learning, Isolation forests, Random forests},
abstract = {In today’s cyberattacks, botnets are used as an advanced technique to generate sophisticated and coordinated attacks. Infected systems connect to a command and control (C&C) server to receive commands and attack. Thus, detecting infected hosts makes it possible to protect the network’s resources and prevent them from illicit activities toward third parties. This research elaborates on the design, implementation, and results of a bot infection detection system based on Domain Name System (DNS) traffic events for a network corporation. An infection detection feasibility analysis is performed by creating fingerprints. The traces are generated from a numerical analysis of 13 attributes. These attributes are obtained from the DNS logs of a DNS server. It looks for fingerprint anomalies using Isolation Forest to label a host as infected or not. In addition, on the traces cataloged as anomalous, a search will be carried out for queries to domains generated by Domain Generation Algorithms (DGA). Then, Random Forest generates a model that detects future bot infections on hosts. The devised system integrates the ELK stack and Python. This integration facilitates the management, transformation, and storage of events, generation of fingerprints, machine learning application, and analysis of fingerprint classification results with a precision greater than 99%.}
}
@article{BIAZOTTO2024107375,
title = {Technical debt management automation: State of the art and future perspectives},
journal = {Information and Software Technology},
volume = {167},
pages = {107375},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107375},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923002306},
author = {João Paulo Biazotto and Daniel Feitosa and Paris Avgeriou and Elisa Yumi Nakagawa},
keywords = {Systematic mapping study, Technical debt, Technical debt management, Tools, Automation},
abstract = {Context:
Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system’s maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most.
Objectives:
The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM.
Methods:
We conducted a systematic mapping study (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation.
Results:
We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges.
Conclusion:
The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts.}
}
@article{BAI2023120024,
title = {Automating discussion structure re-organization for GitHub issues},
journal = {Expert Systems with Applications},
volume = {225},
pages = {120024},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120024},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423005262},
author = {Shuotong Bai and Lei Liu and Chenkun Meng and Huaxiao Liu},
keywords = {Discussion re-organization, GitHub, Issue, Deep learning},
abstract = {As a popular social code hosting platform, GitHub encourages developers to discuss and leave opinions on issues. However, the linear format of GitHub issue discussions makes popular discussions difficult for developers to organize and extract useful information effectively. In this paper, we propose an issue discussion re-organization approach, aiming at converting an issue discussion with the linear structure into a discussion tree with key information. First, we conduct a motivational study to investigate the current situation of issue discussions in GitHub. Further, to re-organize discussion structures, we employ a Transformer-based model with transfer learning to predict the response relationship between comments for re-building structures and utilize TF–IDF to extract key information from the content with different topics. The experimental results show that our approach outperforms other baselines, and achieves an average improvement of 14.54% on metrics in the task of predicting response relationships, as well as getting an average improvement of 27.19% in terms of metrics of the re-organizing task. To investigate our re-organized results from actual perspectives, we also conduct a human evaluation. The results show that our approach can predict the accurate response relationships for 80.74% of comments from actual perspectives and 63% of topics extracted by our approach are highly rated. Moreover, 90.00% of newcomers from the open-source community approve of re-organized discussion structures.}
}
@article{HOLMES2018118,
title = {Cultivating Metanoia in Twitter Publics: Analyzing and Producing Bots of Protest in the #GamerGate Controversy},
journal = {Computers and Composition},
volume = {48},
pages = {118-138},
year = {2018},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S875546151730049X},
author = {Steve Holmes and Rachael Graham Lussos},
keywords = {kairos, metanoia, GamerGate, bots, social media},
abstract = {This article examines the unique rhetorical affordances of Twitter bots as a way to offer student writers the kairotic means of understanding how networked writing functions in social media public spheres. Specifically, this article discusses how to theorize and construct protest bots in the ad hoc and post hoc Twitter publics of the GamerGate controversy. In addition to kairos, we suggest that the supplementary concept of metanoia offers a highly relevant lens to understand Twitter bots' anonymity and persistence in relationship to the ways in which publics spheres adapt and change over time.}
}
@article{PARADIS2024111967,
title = {Analyzing the Tower of Babel with Kaiaulu},
journal = {Journal of Systems and Software},
volume = {210},
pages = {111967},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.111967},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000104},
author = {Carlos Paradis and Rick Kazman and Damian Tamburri},
keywords = {Socio-smells, Socio-technical smells, Mining-software-repositories, Gitlog, Mailing-list, Issue-tracker, Identity-matching, Networks, Tools},
abstract = {Context:
An extensive body of work has examined socio-technical activities in software development; however, the availability of tools to enable these studies is limited.
Aim:
We extend Kaiaulu, a software package for Mining Software Repositories to enable a broad spectrum analysis of Social Smells and Motifs.
Methods:
We perform a literature review to identify what tools are available which implement graph construction methods and social smell metrics, contextualizing the contributions of our tool.
Results:
The few tools identified in the literature either leverage fewer parts of the software ecosystem, have been archived, or depend on components no longer maintained.
Conclusion:
The socio-technical features in Kaiaulu complement existing tools and related literature, while providing a simple architecture to facilitate ease or use, and ease of learning, benefitting reproducibility.
Tool Repository:
github.com/sailuh/kaiaulu}
}
@article{ABDI2024101255,
title = {A test amplification bot for Pharo/Smalltalk},
journal = {Journal of Computer Languages},
volume = {78},
pages = {101255},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101255},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000655},
author = {Mehrdad Abdi and Henrique Rocha and Alexandre Bergel and Serge Demeyer},
keywords = {Test amplification, Mutation testing, Continuous integration, Crash recovery, Pharo smalltalk},
abstract = {Test amplification exploits the knowledge embedded in an existing test suite to strengthen it. A typical test amplification technique transforms the initial tests into additional test methods that increase the mutation coverage. Although past research demonstrated the benefits, additional steps need to be taken to incorporate test amplifiers in the everyday workflow of developers. This paper describes a proof-of-concept bot integrating Small-Amp with GitHub-Actions. The bot decides for itself which tests to amplify and does so within a limited time budget. To integrate the bot into the GitHub-Actions workflow, we incorporate three special-purpose features: (i) prioritization (to fit the process within a given time budget), (ii) sharding (to split lengthy tests into smaller chunks), and (iii) sandboxing (to make the amplifier crash-resilient). We evaluate our approach by installing the proof-of-concept extension of Small-Amp on five open-source projects deployed on GitHub. Our results show that a test amplification bot is feasible at a project level by integrating it into the build system. Moreover, we quantify the impact of prioritization, sharding, and sandboxing so that other test amplifiers may benefit from these special-purpose features. Our proof-of-concept demonstrates that the entry barrier for adopting test amplification can be significantly lowered.}
}
@article{TUAN2023109508,
title = {UTL_DGA22 - a dataset for DGA botnet detection and classification},
journal = {Computer Networks},
volume = {221},
pages = {109508},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109508},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005424},
author = {Tong Anh Tuan and Nguyen Viet Anh and Tran Thi Luong and Hoang Viet Long},
keywords = {DGA botnets datasets, DGA botnet detection, DGA botnet classification, Feature extraction, Machine learning},
abstract = {The DGA botnet prevention is a burning topic in cybersecurity, with two problems: detection and classification. The DGA botnet dataset plays an essential role in the research allowing researchers to evaluate their proposed solutions. This study introduces a new dataset on DGA botnets named UTL_DGA22. Our proposed dataset not only inherits previous datasets' results but also has got own advantages. First, our new dataset includes only domain records and no other raw network traffic, helping to address the DGA botnet problem. Second, we removed duplicated botnet DGA families and added new botnet families for a total of 76 DGA botnet families presented. Third, we propose a valuable set of attributes as input for classification algorithms. Our experiments using the proposed features with several machine learning algorithms have had good results. It shows that our proposed attributes are firmly suitable for the input of the DGA botnet solution. Finally, we carefully compiled the dataset and attribute description documents to make it easy for researchers to use. The UTL_DGA22 dataset can serve as a database for researchers to develop their algorithms while objectively evaluating different solutions.}
}
@article{MATUKUMALLI20214254,
title = {Augment reality chatbot using cloud},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {4254-4257},
year = {2021},
note = {International Conference on Materials, Manufacturing and Mechanical Engineering for Sustainable Developments-2020 (ICMSD 2020)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321020721},
author = {Viswanath Matukumalli and Sai {Naga Sasidhar Maddi} and Kushwanth {Krishna Angirekula} and Vivek {Reddy Pulicherla} and A.M. {Senthil kumar} and T. Maridurai and T. Sathish and D. Kasinathan},
keywords = {Augmented reality, Cloud, Chat bot, Human, API},
abstract = {A chat is a program that simulates the conversation with humans through text or voice commands. Now days, every company or organization is trying to reduce the manpower in many ways to gain some more profits and to increase the efficiency of the output. Chabot is one of the way to reduce the manpower and to reduce the human intervention. In early days websites and organizations use people to guide their clients and users to their respective outcome. But now days to reduce the manpower organizations are using the chatbots to interact their clients. In our paper, we want to create a chatbot using Augmented Reality and using cloud technologies. Our theme is to create a chatbot which interact with humans like there is another human talking to him and to increase interaction. This makes the organization to produce more accurate results for their clients and uses less human power.}
}
@article{BIERENBROODSPOT2024670,
title = {Phylogenomic insights into the first multicellular streptophyte},
journal = {Current Biology},
volume = {34},
number = {3},
pages = {670-681.e7},
year = {2024},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2023.12.070},
url = {https://www.sciencedirect.com/science/article/pii/S0960982223017700},
author = {Maaike J. Bierenbroodspot and Tatyana Darienko and Sophie {de Vries} and Janine M.R. Fürst-Jansen and Henrik Buschmann and Thomas Pröschold and Iker Irisarri and Jan {de Vries}},
keywords = {Charophyta, streptophyte algae, multicellularity, phylogenomics, plant terrestrialization, plant evolution, ancestral character state},
abstract = {Summary
Streptophytes are best known as the clade containing the teeming diversity of embryophytes (land plants).1,2,3,4 Next to embryophytes are however a range of freshwater and terrestrial algae that bear important information on the emergence of key traits of land plants. Among these, the Klebsormidiophyceae stand out. Thriving in diverse environments—from mundane (ubiquitous occurrence on tree barks and rocks) to extreme (from the Atacama Desert to the Antarctic)—Klebsormidiophyceae can exhibit filamentous body plans and display remarkable resilience as colonizers of terrestrial habitats.5,6 Currently, the lack of a robust phylogenetic framework for the Klebsormidiophyceae hampers our understanding of the evolutionary history of these key traits. Here, we conducted a phylogenomic analysis utilizing advanced models that can counteract systematic biases. We sequenced 24 new transcriptomes of Klebsormidiophyceae and combined them with 14 previously published genomic and transcriptomic datasets. Using an analysis built on 845 loci and sophisticated mixture models, we establish a phylogenomic framework, dividing the six distinct genera of Klebsormidiophyceae in a novel three-order system, with a deep divergence more than 830 million years ago. Our reconstructions of ancestral states suggest (1) an evolutionary history of multiple transitions between terrestrial-aquatic habitats, with stem Klebsormidiales having conquered land earlier than embryophytes, and (2) that the body plan of the last common ancestor of Klebsormidiophyceae was multicellular, with a high probability that it was filamentous whereas the sarcinoids and unicells in Klebsormidiophyceae are likely derived states. We provide evidence that the first multicellular streptophytes likely lived about a billion years ago.}
}
@article{CHEN201688,
title = {oTree—An open-source platform for laboratory, online, and field experiments},
journal = {Journal of Behavioral and Experimental Finance},
volume = {9},
pages = {88-97},
year = {2016},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2015.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214635016000101},
author = {Daniel L. Chen and Martin Schonger and Chris Wickens},
keywords = {Experimental economics, Software, Laboratory experiments, Field experiments, Online experiments, Classroom experiments},
abstract = {oTree is an open-source and online software for implementing interactive experiments in the laboratory, online, the field or combinations thereof. oTree does not require installation of software on subjects’ devices; it can run on any device that has a web browser, be that a desktop computer, a tablet or a smartphone. Deployment can be internet-based without a shared local network, or local-network-based even without internet access. For coding, Python is used, a popular, open-source programming language. www.oTree.org provides the source code, a library of standard game templates and demo games which can be played by anyone.}
}
@article{PHAM2022101771,
title = {Bot2Vec: A general approach of intra-community oriented representation learning for bot detection in different types of social networks},
journal = {Information Systems},
volume = {103},
pages = {101771},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101771},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000302},
author = {Phu Pham and Loan T.T. Nguyen and Bay Vo and Unil Yun},
keywords = {Network representation learning, Network embedding, Social bot detection, Random walk},
abstract = {Recently, due to the rapid growth of online social networks (OSNs) such as Facebook, Twitter, Weibo, etc. the number of machine accounts/social bots that mimic human users has increased. Along with the development of artificial intelligence (AI), social bots are designed to become smarter and more sophisticated in their efforts at replicating the normal behaviors of human accounts. Constructing reliable and effective bot detection mechanisms is this considered crucial to keep OSNs clean and safe for users. Despite the rapid development of social bot detection platforms, recent state-of-the-art systems still encounter challenges which are related to the model’s generalization (and whether it can be adaptable for multiple types of OSNs) as well as the great efforts needed for feature engineering. In this paper, we propose a novel approach of applying network representation learning (NRL) to bot/spammer detection, called Bot2Vec. Our proposed Bot2Vec model is designed to automatically preserve both local neighborhood relations and the intra-community structure of user nodes while learning the representation of given OSNs, without using any extra features based on the user’s profile. By applying the intra-community random walk strategy, Bot2Vec promises to achieve better user node embedding outputs than recent state-of-the-art network embedding baselines for bot detection tasks. Extensive experiments on two different types of real-word social networks (Twitter and Tagged) demonstrate the effectiveness of our proposed model. The source code for implementing the Bot2Vec model is available at: https://github.com/phamtheanhphu/bot2vec}
}
@article{MCGOWAN2024105306,
title = {Can natural language processing be effectively applied for audit data analysis in gynaecological oncology at a UK cancer centre?},
journal = {International Journal of Medical Informatics},
volume = {182},
pages = {105306},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105306},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623003246},
author = {Mark McGowan and Filipe {Correia Martins} and Jodi-Louise Keen and Amelia Whitehead and Ellie Davis and Pubudu Pathiraja and Helen Bolton and Peter Baldwin},
keywords = {Artificial intelligence, Natural language processing, Quality assurance audit, NHS, Gynaecological oncology},
abstract = {Background
The British Gynaecological Cancer Society (BGCS) has highlighted the disparity of ovarian cancer outcomes in the UK compared to other European countries. Therefore, cancer quality assurance audits and subspecialty training are important in improving the UK standard of care for these patients. The current workforce crisis afflicting the NHS creates difficulty in dedicating teams of clinicians to these audits. We present a single institution study to evaluate if NLP-generated code can improve the efficiency of ovarian cancer and subspeciality reaccreditations audits. We used the chat bot Google Bard to write Visual Basic Applications algorithms that utilise Excel files from electronic health records.
Methods
Primary ovarian cancer data from 2019 to 2022 was retrospectively collected from the Cambridge University Hospital electronic health records. The surgical subspecialty reaccreditation audit analysed the 2022 surgical database. A modular coding approach with Google Bard was applied to generate audit algorithms. The time to complete these current audits was compared against the 2016 ovarian cancer and 2020 subspeciality reaccreditation audits.
Results
The previous ovarian cancer audit conducted in 2016 required 3 clinicians for the 135 cases and data collection required 1800 min. Data analysis was completed in 300 min. The current ovarian cancer audit allocated 2 clinicians to the 600 surgical cases. Data collection was completed in 3120 min, 3360 min for code development and 720 min for testing. The 2020 subspecialty reaccreditation audit was completed in 360 min. The 2022 subspecialty reaccreditation audit was completed in 1680 min, with 960 min for code development, 240 for debugging and 480 min for testing.
Conclusion
We have demonstrated that NLP-generated code can significantly increase the efficiency of surgical quality assurance audits by eliminating the need for manual data analysis. With the current trajectory of NLP development, increasingly complex algorithms can be developed with minimal programming knowledge.}
}
@article{NGUYEN2024112059,
title = {GPTSniffer: A CodeBERT-based classifier to detect source code written by ChatGPT},
journal = {Journal of Systems and Software},
volume = {214},
pages = {112059},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112059},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001043},
author = {Phuong T. Nguyen and Juri {Di Rocco} and Claudio {Di Sipio} and Riccardo Rubei and Davide {Di Ruscio} and Massimiliano {Di Penta}},
keywords = {ChatGPT, Code classification, CodeBERT, Pre-trained Models},
abstract = {Since its launch in November 2022, ChatGPT has gained popularity among users, especially programmers who use it to solve development issues. However, while offering a practical solution to programming problems, ChatGPT should be used primarily as a supporting tool (e.g., in software education) rather than as a replacement for humans. Thus, detecting automatically generated source code by ChatGPT is necessary, and tools for identifying AI-generated content need to be adapted to work effectively with code. This paper presents GPTSniffer– a novel approach to the detection of source code written by AI – built on top of CodeBERT. We conducted an empirical study to investigate the feasibility of automated identification of AI-generated code, and the factors that influence this ability. The results show that GPTSniffer can accurately classify whether code is human-written or AI-generated, outperforming two baselines, GPTZero and OpenAI Text Classifier. Also, the study shows how similar training data or a classification context with paired snippets helps boost the prediction. We conclude that GPTSniffer can be leveraged in different contexts, e.g., in software engineering education, where teachers use the tool to detect cheating and plagiarism, or in development, where AI-generated code may require peculiar quality assurance activities.}
}
@article{ILG2023103737,
title = {A survey of contemporary open-source honeypots, frameworks, and tools},
journal = {Journal of Network and Computer Applications},
volume = {220},
pages = {103737},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103737},
url = {https://www.sciencedirect.com/science/article/pii/S108480452300156X},
author = {Niclas Ilg and Paul Duplys and Dominik Sisejkovic and Michael Menth},
keywords = {Honeypot, Honeypot framework, Cybersecurity, Threat intelligence},
abstract = {Automated attacks allow adversaries to exploit vulnerabilities in enterprise IT systems at short notice. To identify such attacks as well as new cybersecurity threats, defenders use honeypot systems; these monitored decoy resources mimic legitimate devices to entice adversaries. The domain of enterprise IT honeypots has been an active area of development and research, especially in the open-source community. In this work, we survey open-source honeypots, honeypot frameworks, and tools that help to develop or discover honeypot deployments. In contrast to existing surveys, our work provides a detailed discussion of the honeypots’ system architecture, software architecture, and cloud-native deployment options. In addition, we cover the most recent academic research in honeypot detection and evasion techniques, and discuss how these advances impact current open-source honeypots. This work helps the reader to make an educated choice when selecting a honeypot for deployment or further development.}
}
@article{SINGH201914,
title = {Detecting bot-infected machines using DNS fingerprinting},
journal = {Digital Investigation},
volume = {28},
pages = {14-33},
year = {2019},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S174228761830272X},
author = {Manmeet Singh and Maninder Singh and Sanmeet Kaur},
keywords = {Botnet detection, Bot detection, DNS fingerprinting, Machine learning, Anomaly detection},
abstract = {The never-ending menace of botnet is causing many serious problems on the Internet. Although there are significant efforts on detecting botnet at the global level which rely heavily on finding failed queries and domain flux information for botnet detection, there are very few efforts being made to detect bot infection at an enterprise level. Detecting bot-infected machines is vital for any organization in combating various security threats. This work proposes a novel anomaly-based detection technique which considers hourly hosts DNS fingerprint and attempts to find anomalous behavior which is quite different from normal machine behavior. This work successfully demonstrates the DNS Anomaly Detection (named BotDAD) technique for detecting bot-infected machine in a network using DNS fingerprinting.}
}
@article{WANG2021106408,
title = {Large-scale intent analysis for identifying large-review-effort code changes},
journal = {Information and Software Technology},
volume = {130},
pages = {106408},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106408},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300033},
author = {Song Wang and Chetan Bansal and Nachiappan Nagappan},
keywords = {Change intent analysis, Review effort, Machine learning},
abstract = {Context: Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. Change intents have been studied for years to help developers understand the rationale behind code commits. However, in most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis. Objective: In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes—changes with large review effort. Method: Specifically, we first propose a feedback-driven and heuristics-based approach to identify change intents of code changes. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on four large-scale projects, one from Microsoft and three are open source projects, i.e., Qt, Android, and OpenStack. Results: Our results show that, (i) code changes with some intents (i.e., Feature and Refactor) are more likely to be LRE changes, (ii) machine learning based prediction models are applicable for identifying LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. Conclusion: The change intent analysis and its application on LRE identification proposed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process. To show how to deploy our approaches in real-world practice, we report a case study of developing and deploying the intent analysis system in Microsoft. Moreover, we also evaluate the usefulness of our approaches by using a questionnaire survey. The feedback from developers demonstrate its practical value.}
}
@article{RAK2024109015,
title = {Effectiveness of an ensemble technique based on the distributivity equation in detecting suspicious network activity},
journal = {Fuzzy Sets and Systems},
volume = {488},
pages = {109015},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109015},
url = {https://www.sciencedirect.com/science/article/pii/S0165011424001611},
author = {Ewa Rak and Jaromir Sarzyński and Rafał Rak},
keywords = {Ensemble learning, Aggregation function, Distributivity equation, Machine learning, Classification measure, Cybersecurity},
abstract = {With the growing complexity and frequency of cyber threats, there is a pressing need for more effective defense mechanisms. Machine learning offers the potential to analyze vast amounts of data and identify patterns indicative of malicious activity, enabling faster and more accurate threat detection. Ensemble methods, by incorporating diverse models with varying vulnerabilities, can increase resilience against adversarial attacks. This study covers the usage and evaluation of the relevance of an innovative approach of ensemble classification for identifying intrusion threats on a large CICIDS2017 dataset. The approach is based on the distributivity equation that appropriately aggregates the underlying classifiers. It combines various standard supervised classification algorithms, including Multilayer Perceptron Network, k-Nearest Neighbors, and Naive Bayes, to create an ensemble. Experiments were conducted to evaluate the effectiveness of the proposed hybrid ensemble method. The performance of the ensemble approach was compared with individual classifiers using measures such as accuracy, precision, recall, F-score, and area under the ROC curve. Additionally, comparisons were made with widely used state-of-the-art ensemble models, including the soft voting method (Weighted Average Probabilities), Adaptive Boosting (AdaBoost), and Histogram-based Gradient Boosting Classification Tree (HGBC) and with existing methods in the literature using the same dataset, such as Deep Belief Networks (DBN), Deep Feature Learning via Graph (Deep GFL). Based on these experiments, it was found that some ensemble methods, such as AdaBoost and Histogram-based Gradient Classification Tree, do not perform reliably for the specific task of identifying network attacks. This highlights the importance of understanding the context and requirements of the data and problem domain. The results indicate that the proposed hybrid ensemble method outperforms traditional algorithms in terms of classification precision and accuracy, and offers insights for improving the effectiveness of intrusion detection systems.}
}
@article{PASTORGALINDO2020106047,
title = {Twitter social bots: The 2019 Spanish general election data},
journal = {Data in Brief},
volume = {32},
pages = {106047},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.106047},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920309410},
author = {Javier Pastor-Galindo and Mattia Zago and Pantaleone Nespoli and Sergio {López Bernal} and Alberto {Huertas Celdrán} and Manuel {Gil Pérez} and José A. Ruipérez-Valiente and Gregorio {Martínez Pérez} and Félix {Gómez Mármol}},
keywords = {Social bots detection, Social bots classification, Machine learning, Sentiment analysis, Social network analysis},
abstract = {The term social bots refer to software-controlled accounts that actively participate in the social platforms to influence public opinion toward desired directions. To this extent, this data descriptor presents a Twitter dataset collected from October 4th to November 11th, 2019, within the context of the Spanish general election. Starting from 46 hashtags, the collection contains almost eight hundred thousand users involved in political discussions, with a total of 5.8 million tweets. The proposed data descriptor is related to the research article available at [1]. Its main objectives are: i) to enable worldwide researchers to improve the data gathering, organization, and preprocessing phases; ii) to test machine-learning-powered proposals; and, finally, iii) to improve state-of-the-art solutions on social bots detection, analysis, and classification. Note that the data are anonymized to preserve the privacy of the users. Throughout our analysis, we enriched the collected data with meaningful features in addition to the ones provided by Twitter. In particular, the tweets collection presents the tweets’ topic mentions and keywords (in the form of political bag-of-words), and the sentiment score. The users’ collection includes one field indicating the likelihood of one account being a bot. Furthermore, for those accounts classified as bots, it also includes a score that indicates the affinity to a political party and the followers/followings list.}
}
@article{PEREZ2023108842,
title = {Dataset of open-source software developers labeled by their experience level in the project and their associated software metrics},
journal = {Data in Brief},
volume = {46},
pages = {108842},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108842},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922010459},
author = {Quentin Perez and Christelle Urtado and Sylvain Vauttier},
keywords = {Empirical software engineering, GitHub contributors, Software metrics, Experienced developers, Software architecture, Java, Spring, Maven},
abstract = {Developers are extracted from 17 open-source projects from GitHub. Projects are chosen that use the java programming language, the Spring framework and Maven/Gradle build tools. Along with these developers, 24 software engineering metrics are extracted for each of them. These metrics are either calculated by analyzing the source code or relative to project management metadata. Each of these developers then are manually searched for in professional social media such as LinkedIn or Twitter to be labeled with their experience level in their project. Outliers are statistically detected and manually re-assigned when needed. The resulting dataset contains 703 anonymized developers qualified by their 24 project-related software engineering metrics and labeled for their experience. It is suitable for empirical software engineering studies that need to connect developers’ level of experience to tangible software engineering metrics.}
}
@article{GANDEDKAR2024,
title = {Understanding nuances of scholarly publishing in orthodontics: A comprehensive guide},
journal = {Seminars in Orthodontics},
year = {2024},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2024.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1073874624000744},
author = {Narayan H. Gandedkar and Veerasathpurush Allareddy and Nikhilesh R. Vaiid},
keywords = {Education, Publication metrics, Dental education, Research grants, Research metrics},
abstract = {This comprehensive manuscript endeavors to furnish orthodontic researchers with the necessary tools and knowledge to adeptly navigate the multifaceted landscape of academic publishing, thereby enhancing the efficacy and reach of their scholarly endeavors. It meticulously imparts critical insights and methodologies for comprehending and leveraging publication metrics, such as citation counts, the h-index, and Journal Impact Factors, to strategically plan research trajectories. Furthermore, it offers guidance on adeptly engaging with evaluation agencies such as the American Dental Association (ADA) and the National Institute of Dental and Craniofacial Research (NIDCR), thereby optimizing alignment with grant opportunities. Through the adept utilization of orthodontic bibliometrics, researchers can gain invaluable insights into prevailing collaboration trends and emerging research domains, thus facilitating informed decision-making and prioritization of scholarly pursuits. Additionally, the manuscript delves into the nuanced optimization of publication guidelines to maximize research impact. Spanning both established domains such as biomechanics, anchorage control, and aligner therapy, as well as burgeoning frontiers including 3D printing and artificial intelligence applications in aligner treatment, this manuscript equips orthodontic researchers with the requisite acumen to embark upon a journey of impactful scholarly contributions, thereby catalyzing advancements in patient care within the discipline.}
}
@article{WATTANAKRIENGKRAI2022111117,
title = {GitHub repositories with links to academic papers: Public access, traceability, and evolution},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111117},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111117},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002144},
author = {Supatsara Wattanakriengkrai and Bodin Chinthanet and Hideaki Hata and Raula Gaikovina Kula and Christoph Treude and Jin Guo and Kenichi Matsumoto},
keywords = {Software documentation, Open science, Open access, Traceability},
abstract = {Traceability between published scientific breakthroughs and their implementation is essential, especially in the case of open-source scientific software which implements bleeding-edge science in its code. However, aligning the link between GitHub repositories and academic papers can prove difficult, and the current practice of establishing and maintaining such links remains unknown. This paper investigates the role of academic paper references contained in these repositories. We conduct a large-scale study of 20 thousand GitHub repositories that make references to academic papers. We use a mixed-methods approach to identify public access, traceability and evolutionary aspects of the links. Although referencing a paper is not typical, we find that a vast majority of referenced academic papers are public access. These repositories tend to be affiliated with academic communities. More than half of the papers do not link back to any repository. We find that academic papers from top-tier SE venues are not likely to reference a repository, but when they do, they usually link to a GitHub software repository. In a network of arXiv papers and referenced repositories, we find that the most referenced papers are (i) highly-cited in academia and (ii) are referenced by repositories written in different programming languages.}
}
@article{VIERHAUSER2023111733,
title = {GRuM — A flexible model-driven runtime monitoring framework and its application to automated aerial and ground vehicles},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111733},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111733},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001280},
author = {Michael Vierhauser and Antonio Garmendia and Marco Stadler and Manuel Wimmer and Jane Cleland-Huang},
keywords = {Cyber-physical systems, Runtime monitoring, Model-driven engineering},
abstract = {Runtime monitoring is critical for ensuring safe operation and for enabling self-adaptive behavior of Cyber-Physical Systems (CPS). Monitors are established by identifying runtime properties of interest, creating probes to instrument the system, and defining constraints to be checked at runtime. For many systems, implementing and setting up a monitoring platform can be tedious and time-consuming, as generic monitoring platforms do not adequately cover domain-specific monitoring requirements. This situation is exacerbated when the System under Monitoring (SuM) evolves, requiring changes in the monitoring platform. Most existing approaches lack support for the automated generation and setup of monitors for diverse technologies and do not provide adequate support for dealing with system evolution. In this paper, we present GRuM (Generating CPS Runtime Monitors), a framework that combines model-driven techniques and runtime monitoring, to automatically generate a customized monitoring platform for a given SuM. Relevant properties are captured in a Domain Model Fragment, and changes to the SuM can be easily accommodated by automatically regenerating the platform code. To demonstrate the feasibility and performance we evaluated GRuM against two different systems using TurtleBot robots and Unmanned Aerial Vehicles. Results show that GRuM facilitates the creation and evolution of a runtime monitoring platform with little effort and that the platform can handle a substantial amount of events and data.}
}
@article{SQUAR20221261,
title = {Content queries and in-depth analysis on version-controlled software},
journal = {Procedia Computer Science},
volume = {207},
pages = {1261-1270},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.182},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201064X},
author = {Jannek Squar and Niclas Schroeter and Anna Fuchs and Michael Kuhn and Thomas Ludwig},
keywords = {GitHub, OpenMP, Software Statistics, Data Mining, Information Search},
abstract = {Writing scientific code usually implies the need to coordinate and conflate the contributions of several scientific programmers. Using Git hosting services eases this process, because the hosting services offer many features, which assist in collaborated work on code. The well-established hosting service GitHub has seen continuous growth in terms of number of users, repositories and commits over the last few years; therefore it offers a large data source of scientific codes as well as social interaction of associated scientific programmers. We present a tool, which allows to easily search through relevant GitHub repositories and perform more advanced analyses, which cannot be conducted solely with the GitHub API. Our tool combines benefits from online as well as offline approaches to retrieve and analyse data to optimise time of execution and consumption of storage. We discuss possible use cases and demonstrate the tool's capabilities by investigating the popularity of OpenMP directives in the scientific community.}
}
@article{DANIEL2024103032,
title = {Applying model-driven engineering to the domain of chatbots: The Xatkit experience},
journal = {Science of Computer Programming},
volume = {232},
pages = {103032},
year = {2024},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2023.103032},
url = {https://www.sciencedirect.com/science/article/pii/S0167642323001144},
author = {Gwendal Daniel and Jordi Cabot},
keywords = {Chatbots, Commercial, Lessons learned, DSL},
abstract = {Chatbots are becoming a common component of many types of software systems. But they are typically developed as a side feature using ad-hoc tools and custom integrations. Moreover, current frameworks are efficient only when designing simple chatbot applications while they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs. In addition, the deployment of a chatbot application usually requires a deep understanding of the targeted platforms, especially back-end connections, increasing the development and maintenance costs. In this paper, we discuss our experiences building, evolving and distributing the Xatkit framework. Xatkit is a model-based framework built around a Domain-Specific Language to define chatbots (and voicebots and bots in general) in a platform-independent way. Xatkit also comes with a runtime engine that automatically deploys the chatbot application and manages the defined conversation logic over the platforms of choice. Xatkit has significantly evolved since its initial release. This paper focuses on describing the evolution and the reasons (technical and non-technical) that triggered them. We believe our lessons learned can be useful to any other initiative trying to build a successful industrial-level chatbot platform, and in general, any type of model-based solution.}
}
@article{WONG202440,
title = {A novel deep learning based cloud service system for automated acupuncture needle counting: a strategy to improve acupuncture safety},
journal = {Digital Chinese Medicine},
volume = {7},
number = {1},
pages = {40-46},
year = {2024},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2024.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2589377724000235},
author = {Tsz Ho Wong and Junyi Wei and Haiyong Chen and Bacon Fung Leung Ng},
keywords = {Artificial intelligence, Computer vision, Object detection, Acupuncture, Patient safety},
abstract = {Objective
The unintentional retention of needles in patients can lead to severe consequences. To enhance acupuncture safety, the study aimed to develop a deep learning-based cloud system for automated process of counting acupuncture needles.
Methods
This project adopted transfer learning from a pre-trained Oriented Region-based Convolutional Neural Network (Oriented R-CNN) model to develop a detection algorithm that can automatically count the number of acupuncture needles in a camera picture. A training set with 590 pictures and a validation set with 1 025 pictures were accumulated for fine-tuning. Then, we deployed the MMRotate toolbox in a Google Colab environment with a NVIDIA Tesla T4 Graphics processing unit (GPU) to carry out the training task. Furthermore, we integrated the model with a newly-developed Telegram bot interface to determine the accuracy, precision, and recall of the needling counting system. The end-to-end inference time was also recorded to determine the speed of our cloud service system.
Results
In a 20-needle scenario, our Oriented R-CNN detection model has achieved an accuracy of 96.49%, precision of 99.98%, and recall of 99.84%, with an average end-to-end inference time of 1.535 s
Conclusion
The speed, accuracy, and reliability advancements of this cloud service system innovation have demonstrated its potential of using object detection technique to improve acupuncture practice based on deep learning.}
}
@article{MYNENI2022108874,
title = {SmartDefense: A distributed deep defense against DDoS attacks with edge computing},
journal = {Computer Networks},
volume = {209},
pages = {108874},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108874},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622000792},
author = {Sowmya Myneni and Ankur Chowdhary and Dijiang Huang and Adel Alshamrani},
keywords = {Distributed Denial of Service (DDoS), Internet of Things (IoT), Botnets, Edge computing, Deep neural networks},
abstract = {The growing number of IoT edge devices have inflicted a change in the cyber-attack space. The DDoS attacks, in particular, have significantly increased in magnitude and intensity. Of the existing DDoS solutions, while the destination-based defense mechanisms incur high false positives due to the seemingly legitimate nature of the attack traffic, defense mechanisms implemented at the source alone do not suffice due to the lack of visibility into ongoing DDoS attacks. This paper proposes a distributed DDoS detection and mitigation framework, SmartDefense, based on edge computing approaches towards detecting and mitigating DDoS attacks at and near the source. By mitigating the DDoS attacks near the source, SmartDefense significantly reduces unnecessary bandwidth otherwise consumed by DDoS traffic going from residential edge networks to the ISP edge network. Furthermore, SmartDefense demonstrates how ISPs can detect botnet devices in their customer’s network by having smart edge devices pass attributes that are processed by the botnet detection engine at the provider’s edge. The evaluation of this work shows that SmartDefense can improve the detection and mitigation rate, with over 90% of DDoS traffic caught at the source and over 97.5% of remaining DDoS traffic caught at the provider’s edge. Our experiments also demonstrate how using a botnet detection engine can further reduce the DDoS traffic by up to 51.95% by facilitating ISPs to detect bot devices in their customers’ edge network.}
}
@article{MAHALAKSHMI2022100555,
title = {Adaptive ambulance monitoring system using IOT},
journal = {Measurement: Sensors},
volume = {24},
pages = {100555},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100555},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001891},
author = {S. Mahalakshmi and T. Ragunthar and N. Veena and S. Sumukha and Pranav R. Deshkulkarni},
abstract = {- With the increase in the number of automobiles in urban cities, the number of accidents has increased manifold. Hence, the need for ambulances is increasing at an alarming rate. In order to increase the survival rates of the patients, an efficient communication of ambulances with the hospital and routing of the ambulances at the signal posts is very essential. Hence, the proposed architecture is distributed in nature. The system not only provides effective communication between the ambulance and the hospital but also helps the ambulance send the signal to nearby traffic signal posts to open up so that the ambulance can easily pass through saving ample amounts of time. The signal posts use a camera to detect the incoming ambulance and open up that lane so that the ambulance need not spend much time waiting for the traffic to get cleared.}
}
@article{OZER2020113337,
title = {Discovering patterns of online popularity from time series},
journal = {Expert Systems with Applications},
volume = {151},
pages = {113337},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113337},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420301627},
author = {Mert Ozer and Anna Sapienza and Andrés Abeliuk and Goran Muric and Emilio Ferrara},
keywords = {Multidimensional time series, Shape-based clustering, Online popularity, Social media},
abstract = {How is popularity gained online? Is being successful strictly related to rapidly becoming viral in an online platform, or is it possible to acquire popularity in a steady and disciplined fashion? What are other temporal characteristics that can unveil the popularity of online content? To answer these questions, we leverage a multifaceted temporal analysis of the evolution of popular online content. We present dipm-SC: a multidimensional shape-based time-series clustering algorithm with a heuristic to find the optimal number of clusters. First, we validate the accuracy of our algorithm on synthetic datasets generated from benchmark time series models. Second, we show that dipm-SC can uncover meaningful clusters of popularity behaviors in real-world GitHub and Twitter datasets. By clustering the multidimensional time-series of the popularity of contents coupled with other domain-specific dimensions, we discover two main patterns of popularity: bursty and steady temporal behaviors. Furthermore, we find that the way popularity is gained over time has no significant impact on the final cumulative popularity.}
}
@article{JOOKEN2023106765,
title = {Mining Recency–Frequency–Monetary enriched insights into resources’ collaboration behavior from event data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106765},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106765},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623009491},
author = {Leen Jooken and Benoît Depaire and Mieke Jans},
keywords = {Event data behavioral analytics, Collaboration behavior, Mining resource behavior, Project mining, RFM, Social network analysis},
abstract = {Organizations increasingly rely on teamwork to achieve their goals. Therefore they continuously strive to improve their teams as their performance is interwoven with that of the organization. To implement beneficial changes, accurate insights into the working of the team are necessary. However, team leaders tend to have an understanding of the team’s collaboration that is subjective and seldom completely accurate. Recently there has been an increase in the adoption of digital support systems for collaborative work that capture objective data on how the work took place in reality. This creates the opportunity for data-driven extraction of insights into the collaboration behavior of a team. This data however, does not explicitly record the collaboration relationships, which many existing techniques expect as input. Therefore, these relationships first have to be discovered. Existing techniques that apply discovery are not generally applicable because their notion of collaboration is tailored to the application domain. Moreover, the information that these techniques extract from the data about the nature of the relationships is often limited to the network level. Therefore, this research proposes a generic algorithm that can discover collaboration relationships between resources from event data on any collaborative project. The algorithm adopts an established framework to provide insights into collaboration on a fine-grained level. To this end, three properties are calculated for both the resources and their collaboration relationships: a recency, frequency, and monetary value. The technique’s ability to provide valuable insights into the team structure and characteristics is empirically validated on two use cases.}
}
@article{TREUDE2018237,
title = {Unusual events in GitHub repositories},
journal = {Journal of Systems and Software},
volume = {142},
pages = {237-247},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300876},
author = {Christoph Treude and Larissa Leite and Maurício Aniche},
keywords = {Awareness, Unusual events, GitHub},
abstract = {In large and active software projects, it becomes impractical for a developer to stay aware of all project activity. While it might not be necessary to know about each commit or issue, it is arguably important to know about the ones that are unusual. To investigate this hypothesis, we identified unusual events in 200 GitHub projects using a comprehensive list of ways in which an artifact can be unusual and asked 140 developers responsible for or affected by these events to comment on the usefulness of the corresponding information. Based on 2,096 answers, we identify the subset of unusual events that developers consider particularly useful, including large code modifications and unusual amounts of reviewing activity, along with qualitative evidence on the reasons behind these answers. Our findings provide a means for reducing the amount of information that developers need to parse in order to stay up to date with development activity in their projects.}
}
@article{BREWER2019104729,
title = {Data and replication supplement for double auction markets with snipers},
journal = {Data in Brief},
volume = {27},
pages = {104729},
year = {2019},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.104729},
url = {https://www.sciencedirect.com/science/article/pii/S2352340919310844},
author = {Paul Brewer and Anmol Ratan},
keywords = {Markets, Double auction, Competitive equilibrium, Efficiency, Inequality, Numerical experiments, Simulations},
abstract = {We provide a dataset for our research article “Profitability, Efficiency and Inequality in Double Auction Markets with Snipers” [1]. This dataset [2] includes configuration files, raw output data, and replications of calculated metrics for our robot-populated market simulations. The raw data is subdivided into a hierarchy of folders corresponding to simulation treatment variables, in a 2 × 2 × 21 design for 84 treatments in total. Treatments variables include: (i) robot population ordering, either “primary” or “reverse”; (ii) two market schedules of agent's values and costs: equal-expected-profit “market 1” and unequal-expected-profit “market 2”; (iii) 21 robot populations identified by the number of Sniper Bots (0–20) on each side of the market. Each treatment directory contains a simulator input file and outputs for 10,000 periods of market data. The outputs include all acceptable buy and sell orders, all trades, profits for each agent, and market metrics such as efficiency-of-allocation, Gini coefficient, and price statistics. An additional public copy in Google Cloud is available for database query by users of Google BigQuery. The market simulator software is a private product created by Paul Brewer at Economic and Financial Technology Consulting LLC. Free open source modules are available for tech-savvy users at GitHub, NPM, and Docker Hub repositories and are sufficient to repeat the simulations. An easier-to-use paid market simulation product will eventually be available online from Econ1.Net. We provide instructions for repeating individual simulations using the free open source simulator and the free container tool Docker.}
}
@article{CUCCHIARELLI2021114551,
title = {Algorithmically generated malicious domain names detection based on n-grams features},
journal = {Expert Systems with Applications},
volume = {170},
pages = {114551},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114551},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420311957},
author = {Alessandro Cucchiarelli and Christian Morbidoni and Luca Spalazzi and Marco Baldi},
keywords = {Domain generation algorithm, Botnet, Machine learning, DNS query, Kullback-Leibner divergence, Jaccard Index},
abstract = {Botnets are one of the major cyber infections used in several criminal activities. In most botnets, a Domain Generation Algorithm (DGA) is used by bots to make DNS queries aimed at establishing the connection with the Command and Control (C&C) server. The identification of such queries by monitoring the network DNS traffic is then crucial for bot detection. In this paper we present a methodology to detect DGA generated domain names based on a supervised machine learning process, trained with a dataset of known benign and malicious domain names. The proposed approach represents the domain names through a set of features which express the similarity between the 2-grams and 3-grams in a single unclassified domain name and those in domain names known as malicious or benign. We used the Kullback-Leibner divergence and the Jaccard Index to estimate the similarity, and we tested different machine learning algorithms to classify each domain name as benign or DGA-based (with both binary and multi-class approach). The results of our experiments demonstrate that the proposed methodology, which only exploits lexical features of domain names, attains a good level of accuracy and results in a general model able to classify previously unseen domains in an effective way. It is also able to outperform some of the state-of-the-art featur eless classification methods based on deep learning.}
}
@article{CHEE2024103534,
title = {IoTSecSim: A framework for modelling and simulation of security in Internet of things},
journal = {Computers & Security},
volume = {136},
pages = {103534},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103534},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004443},
author = {Kok Onn Chee and Mengmeng Ge and Guangdong Bai and Dan Dongseong Kim},
keywords = {Graphical security modelling, Internet of things, Botnet attacks, Simulation, Security evaluation},
abstract = {The proliferation of the Internet of Things (IoT) devices has provided attackers with tremendous opportunities to launch various cyber-attacks. It has been challenging to analyse the impact of cyber-attacks and evaluate the effectiveness of defences in real IoT environments due to the scale and heterogeneity of IoT networks. In this work, we propose a novel simulation framework and a software tool, IoT Security Simulator (IoTSecSim). IoTSecSim is operated based on a framework we propose for modelling and simulating cyber-attacks and various defences in IoT networks. IoTSecSim is not only able to support the creation of an IoT network with flexible settings of IoT devices and topology information but also models the attack behaviours, node-level, and network-level defences. Moreover, a systematic security evaluation can be performed by comparing the results based on the calculation of security metrics. We perform simulations with case studies on Mirai malware and its variants to model cyber-attack behaviours on IoT networks and evaluate the impact of these attacks and the effectiveness of defence techniques via IoTSecSim. Then, we carry out a sensitivity analysis to justify that the simulation results produced by IoTSecSim are accurate and feasible when compared with related works. We also perform a comparative performance analysis with four combinations of cyber-attack behaviours and show that these behaviours can influence IoT malware propagation in different situations. We consider multiple attacker models and deploy conventional defence techniques (including firewall, intrusion detection, and vulnerability patching) to investigate the effectiveness of defence techniques. IoTSecSim provides a generalised and extensible simulation framework that enables users to model emerging cyber-attacks against IoT networks and evaluate the effectiveness of defences against these attacks. This helps users focus on the design and performance evaluation of new defences before the actual implementation and deployment of the defences are required.}
}
@article{DAI2024103718,
title = {DAmpADF: A framework for DNS amplification attack defense based on Bloom filters and NAmpKeeper},
journal = {Computers & Security},
volume = {139},
pages = {103718},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103718},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824000191},
author = {Yunwei Dai and Tao Huang and Shuo Wang},
keywords = {Network security, DDoS, Bloom filter, NAmpKeeper, DNS, Amplification attack},
abstract = {Domain Name System (DNS) amplification attacks exploit botnets and open recursive DNS servers to launch Distributed Denial of Service (DDoS) attacks. During an attack, the attacker leverages infected computers (bots) to perpetually send small spoofed DNS queries to numerous open recursive DNS servers. The servers, in turn, respond with lots of large DNS responses, which are reflected back to the victim. Such responses are usually several times larger than the original queries, and could exhaust the resources of CPUs, memory, and network bandwidth, rendering them unavailable for benign users. However, most in-network DDoS mitigation systems today inevitably cause normal DNS responses to be discarded while scrubbing traffic, as they do not distinguish between legitimate and malicious responses. To address this issue, some existing solutions employ Bloom filters to filter out unsolicited DNS responses, utilizing the “one-to-one mapping” relationship between DNS queries and responses. In this work, we present a framework called DAmpADF, designed to defend against DNS amplification attacks. The framework employs Bloom filters at the edge or core routers of Internet Service Providers (ISPs) or organizations to filter out malicious DNS responses. To reduce the false positives of the Bloom filters, we propose a novel data structure called the Non-amplifier Keeper (NAmpKeeper), which maintains the most frequently queried DNS servers that are not DNS amplifiers. By excluding queries to non-amplifiers from the Bloom filters, the false positives of Bloom filters are decreased significantly. Experimental results show that DAmpADF outperforms previous methods and achieves a superior filtration ratio of illegitimate DNS responses. Furthermore, the proposed approach incurs small, constant processing and memory overhead, enabling support for high line rates.}
}
@article{DECAN2020110573,
title = {GAP: Forecasting commit activity in git projects},
journal = {Journal of Systems and Software},
volume = {165},
pages = {110573},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110573},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300546},
author = {Alexandre Decan and Eleni Constantinou and Tom Mens and Henrique Rocha},
keywords = {Git, Commit activity, Developer abandonment, Distributed software development, Prediction model},
abstract = {Abandonment of active developers poses a significant risk for many open source software projects. This risk can be reduced by forecasting the future activity of contributors involved in such projects. Focusing on the commit activity of individuals involved in git repositories, this paper proposes a practicable probabilistic forecasting model based on the statistical technique of survival analysis. The model is empirically validated on a wide variety of projects accounting for 7528 git repositories and 5947 active contributors. We found that a model based on the last 20 observed days of commit activity per contributor provides the best concordance. We also found that the predictions provided by the model are generally close to actual observations, with slight underestimations for low probability predictions and slight overestimations for higher probability predictions. This model is implemented as part of an open source tool, called GAP, that predicts future commit activity.}
}
@article{GOLZADEH2021110911,
title = {A ground-truth dataset and classification model for detecting bots in GitHub issue and PR comments},
journal = {Journal of Systems and Software},
volume = {175},
pages = {110911},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110911},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100008X},
author = {Mehdi Golzadeh and Alexandre Decan and Damien Legay and Tom Mens},
keywords = {Distributed software development, Bot identification, GitHub repositories, Text similarity, Classification model},
abstract = {Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots.}
}
@article{BALDERAS2023e19517,
title = {Chatbot for communicating with university students in emergency situation},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19517},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19517},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023067257},
author = {Antonio Balderas and Roberto Fermín García-Mena and Milagros Huerta and Nestor Mora and Juan Manuel Dodero},
keywords = {Chatbot, Students, Emergency situation, COVID-19},
abstract = {Chatbots have arrived in higher education, and professors are trying to make the most of them. Typically, chatbots are used to help students learn academic subjects. In times of crisis, such as the COVID-19 pandemic, students who were not living with their families during the course, especially international students, were isolated and in critical situations. The student services offices were in constant contact with these students to solve problems, advise them and support them during their stay, within the constraints of confinement and the guidelines dictated by the country at the time. The student services offices were overwhelmed trying to help these students because, although the students' problems were very recurrent, the government guidelines changed from one day to the next. This article proposes the use of a chatbot to provide initial support to students during crisis situations, and facilitate communication between them and the university. The chatbot was tested by more than 160 students and student services staff. The findings support the use of chatbots as a potential tool to facilitate communication with students in emerging emergency situations, and encourage universities to adopt these types of smart tools to be prepared to respond quickly and efficiently to students in times of crisis.}
}
@article{BERHE2024618,
title = {Triage Software Update Impact via Release Notes Classification},
journal = {Procedia Computer Science},
volume = {238},
pages = {618-622},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.069},
url = {https://www.sciencedirect.com/science/article/pii/S187705092401305X},
author = {Solomon Berhe and Vanessa Kan and Omhier Khan and Nathan Pader and Ali Zain Farooqui and Marc Maynard and Foutse Khomh},
keywords = {Triage, Software, Update, Release Notes, Classifier, Evaluation},
abstract = {In the rapidly evolving domain of Industry 4.0, effective management of software updates is crucial for maintaining system continuity and security. This paper presents a novel machine learning-based approach for a prompt and effective triage of software updates, leveraging an evaluation of six release note classifiers to categorize updates by component type, release type, and security risk. Our methodology, tested on a dataset of 1,000 release notes commonly encountered in Industry 4.0 ecosystems, demonstrates Logistic Regression as the most accurate classifier. The findings not only highlight the practical applicability of our approach in real-world data but also set the foundation for future enhancements to streamline the machine learning triage process further.}
}
@article{SARACOGLU2022100249,
title = {Initialization of profile and social network analyses robot and platform with a concise systematic review},
journal = {Machine Learning with Applications},
volume = {7},
pages = {100249},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100249},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000019},
author = {Burak Omer Saracoglu},
keywords = {Decision Making Trial and Evaluation Laboratory, DEMATEL, Investment, Location, Profile analysis, Social network analysis},
abstract = {This paper presents profile and social network analyses on concise systematic review corpora. It suggests two new robots and platforms for profile and social network analyses, that will serve previously proposed data, expert, and event-driven robots and platforms for energy and power industry. The literature is collected and stored in three topic clusters “location”, “investment”, and “DEMATEL” to prepare corpora. Twenty-five publications are selected in each sample corpus. A sample dataset of each corpus is prepared for thirty-one features such as “author’s full name and surname”, “applied methods”, and “publisher”. Afterward, “authors network matrices” are prepared in spreadsheet software. Data input files (*.csv) are prepared for each dataset. Gephi 0.9.2 201709241107 (free open-source software) is used for social network analyses with built-in layout and statistics algorithms on a desktop Windows 10 Pro, Intel(R) Core(TM) i5 CPU 650 @ 3.20 GHz, 6,00 GB RAM personal computer in an offline and active cybersecurity software environment. Force Atlas, Force Atlas 2, Fruchterman–Reingold, OpenOrd, Yifan Hu, and Yifan Hu Proportional layout algorithms with Noverlap layout algorithm are run one by one. Runtimes range 2–120 s. All default statistic algorithms are run for several metrics like average degree, average weighted degree, betweenness centrality, closeness centrality, harmonic closeness centrality, eccentricity, and density. Authors in “location” cluster have a centralized network, but authors in “investment” and “DEMATEL” clusters have distributed networks. General profile analyses are conducted based on authors’ publications in the literature without any data and information on social media sites and platforms. Two new profile analysis metrics are proposed as “researcher’s past research focus index”, and “researcher’s future research focus prediction index”. Detailed profile analysis is performed for only Burak Omer Saracoglu. All analyses and findings are compared and summarized in the end.}
}
@article{LIAN2024102442,
title = {Public attitudes and sentiments toward ChatGPT in China: A text mining analysis based on social media},
journal = {Technology in Society},
volume = {76},
pages = {102442},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102442},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002476},
author = {Ying Lian and Huiting Tang and Mengting Xiang and Xuefan Dong},
keywords = {ChatGPT, Artificial intelligence, Social media, Public perception, Text mining, Online public opinion},
abstract = {ChatGPT, an innovative artificial intelligence language model, is attracted significant attention around the world, sparking both enthusiasm and controversy, but identifying its societal impact and addressing its potential concerns necessitate an understanding of the prevailing public's attitudes toward the tool. In this study, we leverage text mining techniques to analyze the sentiments and themes prevalent among Chinese social media discussions of ChatGPT. In total, 96,435 comment data and 55,186 repost data were used, and the results show that public discussions mainly focused on ChatGPT's technical support, AI-related effectiveness, impact on human work, and effects on education and technology. Concerns were related to disinformation risks, technological unemployment, and the human–computer relationship. In addition, we found that social media played a prominent role in information dissemination, while official media and government units demonstrated a limited influence. The insights obtained through this study can inform policymakers, industry stakeholders, and the public of the public's prevailing attitude toward AI technologies, and they can facilitate informed decision-making.}
}
@article{ISMAIL2021102380,
title = {A review of amplification-based distributed denial of service attacks and their mitigation},
journal = {Computers & Security},
volume = {109},
pages = {102380},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102380},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002042},
author = {Salih Ismail and Hani Ragab Hassen and Mike Just and Hind Zantout},
keywords = {Amplification attack, Reflection attack, DDoS},
abstract = {The rise of Distributed Denial of Service (DDoS) attacks have been steady in terms of the frequency and the impact of the attack. Traditionally, the attackers required control of a huge amount of resources to launch an attack. This has changed with the use of reflectors and amplifiers in DDoS attacks. A recent shift consisted of using other protocols than the traditional NTP and DNS protocols which were heavily used for ADDoS. In this paper, we review and organize amplification-based DDoS (ADDoS) attacks and associated countermeasures into a new taxonomy. Furthermore, we present a modus operandi of ADDoS attacks and analyze how it differs from traditional DDoS attacks. We also investigate how accessible ADDoS are for attackers with average resources. We survey readily available open-source scripts on GitHub and also the ADDoS features available in hire-to-DDoS platforms. We believe that accessibility and low-cost of hire-to-DDoS platforms are the major reasons for the increase of amplification-based DDoS attacks. Lastly, we provide a list of future directions that might be interesting for the community to focus on.}
}
@article{ESCALEIRA2023100916,
title = {Moving Target Defense for the cloud/edge Telco environments},
journal = {Internet of Things},
volume = {24},
pages = {100916},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100916},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002391},
author = {Pedro Escaleira and Vitor A. Cunha and Diogo Gomes and João P. Barraca and Rui L. Aguiar},
keywords = {Moving Target Defense (MTD), MTD as a Service (MTDaaS), Network Functions Virtualization (NFV), Cloud security, Security as a Service (SECaaS), Zero-day vulnerabilities},
abstract = {The Internet of Things (IoT) paradigm has been one of the main contributors, in recent years, to the growth in the number of connected equipment. This fact has predominantly contributed to IoT being constrained by the 5th Generation Mobile Network (5G) progress and the promises this technology brings. However, this can be a double-edged sword. On the one hand, it will benefit from those progresses, but on the other, it will also be impacted by any security risk associated with 5G. One of the more serious security problems associated with it is the new wave of virtualization and softwarization of networks and analogous appliances, brought to light by paradigms such as Network Functions Virtualization (NFV) and Multi-access Edge Computing (MEC). Considering these predicaments, we propose a state-of-the-art Moving Target Defense (MTD) approach that defends Cloud-based Network Functions (CNFs) launched within MEC and NFV environments. Furthermore, our mechanism follows the famous Everything as a Service (XaaS) ideology, allowing any CNF provider to use this protection system, working agonistically. In the end, we created a Proof of Concept (PoC) of our proposed methodology, which we then used to conduct an extensive practical security analysis against the multiple phases of the Intrusion Kill Chain. Our final results have proven that our MTD as a Service (MTDaaS) approach can effectively delay and, in some cases, stop an attacker from achieving its objectives when trying to attack a CNF, even if the related vulnerability is a zero-day.}
}
@article{KEBEDE2024248,
title = {A modular ontology modeling approach to developing digital product passports to promote circular economy in the built environment},
journal = {Sustainable Production and Consumption},
volume = {48},
pages = {248-268},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924001362},
author = {Rahel Kebede and Annika Moscati and He Tan and Peter Johansson},
keywords = {Circular economy, Digital product passports, Modular ontology, Ontology design pattern, Information requirements, Built environment},
abstract = {The significant impact of the built environment on resource consumption and waste production has led to calls for a shift towards a circular economy model that maximizes the efficient use of resources. This study explores the use of digital product passports (DPPs) to improve how we manage products throughout their lifecycle. However, dealing with the complexity and large volume of data in DPPs can be challenging in terms of effective information management and utilization. We address this issue by adopting a modular ontological approach to systematically capture product lifecycle information from its origin to its end-of-life phase. To ensure interoperability and reusability of the ontology, we annotate key concepts and relationships using International Organization for Standardization (ISO) standards that promote circular economy. Our research led to the development of several ontology modules derived from literature reviews and interviews conducted with industry and academia experts who specialize in sustainability. These modules were then integrated to create a digital product passport ontology. The study demonstrates the feasibility of using a modular ontology approach to manage the complex information inherent in DPPs paving the way for more sustainable management practices in the built environment sector.}
}
@article{CARUCCIO2024121186,
title = {Can ChatGPT provide intelligent diagnoses? A comparative study between predictive models and ChatGPT to define a new medical diagnostic bot},
journal = {Expert Systems with Applications},
volume = {235},
pages = {121186},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121186},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423016883},
author = {Loredana Caruccio and Stefano Cirillo and Giuseppe Polese and Giandomenico Solimando and Shanmugam Sundaramurthy and Genoveffa Tortora},
keywords = {Intelligent diagnosis, Traditional ML, ChatGPT, Google BARD, Comparative analysis},
abstract = {Intelligent diagnosis processes rely on Artificial Intelligence (AI) techniques to provide possible diagnoses by analyzing patient data and medical information. To make accurate and quick diagnoses, it is possible to use AI tools to efficiently analyze huge amounts of data and find patterns that a clinician might miss. In recent years, new large language models (LLMs), such as ChatGPT and Google BARD, have shown remarkable capabilities in several domains, including intelligent diagnostics. This research aims to compare the performances of ChatGPT and traditional machine learning models for making diagnoses of low- and medium- risk diseases only based on their symptoms. On the basis of our study, we defined four research questions: RQ1) What are the benefits and limitations of using ChatGPT in intelligent diagnosis? RQ2) How do traditional machine learning approaches compare to ChatGPT for intelligent diagnosis? RQ3) How does ChatGPT compare with other LLMs and domain-specific natural language processing models in the intelligent diagnosis tasks?, and RQ4) What are the implications of the predictive models and ChatGPT for healthcare, and how can they be used to support people?. To answer these RQs, we first evaluate the performances of different engines of ChatGPT, also introducing a new prompt engineering methodology specifically tailored for achieving accurate diagnostic outcomes. Moreover, we compare these results with those achieved by different predictive models trained for intelligent diagnosis tasks, i.e., Google BARD, and two domain-specific NLP models. Finally, we propose a new interactive bot available for users that relies on the best-performing models evaluated in the previous steps. The experiments have been conducted using two medical datasets for disease prediction consisting of more than 100 symptoms associated with several diagnoses.}
}
@article{ESKANDANI2023111589,
title = {The uphill journey of FaaS in the open-source community},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111589},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111589},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002655},
author = {Nafise Eskandani and Guido Salvaneschi},
keywords = {FaaS, Serverless, Cloud computing},
abstract = {Since its introduction in 2014 by Amazon, the Function as a Service (FaaS) model of serverless computing has set the expectation to fulfill the promise of on-demand, pay-as-you-go, infrastructure-independent processing, originally formulated by cloud computing. Yet, serverless applications are fundamentally different than traditional service-oriented software in that they pose specific performance (e.g., cold start), design (e.g., stateless), and development challenges (e.g., debugging). A growing number of cloud solutions have been continuously attempting to address each of these challenges as a result of the increasing popularity of FaaS. Yet, the characteristics of this model have been poorly understood; therefore, the challenges are poorly tackled. In this paper, we assess the state of FaaS in open-source community with a study on almost 2K real-world serverless applications. Our results show a jeopardized ecosystem, where, despite the hype of serverless solutions in the last years, a number of challenges remain untackled, especially concerning component reuse, support for software development, and flexibility among different platforms — resulting in arguably slow adoption of the FaaS model. We believe that addressing the issues discussed in this paper may help researchers shaping the next generation of cloud computing models.}
}
@article{ZHANG2023103025,
title = {A secure annuli CAPTCHA system},
journal = {Computers & Security},
volume = {125},
pages = {103025},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.103025},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822004175},
author = {Jie Zhang and Min-Yen Tsai and Kotcharat Kitchat and Min-Te Sun and Kazuya Sakai and Wei-Shinn Ku and Thattapon Surasak and Tipajin Thaipisutikul},
keywords = {CAPTCHA, Annuli, Deep learning, Hough transform, Indistinguishable region},
abstract = {Many websites and applications rely on CAPTCHA for protection from bot attacks. Otherwise, users and businesses will be exposed to risks. Although several different CAPTCHA systems have been proposed, the development of deep learning algorithms allows attackers to create more efficient and accurate attack methods. Many studies have shown that existing CAPTCHA systems are no longer safe, especially text-based CAPTCHA. To resolve this issue, a simple, secure, and effective annuli CAPTCHA system is proposed in this paper. In the proposed system, the annuli CAPTCHA image containing the overlapping of circles and ovals is randomly generated. The user wishing to gain access to the system is required to answer correctly the total number of circles and ovals in the image to prove that he/she is not a bot. The security of our proposed CAPTCHA system is verified by three attack methods. Additionally, the usability survey of our CAPTCHA system conducted by anonymous questionnaires shows that our system is user friendly. In other words, the proposed system maintains a high level of usability under the premise of high security. Compared with the existing CAPTCHA system, our CAPTCHA system is significantly better in terms of security, usability and ease of implementation.}
}
@article{GARCIATEODORO2022102845,
title = {Multi-labeling of complex, multi-behavioral malware samples},
journal = {Computers & Security},
volume = {121},
pages = {102845},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102845},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002395},
author = {P. García-Teodoro and J.A. Gómez-Hernández and A. Abellán-Galera},
keywords = {Android, Behavior, Dataset, Labeling, Malware},
abstract = {The use of malware samples is usually required to test cyber security solutions. For that, the correct typology of the samples is of interest to properly estimate the exhibited performance of the tools under evaluation. Although several malware datasets are publicly available at present, most of them are not labeled or, if so, only one class or tag is assigned to each malware sample. We defend that just one label is not enough to represent the usual complex behavior exhibited by most of current malware. With this hypothesis in mind, and based on the varied classification generally provided by automatic detection engines per sample, we introduce here a simple multi-labeling approach to automatically tag the usual multiple behavior of malware samples. In the paper, we first analyze the coherence between the behaviors exhibited by a specific number of well-known malware samples dissected in the literature and the multiple tags provided for them by our labeling proposal. After that, the automatic multi-labeling scheme is executed over four public Android malware datasets, the different results and statistics obtained regarding their composition and representativeness being discussed. We share in a GitHub repository the multi-labeling tool developed, for public usage.}
}