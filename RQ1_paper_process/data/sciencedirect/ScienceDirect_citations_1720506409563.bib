@article{2024I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {17},
number = {1},
pages = {I-CXIV},
year = {2024},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(23)00525-9},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X23005259}
}
@article{KJAERGAARD2020106848,
title = {Current practices and infrastructure for open data based research on occupant-centric design and operation of buildings},
journal = {Building and Environment},
volume = {177},
pages = {106848},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106848},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320302079},
author = {Mikkel B. Kjærgaard and Omid Ardakanian and Salvatore Carlucci and Bing Dong and Steven K. Firth and Nan Gao and Gesche Margarethe Huebner and Ardeshir Mahdavi and Mohammad Saiedur Rahaman and Flora D. Salim and Fisayo Caleb Sangogboye and Jens Hjort Schwee and Dawid Wolosiuk and Yimin Zhu},
keywords = {Open data, Data publishing, Data use, Occupant behavior, FAIR Data, Ontology, Anonymi, z, ation, Metadata schema},
abstract = {Many new tools for improving the design and operation of buildings try to realize the potential of big data. In particular, data is an important element for occupant-centric design and operation as occupants’ presence and actions are affected by a high degree of uncertainty and, hence, are hard to model in general. For such research, data handling is an important challenge, and following an open science paradigm based on open data can increase efficiency and transparency of scientific work. This article reviews current practices and infrastructure for open data-driven research on occupant-centric design and operation of buildings. In particular, it covers related work on open data in general and for the built environment in particular, presents survey results for existing scientific practices, reviews technical solutions for handling data and metadata, discusses ethics and privacy protection and analyses principles for the sharing of open data. In summary, this study establishes the status quo and presents an outlook on future work for methods and infrastructures to support the open data community within the built environment.}
}
@article{GORDON2022107949,
title = {The HITRAN2020 molecular spectroscopic database},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {277},
pages = {107949},
year = {2022},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2021.107949},
url = {https://www.sciencedirect.com/science/article/pii/S0022407321004416},
author = {I.E. Gordon and L.S. Rothman and R.J. Hargreaves and R. Hashemi and E.V. Karlovets and F.M. Skinner and E.K. Conway and C. Hill and R.V. Kochanov and Y. Tan and P. Wcisło and A.A. Finenko and K. Nelson and P.F. Bernath and M. Birk and V. Boudon and A. Campargue and K.V. Chance and A. Coustenis and B.J. Drouin and J.–M. Flaud and R.R. Gamache and J.T. Hodges and D. Jacquemart and E.J. Mlawer and A.V. Nikitin and V.I. Perevalov and M. Rotger and J. Tennyson and G.C. Toon and H. Tran and V.G. Tyuterev and E.M. Adkins and A. Baker and A. Barbe and E. Canè and A.G. Császár and A. Dudaryonok and O. Egorov and A.J. Fleisher and H. Fleurbaey and A. Foltynowicz and T. Furtenbacher and J.J. Harrison and J.–M. Hartmann and V.–M. Horneman and X. Huang and T. Karman and J. Karns and S. Kassi and I. Kleiner and V. Kofman and F. Kwabia–Tchana and N.N. Lavrentieva and T.J. Lee and D.A. Long and A.A. Lukashevskaya and O.M. Lyulin and V.Yu. Makhnev and W. Matt and S.T. Massie and M. Melosso and S.N. Mikhailenko and D. Mondelain and H.S.P. Müller and O.V. Naumenko and A. Perrin and O.L. Polyansky and E. Raddaoui and P.L. Raston and Z.D. Reed and M. Rey and C. Richard and R. Tóbiás and I. Sadiek and D.W. Schwenke and E. Starikova and K. Sung and F. Tamassia and S.A. Tashkun and J. {Vander Auwera} and I.A. Vasilenko and A.A. Vigasin and G.L. Villanueva and B. Vispoel and G. Wagner and A. Yachmenev and S.N. Yurchenko},
keywords = {HITRAN, Spectroscopic database, Molecular spectroscopy, Spectroscopic line parameters, Absorption cross-sections, Collision-induced absorption, Aerosols, Molecular opacities},
abstract = {The HITRAN database is a compilation of molecular spectroscopic parameters. It was established in the early 1970s and is used by various computer codes to predict and simulate the transmission and emission of light in gaseous media (with an emphasis on terrestrial and planetary atmospheres). The HITRAN compilation is composed of five major components: the line-by-line spectroscopic parameters required for high-resolution radiative-transfer codes, experimental infrared absorption cross-sections (for molecules where it is not yet feasible for representation in a line-by-line form), collision-induced absorption data, aerosol indices of refraction, and general tables (including partition sums) that apply globally to the data. This paper describes the contents of the 2020 quadrennial edition of HITRAN. The HITRAN2020 edition takes advantage of recent experimental and theoretical data that were meticulously validated, in particular, against laboratory and atmospheric spectra. The new edition replaces the previous HITRAN edition of 2016 (including its updates during the intervening years). All five components of HITRAN have undergone major updates. In particular, the extent of the updates in the HITRAN2020 edition range from updating a few lines of specific molecules to complete replacements of the lists, and also the introduction of additional isotopologues and new (to HITRAN) molecules: SO, CH3F, GeH4, CS2, CH3I and NF3. Many new vibrational bands were added, extending the spectral coverage and completeness of the line lists. Also, the accuracy of the parameters for major atmospheric absorbers has been increased substantially, often featuring sub-percent uncertainties. Broadening parameters associated with the ambient pressure of water vapor were introduced to HITRAN for the first time and are now available for several molecules. The HITRAN2020 edition continues to take advantage of the relational structure and efficient interface available at www.hitran.org and the HITRAN Application Programming Interface (HAPI). The functionality of both tools has been extended for the new edition.}
}
@article{LAUMANN2022e00258,
title = {Device for measuring part adhesion in FFF process},
journal = {HardwareX},
volume = {11},
pages = {e00258},
year = {2022},
issn = {2468-0672},
doi = {https://doi.org/10.1016/j.ohx.2022.e00258},
url = {https://www.sciencedirect.com/science/article/pii/S2468067222000037},
author = {Daniel Laumann and Dieter Spiehl and Edgar Dörsam},
keywords = {3D-Printing, Fused filament fabrication, Adhesion},
abstract = {The adhesion of parts to the build surface plays a central role in the Fused Filament Fabrication (FFF) process. Without sufficient adhesion, the part will deform (so called warping) due to thermal shrinkage, so that no defined geometries can be created. Nevertheless, there is no established method to measure the adhesion of printed parts and therefore it is not possible to targeted improve it. This article presents a measurement method based on the DIN EN 28510-1 standard and a corresponding test device which makes it possible to identify the optimum build surface for a filament and also to improve the process parameters in a targeted manner. The test device combines a FFF printer with a measuring unit so that all common filaments can be tested close to the process up to a processing temperature of 400 °C in the nozzle and around 150 °C on the build platform. The test device uses only open-source parts and software and costs about 1700€.}
}
@article{ROSELLI202222,
title = {Leveraging Conflicting Constraints in Solving Vehicle Routing Problems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {28},
pages = {22-29},
year = {2022},
note = {16th IFAC Workshop on Discrete Event Systems WODES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.319},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322023564},
author = {Sabino Francesco Roselli and Remco Vader and Martin Fabian and Knut Åkesson},
abstract = {The Conflict-Free Electric Vehicle Routing Problem (CF-EVRP) is a combinatorial optimization problem of designing routes for vehicles to visit customers such that a cost function, typically the number of vehicles or the total travelled distance, is minimized. The CF-EVRP involves constraints such as time windows on the delivery to the customers, limited operating range of the vehicles, and limited capacity on the number of vehicles that a road segment can simultaneously accommodate. In previous work, the compositional algorithm ComSat was introduced and that solves the CF-EVRP by breaking it down into sub-problems and iteratively solve them to build an overall solution. Though ComSat showed good performance in general, some problems took significant time to solve due to the high number of iterations required to find solutions that satisfy the road segments’ capacity constraints. The bottleneck is the Path Changing Problem, i.e., the sub-problem of finding a new set of shortest paths to connect a subset of the customers, disregarding previously found shortest paths. This paper presents an improved version of the PathsChanger function to solve the Path Changing Problem that exploits the unsatisfiable core, i.e., information on which constraints conflict, to guide the search for feasible solutions. Experiments show faster convergence to feasible solutions compared to the previous version of PathsChanger.}
}
@article{2023100784,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {2},
number = {10},
pages = {100784},
year = {2023},
issn = {2772-963X},
doi = {https://doi.org/10.1016/S2772-963X(23)00815-3},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X23008153}
}
@incollection{2017395,
title = {Index},
editor = {Paul Göransson and Chuck Black and Timothy Culver},
booktitle = {Software Defined Networks (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {395-409},
year = {2017},
isbn = {978-0-12-804555-8},
doi = {https://doi.org/10.1016/B978-0-12-804555-8.09984-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045558099841}
}
@incollection{VITEK2020251,
title = {Chapter Five - Intelligent agents in games: Review with an open-source tool},
editor = {Ali R. Hurson and Veljko Milutinović},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {116},
number = {1},
pages = {251-303},
year = {2020},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300312},
author = {Matej Vitek and Peter Peer},
keywords = {Agent, Artificial intelligence, Rationality, Games, Game world, Game development},
abstract = {The field of artificial intelligence has come a long way in the last 50 years, and studies of its methods soon expanded to a field in which they are of great practical value—computer games. The concept of intelligent agents provides a much needed theoretical background for the comparison of various different approaches to intelligent, rational behavior of computer-controlled characters in games. By combining rationality with certain limitations to the capabilities of our agents, we can achieve behavior resembling that of a human player, which is also desirable in games. The goal of this article is to introduce various types of agents that are used in games, show how to implement meaningful, reasonable limitations to agent capabilities into the game world, and provide a freely available, open-source application for the comparison of such agents. Additionally, in this article we show that even the simplest agents can succeed in their tasks in certain task environments, whereas more difficult task environments often require a more sophisticated agent architecture. Our application consists of two task environments with nine agents in total but could easily be extended with additional task environments and agent implementations. In the end, we find the addition of goals into the agent architecture has the biggest impact on the agent's behavior and performance, whereas the state-based approach helps keep our implementation simple and compact.}
}
@article{WANG2023107105,
title = {An empirical study on bugs in JavaScript engines},
journal = {Information and Software Technology},
volume = {155},
pages = {107105},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107105},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922002142},
author = {Ziyuan Wang and Dexin Bu and Nannan Wang and Sijie Yu and Shanyi Gou and Aiyue Sun},
keywords = {Empirical study, JavaScript engine, Software bug, SpiderMonkey, Chakra, V8},
abstract = {Context:
JavaScript is a prototype-based dynamic type scripting language. The correct running of a JavaScript program depends on the correctness of both the program and the JavaScript engine.
Objective:
An in-depth understanding of the characteristics of bugs in JavaScript engines can help detect and fix them.
Methods:
We conduct an empirical study on the bugs in three mainstream JavaScript engines: V8, SpiderMonkey, and Chakra. Such an empirical study involves 19,019 bug reports, 16,437 revisions, 805 test cases, and root causes of randomly selected 540 bugs.
Results:
(1) The Compiler and the DOM are the most buggy component in V8 and SpiderMonkey, respectively. Most of the source files contain only one bug. (2) The scales of the testing programs that reveal bugs are usually small. Most bug fixes involve only limited modifications since the number of modified source files and lines of code modified are small. (3) Most bugs can be fixed within half a year (80.33% for V8 and 91.9% for SpiderMonkey). Only 4.33% of SpiderMonkey bugs need more than a year to fix. Bugs in SpiderMonkey are usually fixed faster than bugs in V8. (4) High priority tends to be assigned to Infrastructure bugs in V8 and Release Automation bugs in SpiderMonkey. The duration of bugs is not strictly correlated with their priorities. (5) Semantic bugs are the most common root causes of bugs. And among semantic bugs, the processing bugs, missing features bugs and function call bugs are more than others.
Conclusion:
This study deepens our understanding of bugs in JavaScript engines, and empirical results could indicate some potential problems during the detecting and fixing of bugs in JavaScript engines, assist developers of JavaScript engines in improving their development quality, assist maintainers in detecting and fixing bugs more effectively, and suggest users of JavaScript evade potential risks.}
}
@article{GARRABE202281,
title = {Probabilistic design of optimal sequential decision-making algorithms in learning and control},
journal = {Annual Reviews in Control},
volume = {54},
pages = {81-102},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000967},
author = {Émiland Garrabé and Giovanni Russo},
keywords = {Sequential decision-making, Data-driven control, Learning, Densities optimization},
abstract = {This survey is focused on certain sequential decision-making problems that involve optimizing over probability functions. We discuss the relevance of these problems for learning and control. The survey is organized around a framework that combines a problem formulation and a set of resolution methods. The formulation consists of an infinite-dimensional optimization problem. The methods come from approaches to search optimal solutions in the space of probability functions. Through the lenses of this overarching framework we revisit popular learning and control algorithms, showing that these naturally arise from suitable variations on the formulation mixed with different resolution methods. A running example, for which we make the code available, complements the survey. Finally, a number of challenges arising from the survey are also outlined.}
}
@article{MARTINEZPLUMED2021101525,
title = {Futures of artificial intelligence through technology readiness levels},
journal = {Telematics and Informatics},
volume = {58},
pages = {101525},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101525},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301842},
author = {Fernando Martínez-Plumed and Emilia Gómez and José Hernández-Orallo},
keywords = {AI technologies, Generality, Capabilities, Technology readiness, TRLs},
abstract = {Artificial Intelligence (AI) offers the potential to transform our lives in radical ways. However, the main unanswered questions about this foreseen transformation are its depth, breadth and timelines. To answer them, not only do we lack the tools to determine what achievements will be attained in the near future, but we even ignore what various technologies in present-day AI are capable of. Many so-called breakthroughs in AI are associated with highly-cited research papers or good performance in some particular benchmarks. However, research breakthroughs do not directly translate into a technology that is ready to use in real-world environments. In this paper, we present a novel exemplar-based methodology to categorise and assess several AI technologies, by mapping them onto Technology Readiness Levels (TRL) (representing their depth in maturity and availability). We first interpret the nine TRLs in the context of AI, and identify several categories in AI to which they can be assigned. We then introduce a generality dimension, which represents increasing layers of breadth of the technology. These two dimensions lead to the new readiness-vs-generality charts, which show that higher TRLs are achievable for low-generality technologies, focusing on narrow or specific abilities, while high TRLs are still out of reach for more general capabilities. We include numerous examples of AI technologies in a variety of fields, and show their readiness-vs-generality charts, serving as exemplars. Finally, we show how the timelines of several AI technology exemplars at different generality layers can help forecast some short-term and mid-term trends for AI.}
}
@article{N2024101215,
title = {Secure pharmaceutical supply chain using blockchain in IoT cloud systems},
journal = {Internet of Things},
volume = {26},
pages = {101215},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101215},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001562},
author = {Mangala N. and Naveen D.R. and B. Eswara Reddy and Rajkumar Buyya and Venugopal K.R. and S.S. Iyengar and L.M. Patnaik},
keywords = {Internet of Things (IoT), Blockchain, Supply Chain Management (SCM), Cloud computing (CC), Artificial Intelligence (AI)},
abstract = {Supply Chain Management (SCM) systems require time sequencing, coordination and tracking the movement of goods and processes. Internet of Things (IoT) and Blockchain technologies are useful to develop a secure automated SCM. IoT devices with in-built sensors and actuators help to keep track of the state, location, temperature or other parameters of an entity, and control the automation of routine as well as hazardous tasks. Blockchain technology supports time-stamping, authentication, process coordination, non-repudiation, commercial transactions, and also provides security for transactions and storage. The pharmaceutical SCM demands accurate, immediate and secure control system. Additionally, the supply chain process data from IoTs is stored and processed in Cloud by Analytics applications, for business planning and improvement. An efficient and secure IoT-Cloud-Blockchain based system for both SCM automation and analytics has been proposed in this work. It leverages a hierarchical IoT, Mist, Edge, Fog, Cloud computing (IMEFC) architecture to enhance Communication-Response-Compute-Security-Storage (CRCSS) in the system. Blockchain technology provides security for the SCM transactions and data. The efficiency of the Blockchain is measured in terms of upload time, download time and transaction fees for Bitcoin, Ethereum and Filecoin platforms. The Filecoin blockchain platform is quicker and cost-effective for larger file sizes, compared to Ethereum and Bitcoin, making it suitable for Pharmaceutical SCM systems.}
}
@article{JAHANIYEKTA2024100078,
title = {The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance},
journal = {Meta-Radiology},
volume = {2},
number = {2},
pages = {100078},
year = {2024},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2950162824000316},
author = {Mohammad Mahdi {Jahani Yekta}},
keywords = {GPT–4, Artificial general intelligence, Knowledge diffusion, Interpretability and explainability, Societal influences, Governance},
abstract = {Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.}
}
@article{WANG2022108841,
title = {An entity-weights-based convolutional neural network for large-sale complex knowledge embedding},
journal = {Pattern Recognition},
volume = {131},
pages = {108841},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108841},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322003223},
author = {Zhengdi Wang and Lvqing Yang and Zhenfeng Lei and Anwar {Ul Haq} and Defu Zhang and Shuangyuan Yang and Akindipe Olusegun Francis},
keywords = {Graph-based finance, Representation learning, Complete incidence matrix, Convolutional neural network, Matrix factorization},
abstract = {Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications (e.g., risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing. With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.}
}
@incollection{ZOHURI2022837,
title = {Chapter 27 - Artificial intelligence, machine learning, and deep learning driving big data},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {837-888},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000271},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Artificial intelligence, Deep learning, Internet of things, Machine learning},
abstract = {Artificial Intelligence (AI) is one of those technologies that seem to be expanding outward in every direction, and this expansion is driven by sheer volume of data that we are encountering in our daily routine life in these days, where technology is deviating from its tradition format to the more modern world of electronic gadget. In today's modern technological world, we are accumulating so much data in real time with speed of electron through the world of Internet of Things (IoT), and it comes to us in Omni-Direction space. Dealing with these data in the form of structured and unstructured, we have no choice except to turn to AI for assistant, in order to stay on top of all the information that is collected from these data for us to have a knowledge that allows us to consequently have power of right and accurate decision-making in real time in our daily routine as stakeholder. However, comes with AI needs its two other integrated components, namely Machine Learning (ML) and Deep Learning (DL) to be more effective to us as human being, where we are relying on Artificial Intelligence to run our day-to-day operation in very resilience form.}
}
@article{2023I,
title = {Full Issue PDF},
journal = {JACC: CardioOncology},
volume = {5},
number = {6},
pages = {I-CXLII},
year = {2023},
issn = {2666-0873},
doi = {https://doi.org/10.1016/S2666-0873(23)00346-0},
url = {https://www.sciencedirect.com/science/article/pii/S2666087323003460}
}
@incollection{HOLMBERG201655,
title = {2 - The Present},
editor = {Kim Holmberg},
booktitle = {Altmetrics for Information Professionals},
publisher = {Chandos Publishing},
pages = {55-104},
year = {2016},
isbn = {978-0-08-100273-5},
doi = {https://doi.org/10.1016/B978-0-08-100273-5.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002735000028},
author = {Kim Holmberg},
keywords = {scholarly communication, web, internet, data, information dissemination, impact, data, blogs, microblogs, Twitter, social networking sites, social reference managers, social peer review, peer review, recommendation systems, Wikipedia, data repositories, data sharing, service providers, aggregators, stakeholders, researchers, universities, libraries, publishers, funders, public, audience, altmetrics, webometrics, social media, social media metrics, social media analysis, awareness, attention},
abstract = {In the second part the current altmetric research is presented. This part begins with an overview of scholarly communication on the web, its potential, and current status. This is followed by an overview of some of the sources of these new online metrics. The service providers or aggregators of altmetrics are briefly presented, followed by a discussion of the different stakeholders. There are many stakeholders connected to altmetrics, all of whom can use them somewhat differently and benefit from them in various ways. Some of the earlier research of altmetrics will be presented, research that has pushed the development of altmetrics forward and continues to push it as the web evolves and the way we use the web changes.}
}
@article{J2023103633,
title = {Blockchain for healthcare systems: Architecture, security challenges, trends and future directions},
journal = {Journal of Network and Computer Applications},
volume = {215},
pages = {103633},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103633},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523000528},
author = {Andrew J and Deva Priya Isravel and K. Martin Sagayam and Bharat Bhushan and Yuichi Sei and Jennifer Eunice},
keywords = {Blockchain, Consensus algorithms, Cryptocurrency, Distributed ledger, Healthcare data, Security & privacy},
abstract = {Blockchain has become popular in recent times through its data integrity and wide scope of applications. It has laid the foundation for cryptocurrencies such as Ripple, Bitcoin, Ethereum, and so on. Blockchain provides a platform for decentralization and trust in various applications such as finance, commerce, IoT, reputation systems, and healthcare. However, prevailing challenges like scalability, resilience, security and privacy are yet to be overcome. Due to rigorous regulatory constraints such as HIPAA, blockchain applications in the healthcare industry usually require more stringent authentication, interoperability, and record sharing requirements. This article presents an extensive study to showcase the significance of blockchain technology from both application and technical perspectives for healthcare domain. The article discusses the features and use-cases of blockchain in different applications along with the healthcare domain interoperability. The detailed working operation of the blockchain and the consensus algorithms are presented in the context of healthcare. An outline of the blockchain architecture, platforms, and classifications are discussed to choose the right platform for healthcare applications. The current state-of-the-art research in healthcare blockchain and available blockchain based healthcare applications are summarized. Furthermore, the challenges and future research opportunities along with the performance evaluation metrics in realizing the blockchain technology for healthcare are presented to provide insight for future research. We also layout the various security attacks on the blockchain protocol with the classifications of threat models and presented a comparative analysis of the detection and protection techniques. Techniques to enhance the security and privacy of the blockchain network is also discussed.}
}
@article{2023I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {16},
number = {8},
pages = {I-CXXXI},
year = {2023},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(23)00322-4},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X23003224}
}
@article{GAROUSI2022111136,
title = {Mining user reviews of COVID contact-tracing apps: An exploratory analysis of nine European apps},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111136},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111136},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002338},
author = {Vahid Garousi and David Cutting and Michael Felderer},
keywords = {Mobile apps, COVID, Contact-tracing, User reviews, Software engineering, Software in society, Data mining},
abstract = {Context:
More than 78 countries have developed COVID contact-tracing apps to limit the spread of coronavirus. However, many experts and scientists cast doubt on the effectiveness of those apps. For each app, a large number of reviews have been entered by end-users in app stores.
Objective:
Our goal is to gain insights into the user reviews of those apps, and to find out the main problems that users have reported. Our focus is to assess the “software in society” aspects of the apps, based on user reviews.
Method:
We selected nine European national apps for our analysis and used a commercial app-review analytics tool to extract and mine the user reviews. For all the apps combined, our dataset includes 39,425 user reviews.
Results:
Results show that users are generally dissatisfied with the nine apps under study, except the Scottish (“Protect Scotland”) app. Some of the major issues that users have complained about are high battery drainage and doubts on whether apps are really working.
Conclusion:
Our results show that more work is needed by the stakeholders behind the apps (e.g., app developers, decision-makers, public health experts) to improve the public adoption, software quality and public perception of these apps.}
}
@article{2024I,
title = {Full Issue PDF},
journal = {JACC: Basic to Translational Science},
volume = {9},
number = {5},
pages = {I-CLXXIX},
year = {2024},
issn = {2452-302X},
doi = {https://doi.org/10.1016/S2452-302X(24)00173-6},
url = {https://www.sciencedirect.com/science/article/pii/S2452302X24001736}
}
@article{GARCIA201727,
title = {Static analysis of cloud elasticity},
journal = {Science of Computer Programming},
volume = {147},
pages = {27-53},
year = {2017},
note = {Selected and Extended papers from the International Symposium on Principles and Practice of Declarative Programming 2015},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167642317300679},
author = {Abel Garcia and Cosimo Laneve and Michael Lienhardt},
keywords = {Resource consumption analysis, Cloud computing, Behavioural type system, Subject reduction, Concurrent programming},
abstract = {We propose a static analysis technique that computes upper bounds of virtual machine usages in a concurrent language with explicit acquire and release operations of virtual machines. In our language it is possible to delegate other (ad-hoc or third party) concurrent code to release virtual machines (by passing them as arguments of invocations). Our technique is modular and consists of (i) a type system associating programs with behavioural types that record relevant information for resource usage (creations, releases, and concurrent operations), (ii) a translation function that takes behavioural types and returns cost equations, and (iii) an automatic off-the-shelf solver for the cost equations. A soundness proof of the type system establishes the correctness of our technique with respect to the cost equations. We have experimentally evaluated our technique using a cost analysis solver and we report some results.}
}
@article{YANG2022105366,
title = {IoT data analytics in dynamic environments: From an automated machine learning perspective},
journal = {Engineering Applications of Artificial Intelligence},
volume = {116},
pages = {105366},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105366},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622003803},
author = {Li Yang and Abdallah Shami},
keywords = {IoT data analytics, AutoML, Concept drift, Machine learning},
abstract = {With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.}
}
@article{MAN2024102427,
title = {Neural networks for intelligent multilevel control of artificial and natural objects based on data fusion: A survey},
journal = {Information Fusion},
volume = {110},
pages = {102427},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102427},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002057},
author = {Tianxing Man and Vasily Yu. Osipov and Nataly Zhukova and Alexey Subbotin and Dmitry I. Ignatov},
keywords = {Data fusion, Neural networks, Intelligent control, Requirements},
abstract = {Today the tasks of complex artificial and natural objects control have come to the fore in the majority of subject domains. The efficiency and effectiveness of solving these tasks directly depends of the efficiency and effectiveness of data fusion (DF). Data fusion methods are designed to integrate data from multiple sources and transform it in order to produce more consistent, accurate, and useful information than that provided by any individual data source. Although DF has been extensively studied for a considerable period of time it is still hardly applicable in practice in the processes of the control of the real world objects with complex structure and behavior as the data produced by the objects is, in the majority of cases, heterogeneous, multimodal, and imperfect, has huge volume. To ensure proper response to the changes in the state and behavior of the controlled objects that can be caused by both internal and external influencing factors the data should be processed with high accuracy and with minimum delays. Despite the importance of the tasks of complex objects control till now there are no researches that clarify to what extent the DF problem has been solved from the perspective of its application in the processes of objects control based on the data received from the objects. In the survey we define the requirements to DF in the interests of the control of complex artificial and natural objects, consider the structure of the multilevel process of intelligent object control, identify the neural networks that can be used in the control process for data fusion. Despite the wide capabilities of the existing NN we reveal that they still do not meet all the requirements to DF for complex objects control. Based on the analysis of NN architectures, we define requirements for advanced NN architectures and discuss future research directions. To facilitate our literature analyses, we also perform conceptual exploration of collected papers with lattices of closed itemsets and implications from Formal Concept Analysis and Data Mining used for knowledge processing in similar large-scale studies.}
}
@article{PERALTA2020101479,
title = {Detecting coherent explorations in SQL workloads},
journal = {Information Systems},
volume = {92},
pages = {101479},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101479},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919305319},
author = {Verónika Peralta and Patrick Marcel and Willeme Verdeaux and Aboubakar Sidikhy Diakhaby},
abstract = {This paper presents a proposal aiming at better understanding a workload of SQL queries and detecting coherent explorations hidden within the workload. In particular, our work investigates SQLShare (Jain et al., 2016), a database-as-a-service platform targeting scientists and data scientists with minimal database experience, whose workload was made available to the research community. According to the authors of (Jain et al., 2016), this workload is the only one containing primarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed this workload by extracting features that characterize SQL queries and we investigate three different machine learning approaches to use these features to separate sequences of SQL queries into meaningful explorations. The first approach is unsupervised and based only on similarity between contiguous queries. The second approach uses transfer learning to apply a model trained over a dataset where ground truth is available. The last approach uses weak labeling to predict the most probable segmentation from heuristics meant to label a training set. We ran several tests over various query workloads to evaluate and compare the proposed methods.}
}
@article{2021I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {14},
number = {2},
pages = {I-CCVIII},
year = {2021},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(21)00040-1},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X21000401}
}
@article{WANG2019108,
title = {Why is my code change abandoned?},
journal = {Information and Software Technology},
volume = {110},
pages = {108-120},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300424},
author = {Qingye Wang and Xin Xia and David Lo and Shanping Li},
keywords = {Code review, Empirical study, Abandoned change},
abstract = {Context: Software developers contribute numerous changes every day to the code review systems. However, not all submitted changes are merged into a codebase because they might not pass the code review process. Some changes would be abandoned or be asked for resubmission after improvement, which results in more workload for developers and reviewers, and more delays to deliverables. Objective: To understand the underlying reasons why changes are abandoned, we conduct an empirical study on the code review of four open source projects (Eclipse, LibreOffice, OpenStack, and Qt). Method: First, we manually analyzed 1459 abandoned changes. Second, we leveraged the open card sorting method to label these changes with reasons why they were abandoned, and we identified 12 categories of reasons. Next, we further investigated the frequency distribution of the categories across projects. Finally, we studied the relationship between the categories and time-to-abandonment. Results: Our findings include the following: (1) Duplicate changes are the majority of the abandoned changes; (2) the frequency distribution of abandoned changes across the 12 categories is similar for the four open source projects; (3) 98.39% of the changes are abandoned within a year. Conclusion: Our study concluded the root causes of abandoned changes, which will help developers submit high-quality code changes.}
}
@article{QIU201756,
title = {Global Flow Table: A convincing mechanism for security operations in SDN},
journal = {Computer Networks},
volume = {120},
pages = {56-70},
year = {2017},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617301251},
author = {Xiaofeng Qiu and Kai Zhang and Qiuzheng Ren},
keywords = {Security operations, Software defined networking, Global flow, Weak vertex cover},
abstract = {One of the key challenges of network security is that security middle boxes, such as firewalls and Intrusion Detection Systems (IDSs), only have local view of the network. This lowers the efficiency of security detection and makes it difficult to locate the sources of the threats. There have been growing demands for security operations and appliances that are aware of the distribution and behavior of flows in the whole network; logically centralized control ability of Software-Defined Network (SDN) makes it possible for the network controller to acquire the global view of the network. In this paper, we propose a mechanism named Global Flow Table (GFT) which can provide security appliances and operators with paths of all the flows in SDN network, in addition to their sources, destinations, setup and terminate time, traffic volume and directions. A weak vertex cover based GFT algorithm which sacrifices less than 5% accuracy is also provided to improve scalability. Tests with different network topologies of cloud computing center and enterprise networks show promising performance. Utilizing the Global Flow Table, we built several applications to illustrate how GFT could benefit the security operations.}
}
@article{TRAINER2018328,
title = {Bridging the gap between awareness and trust in globally distributed software teams},
journal = {Journal of Systems and Software},
volume = {144},
pages = {328-341},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301250},
author = {Erik H. Trainer and David F. Redmiles},
keywords = {Trust, Awareness, Attribution error, Perceived trustworthiness, Global software engineering, Empirical studies},
abstract = {Trust remains a key challenge for globally distributed teams despite decades of research. Awareness, a key component of collaboration, has even more research around it. However, detailed accounts of the interrelationship of awareness and trust are still lacking in the literature, particularly in the setting of software teams. The gap we seek to fill with this article is to examine how software tool support for awareness can engender trust among globally distributed software developers. We highlight qualitative results from a previous and extensive field study that shows how trust is still a problem in contemporary teams. These results motivate a specific examination of how developers form attributions of one another. We describe a collection of visualization widgets designed to address the specific issues found in the field. To evaluate their effectiveness, we performed a controlled laboratory study with 28 students and 12 professional software developers who used these visualizations collected into a tool environment called Theseus. The results show that in general, participants using the visualizations make more accurate attributions, and their perceived trustworthiness of their remote teammates more accurately reflects actual circumstances. We conclude with a discussion of the implications of our results for theory and practice.}
}
@article{2023100310,
title = {Pathology Visions 2022 Overview},
journal = {Journal of Pathology Informatics},
volume = {14},
pages = {100310},
year = {2023},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2023.100310},
url = {https://www.sciencedirect.com/science/article/pii/S2153353923001244}
}
@article{2024I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Interventions},
volume = {17},
number = {10},
pages = {I-CXI},
year = {2024},
issn = {1936-8798},
doi = {https://doi.org/10.1016/S1936-8798(24)00738-6},
url = {https://www.sciencedirect.com/science/article/pii/S1936879824007386}
}
@article{UBAN2021480,
title = {An emotion and cognitive based analysis of mental health disorders from social media data},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {480-494},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001825},
author = {Ana-Sabina Uban and Berta Chulvi and Paolo Rosso},
keywords = {Mental health disorders, Early risk prediction, Emotions, Cognitive styles, Deep learning, Social media},
abstract = {Mental disorders can severely affect quality of life, constitute a major predictive factor of suicide, and are usually underdiagnosed and undertreated. Early detection of signs of mental health problems is particularly important, since unattended, they can be life-threatening. This is why a deep understanding of the complex manifestations of mental disorder development is important. We present a study of mental disorders in social media, from different perspectives. We are interested in understanding whether monitoring language in social media could help with early detection of mental disorders, using computational methods. We developed deep learning models to learn linguistic markers of disorders, at different levels of the language (content, style, emotions), and further try to interpret the behavior of our models for a deeper understanding of mental disorder signs. We complement our prediction models with computational analyses grounded in theories from psychology related to cognitive styles and emotions, in order to understand to what extent it is possible to connect cognitive styles with the communication of emotions over time. The final goal is to distinguish between users diagnosed with a mental disorder and healthy users, in order to assist clinicians in diagnosing patients. We consider three different mental disorders, which we analyze separately and comparatively: depression, anorexia, and self-harm tendencies.}
}
@article{NOLTE2023677,
title = {Secure HPC: A workflow providing a secure partition on an HPC system},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {677-691},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200423X},
author = {Hendrik Nolte and Nicolai Spicher and Andrew Russel and Tim Ehlers and Sebastian Krey and Dagmar Krefting and Julian Kunkel},
keywords = {High performance computing, Sensitive data, Secure computing, Data encryption},
abstract = {Driven by the progress of data and compute-intensive methods in various scientific domains, there is an increasing demand from researchers working with highly sensitive data to have access to the necessary computational resources to be able to adapt those methods in their respective fields. To satisfy the computing needs of those researchers cost-effectively, it is an open quest to integrate reliable security measures on existing High Performance Computing (HPC) clusters. The fundamental problem with securely working with sensitive data is, that HPC systems are shared systems that are typically trimmed for the highest performance—not for high security. For instance, there are commonly no additional virtualization techniques employed, thus, users typically have access to the host operating system. Since new vulnerabilities are being continuously discovered, solely relying on the traditional Unix permissions is not secure enough. In this paper, we discuss Secure HPC, a workflow allowing users to transfer, store and analyze data with the highest privacy requirements. Our contributions are the design of a multi-node secure workflow with parallel I/O, a strict security model enforced by the system and network features, and lastly the demonstration of a medical use case. In our experiments, we see an advantage in the asynchronous execution of IO requests in dm_crypt, while reaching 80% of the ideal performance. When comparing eCryptFS with GoCryptFS as two representative filesystem-level encryption stacks, eCryptFS was twice as fast. In a real use case, we observed on average 97% of the native performance.}
}
@article{2023I,
title = {Full Issue PDF},
journal = {JACC: Basic to Translational Science},
volume = {8},
number = {3},
pages = {I-CXXXVII},
year = {2023},
issn = {2452-302X},
doi = {https://doi.org/10.1016/S2452-302X(23)00089-X},
url = {https://www.sciencedirect.com/science/article/pii/S2452302X2300089X}
}
@article{2021I,
title = {Full Issue PDF},
journal = {JACC: Basic to Translational Science},
volume = {6},
number = {11},
pages = {I-CXXIII},
year = {2021},
issn = {2452-302X},
doi = {https://doi.org/10.1016/S2452-302X(21)00355-7},
url = {https://www.sciencedirect.com/science/article/pii/S2452302X21003557}
}
@article{LIU2022106416,
title = {Machine-Learning-enhanced systemic risk measure: A Two-Step supervised learning approach},
journal = {Journal of Banking & Finance},
volume = {136},
pages = {106416},
year = {2022},
issn = {0378-4266},
doi = {https://doi.org/10.1016/j.jbankfin.2022.106416},
url = {https://www.sciencedirect.com/science/article/pii/S0378426622000164},
author = {Ruicheng Liu and Chi Seng Pun},
keywords = {Systemic risk measure, Machine learning, Cross-sectional measures, Conditional capital shortfall, Marginal expected shortfall (MES), SRISK},
abstract = {This paper explores ways to improve the existing systemic risk measures by incorporating machine learning algorithms into the measurement. We aim to overcome the shortcomings of existing methods that rely on restricted modeling and are difficult to tap into various data resources. To this end, this paper unifies a dynamic quantification framework for systemic risk and links it to a two-step supervised learning problem, which allows for hierarchical structure of the systemic event and the return dependence. We leverage the generalization and predictive powers of machine learning to statistically model the tail events and the co-movements of the equity returns during the shocks to the macro-economy. Our results show that most machine learning algorithms enhance the systemic risk measure’s predictive power. Numerous comparative and sensitivity backtesting studies for United States and Hong Kong markets are conducted, from which we recommend the best machine learning algorithm for systemic risk measurement.}
}
@article{CASHEEKAR2024100632,
title = {A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions},
journal = {Computer Science Review},
volume = {52},
pages = {100632},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100632},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000169},
author = {Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan},
keywords = {Computational intelligence, Artificial intelligence, Chatbots, Conversational agents, ChatGPT},
abstract = {This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.}
}
@article{SENGUPTA2020102481,
title = {A Comprehensive Survey on Attacks, Security Issues and Blockchain Solutions for IoT and IIoT},
journal = {Journal of Network and Computer Applications},
volume = {149},
pages = {102481},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102481},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303418},
author = {Jayasree Sengupta and Sushmita Ruj and Sipra {Das Bit}},
keywords = {IIoT, Security, Privacy, Blockchain, Smart Factory, Smart Grid, Supply Chain, E-Healthcare, VANET},
abstract = {In recent years, the growing popularity of Internet of Things (IoT) is providing a promising opportunity not only for the development of various home automation systems but also for different industrial applications. By leveraging these benefits, automation is brought about in the industries giving rise to the Industrial Internet of Things (IIoT). IoT is prone to several cyberattacks and needs challenging approaches to achieve the desired security. Moreover, with the emergence of IIoT, the security vulnerabilities posed by it are even more devastating. Therefore, in order to provide a guideline to researchers, this survey primarily attempts to classify the attacks based on the objects of vulnerability. Subsequently, each of the individual attacks is mapped to one or more layers of the generalized IoT/IIoT architecture followed by a discussion on the countermeasures proposed in literature. Some relevant real-life attacks for each of these categories are also discussed. We further discuss the countermeasures proposed for the most relevant security threats in IIoT. A case study on two of the most important industrial IoT applications is also highlighted. Next, we explore the challenges brought by the centralized IoT/IIoT architecture and how blockchain can effectively be used towards addressing such challenges. In this context, we also discuss in detail one IoT specific Blockchain design known as Tangle, its merits and demerits. We further highlight the most relevant Blockchain-based solutions provided in recent times to counter the challenges posed by the traditional cloud-centered applications. The blockchain-related solutions provided in the context of two of the most relevant applications for each of IoT and IIoT is also discussed. Subsequently, we design a taxonomy of the security research areas in IoT/IIoT along with their corresponding solutions. Finally, several open research directions relevant to the focus of this survey are identified.}
}
@article{EGGER2022106874,
title = {Medical deep learning—A systematic meta-review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {221},
pages = {106874},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106874},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722002565},
author = {Jan Egger and Christina Gsaxner and Antonio Pepe and Kelsey L. Pomykala and Frederic Jonske and Manuel Kurz and Jianning Li and Jens Kleesiek},
keywords = {Deep learning, Artificial neural networks, Machine learning, Data analysis, Image analysis, Medical image analysis, Medical image processing, Medical imaging, Patient data, Pathology, Detection, Segmentation, Registration, Generative adversarial networks, PubMed, Systematic, Review, Survey, Meta-review, Meta-survey},
abstract = {Deep learning has remarkably impacted several different scientific disciplines over the last few years. For example, in image processing and analysis, deep learning algorithms were able to outperform other cutting-edge methods. Additionally, deep learning has delivered state-of-the-art results in tasks like autonomous driving, outclassing previous attempts. There are even instances where deep learning outperformed humans, for example with object recognition and gaming. Deep learning is also showing vast potential in the medical domain. With the collection of large quantities of patient records and data, and a trend towards personalized treatments, there is a great need for automated and reliable processing and analysis of health information. Patient data is not only collected in clinical centers, like hospitals and private practices, but also by mobile healthcare apps or online websites. The abundance of collected patient data and the recent growth in the deep learning field has resulted in a large increase in research efforts. In Q2/2020, the search engine PubMed returned already over 11,000 results for the search term ‘deep learning’, and around 90% of these publications are from the last three years. However, even though PubMed represents the largest search engine in the medical field, it does not cover all medical-related publications. Hence, a complete overview of the field of ‘medical deep learning’ is almost impossible to obtain and acquiring a full overview of medical sub-fields is becoming increasingly more difficult. Nevertheless, several review and survey articles about medical deep learning have been published within the last few years. They focus, in general, on specific medical scenarios, like the analysis of medical images containing specific pathologies. With these surveys as a foundation, the aim of this article is to provide the first high-level, systematic meta-review of medical deep learning surveys.}
}
@article{DEBRUIN20208049,
title = {Fine-tuning Deep RL with Gradient-Free Optimization⁎⁎This work is part of the research programme Deep Learning for Robust Robot Control (DL-Force) with project number 656.000.003, which is (partly) financed by the Netherlands Organisation for Scientific Research (NWO).},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8049-8056},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2240},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329001},
author = {Tim {de Bruin} and Jens Kober and Karl Tuyls and Robert Babuška},
keywords = {Reinforcement Learning, Deep Learning, Optimization, Neural Networks, Control},
abstract = {Deep reinforcement learning makes it possible to train control policies that map high-dimensional observations to actions. These methods typically use gradient-based optimization techniques to enable relatively efficient learning, but are notoriously sensitive to hyperparameter choices and do not have good convergence properties. Gradient-free optimization methods, such as evolutionary strategies, can offer a more stable alternative but tend to be much less sample efficient. In this work we propose a combination, using the relative strengths of both. We start with a gradient-based initial training phase, which is used to quickly learn both a state representation and an initial policy. This phase is followed by a gradient-free optimization of only the final action selection parameters. This enables the policy to improve in a stable manner to a performance level not obtained by gradient-based optimization alone, using many fewer trials than methods using only gradient-free optimization. We demonstrate the effectiveness of the method on two Atari games, a continuous control benchmark and the CarRacing-v0 benchmark. On the latter we surpass the best previously reported score while using significantly fewer episodes.}
}
@article{BENMOUSSA2020293,
title = {MSIDN: Mitigation of Sophisticated Interest flooding-based DDoS attacks in Named Data Networking},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {293-306},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.01.043},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19328729},
author = {Ahmed Benmoussa and Abdou el Karim Tahari and Chaker Abdelaziz Kerrache and Nasreddine Lagraa and Abderrahmane Lakas and Rasheed Hussain and Farhan Ahmad},
keywords = {Named Data Networking, Denial-of-Service attacks, Interest Flooding Attack, NDN Security},
abstract = {Named Data Networking (NDN) is a promising candidate for Future Internet Architecture (FIA), where the focus of communication is the content itself rather than the source of the requested content. NDN is one of the implementations of Information-Centric Networking (ICN). Among other salient features, NDN provides intrinsic security where security is provided to the content directly, rather than securing the communication channel. However, despite promising features offered by NDN, it is still susceptible to various Denial of Service (DoS) attacks, mainly Interest Flooding Attacks (IFA). Various mitigation solutions exist in the literature; however, legitimate users and their traffic are usually affected by these solutions. In this paper, we propose a lightweight mechanism called MSIDN, to mitigate sophisticated interest flooding-based DoS and Distributed DoS (DDoS) attacks in NDN. MSIDN aims to mitigate attacks at the source of communication without affecting the legitimate users. MSIDN relies on data producers’ feedback which is used by the routers to employ precise rate-limiting and block the attackers. Extensive simulations were conducted to evaluate the proposed MSIDN in terms of its robustness during various attack scenarios, dealing with malicious traffic without affecting the legitimate requests, and mitigating attacks at the source side of the communication.}
}
@article{BORNMANN2019325,
title = {Do altmetrics assess societal impact in a comparable way to case studies? An empirical test of the convergent validity of altmetrics based on data from the UK research excellence framework (REF)},
journal = {Journal of Informetrics},
volume = {13},
number = {1},
pages = {325-340},
year = {2019},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1751157718302700},
author = {Lutz Bornmann and Robin Haunschild and Jonathan Adams},
keywords = {Bibliometrics, Altmetrics, MHq, Societal impact, Case studies, Research excellence framework, REF2014},
abstract = {Altmetrics have been proposed as a way to assess the societal impact of research. Although altmetrics are already in use as impact or attention metrics in different contexts, it is still not clear whether they really capture or reflect societal impact. This study is based on altmetrics, citation counts, research output and case study data from the UK Research Excellence Framework (REF), and peers’ REF assessments of research output and societal impact. We investigated the convergent validity of altmetrics by using two REF datasets: publications submitted as research output (PRO) to the REF and publications referenced in case studies (PCS). Case studies, which are intended to demonstrate societal impact, should cite the most relevant research papers. We used the MHq’ indicator for assessing impact – an indicator which has been introduced for count data with many zeros. The results of the first part of the analysis show that news media as well as mentions on Facebook, in blogs, in Wikipedia, and in policy-related documents have higher MHq’ values for PCS than for PRO. Thus, the altmetric indicators seem to have convergent validity for these data. In the second part of the analysis, altmetrics have been correlated with REF reviewers’ average scores on PCS. The negative or close to zero correlations question the convergent validity of altmetrics in that context. We suggest that they may capture a different aspect of societal impact (which can be called unknown attention) to that seen by reviewers (who are interested in the causal link between research and action in society).}
}
@article{LEMAY201826,
title = {Survey of publicly available reports on advanced persistent threat actors},
journal = {Computers & Security},
volume = {72},
pages = {26-59},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817301608},
author = {Antoine Lemay and Joan Calvet and François Menet and José M. Fernandez},
keywords = {Advanced Persistent Threat (APT), Cyber espionage, Cyber attacks, Targeted attacks, Targeted malware},
abstract = {The increase of cyber attacks for the purpose of espionage is a growing threat. Recent examples, such as hacking of the Democratic National Committee and indicting by the FBI of Chinese military personnel for cyber economic espionage, are testaments of the severity of the problem. Unfortunately, research on the topic of Advanced Persistent Threats (APT) is complicated due to the fact that information is fragmented across a large number of Internet resources. This paper aims at providing a comprehensive survey of open source publications related to APT actors and their activities, focusing on the APT activities, rather than research on defensive or detective measures. It is intended to serve as a quick reference on the state of the knowledge of APT actors, where interested researchers can find what primary sources are most relevant to their research. The paper covers publications related to around 40 APT groups from multiple regions across the globe. A short summary of the main findings of each publication is presented.}
}
@article{WILCOCK2022110995,
title = {Oxidative stress from DGAT1 oncoprotein inhibition in melanoma suppresses tumor growth when ROS defenses are also breached},
journal = {Cell Reports},
volume = {39},
number = {12},
pages = {110995},
year = {2022},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2022.110995},
url = {https://www.sciencedirect.com/science/article/pii/S2211124722007811},
author = {Daniel J. Wilcock and Andrew P. Badrock and Chun W. Wong and Rhys Owen and Melissa Guerin and Andrew D. Southam and Hannah Johnston and Brian A. Telfer and Paul Fullwood and Joanne Watson and Harriet Ferguson and Jennifer Ferguson and Gavin R. Lloyd and Andris Jankevics and Warwick B. Dunn and Claudia Wellbrock and Paul Lorigan and Craig Ceol and Chiara Francavilla and Michael P. Smith and Adam F.L. Hurlstone},
keywords = {melanoma, lipid droplets, DGAT1, fatty acids, reactive oxygen species, SOD1, oxidative stress},
abstract = {Summary
Dysregulated cellular metabolism is a cancer hallmark for which few druggable oncoprotein targets have been identified. Increased fatty acid (FA) acquisition allows cancer cells to meet their heightened membrane biogenesis, bioenergy, and signaling needs. Excess FAs are toxic to non-transformed cells but surprisingly not to cancer cells. Molecules underlying this cancer adaptation may provide alternative drug targets. Here, we demonstrate that diacylglycerol O-acyltransferase 1 (DGAT1), an enzyme integral to triacylglyceride synthesis and lipid droplet formation, is frequently up-regulated in melanoma, allowing melanoma cells to tolerate excess FA. DGAT1 over-expression alone transforms p53-mutant zebrafish melanocytes and co-operates with oncogenic BRAF or NRAS for more rapid melanoma formation. Antagonism of DGAT1 induces oxidative stress in melanoma cells, which adapt by up-regulating cellular reactive oxygen species defenses. We show that inhibiting both DGAT1 and superoxide dismutase 1 profoundly suppress tumor growth through eliciting intolerable oxidative stress.}
}
@article{BASILE2023103321,
title = {Design, implementation, and automation of a risk management approach for man-at-the-End software protection},
journal = {Computers & Security},
volume = {132},
pages = {103321},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103321},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823002316},
author = {Cataldo Basile and Bjorn {De Sutter} and Daniele Canavese and Leonardo Regano and Bart Coppens},
keywords = {Software protection, Standardization, Risk framing, Risk assessment, Risk mitigation},
abstract = {The last years have seen an increase in Man-at-the-End (MATE) attacks against software applications, both in number and severity. However, software protection, which aims at mitigating MATE attacks, is dominated by fuzzy concepts and security-through-obscurity. This paper presents a rationale for adopting and standardizing the protection of software as a risk management process according to the NIST SP800-39 approach. We examine the relevant constructs, models, and methods needed for formalizing and automating the activities in this process in the context of MATE software protection. We highlight the open issues that the research community still has to address. We discuss the benefits that such an approach can bring to all stakeholders. In addition, we present a Proof of Concept (PoC) decision support system that instantiates many of the discussed construct, models, and methods and automates many activities in the risk analysis methodology for the protection of software. Despite being a prototype, the PoC’s validation with industry experts indicated that several aspects of the proposed risk management process can already be formalized and automated with our existing toolbox and that it can actually assist decision making in industrially relevant settings.}
}
@incollection{TCHAKOUNTE2017101,
title = {Chapter 6 - Supervised Learning Based Detection of Malware on Android},
editor = {Man Ho Au and Kim-Kwang Raymond Choo},
booktitle = {Mobile Security and Privacy},
publisher = {Syngress},
address = {Boston},
pages = {101-154},
year = {2017},
isbn = {978-0-12-804629-6},
doi = {https://doi.org/10.1016/B978-0-12-804629-6.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046296000067},
author = {F. Tchakounté and F. Hayata},
keywords = {Detection, Permission, Resource, Risk, Supervised learning},
abstract = {This chapter aims to present a new approach for detecting Android malware by relying permissions and supervised learning techniques. For that, we present security and its flaws in the Android system. Then we present concepts around machine learning and how they can be used for malware detection in general. We discuss works using permissions as key feature for the characterization of applications to detect malicious behavior. We present a detection system combining the proportion of requested permissions and risks induced on resources. This system requires the user to specify resources to protect and inform in an understandable way, activities performed in background with those permissions. We pass through some graphical interfaces of the implementation, then elucidate results concerning detection and prediction performance with the support of learning algorithms. We compare these results against well-known antiviruses and related solutions on the same collected datasets of malicious and benign applications. It is revealed that our system outperforms most of them and it is able to detect zero-day malware. Therefore it constitutes an interesting step forward to help users understanding the risks induced on resources and to help them detecting malware.}
}
@article{2023I,
title = {Full Issue PDF},
journal = {JACC: Clinical Electrophysiology},
volume = {9},
number = {12},
pages = {I-CCLXXII},
year = {2023},
issn = {2405-500X},
doi = {https://doi.org/10.1016/S2405-500X(23)00891-5},
url = {https://www.sciencedirect.com/science/article/pii/S2405500X23008915}
}
@article{KHODATARS2021104949,
title = {Deep learning for neuroimaging-based diagnosis and rehabilitation of Autism Spectrum Disorder: A review},
journal = {Computers in Biology and Medicine},
volume = {139},
pages = {104949},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104949},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521007435},
author = {Marjane Khodatars and Afshin Shoeibi and Delaram Sadeghi and Navid Ghaasemi and Mahboobeh Jafari and Parisa Moridian and Ali Khadem and Roohallah Alizadehsani and Assef Zare and Yinan Kong and Abbas Khosravi and Saeid Nahavandi and Sadiq Hussain and U. Rajendra Acharya and Michael Berk},
keywords = {Autism spectrum disorder, Diagnosis, Rehabilitation, Deep learning, Neuroimaging, Neuroscience},
abstract = {Accurate diagnosis of Autism Spectrum Disorder (ASD) followed by effective rehabilitation is essential for the management of this disorder. Artificial intelligence (AI) techniques can aid physicians to apply automatic diagnosis and rehabilitation procedures. AI techniques comprise traditional machine learning (ML) approaches and deep learning (DL) techniques. Conventional ML methods employ various feature extraction and classification techniques, but in DL, the process of feature extraction and classification is accomplished intelligently and integrally. DL methods for diagnosis of ASD have been focused on neuroimaging-based approaches. Neuroimaging techniques are non-invasive disease markers potentially useful for ASD diagnosis. Structural and functional neuroimaging techniques provide physicians substantial information about the structure (anatomy and structural connectivity) and function (activity and functional connectivity) of the brain. Due to the intricate structure and function of the brain, proposing optimum procedures for ASD diagnosis with neuroimaging data without exploiting powerful AI techniques like DL may be challenging. In this paper, studies conducted with the aid of DL networks to distinguish ASD are investigated. Rehabilitation tools provided for supporting ASD patients utilizing DL networks are also assessed. Finally, we will present important challenges in the automated detection and rehabilitation of ASD and propose some future works.}
}
@article{ZAVALA2019544,
title = {Visual analytics for identifying product disruptions and effects via social media},
journal = {International Journal of Production Economics},
volume = {208},
pages = {544-559},
year = {2019},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2018.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925527318304985},
author = {Araceli Zavala and Jose Emmanuel Ramirez-Marquez},
keywords = {Visual analytics, Product recall, Social media, Statistical Process Control},
abstract = {In the last decade, there have been high profile product safety events that captured public attention on social networks. Researchers have attempted various studies on consumers' reaction to product recalls but hardly any studies were conducted to find out a way to identify recalls by using users' comments specifically on social media. The earlier a company can detect a product disruption, the more a company can do in preparation to reduce its impact. In this paper, we propose a visualization framework capable of identifying a possible product recall via social networks, like Facebook or Twitter. Customers' comments found in data that express a negative sentiment are considered as non-conforming observations and plotted on a p-chart, which helps to identify when the proportion of negative comments get out of control and, as a result, a company can diminish the response time. To check its viability, we conducted three event studies of well-known companies that have experienced product recalls. The results show that customers’ negative sentiments could be monitored with the aim of predicting when a product might necessitate a recall as well as reducing decision-makers response time.}
}
@incollection{CHANG20243,
title = {Chapter 1 - Introduction to artificial intelligence for cardiovascular clinicians},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {3-120},
year = {2024},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390534300010X},
author = {Anthony C. Chang and Alfonso Limon},
keywords = {Artificial intelligence, Cardiovascular clinicians, Deep learning technology, Human-machine intelligence continuum, Machine learning, Neuroscience},
abstract = {The impressive gains in deep learning (DL) started in 2012 and its successful utilization in image interpretation have led to the current momentum for artificial intelligence (AI) awareness and adoption. In 2016, Google DeepMind's AlphaGo software soundly defeated the best human Go champion Lee Sedol to introduce the capability of DL outside of image interpretation. More recently, there have been impressive exponential advances in natural language processing with transformer tools such as GPT-3, GPT-4, and now ChatGPT. DeepMind and its AlphaFold AI tool has been able to predict the three-dimensional (3D) structure of proteins since 2021 and was Science magazine's “Breakthrough of the Year.” All of these AI accomplishments heralded the recent new era in AI. Major universities with AI departments (such as Stanford, MIT, and Carnegie Mellon) and technology giants (such as IBM, Apple, Facebook, and Microsoft in the United States as well as other large companies such as Baidu, Alibaba, and Tencent [BAT] in China) are all fervidly exploring real-life applications of AI. There is also a movement to democratize AI so that “no-code platforms” can accommodate people who do not know how to code [1].}
}
@article{GONCALVES201938,
title = {Deep learning in exchange markets},
journal = {Information Economics and Policy},
volume = {47},
pages = {38-51},
year = {2019},
note = {The Economics of Artificial Intelligence and Machine Learning},
issn = {0167-6245},
doi = {https://doi.org/10.1016/j.infoecopol.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167624518300702},
author = {Rui Gonçalves and Vitor Miguel Ribeiro and Fernando Lobo Pereira and Ana Paula Rocha},
keywords = {Deep learning, Betting exchange, Market depth, Classification},
abstract = {We present the implementation of a short-term forecasting system of price movements in exchange markets using market depth data and a systematic procedure to enable a fully automated trading system. Three types of Deep Learning (DL) Neural Network (NN) methodologies are trained and tested: Deep NN Classifier (DNNC), Long Short-Term Memory (LSTM) and Convolutional NN (CNN). Although the LSTM is more suitable for multivariate time series analysis from a theoretical point of view, test results indicate that the CNN has on average the best predictive power in the case study under analysis, which is the UK to Win Horse Racing market during pre-live stage in the world’s most relevant betting exchange. Implications from the generalized use of automated trading systems in betting exchange markets are discussed.}
}
@article{ALEIADEH2024103927,
title = {GeniGraph: A genetic-based novel security defense resource allocation method for interdependent systems modeled by attack graphs},
journal = {Computers & Security},
volume = {144},
pages = {103927},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103927},
url = {https://www.sciencedirect.com/science/article/pii/S016740482400230X},
author = {Mohammad Ryiad Al-Eiadeh and Mustafa Abdallah},
keywords = {Attack graphs, Security resource allocation, Guiding security decision-makers, Proactive security defense, Meta-heuristic for learning attacks, Markov Random Field, Intedependent systems},
abstract = {We design a resource allocation framework for securing interdependent systems managed by multiple defenders. Our framework models these multi-defender interdependent systems with the notion of attack graphs. We propose three defense scenarios that are derived from the top attack paths that defenders predict, based on their system knowledge, which attackers may consider to launch their attacks. Furthermore, we propose a defense method with low sensitivity to the number of concurrent attacks, based on a graph-theoretical notion known as the Markov random field (MRF). We elucidate the advantages gained from our decision-making framework through comprehensive evaluation experiments on fourteen attack graphs (that includes multiple real-world interdependent systems). In our evaluation, we compare different defense scenarios and provide information about the most effective resource allocation approach against each attack scenario. In particular, we quantify the level of security improvement under our defense methods compared to three well-known resource allocation algorithms. Our experimental results show that our framework surpasses these resource allocation algorithms. In particular, our proposed defense leads to an average relative reduction in the expected security cost of 72% under equal initial investments on all edges. Moreover, it leads to an average relative reduction in the expected security cost of 78% under random initial investments on all edges. Under high security budget, our proposed defense approach has a relative reduction above 94% for 10 of the 14 attack graphs. These results signify notable enhancements in security resource allocation which contributes to improved security decision-making.}
}
@article{CHIARELLO2021121177,
title = {Towards ESCO 4.0 – Is the European classification of skills in line with Industry 4.0? A text mining approach},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121177},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121177},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006107},
author = {Filippo Chiarello and Gualtiero Fantoni and Terence Hogarth and Vito Giordano and Liga Baltina and Irene Spada},
keywords = {Industry 4.0, Technological change, Employment, Skill analysis, Text mining},
abstract = {ESCO is a multilingual classification of Skills, Competences, Qualifications, and Occupations created by the European Commission to improve the supply of information on skills demand in the labour market. It is designed to assist individuals, employers, universities and training providers by giving them up to date and standardized information on skills. Rapid technological change means that ESCO needs to be updated in a timely manner. Evidence is presented here of how text-mining techniques can be applied to the analysis of data on emerging skill needs arising from Industry 4.0 to ensure that ESCO provides information which is current. The alignment between ESCO and Industry 4.0 technological trends is analysed. Using text mining techniques, information is extracted on Industry 4.0 technologies from: (i) two versions of ESCO (v1.0 - v1.1.); and (ii) from the 4.0 related scientific literature. These are then compared to identify potential data gaps in ESCO. The findings demonstrate that text mining applied on scientific literature to extract technology trends, can help policy makers to provide more up-to-date labour market intelligence.}
}
@article{2024I,
title = {Full issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {17},
number = {7},
pages = {I-CXXXV},
year = {2024},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(24)00224-9},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X24002249}
}
@article{JANSCHPORTO20207418,
title = {Robust Decentralized Switching Control of UAVs using UWB-based Localization and Cooperation⁎⁎The authors were partially supported by NSF Grant 1629949.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {7418-7423},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1281},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320316839},
author = {Joao P. Jansch-Porto and Geir E. Dullerud},
keywords = {Control of switched systems, Decentralized control, Flying robots, Multi-vehicle systems, Networked robotic systems, Robust control applications},
abstract = {In this paper, we implement a switched decentralized controller, along with a proposed communication protocol, to control a nested multi-agent system without the need for a centralized processing node. More specifically, we apply a recently developed method for switched systems synthesis, which gives exact conditions for existence of a block-lower triangular path-dependent controller with L2 induced norm performance. The synthesis conditions are given in the form of a semidefinite program (SDP), which is computed offline for a predefined switching sequence. Each robot is equipped with a ultra-wideband (UWB) unit, which allows it to both estimate its position and communicate with other robots.}
}
@article{BHATTACHARYA2022103115,
title = {DDoS attack resisting authentication protocol for mobile based online social network applications},
journal = {Journal of Information Security and Applications},
volume = {65},
pages = {103115},
year = {2022},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2022.103115},
url = {https://www.sciencedirect.com/science/article/pii/S2214212622000084},
author = {Munmun Bhattacharya and Sandip Roy and Ashok Kumar Das and Samiran Chattopadhyay and Soumya Banerjee and Ankush Mitra},
keywords = {Mobile online social networks, DDoS attacks, Authentication, Key-refilling, Machine learning, NS3 simulation, ProVerif simulation},
abstract = {The rapid development of smartphone technology and the Internet services in mobile devices facilitates easy access to online social networking (OSN) sites anytime, anywhere. At the same time, this allures the adversaries to exploit the OSNs as a soft target for easy execution of various attacks that can quickly spread to a large number of users. In distributed denial-of-service (DDoS) attacks, an adversary aims to overwhelm the normal traffic of a targeted server with a flood of fake login messages so that the associated Internet service or website turns inoperable. In this paper, we propose a secure and lightweight authentication scheme (PRDoS) that resists DDoS and other security attacks in mobile OSN environments. We provide a multi-faceted solution towards the remedy of DDoS attacks in the OSN environment. After a certain threshold, the scheme discards further user login attempts and blocks an adversary who intends to overload the network server. We use the pre-loaded shadow identity and emergency key pairs, and a key-refilling strategy that rebuilds the essential synchronization between a blocked naive user and the OSN server. This technique restores the intended un-linkability property of the protocol. Using NS3 simulation, we study the impact of DDoS attackers on network throughput and network delay. Moreover, we validate and compare the proposed scheme against state-of-the-art solutions using the real attacks and benign datasets. We use the Canadian Institute for Cybersecurity (CIC) DoS dataset 2017, which is generated by capturing the normal and DoS attack packets separately with subsequent pre-processed for testing. We also use the machine learning (ML) algorithms, such as K-Nearest Neighbor (KNN), Gaussian Naive Bayes, and Multilayer Perceptron (MLP) to demonstrate the performance of the proposed solution in a practical attack detection scenario. We observe that these algorithms provide 97.05%, 95.48%, and 96.6% DDoS attack detection accuracy, respectively.}
}
@article{DOUHA2022100588,
title = {A survey on blockchain, SDN and NFV for the smart-home security},
journal = {Internet of Things},
volume = {20},
pages = {100588},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100588},
url = {https://www.sciencedirect.com/science/article/pii/S2542660522000750},
author = {N’guessan Yves-Roland Douha and Monowar Bhuyan and Shigeru Kashihara and Doudou Fall and Yuzo Taenaka and Youki Kadobayashi},
keywords = {Smart homes, IoT, Privacy, Security, Trust, Blockchain, SDN, NFV},
abstract = {Due to millions of loosely coupled devices, the smart-home security is gaining the attention of industry professionals, attackers, and academic researchers. The smart home is a typical home where many sensors, actuators, and IoT devices are used to automate home users’ daily activities. Although a smart home provides comfort, safety, and satisfaction to users, it opens up multiple challenging security issues when automating and offering intelligent services. Recent studies have investigated not only blockchain but SDN and NFV to address these challenges. We present a comprehensive survey on blockchain, SDN, and NFV for smart-home security. The paper also proposes a new architecture of the smart-home security. First, we describe the features of the smart home and its current security issues. Next, we outline the characteristics of blockchain, SDN, and NFV, including their contribution to improving the smart-home security. While SDN enhances the management and access control of the home network by providing a programmable controller to home nodes, NFV implements the functions of network appliances (e.g., network monitoring, firewall) as virtual machines and ensures the high availability of the network. Blockchain reinforces IoT data’s privacy, integrity, and security and improves the trust in transactions among untrusted IoT devices. Finally, we discuss open issues and challenges in the field and propose recommendations towards high-level security for the smart home.}
}
@article{2024100875,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {3},
number = {2},
pages = {100875},
year = {2024},
issn = {2772-963X},
doi = {https://doi.org/10.1016/S2772-963X(24)00053-X},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X2400053X}
}
@article{SAQUETE2020112943,
title = {Fighting post-truth using natural language processing: A review and open challenges},
journal = {Expert Systems with Applications},
volume = {141},
pages = {112943},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112943},
url = {https://www.sciencedirect.com/science/article/pii/S095741741930661X},
author = {Estela Saquete and David Tomás and Paloma Moreda and Patricio Martínez-Barco and Manuel Palomar},
keywords = {Natural language processing, Fake news, Post-truth, Deception detection, Automatic fact-checking, Clickbait detection, Stance detection, Credibility, Human language technologies, Applied computing, Document management and text processing, Document capture, Document analysis},
abstract = {Post-truth is a term that describes a distorting phenomenon that aims to manipulate public opinion and behavior. One of its key engines is the spread of Fake News. Nowadays most news is rapidly disseminated in written language via digital media and social networks. Therefore, to detect fake news it is becoming increasingly necessary to apply Artificial Intelligence (AI) and, more specifically Natural Language Processing (NLP). This paper presents a review of the application of AI to the complex task of automatically detecting fake news. The review begins with a definition and classification of fake news. Considering the complexity of the fake news detection task, a divide-and-conquer methodology was applied to identify a series of subtasks to tackle the problem from a computational perspective. As a result, the following subtasks were identified: deception detection; stance detection; controversy and polarization; automated fact checking; clickbait detection; and, credibility scores. From each subtask, a PRISMA compliant systematic review of the main studies was undertaken, searching Google Scholar. The various approaches and technologies are surveyed, as well as the resources and competitions that have been involved in resolving the different subtasks. The review concludes with a roadmap for addressing the future challenges that have emerged from the analysis of the state of the art, providing a rich source of potential work for the research community going forward.}
}
@article{2020I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {13},
number = {2, Part 1},
pages = {I-CXCIV},
year = {2020},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(20)30043-7},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20300437}
}
@article{WANG2021102859,
title = {Survey of security supervision on blockchain from the perspective of technology},
journal = {Journal of Information Security and Applications},
volume = {60},
pages = {102859},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102859},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621000922},
author = {Yu Wang and Gaopeng Gou and Chang Liu and Mingxin Cui and Zhen Li and Gang Xiong},
keywords = {Blockchain technology, Security supervision, Peer-to-peer (P2P) network, Transaction, User identity, Heuristics, Graph theory},
abstract = {Due to decentralization, immutability, circulation, anonymity, blockchain technology has become a hot topic but also a hotbed of various cyber-crimes. Many perpetrators attack blockchain to steal cryptocurrencies or use anonymous addresses to conduct illicit financial transactions or receive ransoms while hiding their identities. Since blockchain technology has not developed for a long time, the security issues in this area cannot be well resolved through its own mechanisms, which brings great challenges to protect the security of blockchain and users. Although there are some protective measures to prevent attackers from attacking, most of them are proposed after attacks, and it is impossible to find the masterminds behind modern cyber-crimes, so it is necessary to continuously monitor suspicious nodes or users. In this paper, we first present a systematic overview of blockchain technology and security issues according to the four-layer structure, and explain the problem of security supervision of blockchain. Subsequently, we divide the key technologies for security supervision of blockchain into three aspects: node discovery technology on the network layer, data analysis technology of transaction records on the transaction layer, and network traffic analysis technology on the application layer. In terms of each aspect, we summarize the studies from various angles according to its characteristics. In the end, we discuss the relationship between blockchain and traditional law. Moreover, we present the challenges of security supervision and possible future research directions in this field.}
}
@article{ABOUKHALIL2021110882,
title = {On the impact of release policies on bug handling activity: A case study of Eclipse},
journal = {Journal of Systems and Software},
volume = {173},
pages = {110882},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110882},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302727},
author = {Zeinab {Abou Khalil} and Eleni Constantinou and Tom Mens and Laurence Duchien},
keywords = {Bug handling process, Rapid release cycle, Feature freeze, Continuous software development, Software maintenance, Empirical software engineering},
abstract = {Large software projects follow a continuous development process with regular releases during which bugs are handled. In recent years, many software projects shifted to rapid releases that reduce time-to-market and claim a faster delivery of fixed issues, but also have a shorter period to address bugs. To better understand the impact of rapid releases on bug handling activity, we empirically analyze successive releases of the Eclipse Core projects, focusing on the bug handling rates and durations as well as the feature freeze period. We study the impact of Eclipse’s transition from a yearly to quarterly release cycle. We confirm our findings through feedback received from five Eclipse Core maintainers. Among others, our results reveal that Eclipse’s bug handling process is becoming more stable over time, with a decreasing number of reported bugs before releases, an increasing bug fixing rate and an increasingly balanced bug handling workload before and after releases. The transition to a quarterly release cycle continued to improve bug handling. In addition, more effort is spent on bug fixing during the feature freeze period, while the bug handling rates do not differ between both periods.}
}
@article{BUI2024103754,
title = {Agriculture 4.0 and beyond: Evaluating cyber threat intelligence sources and techniques in smart farming ecosystems},
journal = {Computers & Security},
volume = {140},
pages = {103754},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103754},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824000555},
author = {Hang Thanh Bui and Hamed Aboutorab and Arash Mahboubi and Yansong Gao and Nazatul Haque Sultan and Aufeef Chauhan and Mohammad Zavid Parvez and Michael Bewong and Rafiqul Islam and Zahid Islam and Seyit A. Camtepe and Praveen Gauravaram and Dineshkumar Singh and M. {Ali Babar} and Shihao Yan},
keywords = {Cyber threat intelligence (CTI), Systematic literature review, virtual Chief Information Security Officer (vCISO), Agriculture 4.0, Agriculture 5.0, Smart farming infrastructures (SFIs), Digital twin technology},
abstract = {The digitisation of agriculture, integral to Agriculture 4.0, has brought significant benefits while simultaneously escalating cybersecurity risks. With the rapid adoption of smart farming technologies and infrastructure, the agricultural sector has become an attractive target for cyberattacks. This paper presents a systematic literature review that assesses the applicability of existing cyber threat intelligence (CTI) techniques within smart farming infrastructures (SFIs). We develop a comprehensive taxonomy of CTI techniques and sources, specifically tailored to the SFI context, addressing the unique cyber threat challenges in this domain. A crucial finding of our review is the identified need for a virtual Chief Information Security Officer (vCISO) in smart agriculture. While the concept of a vCISO is not yet established in the agricultural sector, our study highlights its potential significance. The implementation of a vCISO could play a pivotal role in enhancing cybersecurity measures by offering strategic guidance, developing robust security protocols, and facilitating real-time threat analysis and response strategies. This approach is critical for safeguarding the food supply chain against the evolving landscape of cyber threats. Our research underscores the importance of integrating a vCISO framework into smart farming practices as a vital step towards strengthening cybersecurity. This is essential for protecting the agriculture sector in the era of digital transformation, ensuring the resilience and sustainability of the food supply chain against emerging cyber risks.}
}
@article{NASAR2019102088,
title = {Textual keyword extraction and summarization: State-of-the-art},
journal = {Information Processing & Management},
volume = {56},
number = {6},
pages = {102088},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102088},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319300044},
author = {Zara Nasar and Syed Waqar Jaffry and Muhammad Kamran Malik},
keywords = {Automatic keyword extraction, Text summarization, Deep Learning},
abstract = {With the advent of Web 2.0, there exist many online platforms that results in massive textual data production such as social networks, online blogs, magazines etc. This textual data carries information that can be used for betterment of humanity. Hence, there is a dire need to extract potential information out of it. This study aims to present an overview of approaches that can be applied to extract and later present these valuable information nuggets residing within text in brief, clear and concise way. In this regard, two major tasks of automatic keyword extraction and text summarization are being reviewed. To compile the literature, scientific articles were collected using major digital computing research repositories. In the light of acquired literature, survey study covers early approaches up to all the way till recent advancements using machine learning solutions. Survey findings conclude that annotated benchmark datasets for various textual data-generators such as twitter and social forms are not available. This scarcity of dataset has resulted into relatively less progress in many domains. Also, applications of deep learning techniques for the task of automatic keyword extraction are relatively unaddressed. Hence, impact of various deep architectures stands as an open research direction. For text summarization task, deep learning techniques are applied after advent of word vectors, and are currently governing state-of-the-art for abstractive summarization. Currently, one of the major challenges in these tasks is semantic aware evaluation of generated results.}
}
@article{2022I,
title = {Full issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {15},
number = {4},
pages = {I-CLXVIII},
year = {2022},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(22)00139-5},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X22001395}
}
@incollection{2016521,
title = {Glossary},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP Study Guide (Third Edition)},
publisher = {Syngress},
edition = {Third Edition},
address = {Boston},
pages = {521-557},
year = {2016},
isbn = {978-0-12-802437-9},
doi = {https://doi.org/10.1016/B978-0-12-802437-9.00011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128024379000114}
}
@article{LIU2021103079,
title = {A fast all-packets-based DDoS attack detection approach based on network graph and graph kernel},
journal = {Journal of Network and Computer Applications},
volume = {185},
pages = {103079},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103079},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001016},
author = {Xinqian Liu and Jiadong Ren and Haitao He and Bing Zhang and Chen Song and Yunxue Wang},
keywords = {Network security, Fast DDoS attack detection, Network graph based all packets, Directed weisfeiler-lehman graph kernel, Dynamic threshold mechanism},
abstract = {DDoS attack detection methods play a very important role in protecting computer network security. However, the existing flow-based DDoS attack detection methods face the non-negligible time delay and are not general for different types of DDoS attacks at different rates. In order to fill this research gap, a fast all-packets-based DDoS attack detection approach (FAPDD) is proposed. The FAPDD firstly designs a new time series network graph model to effectively simplify the processing of network traffic handling compared with the flow-based detections. Furthermore, it is the first time that the directed Weisfeiler-Lehman graph kernel is built for measuring the divergence between the current network graph and the normalization network graphs. Due to the new graph model and kernel measurement method to judge network changes, the different types and rates of DDoS attacks can be especially detected. In addition, the dynamic threshold and freezing mechanism are constructed to display standard traffic changes and prevent the pollution of attack traffic to the standard network. Finally, a number of real DDoS attack datasets are applied to evaluate the effectiveness of the proposed method, as well as the overall time efficiency and detection effect. Compared with other methods, the FAPDD can better meet the real-time requirements and achieve good detection effects in different types of DDoS attacks with different attack rates.}
}
@article{LIMA2021103724,
title = {Next generation antivirus endowed with bitwise morphological extreme learning machines},
journal = {Microprocessors and Microsystems},
volume = {81},
pages = {103724},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103724},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120308693},
author = {Sidney M.L. Lima and Danilo M. Souza and Ricardo P. Pinheiro and Sthéfano H.M.T. Silva and Petrônio G. Lopes and Rafael D.T. {de Lima} and Jemerson R. {de Oliveira} and Thyago de A. Monteiro and Sérgio M.M. Fernandes and Edison de Q. Albuquerque and Washington W.A. {da Silva} and Wellington P. {dos Santos}},
keywords = {High-performance, NGAV, Malwares, Artificial neural networks, Real-time malware detection, Computer forensics},
abstract = {Background and Objective
Every second, on average, 8 (eight) new malware are created. So, our goal is to propose an antivirus, endowed with artificial intelligence, able of identifying malwares through models based on fast training and high-performance neural networks.
Methods
Our NGAV (Next Generation Antivirus) is equipped with an authorial ELM (Extreme Learning Morphological) machine. Our bmELMs (Bitwise-Morphological ELMs) are inspired by the image processing theory of Mathematical Morphology. We claim that bmELMs are able to adapt in any machine learning dataset. Inspired by Mathematical Morphology, our bmELMs are capable of modeling any form present at the decisions boundaries of neural networks.
Results
Our bmELMs results are compared with classical ELMs and evaluated through widely used classification metrics. Our antivirus, provided with Bitwise-Morphology, achieves an average accuracy of 97.88%, 93.07%, 93.07% and 91.74% in malware detection of PE (Portable Executable), Java, JavaScript and PHP, respectively.
Conclusions
Our NGAV enables high performance, large capacity of parallelism, and simple, low-power architecture with low power consumption. We concluded that our Bitwise-Morphology assists to the main requirements for the proper operation and confection of antivirus in hardware.}
}
@incollection{2020503,
title = {Index},
editor = {Anthony C. Chang},
booktitle = {Intelligence-Based Medicine},
publisher = {Academic Press},
pages = {503-521},
year = {2020},
isbn = {978-0-12-823337-5},
doi = {https://doi.org/10.1016/B978-0-12-823337-5.00026-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233375000263}
}
@article{DWIVEDI2021101994,
title = {Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
journal = {International Journal of Information Management},
volume = {57},
pages = {101994},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121930917X},
author = {Yogesh K. Dwivedi and Laurie Hughes and Elvira Ismagilova and Gert Aarts and Crispin Coombs and Tom Crick and Yanqing Duan and Rohita Dwivedi and John Edwards and Aled Eirug and Vassilis Galanos and P. Vigneswara Ilavarasan and Marijn Janssen and Paul Jones and Arpan Kumar Kar and Hatice Kizgin and Bianca Kronemann and Banita Lal and Biagio Lucini and Rony Medaglia and Kenneth {Le Meunier-FitzHugh} and Leslie Caroline {Le Meunier-FitzHugh} and Santosh Misra and Emmanuel Mogaji and Sujeet Kumar Sharma and Jang Bahadur Singh and Vishnupriya Raghavan and Ramakrishnan Raman and Nripendra P. Rana and Spyridon Samothrakis and Jak Spencer and Kuttimani Tamilmani and Annie Tubadji and Paul Walton and Michael D. Williams},
keywords = {Artificial intelligence, AI, Cognitive computing, Expert systems, Machine learning, Research agenda},
abstract = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.}
}
@incollection{REDHU2022641,
title = {Chapter 37 - Artificial intelligence: a way forward for agricultural sciences},
editor = {Pradeep Sharma and Dinesh Yadav and Rajarshi Kumar Gaur},
booktitle = {Bioinformatics in Agriculture},
publisher = {Academic Press},
pages = {641-668},
year = {2022},
isbn = {978-0-323-89778-5},
doi = {https://doi.org/10.1016/B978-0-323-89778-5.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897785000076},
author = {Neeru S. Redhu and Zoozeal Thakur and Shikha Yashveer and Poonam Mor},
keywords = {Artificial intelligence, machine learning, deep learning, neural network, precision farming, yield monitoring, field mapping, crop scouting, weather tracking & forecasting, drone analytics, agriculture robot, smart greenhouse management, soil management, moisture monitoring, nutrient monitoring},
abstract = {Artificial intelligence (AI) mimics the cognitive functions of the human brain, that is, learning from experiences using a set of algorithms by identifying hidden patterns. AI has already demonstrated its game-changing capabilities in many fields such as the banking sector, healthcare, e-commerce, among others. Recent advances in both computer hardware and large-scale generation of biological data have created room for the applicability of AI in agriculture by employing methods like machine learning, deep learning, natural language processing, artificial or convoluted neural networks either independently or in combination thereof. Currently, AI is being utilized in multiple domains of agriculture and contributing toward increasing crop yield as well as minimizing the risk associated with crop cultivation. Data generated from different agricultural domains such as weather, soil, temperature, humidity, irrigation, sowing requirements, and other crop-related criterion are being analyzed and integrated for agriculture-related predictions. An increase in crop yield is being achieved not only by predicting the best time for sowing, harvesting, and monitoring crop health but also by reducing the cost of agricultural inputs like chemicals, fertilizers, irrigation, etc., through precision farming. Correspondingly, agricultural risks are also being minimized by addressing issues such as poor rainfall, weed growth, pest attacks, and postharvest losses. Besides, AI is also being used for crop/commodity price forecasting and agricultural automation via robots. The predictive agricultural analytics and chatbots are used to disseminate information to farmers as a part of advisory services by these agribusinesses. This chapter will discuss AI technologies and their various applications for the development of better farm practices to improve plant productivity as well as technological advancements due to AI and its anticipated challenges in the future will be explored.}
}
@incollection{GORALSKI2017587,
title = {Chapter 23 - The Domain Name System},
editor = {Walter Goralski},
booktitle = {The Illustrated Network (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {587-612},
year = {2017},
isbn = {978-0-12-811027-0},
doi = {https://doi.org/10.1016/B978-0-12-811027-0.00023-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128110270000230},
author = {Walter Goralski},
keywords = {DNS servers, IP addresses, DDOS, HTTp, DHCP, LANs, ISPs},
abstract = {In this chapter, you will learn how DNS gives the Internet a more user-friendly way to access resources. We’ll see how names are associated with IP addresses and how applications find this information. We will also see how the openness and necessity of DNS makes the whole Internet more vulnerable to attack. You will learn how DNS servers provide information about local networks, and how this information is distributed and shared on the Internet. We’ll also use show tools to help examine DNS.}
}
@article{SMART202073,
title = {Planet Braitenberg: Experiments in virtual psychology},
journal = {Cognitive Systems Research},
volume = {64},
pages = {73-95},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300358},
author = {Paul R. Smart},
keywords = {Virtual environment, Virtual robotics, Artificial intelligence, Game engine, Computational simulation, Artificial life, Situated cognition, Embodied cognition, Unity},
abstract = {Braitenberg vehicles are simple robotic platforms, equipped with rudimentary sensor and motor components. Such vehicles have typically featured as part of thought experiments that are intended to show how complex behaviours are apt to emerge from the interaction of inner control mechanisms with aspects of bodily structure and features of the wider (extra-agential) environment. The present paper describes a framework for creating Braitenberg-like vehicles, which is built on top of a widely used and freely available game engine, namely, the Unity game engine. The framework can be used to study the behaviour of virtual vehicles within a multiplicity of virtual environments. All aspects of the vehicle’s design, as well as the wider virtual environment in which the vehicle is situated, can be modified during the design phase, as well as at runtime. The result is a general-purpose simulation capability that is intended to provide the foundation for studies in so-called computational situated cognition—a field of study whose primary objective is to support the computational modelling of cognitive processes associated with the physically-embodied, environmentally-embedded, and materially-extended mind.}
}
@article{DAUD2020102716,
title = {Applications of link prediction in social networks: A review},
journal = {Journal of Network and Computer Applications},
volume = {166},
pages = {102716},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102716},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520301909},
author = {Nur Nasuha Daud and Siti Hafizah {Ab Hamid} and Muntadher Saadoon and Firdaus Sahran and Nor Badrul Anuar},
keywords = {, , },
abstract = {Link prediction methods anticipate the likelihood of a future connection between two nodes in a given network. The methods are essential in social networks to infer social interactions or to suggest possible friends to the users. Rapid social network growth trigger link prediction analysis to be more challenging especially with the significant advancement in complex social network modeling. Researchers implement numerous applications related to link prediction analysis in different network contexts such as dynamic network, weighted network, heterogeneous network and cross network. However, link prediction applications namely, recommendation system, anomaly detection, influence analysis and community detection become more strenuous due to network diversity, complex and dynamic network contexts. In the past decade, several reviews on link prediction were published to discuss the algorithms, state-of-the-art, applications, challenges and future directions of link prediction research. However, the discussion was limited to physical domains and had less focus on social network perspectives. To reduce the gap of the existing reviews, this paper aims to provide a comprehensive review and discuss link prediction applications in different social network contexts and analyses, focusing on social networks. In this paper, we also present conventional link prediction measures based on previous researches. Furthermore, we introduce various link prediction approaches and address how researchers combined link prediction as a base method to perform other applications in social networks such as recommender systems, community detection, anomaly detection and influence analysis. Finally, we conclude the review with a discussion on recent researches and highlight several future research directions of link prediction in social networks.}
}
@article{PUSAPATI2018113,
title = {CRISPR Screens Uncover Genes that Regulate Target Cell Sensitivity to the Morphogen Sonic Hedgehog},
journal = {Developmental Cell},
volume = {44},
number = {1},
pages = {113-129.e8},
year = {2018},
issn = {1534-5807},
doi = {https://doi.org/10.1016/j.devcel.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1534580717309887},
author = {Ganesh V. Pusapati and Jennifer H. Kong and Bhaven B. Patel and Arunkumar Krishnan and Andreas Sagner and Maia Kinnebrew and James Briscoe and L. Aravind and Rajat Rohatgi},
keywords = {Hedgehog signaling, CRISPR screen, Smoothened, neural tube patterning, primary cilia, protein trafficking, morphogen signaling, ciliopathy, congenital heart disease, heterotaxy},
abstract = {Summary
To uncover regulatory mechanisms in Hedgehog (Hh) signaling, we conducted genome-wide screens to identify positive and negative pathway components and validated top hits using multiple signaling and differentiation assays in two different cell types. Most positive regulators identified in our screens, including Rab34, Pdcl, and Tubd1, were involved in ciliary functions, confirming the central role for primary cilia in Hh signaling. Negative regulators identified included Megf8, Mgrn1, and an unannotated gene encoding a tetraspan protein we named Atthog. The function of these negative regulators converged on Smoothened (SMO), an oncoprotein that transduces the Hh signal across the membrane. In the absence of Atthog, SMO was stabilized at the cell surface and concentrated in the ciliary membrane, boosting cell sensitivity to the ligand Sonic Hedgehog (SHH) and consequently altering SHH-guided neural cell-fate decisions. Thus, we uncovered genes that modify the interpretation of morphogen signals by regulating protein-trafficking events in target cells.}
}
@article{SPOLADORE2022103690,
title = {An evaluation of agile Ontology Engineering Methodologies for the digital transformation of companies},
journal = {Computers in Industry},
volume = {140},
pages = {103690},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103690},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000872},
author = {Daniele Spoladore and Elena Pessot},
keywords = {Digital transformation, Industry 4.0, Ontology Engineering, Agile methodologies, Organisational learning},
abstract = {Ontologies are increasingly recognised among the key enablers of the digital transformation of knowledge management processes, but still with a low level of adoption in manufacturing companies. Because ontologies and underlying technologies are complex, Ontology Engineering Methodologies (OEMs) provide a set of guidelines to move from an informal to a formal representation of the company’s knowledge base. This study evaluates three agile OEMs, i.e. UPONLite, SAMOD and RapidOWL, in terms of their process and outcome features, i.e. the OEM steps and the expected quality of the ontological models produced. The assessment is performed from the viewpoint of developers of ontology-based technologies in real industrial use cases. Results show that the three agile OEMs reflect different features to effectively support the digital transformation of companies' knowledge management; thus, they cannot be interchangeable. UPONLite is more effective in contexts where there is a lack of skills in OE, with the need for a structured approach in involving domain experts and generating documentation. SAMOD requires a more extended development period, but with several cycles that allow to map different types of knowledge and enable a “try-and-learn” approach. Conversely, RapidOWL lacks a structured sequence of modelling activities and encourages developers to be creative, but at the same time requires higher expertise in OE. Thus, companies and personnel dedicated to OE should choose the methodology according to the main aims guiding their digitalisation process, the current development status, and the level of expertise.}
}
@article{DWIVEDI2022102456,
title = {Climate change and COP26: Are digital technologies and information management part of the problem or the solution? An editorial reflection and call to action},
journal = {International Journal of Information Management},
volume = {63},
pages = {102456},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102456},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221001493},
author = {Yogesh K. Dwivedi and Laurie Hughes and Arpan Kumar Kar and Abdullah M. Baabdullah and Purva Grover and Roba Abbas and Daniela Andreini and Iyad Abumoghli and Yves Barlette and Deborah Bunker and Leona {Chandra Kruse} and Ioanna Constantiou and Robert M. Davison and Rahul De’ and Rameshwar Dubey and Henry Fenby-Taylor and Babita Gupta and Wu He and Mitsuru Kodama and Matti Mäntymäki and Bhimaraya Metri and Katina Michael and Johan Olaisen and Niki Panteli and Samuli Pekkola and Rohit Nishant and Ramakrishnan Raman and Nripendra P. Rana and Frantz Rowe and Suprateek Sarker and Brenda Scholtz and Maung Sein and Jeel Dharmeshkumar Shah and Thompson S.H. Teo and Manoj Kumar Tiwari and Morten Thanning Vendelø and Michael Wade},
keywords = {Climate change, COP26, Digital world, Information management, Information systems, Information technology, Sustainability, Sustainable Development Goals (SDGs)},
abstract = {The UN COP26 2021 conference on climate change offers the chance for world leaders to take action and make urgent and meaningful commitments to reducing emissions and limit global temperatures to 1.5 °C above pre-industrial levels by 2050. Whilst the political aspects and subsequent ramifications of these fundamental and critical decisions cannot be underestimated, there exists a technical perspective where digital and IS technology has a role to play in the monitoring of potential solutions, but also an integral element of climate change solutions. We explore these aspects in this editorial article, offering a comprehensive opinion based insight to a multitude of diverse viewpoints that look at the many challenges through a technology lens. It is widely recognized that technology in all its forms, is an important and integral element of the solution, but industry and wider society also view technology as being part of the problem. Increasingly, researchers are referencing the importance of responsible digitalization to eliminate the significant levels of e-waste. The reality is that technology is an integral component of the global efforts to get to net zero, however, its adoption requires pragmatic tradeoffs as we transition from current behaviors to a more climate friendly society.}
}
@article{VIDANARALAGE2022100109,
title = {AI-based multidisciplinary framework to assess the impact of gamified video-based learning through schema and emotion analysis},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100109},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100109},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000649},
author = {Anjana Junius Vidanaralage and Anuja Thimali Dharmaratne and Shamsul Haque},
keywords = {Artificial intelligence, Education technology, Emotion recognition, Gamification, Schema theory, Video-based learning},
abstract = {Background
As a natural and continual process, the initial learning stages encompass mastering and recalling basic facts. The process proves effective with the integration of new information with pre-existing knowledge characterised as schema to facilitate memory encoding. Additionally, emotions also have the ability to modulate human cognition in terms of learning and memory. The recent advent of gamification in e-learning, which has garnered much scholarly and industrial interest, necessitates a thorough examination between video-based learning and its subsequent implications on schema, emotions, and gamification.
Objectives
The current multidisciplinary research triangulated cognitive psychology, affective science, and education technology with artificial intelligence for evaluating digital learning pedagogy based on memory retrieval accuracy, response time, and emotional valence.
Design
This three-way (2 x 2 x 2) mixed factorial experiment design with repeated measures entailed 64 healthy young adult volunteers (n = 64) with 32 in the schema congruent group and 32 in the schema incongruent group. Additionally, 27 (42%) of the volunteers were males, while 37 (58%) were females with an age range between 20 and 39 years old (mean age 27.78 years, SD = 4.77 years).
Results
The findings demonstrate that the schema congruent group attained a statistically significant and higher retrieval accuracy (p < .001). The delayed recall response time was faster than its immediate recall counterpart (p < .001). Overall, the gamified learning mode depicted more positive emotions compared to non-gamified learning, although both groups primarily portrayed more negative emotions (p = .05).
Implications
The synthesis of current research aimed to recommend an AI-based multidisciplinary framework to assess the impact on adult learners in terms of schema and evaluate their emotions in experiencing gamified or non-gamified video materials as a learning medium. The implications expedited from this research offer valuable insights for diverse stakeholders engaged in the video-based learning ecosystem.}
}