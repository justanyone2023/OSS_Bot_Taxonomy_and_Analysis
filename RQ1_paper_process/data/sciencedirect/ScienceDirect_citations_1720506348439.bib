@article{SMIRNOV202362,
title = {Multimodal prediction of profanity based on speech analysis},
journal = {Procedia Computer Science},
volume = {229},
pages = {62-69},
year = {2023},
note = {12th International Young Scientists Conference in Computational Science, YSC2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923019981},
author = {Ivan Smirnov and Anastasia Laushkina},
keywords = {Speech analysis, profanity prediction, toxic audio, ASR},
abstract = {With increasing multimedia content and social activities, moderation problems increase. There are different approaches to moderation and automation. However, they have limitations in terms of usage in real-time. The analysis of scientific papers revealed that most of the more common approaches solve the task of detection instead of prediction by considering the final utterance. For this reason, calls are unprotected in toxic languages, and online broadcasts can be unpredictable. In this work, a new way for automatic speech moderation in terms of dynamic word prediction was suggested. The considered task involves the analysis of the auditory and textual channels of speech. Words can have different meanings depending on the context, so in solving the problem it is planned to consider profanity, which is socially unacceptable regardless of the context. In this paper approaches for working with speech stream in the task of profanity prediction were proposed. It can be possible to have smaller latency with usage of audio features. We also suggest the pipeline for real-time (with the ability to predict the sequence with a higher duration than the latency of the processing) prediction for multimodal prediction, which compensates the latency of ASR systems. As a result, in this paper, we compared different solutions for the next color prediction task for English speech and reached the F1 score of 86.6 for 3 class prediction.}
}
@article{AHMAD2022103196,
title = {A Deep Learning Ensemble Approach to Detecting Unknown Network Attacks},
journal = {Journal of Information Security and Applications},
volume = {67},
pages = {103196},
year = {2022},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2022.103196},
url = {https://www.sciencedirect.com/science/article/pii/S2214212622000771},
author = {Rasheed Ahmad and Izzat Alsmadi and Wasim Alhamdani and Lo'ai Tawalbeh},
keywords = {Intrusion Detection System (IDS), Deep learning, Unknown attacks, Internet of Things (IoT), Benchmark network datasets},
abstract = {The majority of the intrusion detection solutions proposed using machine learning and deep learning approaches are based on known attack classes only. Comprehensive threat detection systems should consider both known and unknown attacks. Rapidly changing network environment and the advanced tools and techniques used by adversaries to launch new sophisticated attacks highlight a growing need to build intrusion detection systems that are more realistic, diverse, and robust to detect known and unknown attacks. We employed deep-learning models in our experiments to detect unknown threats, never introduced before to the model. This paper also studied the bias issues in connection with unknown threats detection. Many recent research studies based on conventional machine learning may report biased results and restricted training due to relying only on a single dataset; thus, there are existing threats that the model is unaware of, although the model may have high accuracy (in the known territories). This study presents a realistic IDS approach in which a deep learning classifiers' ensemble is trained on four benchmark IDS datasets for testing the unknown attack instances. Specifically, the model has no prior knowledge of some labels and traffic patterns in those experiments. The architecture proposed builds a deep learning ensemble using classifiers well-known to process and produce good results for sequential data. Our empirical results indicate that the proposed ensemble model can detect a range of unknown attacks with reasonable performance measures and a practical approach towards building a comprehensive IDS solution.}
}
@article{CETIN2021102652,
title = {A review of code reviewer recommendation studies: Challenges and future directions},
journal = {Science of Computer Programming},
volume = {208},
pages = {102652},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102652},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321000459},
author = {H. Alperen Çetin and Emre Doğan and Eray Tüzün},
keywords = {Systematic literature review, Code reviewer recommendation, Reviewer recommendation, Modern code review, Pull request},
abstract = {Code review is the process of inspecting code changes by a developer who is not involved in the development of the changeset. One of the initial and important steps of code review process is selecting code reviewer(s) for a given code change. To maximize the benefits of the code review process, the appropriate selection of the reviewer is essential. Code reviewer recommendation has been an active research area over the last few years, and many recommendation models have been proposed in the literature. In this study, we conduct a systematic literature review by inspecting 29 primary studies published from 2009 to 2020. Based on the outcomes of our review: (1) most preferred approaches are heuristic approaches closely followed by machine learning approaches, (2) the majority of the studies use open source projects to evaluate their models, (3) the majority of the studies prefer incremental training set validation techniques, (4) most studies suffer from reproducibility problems, (5) model generalizability and dataset integrity are the most common validity threats for the models and (6) refining models and conducting additional experiments are the most common future work discussions in the studies.}
}
@article{HAUPT2022100235,
title = {The influence of social media affordances on drug dealer posting behavior across multiple social networking sites (SNS)},
journal = {Computers in Human Behavior Reports},
volume = {8},
pages = {100235},
year = {2022},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2022.100235},
url = {https://www.sciencedirect.com/science/article/pii/S2451958822000690},
author = {Michael Robert Haupt and Raphael Cuomo and Jiawei Li and Matthew Nali and Tim K. Mackey},
keywords = {Affordances, Drug dealers, Opioids, Public health, Social networking sites, Natural language processing, Text analysis},
abstract = {Social media has been documented as widely used for initiating online sales of illicit drugs such as opioids. However, not much is known about how affordances of social networking sites (SNS) influence how dealers advertise their supplies. To explore this topic, social media posts across 5 online platforms (Google Groups, Instagram, Twitter, Reddit, and Tumblr) were collected during 2020–2021. Biterm topic modeling (BTM) was used to identify signal posts specifically associated with the illegal online sale of opioids from drug selling social media accounts. Posts were analyzed by conducting a word count for drug names or slang terms associated with 5 categories: Opioids, Non-Opioid Prescription Controlled Drugs (e.g., Xanax, Valium), Other Illicit Drugs (e.g., Meth, Cocaine), Synthetic Opioids (Fentanyl), and Synthetic Marijuana. Number of mentions per post were calculated for each drug category and compared across platforms. Identifiers (e.g., publicly available email address) associated with posts were used to track dealers across different user accounts. Platforms with affordances for longer messages (e.g., Tumblr) had higher concentrations of drug mentions per post and higher variety of drug type mentions compared to SNS platforms Instagram and Twitter. Google Groups had the most drug mentions per post across all 5 categories. Additionally, each identifier was associated with multiple user accounts on a given platform. These results indicate that affordances of anonymity and message length may influence how drug dealers advertise their services on different platforms. Public health implications and strategies to counteract drug dealers and illicit drug diversion via SNS are also discussed.}
}
@article{JEON2021102391,
title = {TZMon: Improving mobile game security with ARM trustzone},
journal = {Computers & Security},
volume = {109},
pages = {102391},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102391},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002157},
author = {Sanghoon Jeon and Huy Kang Kim},
keywords = {Mobile Game Security, ARM TrustZone, OPTEE, Application Integrity, Secure Update},
abstract = {As the game industry is moving from PC to smartphone platforms, security problems related to mobile games are becoming critical. Considering the characteristics of mobile games such as having short life-cycles and high communication costs, the server/network-side security technologies designed for PC games are not appropriate for mobile games. In this study, we propose TZMon, a client-side game protection mechanism based on the ARM TrustZone, which protects the confidentiality and integrity of mobile games. TZMon is composed of application integrity protocol, secure update protocol, data hiding protocol, and timer synchronization protocol. To adequately safeguard game codes and data, TZMon is designed considering an environment of frequent communications with the game server, a stand-alone operation environment, and an unreliable environment using a rooted OS. Furthermore, flexibility is provided to game application developers who apply security policies by using the Java Native Interface (JNI). In this study, we use Android and the Open Portable Trusted Execution Environment (OPTEE) as the OS platforms for Normal World and Secure World, respectively. After implementing a full-featured prototype of TZMon, we apply it to several open-source mobile games. We prove through the experiments that the application of the proposed TZMon does not cause any noticeable performance degradation and can detect major cheating techniques of mobile games.}
}
@article{LEE2023102320,
title = {WaveMAP for identifying putative cell types from in vivo electrophysiology},
journal = {STAR Protocols},
volume = {4},
number = {2},
pages = {102320},
year = {2023},
issn = {2666-1667},
doi = {https://doi.org/10.1016/j.xpro.2023.102320},
url = {https://www.sciencedirect.com/science/article/pii/S2666166723002873},
author = {Kenji Lee and Nicole Carr and Alec Perliss and Chandramouli Chandrasekaran},
keywords = {Cognitive Neuroscience, Behavior},
abstract = {Summary
Action potential spike widths are used to classify cell types as either excitatory or inhibitory; however, this approach obscures other differences in waveform shape useful for identifying more fine-grained cell types. Here, we present a protocol for using WaveMAP to generate nuanced average waveform clusters more closely linked to underlying cell types. We describe steps for installing WaveMAP, preprocessing data, and clustering waveform into putative cell types. We also detail cluster evaluation for functional differences and interpretation of WaveMAP output. For complete details on the use and execution of this protocol, please refer to Lee et al. (2021).1}
}
@article{SNEHI2021100371,
title = {Vulnerability retrospection of security solutions for software-defined Cyber–Physical System against DDoS and IoT-DDoS attacks},
journal = {Computer Science Review},
volume = {40},
pages = {100371},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100371},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000113},
author = {Manish Snehi and Abhinav Bhandari},
keywords = {Cyber–Physical System, Internet of Things, IoT, Software-defined networks, SDN, Fog Computing, Distributed Denial of Service, DDoS, IoT-DDoS},
abstract = {The wide dispersion of the Internet of Things (IoT), Software-defined Networks and Cloud Computing have given the wings to Cyber–Physical System adoption. The newfangled society relies so much on Cyber–Physical Systems, such as Smart Cities, Smart Agriculture, Medical Cyber System, that a dearth to any of the available services may lead to severe concerns. The IoT devices are unwittingly contributing to the denial of service attacks. Though the neoteric Software-defined Anything (SDx) paradigm has offered effective solution approaches to catastrophic IoT-based DDoS attacks, the novel designed solutions confront various vulnerabilities due to less secure IoT devices, high-volume real-time network traffic generated by the colossal amount of IoT devices, etc. In this paper, we present a comprehensive survey on vulnerability analysis of security solutions for Software-defined Cyber–Physical System. The paper delineates the architectural details of the Software-defined Cyber–Physical System and recommends amalgamation of Fog Computing as one of the architectural layers for overcoming a number of vulnerabilities. As contemporary technologies like IoT, Software-defined Networking and Cloud Computing are the soup ingredients of the Software-defined Cyber–Physical System, each of the individual components has been auscultated individually for security vulnerabilities with a focus on Distributed Denial of Service (DDoS and IoT-based DDoS) attacks. To anticipate the future recasting of the novel paradigm, we discuss the ongoing research and detailed vulnerability analysis with a focus on resiliency, performance, and scalability. Last but not least, we discuss the lessons learned and prospects to conclude.}
}
@article{WERBROUCK2023102136,
title = {A generic framework for federated CDEs applied to Issue Management},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102136},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102136},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002641},
author = {Jeroen Werbrouck and Oliver Schulz and Jyrki Oraskari and Erik Mannens and Pieter Pauwels and Jakob Beetz},
keywords = {Common Data Environment, Federation, Issue Management, BIM Collaboration Format, Solid},
abstract = {This paper analyses the requirements for managing interoperable building data in a federated Common Data Environment (CDE). We discuss the need for generic (meta)data storage patterns, semantic query interfaces, decentral authentication, data aggregation, and adaptation and prove that their combination is feasible with current-day technologies. We illustrate the mechanisms of such federated CDE by considering the topic of digital Issue Management, one of the primary functions of a CDE. In an exemplary data flow process, we show how generic (federated, Semantic Web-based) data patterns for Issue Management can be aggregated and restructured to match existing industry standards like buildingSMART’s BIM Collaboration Format (BCF) API. Finally, we show the methodology is compatible with current-day practice by implementing this process in a proof of concept. The main contribution of this research is a generic, federated framework for project-related, interdisciplinary collaboration for CDEs.}
}
@article{MERAYO2024101651,
title = {Applying machine learning to assess emotional reactions to video game content streamed on Spanish Twitch channels},
journal = {Computer Speech & Language},
volume = {88},
pages = {101651},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101651},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000342},
author = {Noemí Merayo and Rosalía Cotelo and Rocío Carratalá-Sáez and Francisco J. Andújar},
keywords = {Emotional response, Corpus, Machine learning, Twitch, Video games},
abstract = {This research explores for the first time the application of machine learning to detect emotional responses in video game streaming channels, specifically on Twitch, the most widely used platform for broadcasting content. Analyzing sentiment in gaming contexts is difficult due to the brevity of messages, the lack of context, and the use of informal language, which is exacerbated in the gaming environment by slang, abbreviations, memes, and jargon. First, a novel Spanish corpus was created from chat messages on Spanish video game Twitch channels, manually labeled for polarity and emotions. It is noteworthy as the first Spanish corpus for analyzing social responses on Twitch. Secondly, machine learning algorithms were used to classify polarity and emotions offering promising evaluations. The methodology followed in this work consists of three main steps: (1) Extracting Twitch chat messages from Spanish streamers’ channels related to gaming events and gameplays; (2) Processing and selecting the messages to form the corpus and manually annotating polarity and emotions; and (3) Applying machine learning models to detect polarity and emotions in the created corpus. The results have shown that a Bidirectional Encoder Representation from Transformers (BERT) based model excels with 78% accuracy in polarity detection, while deep learning and Random Forest models reach around 70%. For emotion detection, the BERT model performs best with 68%, followed by deep learning with 55%. It is worth noting that emotion detection is more challenging due to the subjective interpretation of emotions in the complex communicative context of video gaming on platforms such as Twitch. The use of supervised learning techniques, together with the rigorous corpus labeling process and the subsequent corpus pre-processing methodology, has helped to mitigate these challenges, and the algorithms have performed well. The main limitations of the research involve category and video game representation balance. Finally, it is important to stress that the integration of machine learning in video games and on Twitch is innovative, by allowing the identification of viewers’ emotions on streamers’ channels. This innovation could bring benefits such as a better understanding of audience sentiment, improving content and audience retention, providing personalized recommendations and detecting toxic behavior in chats.}
}
@article{ASRI201937,
title = {An empirical study of sentiments in code reviews},
journal = {Information and Software Technology},
volume = {114},
pages = {37-54},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301387},
author = {Ikram El Asri and Noureddine Kerzazi and Gias Uddin and Foutse Khomh and M.A. {Janati Idrissi}},
keywords = {Empirical software engineering, Code review, Sentiment analysis, Opinion mining, Affective analysis, Propensity score matching},
abstract = {Context
Modern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.
Objective
In this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the code review process.
Method
Based on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.
Results
We found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (e.g., core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.
Conclusion
Through this work, we provide evidences that text-based sentiments have an impact on the duration of the code review process as well as the acceptance or rejection of the suggested changes.}
}
@article{20183,
title = {In brief},
journal = {Network Security},
volume = {2018},
number = {3},
pages = {3},
year = {2018},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(18)30021-7},
url = {https://www.sciencedirect.com/science/article/pii/S1353485818300217}
}
@article{NGUYEN2022107525,
title = {A collaborative approach to early detection of IoT Botnet},
journal = {Computers & Electrical Engineering},
volume = {97},
pages = {107525},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107525},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621004717},
author = {Giang L. Nguyen and Braulio Dumba and Quoc-Dung Ngo and Hai-Viet Le and Tu N. Nguyen},
keywords = {Machine learning, Collaborative model, Malware, IoT Botnet, Early detection},
abstract = {With the rapid growth of threats and diversity in the manner of attack, Internet of things (IoT) systems has major challenges in providing methods to detect security vulnerabilities and attacks. There have been increasing developments of many detection tools and methods using full-time series data during malware execution based on machine learning/deep learning. However, the effectiveness of existing works is tightly bound by the requirement to use full-time series data. On the other hand, an earlier detection would help propose better solutions to respond to the IoT Botnet. Therefore, it mitigating the damage from potential attacks. In this paper, going beyond the full-time series data-based methods, we propose a collaborative machine learning model to effectively automate the early detection of IoT Botnet based on many features. The proposed model is 99.37% accurate on a dataset of 5023 IoT botnet and 3888 benign samples.}
}
@article{GONCALVES2022107,
title = {How Digital Tools Align with Organizational Agility and Strengthen Digital Innovation in Automotive Startups},
journal = {Procedia Computer Science},
volume = {196},
pages = {107-116},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.11.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022183},
author = {Dulce Gonçalves and Magnus Bergquist and Sverker Alänge and Richard Bunk},
keywords = {Digital Tools, Organizational Agility, Digital Innovation Capability, Agile Culture, Automotive Startups},
abstract = {Digital tools can be an enabler for automotive startups to strengthen their digital innovation capability. Still, few empirical studies describe how automotive startups apply digital tools to do this. Digital innovation capability is essential for survival in a volatile global digital marketplace. Therefore, we conducted a qualitative study based on 23 interviews with nine global automotive startups to understand how they apply digital tools to strengthen their digital innovation. The results showed that automotive startups use cloud services almost exclusively for their business. We conclude that startups choose to use digital tools as SaaS to strengthen their organizational agility and digital innovation initiatives. It harmonizes with their agile culture, effectively enabling innovation collaborations between employees internally and with external actors enabling rapidness to market. SaaS providers’ startup programs enabled startups to remain focused on their innovation initiatives and not worry about scalability since the solutions scaled from the start.}
}
@article{20143,
title = {In brief},
journal = {Network Security},
volume = {2014},
number = {11},
pages = {3},
year = {2014},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(14)70108-4},
url = {https://www.sciencedirect.com/science/article/pii/S1353485814701084}
}
@article{ANZT2021101278,
title = {Crediting pull requests to open source research software as an academic contribution},
journal = {Journal of Computational Science},
volume = {49},
pages = {101278},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101278},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320305743},
author = {Hartwig Anzt and Eileen Kuehn and Goran Flegar},
keywords = {Scientific excellence paradigms, Conference contributions, Scientific reputation, Community software development},
abstract = {Like any other scientific discipline, the High Performance Computing community suffers under the publish or perish paradigm. As a result, a significant portion of novel algorithm designs and hardware-optimized implementations never make it into production code but are instead abandoned once they served the purpose of yielding (another) publication. At the same time, community software packages driving scientific research lack the addition of new technology and hardware-specific implementations. This results in a very unsatisfying situation where researchers and software developers are working independently, and the traditional peer reviewing is reaching its capacity limits. A paradigm shift that accepts high-quality software pull requests to open source research software as conference contributions may create incentives to realize new and/or improved algorithms in community software ecosystems. In this paper, we propose to complement code reviews on pull requests to scientific open source software with scientific reviews, and allow the presentation and publication of high quality software contributions that present an academic improvement to the state-of-the-art at scientific conferences.}
}
@article{CORTESRIOS2022106811,
title = {A unifying framework for the systematic analysis of Git workflows},
journal = {Information and Software Technology},
volume = {145},
pages = {106811},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106811},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002433},
author = {Julio César {Cortés Ríos} and Suzanne M. Embury and Sukru Eraslan},
keywords = {Git, Workflows, Feature-based modelling, Version control, Branching},
abstract = {Context:
Git is a popular distributed version control system that provides flexibility and robustness for software development projects. Several workflows have been proposed to codify the way project contributors work collaboratively with Git. Some workflows are highly prescriptive while others allow more leeway but do not provide the same level of code quality assurance, thus, preventing their comparison to determine the most suitable for a specific set of requirements, or to ascertain if a workflow is being properly followed.
Objective:
In this paper, we propose a novel feature-based framework for describing Git workflows, based on a study of 26 existing instances. The framework enables workflows’ comparison, to discern how, and to what extent, they exploit Git capabilities for collaborative software development.
Methods:
The framework uses feature-based modelling to map Git capabilities, regularly expressed as contribution guidelines, and a set of features that can be impartially applied to all the workflows considered. Through this framework, each workflow was characterised based on their publicly available descriptions. The characterisations were then vectorised and processed using hierarchical clustering to determine workflows’ similarities and to identify which features are most popular, and more relevant for discriminatory purposes.
Results:
Comparative analysis evidenced that some workflows claiming to be closely related, when described and then characterised, turned out to have more differences than similarities. The analysis also showed that most workflows focus on the branching and code integration strategies, whilst others emphasise subtle differences from other popular workflows or describe a specific development route and are, thus, widely reused.
Conclusion:
The characterisation and clustering analysis demonstrated that our framework can be used to compare and analyse Git workflows.}
}
@article{ORIOLTORDERA2022103956,
title = {Epigenetic landscape in the kick-and-kill therapeutic vaccine BCN02 clinical trial is associated with antiretroviral treatment interruption (ATI) outcome},
journal = {eBioMedicine},
volume = {78},
pages = {103956},
year = {2022},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2022.103956},
url = {https://www.sciencedirect.com/science/article/pii/S2352396422001402},
author = {Bruna Oriol-Tordera and Anna Esteve-Codina and María Berdasco and Míriam Rosás-Umbert and Elena Gonçalves and Clara Duran-Castells and Francesc Català-Moll and Anuska Llano and Samandhy Cedeño and Maria C. Puertas and Martin Tolstrup and Ole S. Søgaard and Bonaventura Clotet and Javier Martínez-Picado and Tomáš Hanke and Behazine Combadiere and Roger Paredes and Dennis Hartigan-O'Connor and Manel Esteller and Michael Meulbroek and María Luz Calle and Alex Sanchez-Pla and José Moltó and Beatriz Mothe and Christian Brander and Marta Ruiz-Riol},
keywords = {HIV-1 vaccine, Epigenetics, DNA methylation},
abstract = {Summary
Background
The BCN02-trial combined therapeutic vaccination with a viral latency reversing agent (romidepsin, RMD) in HIV-1-infected individuals and included a monitored antiretroviral pause (MAP) as an efficacy read-out identifying individuals with an early or late (< or > 4weeks) viral-rebound. Integrated -omics analyses were applied prior treatment interruption to identify markers of virus control during MAP.
Methods
PBMC, whole-genome DNA methylation and transcriptomics were assessed in 14 BCN02 participants, including 8 Early and 4 Late viral-rebound individuals. Chromatin state, histone marks and integration analysis (histone-3 acetylation (H3Ac), viral load, proviral levels and HIV-specific T cells responses) were included. REDUC-trial samples (n = 5) were included as a control group for RMD administration alone.
Findings
DNA methylation imprints after receiving the complete intervention discriminated Early versus Late viral-rebound individuals before MAP. Also, differential chromatin accessibility and histone marks at DNA methylation level were detected. Importantly, the differential DNA methylation positions (DMPs) between Early and Late rebounders before MAP were strongly associated with viral load, proviral levels as well as the HIV-specific T-cell responses. Most of these DMPs were already present prior to the intervention and accentuated after RMD infusion.
Interpretation
This study identifies host DNA methylation profiles and epigenetic cascades that are predictive of subsequent virus control in a kick-and-kill HIV cure strategy.
Funding
European Union Horizon 2020 Framework Programme for Research and Innovation under Grant Agreement N°681137-EAVI2020 and N°847943-MISTRAL, the Ministerio de Ciencia e Innovación (SAF2017_89726_R), and the National Institutes of Health–National Institute of Allergy and Infectious Diseases Program Grant P01-AI131568.}
}
@article{CHATZOGLOU2023103051,
title = {A hands-on gaze on HTTP/3 security through the lens of HTTP/2 and a public dataset},
journal = {Computers & Security},
volume = {125},
pages = {103051},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.103051},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822004436},
author = {Efstratios Chatzoglou and Vasileios Kouliaridis and Georgios Kambourakis and Georgios Karopoulos and Stefanos Gritzalis},
keywords = {HTTP/2, HTTP/3, QUIC, IDS, Machine Learning, Anomaly Detection, Vulnerabilities, DDoS, Attack, Dataset},
abstract = {Following QUIC protocol ratification on May 2021, the third major version of the Hypertext Transfer Protocol, namely HTTP/3, was published around one year later in RFC 9114. In light of these consequential advancements, the current work aspires to provide a full-blown coverage of the following issues, which to our knowledge have received feeble or no attention in the literature so far. First, we provide a complete review of attacks against HTTP/2, and elaborate on if and in which way they can be migrated to HTTP/3. Second, through the creation of a testbed comprising the at present six most popular HTTP/3-enabled servers, we examine the effectiveness of a quartet of attacks, either stemming directly from the HTTP/2 relevant literature or being entirely new. This scrutiny led to the assignment of at least one CVE ID with a critical base score by MITRE. No less important, by capitalizing on a realistic, abundant in devices testbed, we compiled a voluminous, labeled corpus containing traces of ten diverse attacks against HTTP and QUIC services. An initial evaluation of the dataset mainly by means of machine learning techniques is included as well. Given that the 30 GB dataset is made available in both pcap and CSV formats, forthcoming research can easily take advantage of any subset of features, contingent upon the specific network topology and configuration.}
}
@article{KUCEK2020102470,
title = {An Empirical Survey of Functions and Configurations of Open-Source Capture the Flag (CTF) Environments},
journal = {Journal of Network and Computer Applications},
volume = {151},
pages = {102470},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102470},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303303},
author = {Stela Kucek and Maria Leitner},
keywords = {CTF, Capture the Flag, Cyber range, Computer network operations, Cyber security exercises, Cyber security training},
abstract = {Capture the Flag (CTF) is a computer security competition that is generally used to give participants experience in securing (virtual) machines and responding to cyber attacks. CTF contests have been getting larger and are receiving many participants every year (e.g., DEFCON, NYU-CSAW). CTF competitions are typically hosted in virtual environments, specifically set up to fulfill the goals and scenarios of the CTF. This article investigates the underlying infrastructures and CTF environments, specifically open-source CTF environments. A systematic review is conducted to assess functionality and game configuration in CTF environments where the source code is available on the web (i.e., open-source software). In particular, from out of 28 CTF platforms, we found 12 open-source CTF environments. As four platforms were not installable for several reasons, we finally examined 8 open-source CTF environments (PicoCTF, FacebookCTF, HackTheArch, WrathCTF, Pedagogic-CTF, RootTheBox, CTFd and Mellivora) regarding their features and functions for hosting CTFs (e.g., scoring, statistics or supported challenge types) and providing game configurations (e.g., multiple flags, points, hint penalities). Surprisingly, while many platforms provide similar base functionality, game configurations between the platforms varied strongly. For example, hint penalty, time frames for solving challenges, limited number of attempts or dependencies between challenges are game options that might be relevant for potential CTF organizers and for choosing a technology. This article contributes to the general understanding of CTF software configurations and technology design and implementation. Potential CTF organizers and participants may use this as a reference for challenge configurations and technology utilization. Based on our analysis, we would like to further review commercial and other platforms in order to establish a golden standard for CTF environments and further contribute to a better understanding of CTF design and development.}
}
@article{GRONIER2022100079,
title = {Platform for transverse evaluation of control strategies for multi-energy smart grids},
journal = {Smart Energy},
volume = {7},
pages = {100079},
year = {2022},
issn = {2666-9552},
doi = {https://doi.org/10.1016/j.segy.2022.100079},
url = {https://www.sciencedirect.com/science/article/pii/S266695522200017X},
author = {Timothé Gronier and Erwin Franquet and Stéphane Gibout},
keywords = {Smart multi-energy grids, Multi-carrier energy flows, DSM, Aggregator, Energy modeling, Renewable generation, Energy storage},
abstract = {This paper presents the PEACEFULNESS software platform (Platform for transvErse evAluation of Control stratEgies For mULti-eNErgy Smart gridS), an open framework dedicated to multi-energy smart-grids, based on a techno-economic model that integrates economic considerations (contracts). As such, it is mainly oriented towards the evaluation of multi-energy grid supervision strategies, that is, energy management, and the corresponding policies and legal organization. The main goal is then to highlight the various possible behaviors and strategies to organize the probable future interconnections between the different energy carriers. In particular, it aims at investigating how to maximize the use of renewable energy sources (RES), using Demand Side Management (DSM) techniques and energy storage, in a shared economy context. The open-source tool PEACEFULNESS, written in Python, is described here in detail. It combines a top-down description of the energy networks and connections between the various agents (energy providers, distribution system operators, aggregators, consumers, producers, prosumers, etc.), together with a techno-economic bottom-up description for all devices. Here, both public databases and users’ data (basic heating demands or based on building modeling) can be used, as well as generic or more specific models (e.g., PV panels with constant or temperature-dependent efficiency). One of its major unique features compared with other tools is that it extends the use of DSM techniques to various energy grids which can also interact together. Furthermore, different economic models can be set for both the aggregators and the customers, and even within these groups. As a last competitive advantage, PEACEFULNESS allows the user to simulate the operation and supervision of tens up to hundreds of thousands of agents. It also provides a reporting system giving access to all the data, with a configurable granularity and frequency for the retained indicators. Finally, several validation cases are presented, followed by a series of test cases with increasing size: a smart home, a smart district (2 000 dwellings) and a smart community (50 000 dwellings).}
}
@article{YU2021108117,
title = {PBCNN: Packet Bytes-based Convolutional Neural Network for Network Intrusion Detection},
journal = {Computer Networks},
volume = {194},
pages = {108117},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108117},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001948},
author = {Lian Yu and Jingtao Dong and Lihao Chen and Mengyuan Li and Bingfeng Xu and Zhao Li and Lin Qiao and Lijun Liu and Bei Zhao and Chen Zhang},
keywords = {Hierarchical Byte-based CNN, Network intrusion detection, Few sample problem, Auto-extraction of abstract features},
abstract = {Network intrusion detection system (IDS) protects the target network from the threats of data breaches and the insecurity of people’s privacy. However, most of existing researches on network intrusion detection cannot fulfil effectively the protection of targets, especially, depending heavily on the statistical features that are manually designed with domain experts’ knowledge and experiences, and failing to address the few sample data problem. Network traffic has a hierarchical structure, i.e., byte-packet-flow, which is similar to phrase-sentence-article in an article. This paper proposes a hierarchical packet byte-based CNN, called PBCNN, where the first level extracts abstract features automatically from bytes in a packet in raw Pcap files, and then the second level further constructs the representation from packets in a flow or session, instead of using feature-ready CSV files, to make full use of original data information. Multiple convolution-pooling modules are cascaded with byte-friendly sizes of multiple filters, and one-layer TextCNN to obtain the representation of traffic flow, feeding the representation to 3 layers of fully connected networks for intrusion classification. PBCNN-based few shot learning is applied to improve the detection reliability of network attack categories with the few sample problem. Several experiments are performed and the results show that the evaluation metrics are superior to the existing researches in regard to CIC-IDS2017 and CSE-CIC-IDS2018 datasets.}
}
@article{MELTON20211505,
title = {Public sentiment analysis and topic modeling regarding COVID-19 vaccines on the Reddit social media platform: A call to action for strengthening vaccine confidence},
journal = {Journal of Infection and Public Health},
volume = {14},
number = {10},
pages = {1505-1512},
year = {2021},
note = {Special Issue on COVID-19 – Vaccine, Variants and New Waves},
issn = {1876-0341},
doi = {https://doi.org/10.1016/j.jiph.2021.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S1876034121002288},
author = {Chad A. Melton and Olufunto A. Olusanya and Nariman Ammar and Arash Shaban-Nejad},
keywords = {Misinformation, COVID-19, Vaccine hesitancy, Sentiment analysis, Topic modeling},
abstract = {Background
The COVID-19 pandemic fueled one of the most rapid vaccine developments in history. However, misinformation spread through online social media often leads to negative vaccine sentiment and hesitancy.
Methods
To investigate COVID-19 vaccine-related discussion in social media, we conducted a sentiment analysis and Latent Dirichlet Allocation topic modeling on textual data collected from 13 Reddit communities focusing on the COVID-19 vaccine from Dec 1, 2020, to May 15, 2021. Data were aggregated and analyzed by month to detect changes in any sentiment and latent topics.
Results
Polarity analysis suggested these communities expressed more positive sentiment than negative regarding the vaccine-related discussions and has remained static over time. Topic modeling revealed community members mainly focused on side effects rather than outlandish conspiracy theories.
Conclusion
Covid-19 vaccine-related content from 13 subreddits show that the sentiments expressed in these communities are overall more positive than negative and have not meaningfully changed since December 2020. Keywords indicating vaccine hesitancy were detected throughout the LDA topic modeling. Public sentiment and topic modeling analysis regarding vaccines could facilitate the implementation of appropriate messaging, digital interventions, and new policies to promote vaccine confidence.}
}
@article{VAYSSADE2023107831,
title = {Wizard: Unsupervised goats tracking algorithm},
journal = {Computers and Electronics in Agriculture},
volume = {209},
pages = {107831},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.107831},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923002193},
author = {Jehan-Antoine Vayssade and Xavier Godard and Mathieu Bonneau},
keywords = {Computer vision, Tracking, Monitoring, Goats, Behavior},
abstract = {Computer vision is an interesting tool for animal behavior monitoring, mainly because it limits animal handling and it can be used to record various traits using only one sensor. From previous studies, this technic has shown to be suitable for various species and behavior. However it remains challenging to collect individual information, i.e. not only to detect animals and behavior on the video frames, but also to identify them. Animal identification is a prerequisite to gather individual information in order to characterize individuals and compare them. A common solution to this problem, known as multiple objects tracking, consists in detecting the animals on each video frame, and then associate detections to a unique animal ID. Association of detections between two consecutive frames are generally made to maintain coherence of the detection locations and appearances. To extract appearance information, a common solution is to use a convolutional neural network (CNN), trained on a large dataset before running the tracking algorithm. For farmed animals, designing such network is challenging as far as large training dataset are still lacking. In this article, we proposed an innovative solution, where the CNN used to extract appearance information is parameterized using offline unsupervised training. The algorithm, named Wizard, was evaluated for the purpose of goats monitoring in outdoor conditions. 17 annotated videos were used, for a total of 4H30, with various number of animals on the video (from 3 to 8) and different level of color differences between animals. First, the ability of the algorithm to track the detected animals was evaluated. When animals were detected, the algorithm found the correct animal ID in 94.82% of the frames. When tracking and detection were evaluated together, we found that Wizard found the correct animal ID in 86.18% of the video length. In situations where the animal detection rate could be high, Wizard seems to be a suitable solution for individual behavior analysis experiments based on computer vision.}
}
@article{JING2024121373,
title = {Automated cryptocurrency trading approach using ensemble deep reinforcement learning: Learn to understand candlesticks},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121373},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121373},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423018754},
author = {Liu Jing and Yuncheol Kang},
keywords = {Automated trading, Candlestick images, Cryptocurrency, Deep reinforcement learning, Ensemble approach},
abstract = {Despite their high risk, cryptocurrencies have gained popularity as viable trading options. Cryptocurrencies are digital assets that experience significant fluctuations in a market operating 24 h a day. Recently, considerable attention has been paid to developing trading bots using machine-learning-based artificial intelligence. Previous studies have employed machine learning techniques to predict financial market trends or make trading decisions, primarily using numerical data extracted from candlesticks. However, these data often overlook the temporal and spatial information of candlesticks, leading to a limited understanding of their significance. In this study, we utilize multi-resolution candlestick images containing temporal and spatial information. Our rationale for using visual information from candlestick charts is to replicate the decision-making processes of human trading experts. To achieve this, we employ deep reinforcement learning algorithms to generate trading signals based on a state vector that includes embedded candlestick-chart images. The trading signal is generated using a multi-agent weighted voting ensemble approach. We test the proposed approach on two BTC/USDT datasets under both bullish and bearish market scenarios. Additionally, we use an attention-based technique to identify significant areas in the candlestick images targeted by the proposed approach. Our findings demonstrate that models using candlestick images 'as-is', outperform those using raw numeric data and other baseline models.}
}
@article{RONZANI20239360,
title = {Vibration Free Flexible Object Handling with a Robot Manipulator Using Learning Control},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9360-9365},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.225},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323005761},
author = {Daniele Ronzani and Shamil Mamedov and Jan Swevers},
keywords = {Iterative Learning Control, Vibration Suppression, Robotic Manipulation},
abstract = {Many industries extensively use flexible materials. effective approaches for handling flexible objects with a robot manipulator must address residual vibrations. Existing solutions rely on complex models, use additional instrumentation for sensing the vibrations, or do not exploit the repetitive nature of most industrial tasks. This paper develops an iterative learning control approach that jointly learns model parameters and residual dynamics using only the interoceptive sensors of the robot. The learned model is subsequently utilized to design optimal point-to-point (PTP) trajectories that accounts for residual vibration, nonlinear kinematics of the manipulator and joint limits. We experimentally show that the proposed approach reduces the residual vibrations by an order of magnitude compared with optimal vibration suppression using the analytical model and threefold compared with the available state-of-the-art method. These results demonstrate that effective handling of a flexible object does not require neither complex models nor additional instrumentation.}
}
@article{CZERANOWSKA2023100059,
title = {Migrants vs. stayers in the pandemic – A sentiment analysis of Twitter content},
journal = {Telematics and Informatics Reports},
volume = {10},
pages = {100059},
year = {2023},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2023.100059},
url = {https://www.sciencedirect.com/science/article/pii/S2772503023000191},
author = {Olga Czeranowska and Karol Chlasta and Piotr Miłkowski and Izabela Grabowska and Jan Kocoń and Krzysztof Hwaszcz and Jan Wieczorek and Agata Jastrzębowska},
keywords = {Sentiment analysis, Text mining, Text analytics, Social media, Twitter, Migrants},
abstract = {In this paper, we propose a sentiment analysis of Twitter data focused on the attitudes and sentiments of Polish migrants and stayers during the pandemic. We collected 9 million tweets and retweets between January and August 2021, and analysed them using MultiEmo, the multilingual, multilevel, multi-domain sentiment analysis corpus. We discovered that the sentiment of tweets differs between migrants and stayers over time, and it relates to the country of migration. The general sentiment is similar for migrants and stayers, but a more detailed analysis reveals that hashtags related to staying safe and staying at home, as well as vaccinations are more polarised for migrants than for stayers, and they reflect the general development trend of the pandemic in Europe. In addition to comparing migrants with stayers, we also compared migrants staying in different countries. amongst the countries of migration, for which we collected at least 3000 tweets, the most positive sentiment of Polish migrants’ tweets was observed in Belgium, with the most negative sentiment coming from Estonia. We also observed that the sentiment of tweets written in Polish by stayers in Poland is less negative when compared to Polish migrants in most of the countries with the highest number of tweets.}
}
@article{NAGAR2021100110,
title = {Information and communication technology platforms as an experimental paradigm in cyber-bystander research: A critique of methodology},
journal = {Computers in Human Behavior Reports},
volume = {4},
pages = {100110},
year = {2021},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2021.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2451958821000580},
author = {Pooja Megha Nagar and Victoria Talwar},
keywords = {Cyberbullying, Information and communication technology, Bystander behavior, Behavior change, Experimental paradigm},
abstract = {The investigation of bystander behavior in response to cyberbullying is a developing area of research that is still in its infancy. To advance this area of inquiry, researchers can use information and communication technology (ICT) platforms, such as simulated social media websites, as an experimental paradigm to facilitate and measure the behavior change of cyber-bystanders in a controlled virtual environment. However, this is a method that remains under-utilized by researchers and it remains unclear why. Thus, the purpose of this paper is to use the '5 principles of cyberbullying research' as an informed and empirical framework to systematically identify the methodological shortcomings that contribute to the underutilization of ICT platforms in cyber-bystander research. The final section of the paper builds on these 5 principles by critically analyzing the unique features of ICT platforms to outline ways in which researchers can design paradigms that are informed by both theory and practice. Overall, this paper aims to further develop the types of experimental methods that are used in the field of cyberbullying to create new avenues of research.}
}
@article{20143,
title = {In brief},
journal = {Network Security},
volume = {2014},
number = {1},
pages = {3},
year = {2014},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(14)70002-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485814700029}
}
@article{20213,
title = {Threatwatch},
journal = {Network Security},
volume = {2021},
number = {1},
pages = {3},
year = {2021},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(21)00002-7},
url = {https://www.sciencedirect.com/science/article/pii/S1353485821000027}
}
@article{RAHMAN2024100085,
title = {Cutting through the noise to motivate people: A comprehensive analysis of COVID-19 social media posts de/motivating vaccination},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100085},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100085},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000335},
author = {Ashiqur Rahman and Ehsan Mohammadi and Hamed Alhoori},
keywords = {Misinformation, Motivation, Vaccine hesitancy, Science communication, Social media, Social psychology},
abstract = {The COVID-19 pandemic exposed significant weaknesses in the healthcare information system. The overwhelming volume of misinformation on social media and other socioeconomic factors created extraordinary challenges to motivate people to take proper precautions and get vaccinated. In this context, our work explored a novel direction by analyzing an extensive dataset collected over two years, identifying the topics de/motivating the public about COVID-19 vaccination. We analyzed these topics based on time, geographic location, and political orientation. We noticed that while the motivating topics remain the same over time and geographic location, the demotivating topics change rapidly. We also identified that intrinsic motivation, rather than external mandate, is more advantageous to inspire the public. This study addresses scientific communication and public motivation in social media. It can help public health officials, policymakers, and social media platforms develop more effective messaging strategies to cut through the noise of misinformation and educate the public about scientific findings.}
}
@article{ISLAM2022106756,
title = {Early prediction for merged vs abandoned code changes in modern code reviews},
journal = {Information and Software Technology},
volume = {142},
pages = {106756},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106756},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002032},
author = {Khairul Islam and Toufique Ahmed and Rifat Shahriyar and Anindya Iqbal and Gias Uddin},
keywords = {Code review, Patch, Early prediction, Merged, Abandoned},
abstract = {Context:
The modern code review process is an integral part of the current software development practice. Considerable effort is given here to inspect code changes, find defects, suggest an improvement, and address the suggestions of the reviewers. In a code review process, several iterations usually take place where an author submits code changes and a reviewer gives feedback until is happy to accept the change. In around 12% cases, the changes are abandoned, eventually wasting all the efforts.
Objective:
In this research, our objective is to design a tool that can predict whether a code change would be merged or abandoned at an early stage to reduce the waste of efforts of all stakeholders (e.g., program author, reviewer, project management, etc.) involved. The real-world demand for such a tool was formally identified by a study by Fan et al. (2018).
Method:
We have mined 146,612 code changes from the code reviews of three large and popular open-source software and trained and tested a suite of supervised machine learning classifiers, both shallow and deep learning-based. We consider a total of 25 features in each code change during the training and testing of the models. The features are divided into five dimensions: reviewer, author, project, text, and code.
Results:
The best performing model named PredCR (Predicting Code Review), a LightGBM-based classifier achieves around 85% AUC score on average and relatively improves the state-of-the-art (Fan et al., 2018) by 14%–23%. In our extensive empirical study involving PredCR on the 146,612 code changes from the three software projects, we find that (1) The new features like reviewer dimensions that are introduced in PredCR are the most informative. (2) Compared to the baseline, PredCR is more effective towards reducing bias against new developers. (3) PredCR uses historical data in the code review repository and as such the performance of PredCR improves as a software system evolves with new and more data.
Conclusion:
PredCR can help save time and effort by helping developers/code reviewers to prioritize the code changes that they are asked to review. Project management can use PredCR to determine how code changes can be assigned to the code reviewers (e.g., select code changes that are more likely to be merged for review before the changes that might be abandoned).}
}
@article{CONTEDELEON201812,
title = {ADLES: Specifying, deploying, and sharing hands-on cyber-exercises},
journal = {Computers & Security},
volume = {74},
pages = {12-40},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817302742},
author = {Daniel {Conte de Leon} and Christopher E. Goes and Michael A. Haney and Axel W. Krings},
keywords = {Cybersecurity, Cyber-defense competitions, Hands-on education, Information technology, Instructional computing environments, Tutorials and exercises},
abstract = {Hands-on tutorials and exercises are recognized as an effective means for gaining much needed cybersecurity and communication and information technology skills. These exercises must be performed in dedicated and virtually isolated computing environments or laboratories, most of which make use of virtualization technology. Building, modifying, and deploying the virtual environments that enable hands-on instruction is currently very time consuming. A new complete exercise instance must be deployed and configured for each course or module, tutorial or exercise, and student. In addition, efficient sharing and reuse of hands-on exercises between organizations is currently extremely difficult, unless the computing resources and virtualization environment are also shared. ADLES is a specification language and associated deployment system created to address these issues up-front. ADLES enables: (1) the formal specification of hands-on virtual computing, networking, and cybersecurity exercises, (2) the automated deployment of specified exercises, and (3) the efficient sharing of such exercises and their computing environment. In this article, we describe in detail the ADLES specification language and deployment system. We also demonstrate ADLES capabilities using two case studies: a pentesting tutorial and a cyber defense competition. The ADLES system is open source and available for all educators to use and improve.}
}
@article{RIZVI2023103259,
title = {Defending Root DNS Servers against DDoS Using Layered Defenses (Extended)},
journal = {Ad Hoc Networks},
volume = {151},
pages = {103259},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103259},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523001798},
author = {ASM Rizvi and Jelena Mirkovic and John Heidemann and Wesley Hardaker and Robert Story},
keywords = {DDoS, DNS, Filtering},
abstract = {Distributed Denial-of-Service (DDoS) attacks exhaust resources, leaving a server unavailable to legitimate clients. The Domain Name System (DNS) is a frequent target of DDoS attacks. Since DNS is a critical infrastructure service, protecting it from DoS is imperative. Many prior approaches have focused on specific filters or anti-spoofing techniques to protect generic services. DNS root nameservers are more challenging to protect, since they use fixed IP addresses, serve very diverse clients and requests, receive predominantly UDP traffic that can be spoofed, and must guarantee high quality of service. In this paper we propose a layered DDoS defense for DNS root nameservers. Our defense uses a library of defensive filters, which can be optimized for different attack types, with different levels of selectivity. We further propose a method that automatically and continuously evaluates and selects the best combination of filters throughout the attack. We show that this layered defense approach provides exceptional protection against all attack types using traces of ten real attacks from a DNS root nameserver. Our automated system can select the best defense within seconds and quickly reduces traffic to the server within a manageable range, while keeping collateral damage lower than 2%. We show our system can successfully mitigate resource exhaustion using replay of a real-world attack. We can handle millions of filtering rules without noticeable operational overhead.}
}
@article{CHALKIADAKIS2022100063,
title = {On-chain analytics for sentiment-driven statistical causality in cryptocurrencies},
journal = {Blockchain: Research and Applications},
volume = {3},
number = {2},
pages = {100063},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100063},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000033},
author = {Ioannis Chalkiadakis and Anna Zaremba and Gareth W. Peters and Michael J. Chantler},
keywords = {Cryptocurrencies, Statistical causality, Blockchain regression, Multiple-output Gaussian process, Natural language processing, Cryptonews sentiment},
abstract = {This paper establishes a new framework for assessing multimodal statistical causality between cryptocurrency market (cryptomarket) sentiment and cryptocurrency price processes. In order to achieve this, we present an efficient algorithm for multimodal statistical causality analysis based on Multiple-Output Gaussian Processes. Signals from different information sources (modalities) are jointly modelled as a Multiple-Output Gaussian Process, and then using a novel approach to statistical causality based on Gaussian Processes (GPs), we study linear and non-linear causal effects between the different modalities. We demonstrate the effectiveness of our approach in a machine learning application by studying the relationship between cryptocurrency spot price dynamics and sentiment time-series data specific to the crypto sector, which we conjecture influences retail investor behaviour. The investor sentiment is extracted from cryptomarket news data via methods developed in the area of statistical machine learning known as Natural Language Processing (NLP). To capture sentiment, we present a novel framework for text to time-series embedding, which we then use to construct a sentiment index from publicly available news articles. We conduct a statistical analysis of our sentiment statistical index model and compare it to alternative state-of-the-art sentiment models popular in the NLP literature. In regard to the multimodal causality, the investor sentiment is our primary modality of exploration, in addition to price and a blockchain technology-related indicator (hash rate). Analysis shows that our approach is effective in modelling causal structures of variable degree of complexity between heterogeneous data sources and illustrates the impact that certain modelling choices for the different modalities can have on detecting causality. A solid understanding of these factors is necessary to gauge cryptocurrency adoption by retail investors and provide sentiment- and technology-based insights about the cryptocurrency market dynamics.}
}
@article{20133,
title = {In brief},
journal = {Network Security},
volume = {2013},
number = {2},
pages = {3},
year = {2013},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(13)70025-4},
url = {https://www.sciencedirect.com/science/article/pii/S1353485813700254}
}
@article{WILSON201757,
title = {Using the Launcher for Executing High Throughput Workloads},
journal = {Big Data Research},
volume = {8},
pages = {57-64},
year = {2017},
note = {Tutorials on Tools and Methods using High Performance Computing resources for Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579616300648},
author = {Lucas A. Wilson},
keywords = {High throughput computing, Distributed computing, Parallel computing},
abstract = {For many scientific disciplines, the transition to using advanced cyberinfrastructure comes not out of a desire to use the most advanced or most powerful resources available, but because their current operational model is no longer sufficient to meet their computational needs. Many researchers begin their computations on their desktop or local workstation, only to discover that the time required to simulate their problem, analyze their instrument data, or score the multitude of entities that they want to would require far more time than they have available. Launcher is a simple utility which enables the execution of high throughput computing workloads on managed HPC systems quickly and with as little effort as possible on the part of the user. Basic usage of the Launcher is straightforward, but Launcher provides several more advanced capabilities including use of Intel® Xeon Phi™ coprocessor cards and task binding support for multi-/many-core architectures. We step through the processes of setting up a basic Launcher job, including creating a job file, setting appropriate environment variables, and using scheduler integration. We also describe how to enable use of the Intel® Xeon Phi™ coprocessor cards, take advantage of Launcher's task binding system, and execute many parallel (OpenMP/MPI) applications at once.}
}
@article{NARCROSS2021100265,
title = {Artificial nervous systems—A new paradigm for artificial intelligence},
journal = {Patterns},
volume = {2},
number = {6},
pages = {100265},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100265},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100091X},
author = {Fredric Narcross},
abstract = {Three dissimilar methodologies in the field of artificial intelligence (AI) appear to be following a common path toward biological authenticity. This trend could be expedited by using a common tool, artificial nervous systems (ANS), for recreating the biology underpinning all three. ANS would then represent a new paradigm for AI with application to many related fields.}
}
@article{QIANG2023113054,
title = {A systematic comparison and evaluation of building ontologies for deploying data-driven analytics in smart buildings},
journal = {Energy and Buildings},
volume = {292},
pages = {113054},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.113054},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823002840},
author = {Zhangcheng Qiang and Stuart Hands and Kerry Taylor and Subbu Sethuvenkatraman and Daniel Hugo and Pouya {Ghiasnezhad Omran} and Madhawa Perera and Armin Haller},
keywords = {Data-driven smart buildings, Data interoperability, Ontology comparison and evaluation, Ontology compatibility, Ontology design patterns, Brick Schema, RealEstateCore, Project Haystack, Digital Buildings},
abstract = {Ontologies play a critical role in data exchange, information integration, and knowledge sharing across diverse smart building applications. Yet, semantic differences between the prevailing building ontologies hamper their purpose of bringing data interoperability and restrict the ability to reuse building ontologies in real-world applications. In this paper, we propose and adopt a framework to conduct a systematic comparison and evaluation of four popular building ontologies (Brick Schema, RealEstateCore, Project Haystack, and Digital Buildings) from both axiomatic design and assertions in a use case, namely the Terminological Box (TBox) evaluation and the Assertion Box (ABox) evaluation. In the TBox evaluation, we use the SQuaRE-based Ontology Quality Evaluation (OQuaRE) framework and concede that Project Haystack and Brick Schema are more compact with respect to the ontology axiomatic design. In the ABox evaluation, we apply an empirical study with sample building data that suggests Brick Schema and RealEstateCore have greater completeness and expressiveness in capturing the main concepts and relations within the building domain. The results indicate that there is no universal building ontology for integrating Linked Building Data (LBD). We also discuss ontology compatibility and investigate building ontology design patterns (ODPs) to support ontology matching, alignment, and harmonisation.}
}
@article{BALA2024100631,
title = {AI techniques for IoT-based DDoS attack detection: Taxonomies, comprehensive review and research challenges},
journal = {Computer Science Review},
volume = {52},
pages = {100631},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100631},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000157},
author = {Bindu Bala and Sunny Behal},
keywords = {DDoS, Machine learning, IoT, Deep learning, Cyber security, Review},
abstract = {Distributed Denial of Service (DDoS) attacks in IoT networks are one of the most devastating and challenging cyber-attacks. The number of IoT users is growing exponentially due to the increase in IoT devices over the past years. Consequently, DDoS attack has become the most prominent attack as vulnerable IoT devices are becoming victims of it. In the literature, numerous techniques have been proposed to detect IoT-based DDoS attacks. However, techniques based on Artificial Intelligence (AI) have proven to be effective in the detection of cyber-attacks in comparison to other alternative techniques. This paper presents a systematic literature review of AI-based tools and techniques used for analysis, classification, and detection of the most threatening, prominent, and dreadful IoT-based DDoS attacks between the years 2019 to 2023. A comparative study of real datasets having IoT traffic features has also been illustrated. The findings of this systematic review provide useful insights into the existing research landscape for designing AI-based models to detect IoT-based DDoS attacks specifically. Additionally, the study sheds light on IoT botnet lifecycle, various botnet families, the taxonomy of IoT-based DDoS attacks, prominent tools used to launch DDoS attack, publicly available IoT datasets, the taxonomy of AI techniques, popular software available for ML/DL modeling, a list of numerous research challenges and future directions that may aid in the development of novel and reliable methods for identifying and categorizing IoT-based DDoS attacks.}
}
@article{PANTOS2023e00484,
title = {The ReFiBot makers guide: Fostering academic open science and circularity with a robotic educational kit},
journal = {HardwareX},
volume = {16},
pages = {e00484},
year = {2023},
issn = {2468-0672},
doi = {https://doi.org/10.1016/j.ohx.2023.e00484},
url = {https://www.sciencedirect.com/science/article/pii/S2468067223000913},
author = {Christos Pantos and Jurrian Doornbos and Gonzalo Mier and João Valente},
keywords = {Robotics, Education, Circularity, Open-science, Recycling, Low-cost},
abstract = {The advent of robotics in schools and universities curricula are preparing students to encompass new didactic fields. This article presents ReFiBot which is an education robot that has been used to increase the technical literacy on robotics and bring more awareness to open science at Wageningen University. The ReFiBot combines open-source hardware and software, integrated with a chassis made from recycled plastic from fishnets. The ReFiBot was carefully designed to be easily assembled with off-the-shelf electronic parts and programmed using the Arduino IDE. Moreover, a software library is facilitated to ease its adoption in educational activities from any curricula level. The ReFiBot has been mainly used for education but can also be used for research on swarm robotics. The CAD files, components list, software files, and tutorial within this contribution will guide the reader through the assemblage and best practices of this circular robotics kit.}
}
@article{DAHIYA2020101763,
title = {Multi attribute auction based incentivized solution against DDoS attacks},
journal = {Computers & Security},
volume = {92},
pages = {101763},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101763},
url = {https://www.sciencedirect.com/science/article/pii/S016740482030047X},
author = {Amrita Dahiya and B.B. Gupta},
keywords = {Auction, Critical value condition, DDoS attack, Differential payment, Marginal utility},
abstract = {Complexity and severity of DDoS attacks is increasing day by day. Internet has highly inconsistent structure in terms of resource distribution. Numerous technical solutions are present in this domain but solutions considering economic aspects have not been given attention. Therefore, in this paper, a multi attribute based auction mechanism to mitigate DDoS attacks has been proposed. A reputation based detection mechanism has been proposed where reputation of a user is assessed through his marginal utility. Along with detection mechanism, two payment mechanisms have been proposed for legitimate and malicious users separately. A greedy resource allocation is devised to allocate resources fairly among the legitimate users. Malicious users who manipulate their bid to acquire maximum share of limited resources are charged with penalty according to differential payment scheme. Since, this is a generalized concept to mitigate DDoS attacks on any platform, we have taken our case study on cloud computing. So, simulations have been carried out on CloudSim. Results obtained from simulations clearly showed that proposed approach performs better than existing DDoS attack mitigation techniques.}
}
@incollection{TURCHIN2024341,
title = {Chapter 27 - Natural Language Processing for Diabetes Digital Health},
editor = {David C. Klonoff and David Kerr and Juan C. Espinoza},
booktitle = {Diabetes Digital Health, Telehealth, and Artificial Intelligence},
publisher = {Academic Press},
pages = {341-351},
year = {2024},
isbn = {978-0-443-13244-5},
doi = {https://doi.org/10.1016/B978-0-443-13244-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132445000043},
author = {Alexander Turchin},
keywords = {Artificial intelligence, Data science, Electronic health records, Natural language processing, Predictive modeling, Real-world evidence},
abstract = {Natural language processing (NLP) is used increasingly widely in the field of diabetes, ranging from the identification of hypoglycemic events to building predictive models for adverse clinical outcomes. More recent studies have shown a relationship between quantitative characteristics of text (e.g., the length of a sentence describing a patient-provider discussion) and outcomes like glucose levels, implying that these computational characteristics likely reflect care delivered to the patient. Barriers to the broader use of NLP in diabetes care and research (e.g., scarcity of human and data resources) can be overcome by promoting the training of endocrinologists with expertise in data science and cross-institutional collaborations.}
}
@article{GARCIASANCHEZ2020105032,
title = {Optimizing Hearthstone agents using an evolutionary algorithm},
journal = {Knowledge-Based Systems},
volume = {188},
pages = {105032},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105032},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119304356},
author = {Pablo García-Sánchez and Alberto Tonda and Antonio J. Fernández-Leiva and Carlos Cotta},
keywords = {Evolutionary algorithms, Hearthstone, Videogames, Evolution strategy, Artificial intelligence, Games, Card games, Collectible card games},
abstract = {Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone.}
}
@article{PATOCKA2023118270,
title = {Dynamic reorientation of tidally locked bodies: Application to Pluto},
journal = {Earth and Planetary Science Letters},
volume = {617},
pages = {118270},
year = {2023},
issn = {0012-821X},
doi = {https://doi.org/10.1016/j.epsl.2023.118270},
url = {https://www.sciencedirect.com/science/article/pii/S0012821X23002832},
author = {Vojtěch Patočka and Martin Kihoulou},
keywords = {true polar wander, planetary reorientation, tidal deformation, Pluto},
abstract = {Planets and moons reorient in space due to mass redistribution associated with various types of internal and external processes. While the equilibrium orientation of a tidally locked body is well understood, much less explored are the dynamics of the reorientation process (or true polar wander, TPW, used here for the motion of either the rotation or the tidal pole). TPW dynamics can be non-trivial and are important for predicting the patterns of TPW-induced surface fractures, as well as for assessing whether enough time has passed for the equilibrium orientation to be reached. The only existing and relatively complex numerical method for an accurate evaluation of the reorientation dynamics of a tidally locked body was described in a series of papers by Hu et al., 2017a, Hu et al., 2017b, Hu et al., 2019. Here we demonstrate that an identical solution can be obtained with a simpler approach, denoted as oω||mMIA, because during TPW the tidal and the rotation axes closely follow respectively the minor and the major axes of the total, time-evolving inertia tensor of the body. Motivated by the presumed reorientation of Pluto, the use of the oω||mMIA method is illustrated on several test examples. In particular, we vary the load sign and the mass of the host body and analyze whether TPW paths are curved or straight. When tidal forcing is relatively small, the paths of negative anomalies (e.g. basins) towards the rotation pole are highly curved, while positive loads may reach the sub- or anti-host point straightforwardly. The obtained behavior is explained by the relative timing of longitudinal and latitudinal reorientation. Our results suggest that the Sputnik Planitia basin cannot be a negative anomaly at present day, and that the remnant figure of Pluto must have formed prior to the reorientation. Finally, the presented method is complemented with an energy balance that can be used to test the numerical solution and to quantify the changes in orbital distance due to TPW. A new release of the custom written code LIOUSHELL that is used to perform the simulations is made freely available on GitHub.}
}
@article{NGUYEN2022117267,
title = {DeepLib: Machine translation techniques to recommend upgrades for third-party libraries},
journal = {Expert Systems with Applications},
volume = {202},
pages = {117267},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117267},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422006388},
author = {Phuong T. Nguyen and Juri {Di Rocco} and Riccardo Rubei and Claudio {Di Sipio} and Davide {Di Ruscio}},
keywords = {Mining software repositories, Deep learning, Encoder–decoder neural network, Third-party libraries upgrade},
abstract = {To keep their code up-to-date with the newest functionalities as well as bug fixes offered by third-party libraries, developers often need to replace an old version of third-party libraries (TPLs) with a newer one. However, choosing a suitable version for a library to be upgraded is complex and susceptible to error. So far, Dependabot is the only tool that supports library upgrades; however, it targets only security fixes and singularly analyzes libraries without considering the whole set of related libraries. In this work, we propose DeepLib as a practical approach to learn upgrades for third-party libraries that have been performed by similar clients. Such upgrades are considered safe, i.e., they do not trigger any conflict, since, in the training clients, the libraries already co-exist without causing any compatibility or dependency issues. In this way, the upgrades provided by DeepLib allow developers to maintain a harmonious relationship with other libraries. By mining the development history of projects, we build migration matrices to train deep neural networks. Once being trained, the networks are then used to forecast the subsequent versions of the related libraries, exploiting the well-founded background related to the machine translation domain. As input, DeepLib accepts a set of library versions and returns a set of future versions to which developers should upgrade the libraries. The framework has been evaluated on two real-world datasets curated from the Maven Central Repository. The results show promising outcomes: DeepLib can recommend the next version for a library as well as a set of libraries under investigation. At its best performance, DeepLib gains a perfect match for several libraries, earning an accuracy of 1.0.}
}
@article{DONRATANAPAT2020104828,
title = {A national scale big data analytics pipeline to assess the potential impacts of flooding on critical infrastructures and communities},
journal = {Environmental Modelling & Software},
volume = {133},
pages = {104828},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104828},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220308859},
author = {N. Donratanapat and S. Samadi and J.M. Vidal and S. {Sadeghi Tabas}},
keywords = {Internet of things, Flood analytics information system, Machine learning, Big Data, Flood situational awareness, The Carolinas.},
abstract = {With the rapid development of the Internet of Things (IoT) and Big Data infrastructure, crowdsourcing techniques have emerged to facilitate data processing and problem solving particularly for flood emergences purposes. A Flood Analytics Information System (FAIS) has been developed as a Python Web application to gather Big Data from multiple servers and analyze flooding impacts during historical and real-time events. The application is smartly designed to integrate crowd intelligence, machine learning (ML), and natural language processing of tweets to provide flood warning with the aim to improve situational awareness for flood risk management. FAIS, a national scale prototype, combines flood peak rates and river level information with geotagged tweets to identify a dynamic set of at-risk locations to flooding. The prototype was successfully tested in real-time during Hurricane Dorian flooding as well as for historical event (Hurricanes Florence) across the Carolinas, USA where the storm made extensive disruption to infrastructure and communities.}
}
@article{20203,
title = {Threatwatch},
journal = {Network Security},
volume = {2020},
number = {11},
pages = {3},
year = {2020},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(20)30123-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485820301239}
}
@article{NASHAT2021102315,
title = {Multifractal detrended fluctuation analysis based detection for SYN flooding attack},
journal = {Computers & Security},
volume = {107},
pages = {102315},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102315},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821001395},
author = {Dalia Nashat and Fatma A. Hussain},
keywords = {DDoS attack, SYN Flooding attack, Time series, MFDFA, Exponential weighted moving average},
abstract = {The TCP SYN flooding (half-open connection) attack is a type of DDoS attack, which denies the services by consuming the server resources. This attack prevents legitimate users from using their desired service. The SYN flooding attack exploits the normal TCP three-way handshake by sending stream of SYN packets to the server with spoofed IP addresses. The detection of this attack is hard since the internet routing infrastructure cannot differentiate between legitimate and spoofed SYN packets. In this paper we present a new detection method for the SYN flooding attack based on Multifractal Detrended Fluctuation Analysis (MFDFA) in addition to an adaptive threshold, thus we can detect the abnormal behavior in the TCP protocol time series.}
}
@article{SCHULTZE20243039,
title = {A comparison of four self-controlled study designs in an analysis of COVID-19 vaccines and myocarditis using five European databases},
journal = {Vaccine},
volume = {42},
number = {12},
pages = {3039-3048},
year = {2024},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2024.03.043},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X2400330X},
author = {Anna Schultze and Ivonne Martin and Davide Messina and Sophie Bots and Svetlana Belitser and Juan {José Carreras-Martínez} and Elisa Correcher-Martinez and Arantxa Urchueguía-Fornes and Mar Martín-Pérez and Patricia García-Poza and Felipe Villalobos and Meritxell Pallejà-Millán and Carlo {Alberto Bissacco} and Elena Segundo and Patrick Souverein and Fabio Riefolo and Carlos E. Durán and Rosa Gini and Miriam Sturkenboom and Olaf Klungel and Ian Douglas},
keywords = {(MeSH): Vaccines, Pharmacoepidemiology, Self-controlled Case Series, Self-controlled Risk Interval, Myocarditis, Electronic Health Records, Meta-analysis},
abstract = {Introduction
The aim of this study was to assess the possible extent of bias due to violation of a core assumption (event-dependent exposures) when using self-controlled designs to analyse the association between COVID-19 vaccines and myocarditis.
Methods
We used data from five European databases (Spain: BIFAP, FISABIO VID, and SIDIAP; Italy: ARS-Tuscany; England: CPRD Aurum) converted to the ConcePTION Common Data Model. Individuals who experienced both myocarditis and were vaccinated against COVID-19 between 1 September 2020 and the end of data availability in each country were included. We compared a self-controlled risk interval study (SCRI) using a pre-vaccination control window, an SCRI using a post-vaccination control window, a standard SCCS and an extension of the SCCS designed to handle violations of the assumption of event-dependent exposures.
Results
We included 1,757 cases of myocarditis. For analyses of the first dose of the Pfizer vaccine, to which all databases contributed information, we found results consistent with a null effect in both of the SCRI and extended SCCS, but some indication of a harmful effect in a standard SCCS. For the second dose, we found evidence of a harmful association for all study designs, with relatively similar effect sizes (SCRI pre = 1.99, 1.40 – 2.82; SCRI post 2.13, 95 %CI – 1.43, 3.18; standard SCCS 1.79, 95 %CI 1.31 – 2.44, extended SCCS 1.52, 95 %CI = 1.08 – 2.15). Adjustment for calendar time did not change these conclusions. Findings using all designs were also consistent with a harmful effect following a second dose of the Moderna vaccine.
Conclusions
In the context of the known association between COVID-19 vaccines and myocarditis, we have demonstrated that two forms of SCRI and two forms of SCCS led to largely comparable results, possibly because of limited violation of the assumption of event-dependent exposures.}
}
@article{DENEIRA2023109553,
title = {Distributed denial of service attack prediction: Challenges, open issues and opportunities},
journal = {Computer Networks},
volume = {222},
pages = {109553},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109553},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005874},
author = {Anderson Bergamini {de Neira} and Burak Kantarci and Michele Nogueira},
keywords = {Cybersecurity, DDoS attack prediction, Survey, Network security},
abstract = {Distributed Denial of Service (DDoS) attack is one of the biggest cyber threats. DDoS attacks have evolved in quantity and volume to evade detection and increase damage. Changes during the COVID-19 pandemic have left traditional perimeter-based security measures vulnerable to attackers that have diversified their activities by targeting health services, e-commerce, and educational services. DDoS attack prediction searches for signals of attack preparation to warn about the imminence of the attack. Prediction is necessary to handle high-volumetric DDoS attacks and to increase the time to defend against them. This survey article presents the classification of studies from the literature comprising the current state-of-the-art on DDoS attack prediction. It highlights the results of this extensive literature review categorizing the works by prediction time, architecture, employed methodology, and the type of data utilized to predict attacks. Further, this survey details each identified study and, finally, it emphasizes the research opportunities to evolve the DDoS attack prediction state-of-the-art.}
}
@article{KOLLURI2021100123,
title = {CoVerifi: A COVID-19 news verification system},
journal = {Online Social Networks and Media},
volume = {22},
pages = {100123},
year = {2021},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2021.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2468696421000070},
author = {Nikhil L. Kolluri and Dhiraj Murthy},
keywords = {Infodemic, Misinformation, Media diet, Machine learning, COVID-19},
abstract = {There is an abundance of misinformation, disinformation, and “fake news” related to COVID-19, leading the director-general of the World Health Organization to term this an ‘infodemic’. Given the high volume of COVID-19 content on the Internet, many find it difficult to evaluate veracity. Vulnerable and marginalized groups are being misinformed and subject to high levels of stress. Riots and panic buying have also taken place due to “fake news”. However, individual research-led websites can make a major difference in terms of providing accurate information. For example, the Johns Hopkins Coronavirus Resource Center website has over 81 million entries linked to it on Google. With the outbreak of COVID-19 and the knowledge that deceptive news has the potential to measurably affect the beliefs of the public, new strategies are needed to prevent the spread of misinformation. This study seeks to make a timely intervention to the information landscape through a COVID-19 “fake news”, misinformation, and disinformation website. In this article, we introduce CoVerifi, a web application which combines both the power of machine learning and the power of human feedback to assess the credibility of news. By allowing users the ability to “vote” on news content, the CoVerifi platform will allow us to release labelled data as open source, which will enable further research on preventing the spread of COVID-19-related misinformation. We discuss the development of CoVerifi and the potential utility of deploying the system at scale for combating the COVID-19 “infodemic”.}
}
@article{CHATURVEDI2021228,
title = {Predicting video engagement using heterogeneous DeepWalk},
journal = {Neurocomputing},
volume = {465},
pages = {228-237},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.08.127},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221013382},
author = {Iti Chaturvedi and Kishor Thapa and Sandro Cavallari and Erik Cambria and Roy E. Welsch},
keywords = {Video engagement, DeepWalk, Online behavior, One-class model},
abstract = {Video engagement is important in online advertisements where there is no physical interaction with the consumer. Engagement can be directly measured as the number of seconds after which a consumer skips an advertisement. In this paper, we propose a model to predict video engagement of an advertisement using only a few samples. This allows for early identification of poor quality videos. This can also help identify advertisement frauds where a robot runs fake videos behind the name of well-known brands. We leverage on the fact that videos with high engagement have similar viewing patterns over time. Hence, we can create a similarity network of videos and use a graph-embedding model called DeepWalk to cluster videos into significant communities. The learned embedding is able to identify viewing patterns of fraud and popular videos. In order to assess the impact of a video, we also consider how the view counts increase or decrease over time. This results in a heterogeneous graph where an edge indicates similar video engagement or history of view counts between two videos. Since it is difficult to find labelled samples for ‘fraud’ video, we leverage on a one-class model that can determine ‘fraud’ videos with outlier or abnormal behavior. The proposed model outperforms baselines in F-measure by over 20%.}
}
@article{TUAN2022102549,
title = {On Detecting and Classifying DGA Botnets and their Families},
journal = {Computers & Security},
volume = {113},
pages = {102549},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102549},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003734},
author = {Tong Anh Tuan and Hoang Viet Long and David Taniar},
keywords = {Botnet detection, Dga botnets, Deep learning, Lstm network, Attention Layer, UMUDGA Dataset},
abstract = {Botnets are a frequent threat to information systems on the Internet, capable of launching denial-of-service attacks, spreading spam and malware on a large scale. Detecting and preventing botnets is very important in cybersecurity. Previous studies have suggested anomaly-based, signature-based, or HoneyNet-based botnet detection solutions. This paper presents new solutions for detecting and classifying families of Domain Generation Algorithm (DGA) botnets. Our solution can be applied in practice to disable botnets even if they have infected the computer. Our works help solve two problems, including binary classification and multiclass classification, specifically: (1) Determining whether a domain name is malicious or benign; (2) For malicious domains, identify their DGA botnet family. We proposed two deep learning models called LA_Bin07 and LA_Mul07 by combining the LSTM network and Attention layer. Our evaluation used the UMUDGA dataset recently published in 2020, with 50 DGA botnet families. The experimental results show that the LA_Bin07 and LA_Mul07 models solve the DGA botnets problem for binary and multiclass classification problems with very high accuracy.}
}
@article{MASOOD2018501,
title = {Adapting agile practices in university contexts},
journal = {Journal of Systems and Software},
volume = {144},
pages = {501-510},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301419},
author = {Zainab Masood and Rashina Hoda and Kelly Blincoe},
keywords = {Agile software development, Agile practices, Teaching, University, Adapting, Contextualization},
abstract = {Teaching agile practices has found its place in software engineering curricula in many universities across the globe. As a result, educators and students have embraced different ways to apply agile practices during their courses through lectures, games, projects, workshops and more for effective theoretical and practical learning. Practicing agile in university contexts comes with challenges for students and to counter these challenges, they perform some adaptations to standard agile practices making them effective and easier to use in university contexts. This study describes the constraints the students faced while applying agile practices in a university course taught at the University of Auckland, including difficulty in setting up common time for all team members to work together, limited availability of customer due to busy schedule and the modifications the students introduced to adapt agile practices to suit the university context, such as daily stand-ups with reduced frequency, combining sprint meetings, and rotating scrum master from team. In addition, it summarizes the effectiveness of these modifications based on reflection of the students. Recommendations for educators and students are also provided. Our findings and recommendations will help educators and students better coordinate and apply agile practices on industry-based projects in university contexts.}
}
@article{MICHAIL2022100104,
title = {Detection of fake news campaigns using graph convolutional networks},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100104},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100104},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000477},
author = {Dimitrios Michail and Nikos Kanakaris and Iraklis Varlamis},
keywords = {Fake news, Astroturfing, Graph convolutional networks, Disinformation, Graph attention networks},
abstract = {The detection of organised disinformation campaigns that spread fake news, by first camouflaging them as real ones is crucial in the battle against misinformation and disinformation in social media. This article presents a method for classifying the diffusion graphs of news formed in social media, by taking into account the profiles of the users that participate in the graph, the profiles of their social relations and the way the news spread, ignoring the actual text content of the news or the messages that spread it. This increases the robustness of the method and widens its applicability in different contexts. The results of this study show that the proposed method outperforms methods that rely on textual information only and provide a model that can be employed for detecting similar disinformation campaigns on different context in the same social medium.}
}
@article{BLAISE2020107391,
title = {Detection of zero-day attacks: An unsupervised port-based approach},
journal = {Computer Networks},
volume = {180},
pages = {107391},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107391},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620300761},
author = {Agathe Blaise and Mathieu Bouet and Vania Conan and Stefano Secci},
abstract = {Last years have witnessed more and more DDoS attacks towards high-profile websites, as the Mirai botnet attack on September 2016, or more recently the memcached attack on March 2018, this time with no botnet required. These two outbreaks were not detected nor mitigated during their spreading, but only at the time they happened. Such attacks are generally preceded by several stages, including infection of hosts or device fingerprinting; being able to capture this activity would allow their early detection. In this paper, we propose a technique for the early detection of emerging botnets and newly exploited vulnerabilities, which consists in (i) splitting the detection process over different network segments and retaining only distributed anomalies, (ii) monitoring at the port-level, with a simple yet efficient change-detection algorithm based on a modified Z-score measure. We argue how our technique, named Split-and-Merge, can ensure the detection of large-scale zero-day attacks and drastically reduce false positives. We apply the method on two datasets: the MAWI dataset, which provides daily traffic traces of a transpacific backbone link, and the UCSD Network Telescope dataset which contains unsolicited traffic mainly coming from botnet scans. The assumption of a normal distribution – for which the Z-score computation makes sense – is verified through empirical measures. We also show how the solution generates very few alerts; an extensive evaluation on the last three years allows identifying major attacks (including Mirai and memcached) that current Intrusion Detection Systems (IDSs) have not seen. Finally, we classify detected known and unknown anomalies to give additional insights about them.}
}
@article{KUHN2023103286,
title = {Common vulnerability scoring system prediction based on open source intelligence information sources},
journal = {Computers & Security},
volume = {131},
pages = {103286},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103286},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823001967},
author = {Philipp Kühn and David N. Relke and Christian Reuter},
keywords = {IT Security, Common vulnerability scoring system, Classification, National vulnerability database, Security management, Deep learning},
abstract = {The number of newly published vulnerabilities is constantly increasing. Until now, the information available when a new vulnerability is published is manually assessed by experts using a Common Vulnerability Scoring System (CVSS) vector and score. This assessment is time consuming and requires expertise. Various works already try to predict CVSS vectors or scores using machine learning based on the textual descriptions of the vulnerability to enable faster assessment. However, for this purpose, previous works only use the texts available in databases such as National Vulnerability Database. With this work, the publicly available web pages referenced in the National Vulnerability Database are analyzed and made available as sources of texts through web scraping. A Deep Learning based method for predicting the CVSS vector is implemented and evaluated. The present work provides a classification of the National Vulnerability Database’s reference texts based on the suitability and crawlability of their texts. While we identified the overall influence of the additional texts is negligible, we outperformed the state-of-the-art with our Deep Learning prediction models.}
}
@article{WAGNER2022103927,
title = {Building product ontology: Core ontology for Linked Building Product Data},
journal = {Automation in Construction},
volume = {133},
pages = {103927},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103927},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003782},
author = {Anna Wagner and Wendelin Sprenger and Christoph Maurer and Tilmann E. Kuhn and Uwe Rüppel},
keywords = {Product Data, Linked Data, Semantic Web technologies, Construction industry, Linked Product Data},
abstract = {The digitalisation of the Architecture, Engineering and Construction domain introduced new methods for digital collaboration, i.e. Building Information Modelling (BIM). While this method focuses on building data, the distribution of digital product models is still problematic, complicating uniform product searches and automated product data processing. Existing schemas, such as a subpart of the Industry Foundation Classes or the German VDI 3805, rely on rigid or template-driven schemas, that do not support the description of innovative or multi-functional products or impose a large schema overhead and complexity on manufacturers. Therefore, this article combines flexible and modular product descriptions with Semantic Web technologies and Linked Data. By applying Web-based technologies, the searchability of product data and the applicability of distributed data are expected to be enhanced. More precisely, this article proposes a concept for Linked Building Product Data and introduces the generic Building Product Ontology as a potential core schema of the concept. To demonstrate the feasibility of Linked Building Product Data and the Building Product Ontology, the authors apply both the concept and the data schema to innovative and multi-functional example products that cannot be described with the existing approaches for product descriptions. The evaluation demonstrates the flexibility, modularity and overall suitability of the presented concepts, meeting all collected requirements for digital product descriptions. Hence, Linked Building Product Data may solve existing issues with rigid product description schemas. At the same time, this approach complements the current research trend of Linked Building Data.}
}
@article{SHARMA2022102627,
title = {Orchestration of APT malware evasive manoeuvers employed for eluding anti-virus and sandbox defense},
journal = {Computers & Security},
volume = {115},
pages = {102627},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102627},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000268},
author = {Amit Sharma and Brij B. Gupta and Awadhesh Kumar Singh and V.K. Saraswat},
keywords = {Advanced persistent threat, Anti-Virus evasion, Process injection, Covert communication, Anti-Debug, Anti-Virtual machine, Evasive manoeuvers re-Engineering framework(EMRF), Fileless malware, DLL Hijacking, Code obfuscation, IAT Hooking, Windows management instrumentation(WMI)},
abstract = {The modern day cyber attacks are highly targeted and incorporate advanced tactics, techniques and procedures for greater stealth, impact and success. These attacks are also known as Advanced Persistent Threats(APT) because of their evasive and stealth nature along with longer foothold on the victim’s digital infrastructure. The malware involved in APT attacks are sophisticated and developed with the intention of sabotaging the victim’s digital infrastructure or performing espionage. They are capable of targeting multiple operating environments starting from desktop and server operating systems (Windows, Linux and MacOS), Mobile platforms (Android, iOS), Embedded platforms (IoT Devices), to Industrial control systems (ICS/SCADA Devices). The evolution of evasive tactics and techniques employed in such advanced malware leads to extensive research efforts to develop mechanisms that can counter these evasion techniques. The research primarily aims to demonstrate that evasive manoeuvers are currently over-weighing the security countermeasures deployed by the prevalent security solutions. This paper will first explain the evasion mechanism in a systematic manner employed in modern APT malware and aims to implement a novel Evasive Manoeuvers Re-Engineering Framework(EMRF).EMRF aims to establish and demonstrate combinations of evasive manoeuvers with much known APT malware samples to elude security solutions. The payload variants, i.e., executable, dynamic link library, and shell-code, were experimented through a research-based framework EMRF to demonstrate 36% to 96% of evasive behavior countering the majority of defender engines. The EMRF system with its dynamic user defined evasion manoeuvers is able to transform non-zero-day payloads more potent by evading majority of the modern security solutions. This research clearly demonstrates the attacker’s ability to deliver non-zero-day payloads easily rather than investing resources and time in discovering zero-day exploits and developing zero-day payloads. This important observation can potentially disrupt the Advanced Persistent Threat Defenses incorporated in modern day security solution where focus is mainly on to detect zero-day payloads and exploits. Exhibiting the threat landscape poised due to APT, the paper utilizes a dataset of 4403 APT malware samples to extract and orchestrate the prevalence of evasive manoeuvers like stealth, covert communication, and anti-analysis mechanisms. This paper will contribute towards advanced malware analysis as an avenue to analyzing intrusion, evasion, and deception to prevent detection and verification, an association of responsibility, and determination of intent.}
}
@article{SONG2024632,
title = {A study of the relationship of malware detection mechanisms using Artificial Intelligence},
journal = {ICT Express},
volume = {10},
number = {3},
pages = {632-649},
year = {2024},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2024.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405959524000298},
author = {Jihyeon Song and Sunoh Choi and Jungtae Kim and Kyungmin Park and Cheolhee Park and Jonghyun Kim and Ikkyun Kim},
keywords = {Malware detection, Machine learning, Deep learning},
abstract = {Implementation of malware detection using Artificial Intelligence (AI) has emerged as a significant research theme to combat evolving various types of malwares. Researchers implement various detection mechanisms using shallow and deep learning models to counter new malware, and they continue to develop these mechanisms today. However, in the field of malware detection using AI, there are difficulties in collecting data, and it is difficult to compare research content and performance with related studies. Meanwhile, the number of well-organized papers is not sufficient to understand the overall research flow of these related studies. Before starting new research, researchers need to analyze the current state of research in the malware detection field they want to study. Therefore, based on these requirements, we present a summary of the general criteria related to malware detection and a classification table for detection mechanisms. Additionally, we have organized many studies in the field of various types of malware detection so that they can be viewed at a glance. We hope that the provided survey can help new researchers quickly understand the research flow in the field of AI-based malware detection and establish the direction for future research.}
}
@article{YAO2024105565,
title = {Enhancing cyber risk identification in the construction industry using language models},
journal = {Automation in Construction},
volume = {165},
pages = {105565},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105565},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003017},
author = {Dongchi Yao and Borja {García de Soto}},
keywords = {Cybersecurity, Risk identification, Deep learning, Language model, Construction industry},
abstract = {Modern construction projects are vulnerable to cyber-attacks due to insufficient attention to cybersecurity. Cyber risks in construction projects are not fully recognized, and the relevant literature is limited. To address this gap, the capabilities of a language model were leveraged to analyze extensive text, tailored to identify cyber risks. The model was trained using a curated corpus related to construction cybersecurity, enhanced by Supervised Fine-Tuning and Reinforcement Learning from Human Feedback techniques. The findings demonstrate advancements in the model's ability to understand cybersecurity and generate responses to cybersecurity questions. Using this model, a prioritized checklist of cyber risks across project phases was developed, establishing a new industry benchmark. This checklist can be utilized by various groups, including project managers and risk analysts. The model allows for updates with new data, ensuring the checklist remains current. The upgraded model holds significant promise for industry-wide applications, serving as an intelligent cybersecurity consultant.}
}
@article{COSTASILVA2022111397,
title = {A qualitative analysis of themes in instant messaging communication of software developers},
journal = {Journal of Systems and Software},
volume = {192},
pages = {111397},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111397},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001133},
author = {Camila {Costa Silva} and Matthias Galster and Fabian Gilson},
keywords = {Instant messaging, Developer communication, Reusable knowledge, Software engineering themes, Thematic analysis},
abstract = {Software developers use instant messaging (e.g., Slack, Gitter) to collaboratively discuss software engineering problems and solutions. This communication takes place in chat rooms that generally contain a description of the main topic of discussion and the messages exchanged. To analyze whether and how the knowledge accumulated in these chat rooms is relevant to other developers, we first need to understand the themes discussed in these chat rooms. In this paper, we used thematic analysis to manually identify software engineering themes in the description of 87 chat rooms of Gitter, an instant messaging tool for software developers. Then, we checked whether these themes also occur in 184 public chat rooms of Slack, another instant messaging tool. We identified 47 themes in Gitter chat rooms, and regarding the applicability of themes, we could relate 36 of our themes to 173 Slack chat rooms. Our results indicate that, in the context of our study, chat rooms in developer instant messaging communication are mostly about software development technologies and practices rather than development processes. Furthermore, most chat rooms are topic- rather than project-related (e.g., a chat room used by developers of a particular software development project).}
}
@article{LEWONIEWSKI20223290,
title = {Identification of Important Web Sources of Information on Wikipedia across various Topics and Languages},
journal = {Procedia Computer Science},
volume = {207},
pages = {3290-3299},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.387},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012777},
author = {Włodzimierz Lewoniewski},
keywords = {Wikipedia, reliability, websites, reliable sources, information quality, topic classification},
abstract = {Despite the fact, that over 21 years Wikipedia is edited by volunteers from all over the world with different views, culture, education and competences, this free encyclopedia is one of the most popular source of knowledge in the Internet. Freedom to edit does not mean, that you can put there whatever content you want. One of the core rules of Wikipedia says, that information in its articles should be based on reliable sources and Wikipedia readers must be able to verify particular facts in text. However, reliability is a subjective concept and a reputation of the same source can be assessed diffidently depending on a person (or group of persons), language and topic. So each language version of Wikipedia may have own rules or criteria on how the website must be assessed before it can be used as a source in references. At the same time, nowadays there are over 1 billion websites on the Internet and only few developed language chapters of the encyclopedia contains non-exhaustive lists of less than 1 thousand popular websites with reliability assessment. Additionally, since reputation of the source can be changed during the time, such lists must be updated regularly. This study presents result of important web sources identification based on analysis of over 230 million references that were extracted from over 40 million Wikipedia article of 42 most developed language version. Additionally, general statistics on references usage for each considered Wikipedia language were counted, including average number of references, number of unique references, scientific score, number of websites in references. Next, Wikipedia articles were assigned to different topics in each considered language. This allows to find differences in reliability and popularity of the same sources of information between Wikipedia languages, as well as find important websites in specific areas of knowledge.}
}
@article{BHARDWAJ2021100332,
title = {Distributed denial of service attacks in cloud: State-of-the-art of scientific and commercial solutions},
journal = {Computer Science Review},
volume = {39},
pages = {100332},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100332},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304329},
author = {Aanshi Bhardwaj and Veenu Mangat and Renu Vig and Subir Halder and Mauro Conti},
keywords = {Anomaly based detection, Cloud computing, DDoS attack, Economic denial of sustainability, Machine learning, Deep learning, Statistical methods},
abstract = {Cloud computing model provides on demand, elastic and fully managed computer system resources and services to organizations. However, attacks on cloud components can cause inestimable losses to cloud service providers and cloud users. One such category of attacks is the Distributed Denial of Service (DDoS), which can have serious consequences including impaired customer experience, service outage and in severe cases, complete shutdown and total economic unsustainability. Advances in Internet of Things (IoT) and network connectivity have inadvertently facilitated launch of DDoS attacks which have increased in volume, frequency and intensity. Recent DDoS attacks involving new attack vectors and strategies, have precipitated the need for this survey. In this survey, we mainly focus on finding the gaps, as well as bridging those gaps between the future potential DDoS attacks and state-of-the-art scientific and commercial DDoS attack defending solutions. It seeks to highlight the need for a comprehensive detection approach by presenting the recent threat landscape and major cloud attack incidents, estimates of future DDoS, illustrative use cases, commercial DDoS solutions, and the laws governing DDoS attacks in different nations. An up-to-date survey of DDoS detection methods, particularly anomaly based detection, available research tools, platforms and datasets, has been given. This paper further explores the use of machine learning methods for detection of DDoS attacks and investigates features, strengths, weaknesses, tools, datasets, and evaluates results of the methods in the context of the cloud. A summary comparison of statistical, machine learning and hybrid methods has been brought forth based on detailed analysis. This paper is intended to serve as a ready reference for the research community to develop effective and innovative detection mechanisms for forthcoming DDoS attacks in the cloud environment. It will also sensitize cloud users and providers to the urgent need to invest in deployment of DDoS detection mechanisms to secure their assets.}
}
@article{WEINGARTNER2023100125,
title = {Quantum Tic-Tac-Toe - learning the concepts of quantum mechanics in a playful way},
journal = {Computers and Education Open},
volume = {4},
pages = {100125},
year = {2023},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2023.100125},
url = {https://www.sciencedirect.com/science/article/pii/S2666557323000046},
author = {Maurice Weingärtner and Tim Weingärtner},
keywords = {Quantum computing, Education, Quantum game theory, Tic-Tac-Toe},
abstract = {Quantum mechanics is a complex matter. Nonetheless, given the impending arrival of quantum computers and the necessity for quantum programming abilities, more students should get acquainted with this subject. We provide a game-based learning approach based on Tic-Tac-Toe in a quantum modified version as a result of our study. We created a prototype to demonstrate and evaluate our assumptions using the design science research technique. Qualitative user feedback provided us with vital insights and shown that this game-based method helps to cope with quantum physics in a fun way. A majority of those who took part in the study stated that their interest in the subject had increased. However, for novices, it is necessary to follow them during the initial period of their training. The comments were quite helpful in optimizing the prototype. We found that our strategy, which included the use of a virtual opponent as well as the presentation of extra information about the quantum circuit, was more effective in helping participants comprehend quantum physics than earlier Tic-Tac-Toe-based learning settings.}
}
@article{AZIMJONOV2024103598,
title = {Designing accurate lightweight intrusion detection systems for IoT networks using fine-tuned linear SVM and feature selectors},
journal = {Computers & Security},
volume = {137},
pages = {103598},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103598},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823005084},
author = {Jahongir Azimjonov and Taehong Kim},
keywords = {Determining relevant and efficient feature subsets, Lightweight IDSs, Fine-tuned LSVMs and feature selectors, IoT network security},
abstract = {Intrusion detection systems (IDSs) play a crucial role in ensuring the security and integrity of Internet of Things (IoT) networks by blocking unwanted packets and facilitating secure traffic flow. However, traditional IDSs based on data mining, fuzzy logic, heuristics, rough sets, or conventional machine learning (ML) techniques often lack accuracy and are not energy efficient, primarily due to inappropriate feature selection or the use of all features in datasets. To address these challenges, this study proposes a lightweight, accurate, and high-performance IDSs for IoT networks using fine-tuned Linear Support Vector Machines (LSVMs) and feature selection methods. Four feature selectors, including Importance Coefficient-, Forward- and Backward-Sequential-, and Correlation Coefficient-based approaches, were applied to identify the most important and efficient features from three datasets: KDD Cup-1999, BotIoT-2018, and N-BaIoT-2021. The fine-tuned LSVMs algorithm was then trained on subsets of the selected and full features of the datasets to detect various IoT botnet attacks. Evaluation results show that the IDS models trained with subsets of relevant features outperform those trained with the full feature sets of the datasets in terms of training and test performance and accuracy. The study concludes that it is possible to develop lightweight IDSs by training them with a reduced number of features (6) instead of using the full features (40, 15, 115) in KDD Cup-1999, BotIoT-2018, and N-BaIoT-2021, respectively. The findings highlight a potential for significantly improving the efficiency and accuracy of IDSs on IoT networks using the fine-tuned feature selectors and LSVMs.}
}
@article{SHAHI2021100104,
title = {An exploratory study of COVID-19 misinformation on Twitter},
journal = {Online Social Networks and Media},
volume = {22},
pages = {100104},
year = {2021},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2020.100104},
url = {https://www.sciencedirect.com/science/article/pii/S2468696420300458},
author = {Gautam Kishore Shahi and Anne Dirkson and Tim A. Majchrzak},
keywords = {Misinformation, Twitter, Social media, COVID-19, Fake news, Coronavirus, Diffusion of information},
abstract = {During the COVID-19 pandemic, social media has become a home ground for misinformation. To tackle this infodemic, scientific oversight, as well as a better understanding by practitioners in crisis management, is needed. We have conducted an exploratory study into the propagation, authors and content of misinformation on Twitter around the topic of COVID-19 in order to gain early insights. We have collected all tweets mentioned in the verdicts of fact-checked claims related to COVID-19 by over 92 professional fact-checking organisations between January and mid-July 2020 and share this corpus with the community. This resulted in 1500 tweets relating to 1274 false and 226 partially false claims, respectively. Exploratory analysis of author accounts revealed that the verified twitter handle(including Organisation/celebrity) are also involved in either creating(new tweets) or spreading(retweet) the misinformation. Additionally, we found that false claims propagate faster than partially false claims. Compare to a background corpus of COVID-19 tweets, tweets with misinformation are more often concerned with discrediting other information on social media. Authors use less tentative language and appear to be more driven by concerns of potential harm to others. Our results enable us to suggest gaps in the current scientific coverage of the topic as well as propose actions for authorities and social media users to counter misinformation.}
}
@article{LI2024103879,
title = {Precursor of privacy leakage detection for individual user},
journal = {Computers & Security},
volume = {142},
pages = {103879},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103879},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824001809},
author = {Xuefeng Li and Chensu Zhao and Yi Hu and Honglin Xie and Yuhang Wang and Jingyang Zhao},
keywords = {Privacy, Social networks, Temporal, Predict},
abstract = {The pursuit of dividends from the burgeoning realm of data traffic has emerged as a prevailing trend. Given this current state of affairs, the safeguarding of personal information has become an urgent task. Current methods for personal privacy protection primarily offer alerts when information breaches occur, but at that stage, irreversible breaches have already transpired. Thus, the study of preemptive privacy breach prediction is a task of significant importance within the realm of privacy protection. However, the endeavor to predict privacy breaches in advance remains exceedingly challenging, owing to several factors: (i) The complexity of social networks gives rise to high-dimensional features. (ii) Concerning the comprehensive and precise capture of pathways leading to information leakage. (iii) How to employ time series data effectively to realize early predictions. This study proposes a novel approach for constructing graph neural networks and forecasting privacy breaches, centered on the context of user-generated content within specific time frames, integrates spatial graph structures with temporal series information. The integration can entirely achieve the advance prediction of users' privacy status, thereby preventing information leakage. Empirical tests on real-world datasets demonstrate that our approach surpasses traditional time series forecasting methods in privacy breach predictions, achieving a notable average improvement of 2% in F1 score, Recall, and Precision metrics.}
}
@article{ROUMANI2024103971,
title = {The diffusion of malicious content on Twitter and its impact on security},
journal = {Information & Management},
volume = {61},
number = {5},
pages = {103971},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.103971},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624000533},
author = {Yaman Roumani},
keywords = {Malicious contents, Social media, Vulnerability, Active exploitation},
abstract = {While Twitter remains one of the most popular social media networks within the information security community, threat actors continue to abuse the platform to create, share, and spread malicious contents. In this study, we focus on whether Twitter- and vulnerability-related features can help predict vulnerabilities known to be actively exploited. Using a sample of 6004 tweets, results show that Twitter features (tweets and quote tweets) that combine the benefits of both content creation and sharing can predict active exploitation of vulnerabilities. Furthermore, findings show that certain technical and vulnerability-related features are also capable of predicting active exploitations.}
}
@article{GASPARINI2022108526,
title = {Benchmark dataset of memes with text transcriptions for automatic detection of multi-modal misogynistic content},
journal = {Data in Brief},
volume = {44},
pages = {108526},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108526},
url = {https://www.sciencedirect.com/science/article/pii/S235234092200720X},
author = {Francesca Gasparini and Giulia Rizzi and Aurora Saibene and Elisabetta Fersini},
keywords = {Misogyny detection, Multi-modal content, Memes, Cybersexism, Visual and textual cues},
abstract = {In this paper we present a benchmark dataset generated as part of a project for automatic identification of misogyny within online content, which focuses in particular on memes. The benchmark here described is composed of 800 memes collected from the most popular social media platforms, such as Facebook, Twitter, Instagram and Reddit, and consulting websites dedicated to collection and creation of memes. To gather misogynistic memes, specific keywords that refer to misogynistic content have been considered as search criterion, considering different manifestations of hatred against women, such as body shaming, stereotyping, objectification and violence. In parallel, memes with no misogynist content have been manually downloaded from the same web sources. Among all the collected memes, three domain experts have selected a dataset of 800 memes equally balanced between misogynistic and non-misogynistic ones. This dataset has been validated through a crowdsourcing platform, involving 60 subjects for the labelling process, in order to collect three evaluations for each instance. Two further binary labels have been collected from both the experts and the crowdsourcing platform, for memes evaluated as misogynistic, concerning aggressiveness and irony. Finally for each meme, the text has been manually transcribed. The dataset provided is thus composed of the 800 memes, the labels given by the experts and those obtained by the crowdsourcing validation, and the transcribed texts. This data can be used to approach the problem of automatic detection of misogynistic content on the Web relying on both textual and visual cues, facing phenomenons that are growing every day such as cybersexism and technology-facilitated violence.}
}
@article{KUMARI2023103096,
title = {A comprehensive study of DDoS attacks over IoT network and their countermeasures},
journal = {Computers & Security},
volume = {127},
pages = {103096},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103096},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000068},
author = {Pooja Kumari and Ankit Kumar Jain},
keywords = {Internet of things, DDoS attack, Botnet, Machine learning, Software defined network},
abstract = {IoT offers capabilities to gather information from digital devices, infer from their results, and maintain and optimize these devices in different domains. IoT is heterogeneous in nature, which makes it prone to various security threats like confidentiality and integrity breaches, lack of availability of resources, trust issues, etc. The security concerns lead to different attacks over the system, and the Distributed Denial of Services (DDoS) bout is growing generously. DDoS is an assault that targets the availability of resources and servers of a network by flooding the communication medium from distinct locations by utilizing various IoT devices, which makes it harder to detect. Thus, analyzing and defending DDoS is a protruding field of research these days. The paper gives a thorough knowledge of DDoS over IoT. In this, we have critically analysed the existing DDoS variants, IoT Security issues, the execution of DDoS attempts, along with the exploitation of IoT devices and creation of them in Botnets or zombies. Moreover, the paper will also cover prevailing DDoS defense methodologies as well as their comparative analysis for ease of understanding.}
}
@article{WU2020300999,
title = {Digital forensic tools: Recent advances and enhancing the status quo},
journal = {Forensic Science International: Digital Investigation},
volume = {34},
pages = {300999},
year = {2020},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2020.300999},
url = {https://www.sciencedirect.com/science/article/pii/S2666281720301864},
author = {Tina Wu and Frank Breitinger and Stephen O'Shaughnessy},
keywords = {Digital forensic tools, Published software, Literature review, Open source software, Availability},
abstract = {Publications in the digital forensics domain frequently come with tools – a small piece of functional software. These tools are often released to the public for others to reproduce results or use them for their own purposes. However, there has been no study on the tools to understand better what is available and what is missing. For this paper we analyzed almost 800 articles from pertinent venues from 2014 to 2019 to answer the following three questions (1) what tools (i.e., in which domains of digital forensics): have been released; (2) are they still available, maintained, and documented; and (3) are there possibilities to enhance the status quo? We found 62 different tools which we categorized according to digital forensics subfields. Only 33 of these tools were found to be publicly available, the majority of these were not maintained after development. In order to enhance the status quo, one recommendation is a centralized repository specifically for tested tools. This will require tool researchers (developers) to spend more time on code documentation and preferably develop plugins instead of stand-alone tools.}
}
@article{BUCHANAN201788,
title = {A methodology for the security evaluation within third-party Android Marketplaces},
journal = {Digital Investigation},
volume = {23},
pages = {88-98},
year = {2017},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1742287617300245},
author = {William J. Buchanan and Simone Chiale and Richard Macfarlane},
keywords = {Malware analysis, APK, Android, Marketplace},
abstract = {This paper aims to evaluate possible threats with unofficial Android marketplaces, and geo-localize the malware distribution over three main regions: China; Europe; and Russia. It provides a comprehensive review of existing academic literature about security in Android focusing especially on malware detection systems and existing malware databases. Through the implementation of a methodology for identification of malicious applications it has been collected data revealing a 5% of them as malicious in an overall analysis. Furthermore, the analysis shown that Russia and Europe have a preponderance of generic detections and adware, while China is found to be targeted mainly by riskware and malware.}
}
@article{MOHDAMINUDDIN2023101778,
title = {WFP-Collector: Automated dataset collection framework for website fingerprinting evaluations on Tor Browser},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {9},
pages = {101778},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101778},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823003324},
author = {Mohamad Amar Irsyad {Mohd Aminuddin} and Zarul Fitri Zaaba},
keywords = {Security, Privacy, Website Fingerprinting, Tor Browser, Dataset Collection Framework, Webpage Crawling},
abstract = {Website Fingerprinting (WFP) is a technique that analyses browsing network traffic to infer a webpage that a user browsed in Tor Browser. A sufficiently large and clean dataset is essential for quality and accurate WFP experiments. Thus, there is a corresponding need to automate the dataset collection, filtering, and validation processes. This work introduces a new paradigm, WFP-Collector, an automatic dataset collection framework for WFP experiments. WFP-Collector enables researchers to automatically (1) create a visit database for webpage crawling, (2) collect various data and log for in-depth analysis, (3) webpage visits in tablet and mobile browsing modes, (4) throttle network bandwidth and latency performance, (5) validate and filter the collected data, (6) compress and upload the collected data to cloud storage, and (7) completion notification using Telegram and email. We developed a proof-of-concept of the WFP framework for simulation and comparison. We found that the WFP-Collector framework collects nine data items and produces over 55% larger collected data size than existing approaches. The captured packet size in tablet and mobile browsing modes is up to 57.5% smaller than in desktop mode. Moreover, the file compression in WFP-Collector can reduce up to 39.9% of the storage space required for data collection.}
}
@article{20211,
title = {Critical Java flaw puts millions of organisations at risk},
journal = {Network Security},
volume = {2021},
number = {12},
pages = {1-2},
year = {2021},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(21)00136-7},
url = {https://www.sciencedirect.com/science/article/pii/S1353485821001367},
abstract = {Threat actors are actively exploiting a critical flaw in Apache's Log4j logging library that allows them to execute arbitrary code, often with elevated privileges. The vulnerability (CVE-2021-44228) is trivially easy to exploit. And while a patch has been released, the library is in such widespread use – including by major platforms and services – that it may take some time before the threat is eliminated.}
}
@article{LI2022100113,
title = {A semantic ontology for representing and quantifying energy flexibility of buildings},
journal = {Advances in Applied Energy},
volume = {8},
pages = {100113},
year = {2022},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2022.100113},
url = {https://www.sciencedirect.com/science/article/pii/S2666792422000312},
author = {Han Li and Tianzhen Hong},
keywords = {Building energy flexibility, Semantic interoperability, Ontology, Grid-interactive efficient buildings, Demand side management},
abstract = {Energy flexibility of buildings can be an essential resource for a sustainable and reliable power grid with the growing variable renewable energy shares and the trend to electrify and decarbonize buildings. Traditional demand-side management technologies, advanced building controls, and emerging distributed energy resources (including electric vehicle, energy storage, and on-site power generation) enable the transition of the building stock to grid-interactive efficient buildings (GEBs) that operate efficiently to meet service needs and are responsive to grid pricing or carbon signals to achieve energy and carbon neutrality. Although energy flexibility has received growing attention from industry and the research community, there remains a lack of common ground for energy flexibility terminologies, characterization, and quantification methods. This paper presents a semantic ontology—EFOnt (Energy Flexibility Ontology)—that extends existing terminologies, ontologies, and schemas for building energy flexibility applications. EFOnt aims to serve as a standardized tool for knowledge co-development and streamlining energy flexibility related applications. We demonstrate potential use cases of EFOnt via two examples: (1) energy flexibility analytics with measured data from a residential smart thermostat dataset and a commercial building, and (2) modeling and simulation to evaluate energy flexibility of buildings. The compatibility of EFOnt with existing ontologies and the outlook of EFOnt's role in the building energy data tool ecosystem are discussed.}
}
@article{RUTTEN2024101105,
title = {Bi-allelic NIT1 variants cause a brain small vessel disease characterized by movement disorders, massively dilated perivascular spaces, and intracerebral hemorrhage},
journal = {Genetics in Medicine},
volume = {26},
number = {6},
pages = {101105},
year = {2024},
issn = {1098-3600},
doi = {https://doi.org/10.1016/j.gim.2024.101105},
url = {https://www.sciencedirect.com/science/article/pii/S1098360024000388},
author = {Julie W. Rutten and Minne N. Cerfontaine and Kyra L. Dijkstra and Aat A. Mulder and Jeroen Vreijling and Mark Kruit and Roman I. Koning and Susanne T. {de Bot} and Koen M. {van Nieuwenhuizen} and Hans J. Baelde and Henk W. Berendse and Leon H. Mei and George J.G. Ruijter and Frank Baas and Carolina R. Jost and Sjoerd G. {van Duinen} and Esther A.R. Nibbeling and Gido Gravesteijn and Saskia A.J. {Lesnik Oberstein}},
keywords = {Autosomal recessive inheritance, Etat-criblé, Hemorrhagic stroke, NIT1, Small vessel disease},
abstract = {Purpose
To describe a recessively inherited cerebral small vessel disease, caused by loss-of-function variants in Nitrilase1 (NIT1).
Methods
We performed exome sequencing, brain magnetic resonance imaging, neuropathology, electron microscopy, western blotting, and transcriptomic and metabolic analyses in 7 NIT1-small vessel disease patients from 5 unrelated pedigrees.
Results
The first identified patients were 3 siblings, compound heterozygous for the NIT1 c.727C>T; (p.Arg243Trp) variant and the NIT1 c.198_199del; p.(Ala68∗) variant. The 4 additional patients were single cases from 4 unrelated pedigrees and were all homozygous for the NIT1 c.727C>T; p.(Arg243Trp) variant. Patients presented in mid-adulthood with movement disorders. All patients had striking abnormalities on brain magnetic resonance imaging, with numerous and massively dilated basal ganglia perivascular spaces. Three patients had non-lobar intracerebral hemorrhage between age 45 and 60, which was fatal in 2 cases. Western blotting on patient fibroblasts showed absence of NIT1 protein, and metabolic analysis in urine confirmed loss of NIT1 enzymatic function. Brain autopsy revealed large electron-dense deposits in the vessel walls of small and medium sized cerebral arteries.
Conclusion
NIT1-small vessel disease is a novel, autosomal recessively inherited cerebral small vessel disease characterized by a triad of movement disorders, massively dilated basal ganglia perivascular spaces, and intracerebral hemorrhage.}
}
@article{JONES2021102340,
title = {Random choices facilitate solutions to collective network coloring problems by artificial agents},
journal = {iScience},
volume = {24},
number = {4},
pages = {102340},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.102340},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221003084},
author = {Matthew I. Jones and Scott D. Pauls and Feng Fu},
keywords = {Computer Science, Artificial Intelligence, Human-Computer Interaction},
abstract = {Summary
Global coordination is required to solve a wide variety of challenging collective action problems from network colorings to the tragedy of the commons. Recent empirical study shows that the presence of a few noisy autonomous agents can greatly improve collective performance of humans in solving networked color coordination games. To provide analytical insights into the role of behavioral randomness, here we study myopic artificial agents attempting to solve similar network coloring problems using decision update rules that are only based on local information but allow random choices at various stages of their heuristic reasonings. We show that the resulting efficacy of resolving color conflicts is dependent on the implementation of random behavior of agents and specific population characteristics. Our work demonstrates that distributed greedy optimization algorithms exploiting local information should be deployed in combination with occasional exploration via random choices in order to overcome local minima and achieve global coordination.}
}
@article{MULLICK2024112659,
title = {MatSciRE: Leveraging pointer networks to automate entity and relation extraction for material science knowledge-base construction},
journal = {Computational Materials Science},
volume = {233},
pages = {112659},
year = {2024},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112659},
url = {https://www.sciencedirect.com/science/article/pii/S0927025623006535},
author = {Ankan Mullick and Akash Ghosh and G. Sai Chaitanya and Samir Ghui and Tapas Nayak and Seung-Cheol Lee and Satadeep Bhattacharjee and Pawan Goyal},
keywords = {Material Science entity extraction, Material science relation extraction, Triplet (entity, relation, entity) extraction from material science, Triplet extraction},
abstract = {Material science literature is a rich source of factual information about various categories of entities (like materials and compositions) and various relations between these entities, such as conductivity, voltage, etc. Automatically extracting this information to generate a material science knowledge base is a challenging task. In this paper, we propose MatSciRE (Material Science Relation Extractor), a Pointer Network-based encoder–decoder framework, to jointly extract entities and relations from material science articles as a triplet (entity1,relation,entity2). Specifically, we target the battery materials and identify five relations to work on — conductivity, coulombic efficiency, capacity, voltage, and energy. Our proposed approach achieved a much better F1-score (0.771) than a previous attempt using ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown in Fig. 1. The material information is extracted from material science literature in the form of entity–relation triplets using MatSciRE.}
}
@article{KECHT2023102176,
title = {Quantifying chatbots’ ability to learn business processes},
journal = {Information Systems},
volume = {113},
pages = {102176},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102176},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000121},
author = {Christoph Kecht and Andreas Egger and Wolfgang Kratsch and Maximilian Röglinger},
keywords = {Chatbots, Process mining, Natural language processing, Conformance checking},
abstract = {Chatbots enable organizations in the business-to-customer domain to respond to repetitive requests efficiently. Extant approaches in Natural Language Processing (NLP) already address the essential requirement of understanding user input and synthesizing a response as close as possible to a response a human interlocutor would give. However, we argue that the organizational adoption of chatbots further depends on the underlying model’s capability to learn and comply with organizations’ business processes, for example, authenticating a customer before providing sensitive details. To address this issue, we develop an approach that quantifies chatbots’ ability to learn business processes using standardized process mining metrics. We demonstrate our approach by training chatbots on a dataset of more than 500,000 customer service conversations from three companies on Twitter and show how our approach supports the quantification of a chatbot’s overall ability to learn business processes from the training data. Furthermore, we quantify a chatbot’s ability to learn a particular variant of the underlying process and we show how to compare the chatbot’s executed steps against a given normative process model. Our approach that seamlessly integrates with existing approaches to evaluate NLP-based chatbots mitigates the current hurdles that practitioners face and, therefore, strives to foster the adoption of chatbots in practice.}
}
@article{LEVIN2024104771,
title = {An evaluation of the capabilities of language models and nurses in providing neonatal clinical decision support},
journal = {International Journal of Nursing Studies},
volume = {155},
pages = {104771},
year = {2024},
issn = {0020-7489},
doi = {https://doi.org/10.1016/j.ijnurstu.2024.104771},
url = {https://www.sciencedirect.com/science/article/pii/S002074892400083X},
author = {Chedva Levin and Tehilla Kagan and Shani Rosen and Mor Saban},
keywords = {Artificial Intelligence, ChatGPT, Claude, Clinical reasoning, Neonatal care},
abstract = {Aim
To assess the clinical reasoning capabilities of two large language models, ChatGPT-4 and Claude-2.0, compared to those of neonatal nurses during neonatal care scenarios.
Design
A cross-sectional study with a comparative evaluation using a survey instrument that included six neonatal intensive care unit clinical scenarios.
Participants
32 neonatal intensive care nurses with 5–10 years of experience working in the neonatal intensive care units of three medical centers.
Methods
Participants responded to 6 written clinical scenarios. Simultaneously, we asked ChatGPT-4 and Claude-2.0 to provide initial assessments and treatment recommendations for the same scenarios. The responses from ChatGPT-4 and Claude-2.0 were then scored by certified neonatal nurse practitioners for accuracy, completeness, and response time.
Results
Both models demonstrated capabilities in clinical reasoning for neonatal care, with Claude-2.0 significantly outperforming ChatGPT-4 in clinical accuracy and speed. However, limitations were identified across the cases in diagnostic precision, treatment specificity, and response lag.
Conclusions
While showing promise, current limitations reinforce the need for deep refinement before ChatGPT-4 and Claude-2.0 can be considered for integration into clinical practice. Additional validation of these tools is important to safely leverage this Artificial Intelligence technology for enhancing clinical decision-making.
Impact
The study provides an understanding of the reasoning accuracy of new Artificial Intelligence models in neonatal clinical care. The current accuracy gaps of ChatGPT-4 and Claude-2.0 need to be addressed prior to clinical usage.}
}
@article{LEITER2024100541,
title = {ChatGPT: A meta-analysis after 2.5 months},
journal = {Machine Learning with Applications},
volume = {16},
pages = {100541},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2024.100541},
url = {https://www.sciencedirect.com/science/article/pii/S2666827024000173},
author = {Christoph Leiter and Ran Zhang and Yanran Chen and Jonas Belouadi and Daniil Larionov and Vivian Fresen and Steffen Eger},
keywords = {ChatGPT, Sentiment analysis, Emotion analysis, Science, Large language models},
abstract = {ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and media attention since its release in November 2022. However, little hard evidence is available regarding its perception in various sources. In this paper, we analyze over 300,000 tweets and more than 150 scientific papers to investigate how ChatGPT is perceived and discussed. Our findings show that ChatGPT is generally viewed as of high quality, with positive sentiment and emotions of joy dominating social media. Its perception has slightly decreased since its debut, however, with joy decreasing and (negative) surprise on the rise, and it is perceived more negatively in languages other than English. In recent scientific papers, ChatGPT is characterized as a great opportunity across various fields including the medical domain, but also as a threat concerning ethics and receives mixed assessments for education. Our comprehensive meta-analysis of ChatGPT’s perception after 2.5 months since its release can contribute to shaping the public debate and informing its future development. We make our data available.11https://github.com/NL2G/ChatGPTReview.}
}
@article{MUTER2024110527,
title = {Dutch tweets before, during, and after a black lives matter demonstration in Amsterdam: Expert annotated data, protocol, and labelling tool},
journal = {Data in Brief},
volume = {55},
pages = {110527},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.110527},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924004955},
author = {Laurens H.F. Müter and Christof {van Nimwegen} and Remco C. Veltkamp},
keywords = {Social media analysis, Protest, COVID-19, Natural language processing, Manual labelling},
abstract = {The Netherlands police are looking for measures to examine sentiment on social media related to protest demonstrations. While models exist to detect more subtle expressions of sentiment within tweets, models trained in the Dutch language are scarce. Being able to predict sentiment development during protests is relevant for parties like the Dutch government and the police to get more insight to when and where potential law enforcement is needed for public order and safety. Therefore, to analyse sentiment before, during, and after protest demonstrations, data was collected with tweets related to a Black Lives Matter protest that took place in Amsterdam during the COVID-19 pandemic. All tweets have been manually labelled by a dedicated open-source intelligence (OSINT) team within the Netherlands police following an established protocol. Both the data and the protocol are available, and interesting for researchers in natural language processing, topic detection, sentiment analysis, and protests analysis. The developed labelling tool for the labelling process is publicly available.}
}
@article{CHAN2019138,
title = {Web-based experimental economics software: How do they compare to desirable features?},
journal = {Journal of Behavioral and Experimental Finance},
volume = {23},
pages = {138-160},
year = {2019},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S221463501830090X},
author = {Shu Wing Chan and Steven Schilizzi and Md Sayed Iftekhar and Raymond {Da Silva Rosa}},
keywords = {Experimental economics, Web-based, Software, Online experiments, Web-based experiments, Economic experiments},
abstract = {Web-based experiments that cut across the lab vs. field distinction are increasingly popular with economists. However, non-standardized software features and services hinder comparability and replication. This study reviews a wide selection of experimental economics software packages and evaluates them against criteria based on the logistics and operational requirements of economic experiments. We find that oTree and SoPHIE rank highest across criteria, but Veconlab and classEx might be suitable for those with a dominant need for a large library of ready-made experiments. We find a portability gap: no presently available software allows portability of experiments across platforms because of technical complexity and the challenging coordination needs of experimental economists. As a result, experiments may be replicated only on the same platform or with the same software, but general replicability is slow and costly. This constrains the development of experimental economics as a replicable science.}
}
@article{FREDERICK2021113533,
title = {A thermo-mechanical terrestrial model of Arctic coastal erosion},
journal = {Journal of Computational and Applied Mathematics},
volume = {397},
pages = {113533},
year = {2021},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2021.113533},
url = {https://www.sciencedirect.com/science/article/pii/S0377042721001527},
author = {Jennifer Frederick and Alejandro Mota and Irina Tezaur and Diana Bull},
keywords = {Thermal, Mechanics, Coupling, Arctic coastal erosion, Permafrost},
abstract = {Although the Arctic comprises one-third of the global coastline and has some of the fastest eroding coasts, current tools for quantifying permafrost erosion are unable to explain the episodic, storm-driven erosion events that occur in this region. In this paper, we present a novel multi-physics finite element model for the numerical simulation of Arctic coastal permafrost degradation: the terrestrial component of the Arctic Coastal Erosion (ACE) model. This model is comprised of two main ingredients: (1) a solid mechanics model that calculates the three-dimensional (3D) stress, strain and displacement fields of the underlying permafrost developing in response to a frozen water content dependent plasticity model, and (2) a novel thermal model governing the 3D heat conduction and solid–liquid phase change occurring within the permafrost. These two physics sets are coupled via a sequential thermo-mechanical coupling scheme developed within the Albany LCM open-source finite element code. Unlike prior approaches, our modeling methodology enables failure from any allowable deformation (block failure, thermo-denudation, thermo-abrasion); moreover, failure modes develop from constitutive (rather than empirical) relationships inherent in the underlying finite element model. Elements are dynamically removed from the underlying finite element mesh so as to simulate transient permafrost erosion events. Our thermo-mechanical terrestrial model is evaluated on a pseudo-realistic problem in which a slice of permafrost is exposed to realistic oceanic and atmospheric forcing boundary condition data occurring at Drew Point, Alaska in July 2018.}
}
@article{GAO2024103643,
title = {Latent representation discretization for unsupervised text style generation},
journal = {Information Processing & Management},
volume = {61},
number = {3},
pages = {103643},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103643},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000037},
author = {Yang Gao and Qianhui Liu and Yizhe Yang and Ke Wang},
keywords = {Style transfer, Controllable generation, Discrete space, Text generation},
abstract = {Language models, such as BART and GPT, have been shown to be highly effective at producing quality headlines. However, without clear guidelines for what constitutes a particular writing style, they may generate text that does not meet the desired style criteria (i.e., attention-grabbing), even if the resulting text is grammatically correct and semantically coherent. In this study, we introduce a novel approach called Discretized Style Transfer (DST) for unsupervised style transfer. We argue that the textual style signal is inherently abstract and separate from the text itself. Therefore, we discretize the style representation into a discrete space, where each discrete point corresponds to a particular category of style that can be elicited by the syntactic structure. To evaluate the effectiveness of our approach, we propose two new automatic evaluation metrics along with several conventional criteria, especially STR metric is nearly 0.9 in TechST, 0.87 in GYAFC datasets, and the best PPL metrics. Furthermore, we conduct thorough human evaluations by directly measuring click-through rates as an indicator of attractiveness, showing our model receives the most popularity. Our results demonstrate that DST achieves competitive performance on style transfer and can effectively capture the written structure of specified styles. This approach has the potential to significantly enhance its relevance and is capable of generating appealing content.}
}
@article{AUNG2022111133,
title = {Multi-triage: A multi-task learning framework for bug triage},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111133},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111133},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002302},
author = {Thazin Win Win Aung and Yao Wan and Huan Huo and Yulei Sui},
keywords = {Bug triage, Recommendation system, Multi-task learning, Deep learning},
abstract = {Assigning developers and allocating issue types are two important tasks in the bug triage process. Existing approaches tackle these two tasks separately, which is time-consuming due to repetition of effort and negating the values of correlated information between tasks. In this paper, a multi-triage model is proposed that resolves both tasks simultaneously via multi-task learning (MTL). First, both tasks can be regarded as a classification problem, based on historical issue reports. Second, performances on both tasks can be improved by jointly interpreting the representations of the issue report information. To do so, a text encoder and abstract syntax tree (AST) encoder are used to extract the feature representation of bug descriptions and code snippets accordingly. Finally, due to the disproportionate ratio of class labels in training datasets, the contextual data augmentation approach is introduced to generate syntactic issue reports to balance the class labels. Experiments were conducted on eleven open-source projects to demonstrate the effectiveness of this model compared with state-of-the-art methods.}
}
@article{BONDUEL202034,
title = {Including widespread geometry schemas into Linked Data-based BIM applied to built heritage},
journal = {Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction},
volume = {172},
number = {1},
pages = {34-51},
year = {2020},
issn = {2397-8759},
doi = {https://doi.org/10.1680/jsmic.19.00014},
url = {https://www.sciencedirect.com/science/article/pii/S2397875920000022},
author = {Mathias Bonduel and Anna Wagner and Pieter Pauwels and Maarten Vergauwen and Ralf Klein},
keywords = {Building Information Modelling (BIM), conservation, knowledge management},
abstract = {A reliable data exchange often including geometry-related data between stakeholders is crucial in construction projects. In this regard, data exchange frameworks built on Linked Data principles are very promising for combining disparate data sets. However, existing proposals to combine geometry and Linked Data either demand dedicated applications or support only a limited number of common geometry schemas. If any existing geometry schema could be used in a Linked Data context, error-prone geometry conversions are avoided and stakeholders do not need to invest in new geometry engines. In this paper, the applicability of Resource Description Framework (RDF) literals for including a wide variety of existing geometry schemas is studied and applied in a built heritage context. The uniform linking pattern and related terminology of the Ontology for Managing Geometry are used to implement this approach. Subsequently, the File Ontology for Geometry formats and Geometry Metadata Ontology are developed to ease the reuse of linked geometry descriptions. The effectiveness of the entire data structure is demonstrated in a built heritage case study project. The receiving party is able to create successfully a coordinated view using a demo web application on shared, but disparate, RDF data sets containing geometry descriptions.}
}
@article{CHEW2021102284,
title = {Preserving indomitable DDoS vitality through resurrection social hybrid botnet},
journal = {Computers & Security},
volume = {106},
pages = {102284},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102284},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821001085},
author = {Chit-Jie Chew and Ying-Chin Chen and Jung-San Lee and Chih-Lung Chen and Kuo-Yu Tsai},
keywords = {Botnet, DDoS, Reputation, Resurrection, Social network, 5G},
abstract = {As explosive development of the 5th generation wireless systems (5 G), the traces of botnet could be found in a cellular phone, intelligent equipment, cloud, and even Bitcoin network. Hackers are then able to launch the distributed denial of service (DDoS) attack via mastering these vulnerable devices. No doubt that the enterprise and government must suffer from the tremendous loss once specific services cannot function normally due to the DDoS. Inspired by a counterintuitive thinking, we actively predict a variant of hybrid botnet based on social network. The addressed prototype, resurrection social hybrid botnet (RSHB), could offer cyber researcher to formulate corresponding defense strategy against botnet. According to comprehensive simulations, the survivability has demonstrated that RSHB is much more dangerous than other botnets. Consequently, RSHB might be a coming sample for cyber engineer to explore the defense methodology.}
}
@article{NGUYEN2021100036,
title = {NEU-chatbot: Chatbot for admission of National Economics University},
journal = {Computers and Education: Artificial Intelligence},
volume = {2},
pages = {100036},
year = {2021},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2021.100036},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X21000308},
author = {Trung Thanh Nguyen and Anh Duc Le and Ha Thanh Hoang and Tuan Nguyen},
keywords = {Rasa, Admission chatbot, RNN, BERT},
abstract = {In the last few years, intelligent chatbot systems have been prevalent in various application fields, especially in education. Therefore, the demand for such online consulting services like chatbots is getting higher respectively. However, most communications between potential students and universities are performed manually, which is very time-consuming procedure, becoming a burden on the head of admissions. In this paper, we introduce an AI-based chatbot where students can instantly get daily updates of curriculum, admission for new students, tuition fees, IELTS writing task II score, etc. Our chatbot was developed by Deep Learning models, which are already integrated into the Rasa framework. We also proposed a rational pipeline for Vietnamese chatbots with our data preprocessing to obtain optimal accuracy and to avoid the overfitting of the model. Our model can detect more than fifty types of questions from users' input with an accuracy of 97.1% on test set. The chatbot was applied for National Economics University's official admission Fanpage on the Facebook platform, which is the most famous social network in Vietnam. This research shows detailed guidelines on how to build an AI chatbot from scratch, and the techniques we used, which can be applied to any language globally.}
}
@article{202332,
title = {HOW TO THINK ABOUT AI… AND HOW TO LIVE WITH IT},
journal = {New Scientist},
volume = {259},
number = {3449},
pages = {32-40},
year = {2023},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(23)01427-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407923014276}
}
@article{DASARO2024109101,
title = {An answer set programming-based implementation of epistemic probabilistic event calculus},
journal = {International Journal of Approximate Reasoning},
volume = {165},
pages = {109101},
year = {2024},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2023.109101},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X23002323},
author = {Fabio Aurelio D'Asaro and Antonis Bikakis and Luke Dickens and Rob Miller},
keywords = {Answer set programming (ASP), Epistemic reasoning, Probabilistic reasoning, Event calculus, Knowledge representation, Artificial intelligence},
abstract = {We describe a general procedure for translating Epistemic Probabilistic Event Calculus (EPEC) action language domains into Answer Set Programs (ASP), and show how the Python-driven features of the ASP solver Clingo can be used to provide efficient computation in this probabilistic setting. EPEC supports probabilistic, epistemic reasoning in domains containing narratives that include both an agent's own action executions and environmentally triggered events. Some of the agent's actions may be belief-conditioned, and some may be imperfect sensing actions that alter the strengths of previously held beliefs. We show that our ASP implementation can be used to provide query answers that fully correspond to EPEC's own declarative, Bayesian-inspired semantics.}
}
@article{CARRILLOMONDEJAR2020267,
title = {Characterizing Linux-based malware: Findings and recent trends},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {267-281},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19325002},
author = {J. Carrillo-Mondéjar and J.L. Martínez and G. Suarez-Tangil},
keywords = {Malware forensics, IoT, Embedded systems, Data analytics, Machine learning, Expert systems},
abstract = {Malware targeting interconnected infrastructures has surged in recent years. A major factor driving this phenomenon is the proliferation of large networks of poorly secured IoT devices. This is exacerbated by the commoditization of the malware development industry, in which tools can be readily obtained in specialized hacking forums or underground markets. However, despite the great interest in targeting this infrastructure, there is little understanding of what the main features of this type of malware are, or the motives of the criminals behind it, apart from the classic denial of service attacks. This is vital to modern malware forensics, where analyses are required to measure the trustworthiness of files collected at large during an investigation, but also to confront challenges posed by tech-savvy criminals (e.g., Trojan Horse Defense). In this paper, we present a comprehensive characterization of Linux-based malware. Our study is tailored to IoT malware and it leverages automated techniques using both static and dynamic analysis to classify malware into related threats. By looking at the most representative dataset of Linux-based malware collected by the community to date, we are able to show that our system can accurately characterize known threats. As a key novelty, we use our system to investigate a number of threats unknown to the community. We do this in two steps. First, we identify known patterns within an unlabeled dataset using a classifier trained with the labeled dataset. Second, we combine our features with a custom distance function to discover new threats by clustering together similar samples. We further study each of the unknown clusters by using state-of-the-art reverse engineering and forensic techniques and our expertise as malware analysts. We provide an in-depth analysis of what the most recent unknown trends are through a number of case studies. Among other findings, we observe that: i) crypto-mining malware is permeating the IoT infrastructure, ii) the level of sophistication is increasing, and iii) there is a rapid proliferation of new variants with minimal investment in infrastructure.}
}
@article{ANDRZEJEWSKI2023626,
title = {Innovators and Emulators: China and Russia’s Compounding Influence on Digital Censorship},
journal = {Orbis},
volume = {67},
number = {4},
pages = {626-654},
year = {2023},
note = {The Belt and Road Initiative, the Global South, and US-China Competition},
issn = {0030-4387},
doi = {https://doi.org/10.1016/j.orbis.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0030438723000431},
author = {Catherine Andrzejewski and Ana Horigoshi and Abigail I. Maher and Jonathan A. Solis},
abstract = {This article examines the overlapping influence of China in Russia and five countries that have experienced democratic backsliding: Azerbaijan, Nicaragua, Serbia, Turkey, and Uganda. Drawing on a wide range of data sources, including media watchdog reports, key informant interviews, and quantitative data, the paper maps the portfolio of specific digital censorship tools – legislative, institutional, and technological—that governments in China and Russia use to censor their domestic digital content. Then the digital censorship tools in the five case study countries are documented to examine where their governments’ tactics overlapped with those of the Kremlin and Beijing. These case study countries differ in their levels of development and democracy, with Russia, China, and the West all vying for influence. Key findings include the importance of timing when installing a digital censorship regime, and that Uganda and Nicaragua stand out among the case study countries.}
}
@article{SHIMADA2023129253,
title = {A simple model of edit activity in Wikipedia},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {630},
pages = {129253},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129253},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123008087},
author = {Takashi Shimada and Fumiko Ogushi and János Török and János Kertész and Kimmo Kaski},
keywords = {Wikipedia, Modeling},
abstract = {A simple dynamical model of collective edit activity of Wikipedia articles and their content evolution is introduced. Based on the recent empirical findings, each editor in the model is characterized by an ability to make content edit, i.e., improving the article by adding content and a tendency to make maintenance edit, i.e., dealing with formal aspects and maintaining the edit flow. In addition, each article is characterized by a level of maturity as compared to a potential quality needed to comprehensively cover its topic. This model is found to reproduce the basic structure of the bipartite network between editors and articles of Wikipedia. Furthermore, the relation between the model parameters of editors and articles and the metrics of those calculated from the emergent network turns out to be robust, i.e. depending only on the rate of the introduction of new articles to the editing activity. This results provides us a way to relate observations in the real data to the hidden characteristics of editors and articles. For the nestedness of the networks, systems with weighted parameter distribution gives better match to the empirical one. This suggests the importance of high-dimensional nature of the ability of editors and quality of articles in the real system.}
}
@article{CADEDDU2024108166,
title = {A comparative analysis of knowledge injection strategies for large language models in the scholarly domain},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108166},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108166},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003245},
author = {Andrea Cadeddu and Alessandro Chessa and Vincenzo {De Leo} and Gianni Fenu and Enrico Motta and Francesco Osborne and Diego {Reforgiato Recupero} and Angelo Salatino and Luca Secchi},
keywords = {Knowledge injection, Knowledge graphs, Large language models, Transformers, BERT, Classification, Natural language processing},
abstract = {In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.}
}
@article{BHUIYAN2022104474,
title = {STCA: Utilizing a spatio-temporal cross-attention network for enhancing video person re-identification},
journal = {Image and Vision Computing},
volume = {123},
pages = {104474},
year = {2022},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2022.104474},
url = {https://www.sciencedirect.com/science/article/pii/S0262885622001032},
author = {Amran Bhuiyan and Jimmy Xiangji Huang},
keywords = {Re-identification, Deep learning, 3D-CNNs, Cross attention},
abstract = {Video-based re-identification (ReID) is a crucial task in computer vision that draws increasing attention due to advances in deep learning (DL) and modern computational devices. Despite recent success with CNN architectures, single models (e.g., 2D-CNNs or 3D-CNNs) alone failed to leverage temporal information with spatial cues. This is due to uncontrolled surveillance scenarios and variable poses leading to inevitable misalignment of ROIs across the tracklets, which is accompanied by occlusion and motion blur. In this context, designing temporal and spatial cues for two different models and their combinations can be beneficial, considering the global of a video-tracklet. 3D-CNNs allow encoding of temporal information while 2D-CNNs extract spatial or appearance information. In this paper, we propose a Spatio-Temporal Cross Attention (STCA) network to utilize both 2D-CNNs and 3D-CNNs that calculate the cross attention mapping both from the layer of 3D-CNNs and 2D-CNNs along a person's trajectory to gate the following layers of 2D-CNNs; and highlight relevant appearance features for the person ReID. Given an input tracklet, the proposed cross attention (CA) is able to capture the salient regions that propagate throughout the tracklet to obtain the global view. This provides a spatio-temporal attention approach that can be dynamically aggregated with spatial features of 2D-CNNs to perform finer-grained recognition. Additionally, we exploit the advantage of utilizing cosine similarity while triplet sampling as well as for calculating the final recognition score. Experimental analyses on three challenging benchmark datasets indicate that integrating spatio-temporal cross attention into the state-of-the-art video ReID backbone CNN architecture allows for improving their recognition accuracy.}
}
@article{STRANDBERG2023109512,
title = {The Westermo network traffic data set},
journal = {Data in Brief},
volume = {50},
pages = {109512},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109512},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923006121},
author = {Per Erik Strandberg and David Söderman and Alireza Dehlaghi-Ghadim and Miguel Leon and Tijana Markovic and Sasikumar Punnekkat and Mahshid Helali Moghadam and David Buffoni},
keywords = {Industrial communication system, Cyber-physical systems, Network intrusion detection, Distributed artificial intelligence},
abstract = {There is a growing body of knowledge on network intrusion detection, and several open data sets with network traffic and cyber-security threats have been released in the past decades. However, many data sets have aged, were not collected in a contemporary industrial communication system, or do not easily support research focusing on distributed anomaly detection. This paper presents the Westermo network traffic data set, 1.8 million network packets recorded in over 90 minutes in a network built up of twelve hardware devices. In addition to the raw data in PCAP format, the data set also contains pre-processed data in the form of network flows in CSV files. This data set can support the research community for topics such as intrusion detection, anomaly detection, misconfiguration detection, distributed or federated artificial intelligence, and attack classification. In particular, we aim to use the data set to continue work on resource-constrained distributed artificial intelligence in edge devices. The data set contains six types of events: harmless SSH, bad SSH, misconfigured IP address, duplicated IP address, port scan, and man in the middle attack.}
}
@article{BAGAY2020179,
title = {Information security of Internet things},
journal = {Procedia Computer Science},
volume = {169},
pages = {179-182},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.132},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302556},
author = {Dmitry Bagay},
keywords = {Information security, Internet of things, IoT, DDoS-attacks, Smart house, Cyberthreats, Security at the network level, Application-level security, Vulnerability of software},
abstract = {The development of methods of information protection against cyber threats is a priority and the most labor-intensive direction in the development of the IoT sector. The relevance of the topic is growing due to the growing user interest in the Internet of things. Basic for studying in this work was the traditional IoT architecture format, which consists of three "slices". This perception, network and application levels. Each "cut" characterizes its key problems in the field of information security. The most difficult part is the network layer. The arising difficulties are provoked by the features of the structure (multivariance of things, different methods of networks) and a high numerical index of objects. The Internet of things accumulates information data from a huge number of devices that have different formats and various characteristics. As a result, there are failures of DoS, which arise due to a heavy load on the network, as well as disruptions in the operation of programs.}
}
@article{FAUTH2023102216,
title = {Ontology for building permit authorities (OBPA) for advanced building permit processes},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102216},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102216},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003440},
author = {Judith Fauth and Sebastian Seiß},
keywords = {Building permit, Building official, Building application, Ontology, Assignment process, Shapes Constraint Language (SHACL)},
abstract = {Building permit processes lie on the divide between architecture, engineering, and construction (AEC) and public administration. To ensure consistent and effective digitization in building permitting, it is necessary to consider and merge both areas. Hence, for advanced building permit processes, foundations must be developed, which begins with understanding and formalizing building permit authorities’ organizational structures and processes. Therefore, this study developed an ontology that covers a semantic representation of a building permit authority along with a subprocess of the building permit process called the assignment process. The assignment process describes how and on what basis building applications are assigned to appropriate building officials. Proposing a semantic representation of the assignment process, tacit knowledge from previously conducted data sets was analyzed and implemented in the ontology. As a case study, a sample building permit authority was described and implemented in the ontology. On the one hand, the developed ontology serves as a basis for decision support for building permit processes, while on the other hand, it enables a fully automated assignment process in a building permit authority. The approach not only makes the assignment process more objective and transparent for all parties involved in the building permit process but also allows time and personnel capacities to be used in its other subprocesses.}
}