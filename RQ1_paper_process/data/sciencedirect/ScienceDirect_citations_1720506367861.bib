@article{STEBBINS2014205,
title = {Reviews and Analysis of Special Reports},
journal = {The Journal of Academic Librarianship},
volume = {40},
number = {2},
pages = {205-207},
year = {2014},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0099133314000093},
author = {Leslie Stebbins}
}
@article{SERRANOMAMOLAR2019102416,
title = {Autonomic protection of multi-tenant 5G mobile networks against UDP flooding DDoS attacks},
journal = {Journal of Network and Computer Applications},
volume = {145},
pages = {102416},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102416},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519302504},
author = {Ana {Serrano Mamolar} and Pablo Salvá-García and Enrique Chirivella-Perez and Zeeshan Pervez and Jose M. {Alcaraz Calero} and Qi Wang},
keywords = {Self-managed networks, Autonomic control loop, 5G network, DDoS attack, Multi-tenancy, Self-protection},
abstract = {There is a lack of effective security solutions that autonomously, without any human intervention, detect and mitigate DDoS cyber-attacks. The lack is exacerbated when the network to be protected is a 5G mobile network. 5G networks push multi-tenancy to the edge of the network. Both the 5G user mobility and multi-tenancy are challenges to be addressed by current security solutions. These challenges lead to an insufficient protection of 5G users, tenants and infrastructures. This research proposes a novel autonomic security system, including the design, implementation and empirical validation to demonstrate the efficient protection of the network against Distributed Denial of Service (DDoS) attacks by applying countermeasures decided on and taken by an autonomic system, instead of a human. The self-management architecture provides support for all the different phases involved in a DDoS attack, from the detection of an attack to its final mitigation, through making the appropriate autonomous decisions and enforcing actions. Empirical experiments have been performed to protect a 5G multi-tenant infrastructure against a User Datagram Protocol (UDP) flooding attack, as an example of an attack to validate the design and prototype of the proposed architecture. Scalability results show self-protection against DDoS attacks, without human intervention, in around one second for an attack of 256 simultaneous attackers with 100 Mbps bandwidth per attacker. Furthermore, results demonstrate the proposed approach is flow-, user- and tenant-aware, which allows applying different protection strategies within the infrastructure.}
}
@article{THANGAVEL2023105215,
title = {EAD-DNN: Early Alzheimer's disease prediction using deep neural networks},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105215},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105215},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423006481},
author = {Preethi Thangavel and Yuvaraj Natarajan and K.R. {Sri Preethaa}},
keywords = {Alzheimer disease, Convolutional neural network, Deep neural network, MRI, ResNet},
abstract = {Early Alzheimer’s disease (EAD) diagnosis enables individuals to take preventative actions before irreversible brain damage occurs. Memory and thinking skills get worse in alzheimer disease, making it hard to do basic things. The abnormal buildup of amyloid and tau proteins in and around brain cells is thought to cause it. When amyloid builds up, it forms plaques around brain cells. Inside brain cells, tau tangles form when it accumulates. Healthy brain cells are damaged by the tangles and plagues, which causes them to shrink. The hippocampus, a part of the brain that aids in memory formation, appears to be the location of this damage. There are currently no methods that give the most accurate results and suggestions. With the methods we have now, alzheimer disease is not found early. So, we said that the Early Alzheimer’s disease - Deep Neural Network (EAD-DNN) method has found a way to predict alzheimer disease earlier. The Magnetic Resonance Imaging (MRI) dataset in the Comma Separated Value (CSV) format has been used by the EAD-DNN method. Convolutional Neural Network (CNN), the deep Residual Network (ResNet) has been used to train the MRI image dataset. This ResNet model can get more information from network levels with the help of Deep ResNet.The modified adam optimization has selected the best feature information from MRI scans of alzheimer patients and transferred it to another area while keeping the most important data. Using the EAD-DNN approach, a multi-class classification has been carried out. The extensive experiments show that the suggested method can achieve an accuracy rate of 98%.}
}
@article{ZHENG2021101563,
title = {Value drivers of blockchain technology: A case study of blockchain-enabled online community},
journal = {Telematics and Informatics},
volume = {58},
pages = {101563},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101563},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321000022},
author = {Yujie Zheng and Wai Fong Boh},
keywords = {Blockchain technology, Value driver, Value creation, Online community, Socio-technical approach},
abstract = {There is growing recognition that blockchain technology has significant potential to alter how organizations and people work and communicate. However, theoretical guidance concerning how organizations leverage blockchain technology to enhance value creation for users is still limited. Grounded in the socio-technical perspective and leveraging the rich data obtained from case analyses of blockchain-enabled online communities, this paper develops a theoretical model to identify the core value drivers that blockchain enables for online communities. The core value drivers include: a reputation-value system, data ownership mechanisms, and verification & tracking mechanisms. Our findings suggest that these three value drivers enhance value creation of online communities by motivating participation and protecting contributions.}
}
@article{ALDUWAIRI2020102071,
title = {LogDoS: A Novel logging-based DDoS prevention mechanism in path identifier-Based information centric networks},
journal = {Computers & Security},
volume = {99},
pages = {102071},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102071},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820303448},
author = {Basheer Al-Duwairi and Öznur Özkasap and Ahmet Uysal and Ceren Kocaoğullar and Kaan Yildirim},
keywords = {Information centric networks, DDoS, Packet logging, Path identifiers, Network security},
abstract = {Information Centric Networks (ICNs) have emerged in recent years as a new networking paradigm for the next-generation Internet. The primary goal of these networks is to provide effective mechanisms for content distribution and retrieval based on in-network content caching. Several network architectures were proposed in recent years to realize this communication model. This include Named Data Networks (NDN) and Path-Identifier (PID) based ICN. This paper proposes LogDoS as a novel mechanism to address the problem of data flooding attacks in PID-based ICNs. The proposed LogDoS mechanism is a unique hybrid approach that combines the best of NDN networks and PID-based ICNs, and it is the first to employ Bloom-filter based logging approach in a novel way to filter attack traffic efficiently. In this context, we develop and model three versions of LogDoS with varying levels of storage overhead at LogDoS-enabled routers. Extensive simulation experiments show that LogDoS is very effective against DDoS attacks as it can filter more than 99.98% of attack traffic in different attack scenarios while incurring acceptable storage overhead.}
}
@article{ESTEFO2019226,
title = {The Robot Operating System: Package reuse and community dynamics},
journal = {Journal of Systems and Software},
volume = {151},
pages = {226-242},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300342},
author = {Pablo Estefo and Jocelyn Simmonds and Romain Robbes and Johan Fabry},
keywords = {Robot Operating System, Package management, Software ecosystems},
abstract = {ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem.}
}
@article{CARBONNEL2019341,
title = {Towards complex product line variability modelling: Mining relationships from non-boolean descriptions},
journal = {Journal of Systems and Software},
volume = {156},
pages = {341-360},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301311},
author = {Jessie Carbonnel and Marianne Huchard and Clémentine Nebut},
keywords = {Complex software product line, Reverse engineering, Variability modelling, Extended feature models, Formal concept analysis, Pattern structures},
abstract = {Software product line engineering relies on systematic reuse and mass customisation to reduce the development time and cost of a software system family. The extractive adoption of a product line requires to extract variability information from the description of a collection of existing software systems to model their variability. With the increasing complexity of software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of existing boolean variability models, such as multi-valued attributes or UML-like cardinalities, were proposed to enhance their expressiveness and support variability modelling in complex product lines. In this paper, we propose an approach to extract complex variability information, i.e., involving features as well as multi-valued attributes and cardinalities, in the form of logical relationships. This approach is based on Formal Concept Analysis and Pattern Structures, two mathematical frameworks for knowledge discovery that bring theoretical foundations to complex variability extraction algorithms. We present an application on product comparison matrices representing complex descriptions of software system families. We show that our method does not suffer from scalability issues and extracts all pertinent relationships, but that it also extracts numerous accidental relationships that need to be filtered.}
}
@article{GUAN2024100646,
title = {A two-tiered framework for anomaly classification in IoT networks utilizing CNN-BiLSTM model},
journal = {Software Impacts},
volume = {20},
pages = {100646},
year = {2024},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2024.100646},
url = {https://www.sciencedirect.com/science/article/pii/S2665963824000344},
author = {Yue Guan and Morteza Noferesti and Naser Ezzati-Jivan},
keywords = {IoT security, Deep neural network, CNN-BiLSTM model, Intrusion detection, Anomaly classification},
abstract = {The paper introduces ACS-IoT, an Anomaly Classification System for IoT networks, structured as a two-tiered framework. In the first, it employs a decision tree classifier for anomaly detection. In the second, a CNN-BiLSTM model is utilized for more profound analysis and classification of anomaly types. To address data imbalance, SMOTE is used, and feature selection is enhanced with PSO. The approach showcases strong practical applicability in real-world industrial settings, achieving an accuracy of 88%, precision of 89%, recall of 88%, and F1-score of 88% for multi-class classification, surpassing other machine learning approaches by at least 6% in all metrics.}
}
@article{JUNG2020100103,
title = {IoT botnet detection via power consumption modeling},
journal = {Smart Health},
volume = {15},
pages = {100103},
year = {2020},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2019.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2352648319300674},
author = {Woosub Jung and Hongyang Zhao and Minglong Sun and Gang Zhou},
keywords = {IoT botnet mitigation, Power-based botnet detection method, Convolutional neural network modeling},
abstract = {Many IoT botnets that exploit vulnerabilities of IoT devices have emerged recently. After taking over control of IoT devices, the botnets generate tremendous traffic to attack target nodes. It is also a threat to the smart health area since they have used IoT devices more and more. To detect the malicious IoT botnets, many researchers have proposed botnet detection systems; however, these are not easily applicable to resource-constrained IoT devices. Moreover, since the botnet's early stage makes marginal differences in terms of traffic, it is hard to detect when they first attack the victim nodes. However, we observe that the IoT botnets generate distinguishable power consumption patterns. Thus, we aim to classify whether the IoT device is affected by malign behaviors through power consumption patterns so that we can protect the healthcare ecosystem from the malicious IoT botnets. We propose a CNN-based deep learning model that consists of a data processing module as well as an 8-layer CNN. Prior to applying the CNN model, we segment and normalize the collected power consumption data to help our CNN model to achieve higher accuracy. The 8-layer CNN classifies the processed data into four classes including a botnet class, which is our primary target. To demonstrate the performance, we run self-evaluation, cross-device-evaluation, leave-one-device-out, and leave-one-botnet-out tests on three common types of IoT devices, which are Security Camera, Router, and Voice Assistant devices. The self-tests achieve up to 96.5% classification accuracy whereas the cross-evaluation tests perform about 90% accuracy. Leave-one-out tests also introduce higher than 90% accuracy for botnet detection.}
}
@article{LAI2022107913,
title = {Kansei engineering for new energy vehicle exterior design: An internet big data mining approach},
journal = {Computers & Industrial Engineering},
volume = {165},
pages = {107913},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107913},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221008172},
author = {Xinjun Lai and Sheng Zhang and Ning Mao and Jianjun Liu and Qingxin Chen},
keywords = {Kansei engineering, Automobile exterior, Text mining, Structural equation model, Deep learning},
abstract = {New energy vehicles (NEVs) such as electronic cars represent a major trend in the automobile industry, where most their exterior designs still follow those of convention fuelled vehicles (FVs). It is important to investigate whether NEV users have unique requirements that differ from those of traditional users. Kansei engineering is a practical tool for perceptual demand analysis. However, the conventional method requires questionnaires or surveys to perform limited data collection. In this study, we utilised massive internet data to collect user Kansei requirements for NEV exterior design. The Scrapy crawler was adopted for data collection and a bidirectional long short-term memory, conditional random field, and multilayer perceptron framework was developed for text mining. To quantify design features and Kansei image scores, a hybrid Apriori  + structural equation model (SEM) system is proposed, where the data-driven Apriori algorithm can explore the hidden relationships in big user generated comments, while the SEM model captures the users’ behaviour and decision procedure so that to provide interpretable results. In addition, the association rules mined from user comments by Apriori can facilitate the specification of a complicated SEM model, substantially reducing the modelling and calibration effort. Goodness-of-fit results suggest that the proposed model outperforms conventional models. A case study on 1805 automobiles, 287 brands, and 369105 comments was conducted and the results suggest that some design features that would increase the Kansei image scores for conventional FVs may have the opposite effect on NEVs. Discussions on engineering and managerial insights are presented and the discovered rules and relationships are employed to develop a design-aided system.}
}
@article{SHAHID2021103751,
title = {Machine learning research towards combating COVID-19: Virus detection, spread prevention, and medical assistance},
journal = {Journal of Biomedical Informatics},
volume = {117},
pages = {103751},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103751},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000800},
author = {Osama Shahid and Mohammad Nasajpour and Seyedamin Pouriyeh and Reza M. Parizi and Meng Han and Maria Valero and Fangyu Li and Mohammed Aledhari and Quan Z. Sheng},
keywords = {COVID-19, Machine learning, Artificial intelligence, Healthcare, Drug development, Predictive analysis},
abstract = {COVID-19 was first discovered in December 2019 and has continued to rapidly spread across countries worldwide infecting thousands and millions of people. The virus is deadly, and people who are suffering from prior illnesses or are older than the age of 60 are at a higher risk of mortality. Medicine and Healthcare industries have surged towards finding a cure, and different policies have been amended to mitigate the spread of the virus. While Machine Learning (ML) methods have been widely used in other domains, there is now a high demand for ML-aided diagnosis systems for screening, tracking, predicting the spread of COVID-19 and finding a cure against it. In this paper, we present a journey of what role ML has played so far in combating the virus, mainly looking at it from a screening, forecasting, and vaccine perspective. We present a comprehensive survey of the ML algorithms and models that can be used on this expedition and aid with battling the virus.}
}
@article{STVILIA2022101160,
title = {Seeking and sharing datasets in an online community of data enthusiasts},
journal = {Library & Information Science Research},
volume = {44},
number = {3},
pages = {101160},
year = {2022},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2022.101160},
url = {https://www.sciencedirect.com/science/article/pii/S0740818822000238},
author = {Besiki Stvilia and Leila Gibradze},
keywords = {Dataset practices, Data curation, Online community, Metadata, Data seeking, Data sharing},
abstract = {This study examined discussions of the r/Datasets community on Reddit. It identified three activities in which the community engaged: question answering, data sharing, and community building. Members of the community used 21 types of data and information sources in their activities. The findings of this research enhance our understanding of the activity structures, data and information sources used, and challenges and problems encountered when users search for, share, and make sense of datasets on the web, outside the traditional information and data ecosystems. Data librarians and curators can use the findings of this study in the design of their data management and reference services. The typology of data sources and the metadata model developed through this study can be used in annotating and categorizing data sources and informing the design of metadata schemas and vocabularies for datasets.}
}
@article{KE2024109035,
title = {Discovering e-commerce user groups from online comments: An emotional correlation analysis-based clustering method},
journal = {Computers and Electrical Engineering},
volume = {113},
pages = {109035},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.109035},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623004597},
author = {Jia Ke and Ying Wang and Mingyue Fan and Xiaojun Chen and Wenlong Zhang and Jianping Gou},
keywords = {E-commerce, Commodity review, Sentiment analysis, the emotional correlation analysis model, SOM, User clustering},
abstract = {Platform merchants mine user clusters and their characteristics to assist in precision marketing. In view of the information overload in e-commerce reviews, machine methods are needed to efficiently obtain clustering information from text. This study innovatively integrated the emotional correlation analysis model and Self-organizing Map (SOM) in application, to construct fine-grained user emotion vector based on review text and perform visual cluster analysis, which helped quickly mine user clustering and characteristics from review text. The result of empirical analysis based on real reviews of Amazon books showed that the proposed method had the average precision as 0.71, confirming that the clustering method integrating the emotional correlation analysis model and SOM could efficiently mine user groups and match appropriate marketing strategies, which will help platform merchants carry out precision marketing. The study makes contributions to the application and innovation of researches in the field of user clustering and e-commerce precision marketing.}
}
@article{LASTOVICKA2023109782,
title = {Passive operating system fingerprinting revisited: Evaluation and current challenges},
journal = {Computer Networks},
volume = {229},
pages = {109782},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109782},
url = {https://www.sciencedirect.com/science/article/pii/S138912862300227X},
author = {Martin Laštovička and Martin Husák and Petr Velan and Tomáš Jirsík and Pavel Čeleda},
keywords = {OS fingerprinting, Network monitoring, Network management, Cybersecurity, Machine learning, Survey},
abstract = {Fingerprinting a host's operating system is a very common yet precarious task in network, asset, and vulnerability management. Estimating the operating system via network traffic analysis may leverage TCP/IP header parameters or complex analysis of hosts' behavior using machine learning. However, the existing approaches are becoming obsolete as network traffic evolves which makes the problem still open. This paper discusses various approaches to passive OS fingerprinting and their evolution in the past twenty years. We illustrate their usage, compare their results in an experiment, and list challenges faced by the current fingerprinting approaches. The hosts' differences in network stack settings were initially the most important information source for OS fingerprinting, which is now complemented by hosts' behavioral analysis and combined approaches backed by machine learning. The most impactful reasons for this evolution were the Internet-wide network traffic encryption and the general adoption of privacy-preserving concepts in application protocols. Other changes, such as the increasing proliferation of web applications on handheld devices, raised the need to identify these devices in the networks, for which we may use the techniques of OS fingerprinting.}
}
@article{CALEFATO20191,
title = {A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem},
journal = {Information and Software Technology},
volume = {114},
pages = {1-20},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301216},
author = {Fabio Calefato and Filippo Lanubile and Bogdan Vasilescu},
keywords = {Personality traits, Large-scale distributed projects, Ecosystems, Apache, Big five, Five-Factor model, Open source software, Human aspects, Psychometric analysis, Computational personality detection},
abstract = {Context
Large-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.
Objective
We aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global software engineering — has been found to influence positively the result of code reviews in distributed projects.
Method
In this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.
Results
We find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.
Conclusion
Overall, our findings reinforce the need for future studies on human factors in software engineering to use psychometric tools to control for differences in developers’ personalities.}
}
@article{SMITE2023111509,
title = {From forced Working-From-Home to voluntary working-from-anywhere: Two revolutions in telework},
journal = {Journal of Systems and Software},
volume = {195},
pages = {111509},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111509},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001856},
author = {Darja Šmite and Nils Brede Moe and Eriks Klotins and Javier Gonzalez-Huerta},
keywords = {Working from home, WFH, Telework, COVID-19, Software engineering, Case study, Empirical study},
abstract = {The COVID-19 outbreak has admittedly caused interruptions to production, transportation, and mobility, therefore, having a significant impact on the global supply and demand chain’s well-functioning. But what happened to companies developing digital services, such as software? How has the enforced Working-From-Home (WFH) mode impacted their ability to deliver software, if at all? This article shares our findings from monitoring the WFH during 2020 in an international software company with engineers located in Sweden, the USA, and the UK. We analyzed different aspects of productivity, such as developer job satisfaction and well-being, activity, communication and collaboration, efficiency and flow based on the archives of commit data, calendar invites, Slack communication, the internal reports of WFH experiences, and 30 interviews carried out in April/May and September 2020. We add more objective evidence to the existing COVID-19 studies the vast majority of which are based on self-reported productivity from the early months of the pandemic. We find that engineers continue committing code and carrying out their daily duties, as their routines adjust to “the new norm”. Our key message is that software engineers can work from home and quickly adjust their tactical approaches to the changes of unprecedented scale. Further, WFH has its benefits, including better work-life balance, improved flow, and improved quality of distributed meetings and events. Yet, WFH is not challenge free: not everybody feels equally productive working from home, work hours for many increased, while physical activity, socialization, pairing and opportunities to connect to unfamiliar colleagues decreased. Information sharing and meeting patterns also changed. Finally, experiences gained during the pandemic will have a lasting impact on the future of the workplace. The results of an internal company-wide survey suggest that only 9% of engineers will return to work in the office full time. Our article concludes with the InterSoft’s strategy for work from anywhere (WFX), and a list of useful adjustments for a better WFH.}
}
@article{NGUYEN2020128,
title = {PSI-rooted subgraph: A novel feature for IoT botnet detection using classifier algorithms},
journal = {ICT Express},
volume = {6},
number = {2},
pages = {128-138},
year = {2020},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405959519304412},
author = {Huy-Trung Nguyen and Quoc-Dung Ngo and Doan-Hieu Nguyen and Van-Hoang Le},
keywords = {IoT botnet, IoT security, Static malware detection, PSI-rooted subgraph, Machine learning, Deep learning},
abstract = {It is obvious that IoT devices are widely used more and more in many areas. However, due to limited resources (e.g., memory, CPU), the security mechanisms on many IoT devices such as IP-Camera, router are low. Therefore, botnets are an emerging threat to compromise IoT devices recently. To tackle this, a novel method for IoT botnets detection plays a crucial role. In this paper, we have some contributions for IoT botnet detection: first, we present a novel high-level PSI-rooted subgraph-based feature for the detection of IoT botnets; second, we generate a limited number of features that have precise behavioral descriptions, which require smaller space and reduce processing time; third, The evaluation results show the effectiveness and robustness of PSI-rooted subgraph-based features, as with five machine classifiers consisting of Random Forest, Decision Tree, Bagging, k-Nearest Neighbor, and Support Vector Machine, each classifier achieves more than 97% detection rate and low time-consuming. Moreover, compared to other work, our proposed method obtains better performance. Finally, we publicize all our materials on Github, which will benefit future research (e.g., IoT botnet detection approach).}
}
@article{LAGES2022279,
title = {Remote Teaching of Dynamics and Control of Robots Using ROS 2},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {17},
pages = {279-284},
year = {2022},
note = {13th IFAC Symposium on Advances in Control Education ACE 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.292},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322015348},
author = {Walter Fetter Lages},
keywords = {Robotics, Robot Operating System, ROS 2, Robotic manipulation, Mobile robots, Robot control, Remote teaching},
abstract = {This paper presents the use of ROS 2 as a support platform for the Dynamics and Control of Robots course at the School of Engineering of the Universidade Federal do Rio Grande do Sul, Brazil. The organization of the course is presented and the autonomous activities performed by students at home using ROS 2 are described in detail, including some pitfalls that should be avoided. The results show that ROS 2 is mature enough for production use, at least in an academic environment and that students are able to use sophisticated robotic tools even in undergraduate courses.}
}
@article{HORE2024103928,
title = {A sequential deep learning framework for a robust and resilient network intrusion detection system},
journal = {Computers & Security},
volume = {144},
pages = {103928},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103928},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824002311},
author = {Soumyadeep Hore and Jalal Ghadermazi and Ankit Shah and Nathaniel D. Bastian},
keywords = {Network intrusion detection system (NIDS), Multistage AI-based NIDS, Malicious packet classifier, Anomaly detector, Novelty detector, Sequential deep neural networks, One-shot learning, Transfer learning},
abstract = {Ensuring the security and integrity of computer and network systems is of utmost importance in today’s digital landscape. Network intrusion detection systems (NIDS) play a critical role in continuously monitoring network traffic and identifying unauthorized or potentially malicious activities that could compromise the confidentiality, availability, and integrity of these systems. However, traditional NIDS face a daunting challenge in effectively adapting to the evolving tactics of cyber attackers. To address this challenge, we propose a multistage artificial intelligence enabled framework for intrusion detection in network traffic, capable of handling zero-day, out-of-distribution, and adversarial evasion attacks. Our framework comprises three sequential deep neural network (DNN) architectures: one for the classifier and two for specific autoencoders, designed to effectively detect both known attack patterns and novel, previously unseen samples. We introduce an innovative transfer learning technique where specific combinations of neurons and layers in the DNN architectures are frozen during one-shot learning to enhance the framework’s robustness to novel attacks. To validate the effectiveness of our framework, we conducted extensive experimentation using publicly available benchmark intrusion detection data sets. Leveraging the one-shot learning approach in the transfer learning component of the framework, we demonstrate continuous improvement in detection accuracy for both known and novel network traffic patterns. The results demonstrate the effectiveness of the multiple stages in the framework by achieving, on average, 98.5% accuracy in detecting various attacks.}
}
@article{SHIBATA202311436,
title = {Learning Locally, Communicating Globally: Reinforcement Learning of Multi-robot Task Allocation for Cooperative Transport},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {11436-11443},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.431},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300798X},
author = {Kazuki Shibata and Tomohiko Jimbo and Tadashi Odashima and Keisuke Takeshita and Takamitsu Matsubara},
keywords = {Networked robotic systems, Multi-agent systems, Consensus, Decentralized control, Decentralized Control and Systems},
abstract = {We consider task allocation for multi-object transport using a multi-robot system, in which each robot selects one object among multiple objects with different and unknown weights. The existing centralized methods assume the number of robots and tasks to be fixed, which is inapplicable to scenarios that differ from the learning environment. Meanwhile, the existing distributed methods limit the minimum number of robots and tasks to a constant value, making them applicable to various numbers of robots and tasks. However, they cannot transport an object whose weight exceeds the load capacity of robots observing the object. To make it applicable to various numbers of robots and objects with different and unknown weights, we propose a framework using multi-agent reinforcement learning for task allocation. First, we introduce a structured policy model consisting of 1) predesigned dynamic task priorities with global communication and 2) a neural network-based distributed policy model that determines the timing for coordination. The distributed policy builds consensus on the high-priority object under local observations and selects cooperative or independent actions. Then, the policy is optimized by multi-agent reinforcement learning through trial and error. This structured policy of local learning and global communication makes our framework applicable to various numbers of robots and objects with different and unknown weights, as demonstrated by simulations.}
}
@article{CONTI2023100252,
title = {Turning captchas against humanity: Captcha-based attacks in online social media},
journal = {Online Social Networks and Media},
volume = {36},
pages = {100252},
year = {2023},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2023.100252},
url = {https://www.sciencedirect.com/science/article/pii/S2468696423000113},
author = {Mauro Conti and Luca Pajola and Pier Paolo Tricomi},
keywords = {Online social networks, Automatic content moderator, Adversarial machine learning, Hate speech, Cybersecurity, Instagram, Obfuscation techniques},
abstract = {Nowadays, people generate and share massive amounts of content on online platforms (e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook users posted around 150 thousand photos every minute. Content moderators constantly monitor these online platforms to prevent the spreading of inappropriate content (e.g., hate speech, nudity images). Based on deep learning (DL) advances, Automatic Content Moderators (ACM) help human moderators handle high data volume. Despite their advantages, attackers can exploit weaknesses of DL components (e.g., preprocessing, model) to affect their performance. Therefore, an attacker can leverage such techniques to spread inappropriate content by evading ACM. In this work, we analyzed 4600 potentially toxic Instagram posts, and we discovered that 44% of them adopt obfuscations that might undermine ACM. As these posts are reminiscent of captchas (i.e., not understandable by automated mechanisms), we coin this threat as Captcha Attack (CAPA). Our contributions start by proposing a CAPA taxonomy to better understand how ACM is vulnerable to obfuscation attacks. We then focus on the broad sub-category of CAPA using textual Captcha Challenges, namely CC-CAPA, and we empirically demonstrate that it evades real-world ACM (i.e., Amazon, Google, Microsoft) with 100% accuracy. Our investigation revealed that ACM failures are caused by the OCR text extraction phase. The training of OCRs to withstand such obfuscation is therefore crucial, but huge amounts of data are required. Thus, we investigate methods to identify CC-CAPA samples from large sets of data (originated by three OSN – Pinterest, Twitter, Yahoo-Flickr), and we empirically demonstrate that supervised techniques identify target styles of samples almost perfectly. Unsupervised solutions, on the other hand, represent a solid methodology for inspecting uncommon data to detect new obfuscation techniques.}
}
@article{BLANCO2022102918,
title = {Optimism and pessimism analysis using deep learning on COVID-19 related twitter conversations},
journal = {Information Processing & Management},
volume = {59},
number = {3},
pages = {102918},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102918},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322000437},
author = {Guillermo Blanco and Anália Lourenço},
keywords = {Covid-19 pandemic, Sociome, Conversation, Emotion classification, Emotion shift},
abstract = {This paper proposes a new deep learning approach to better understand how optimistic and pessimistic feelings are conveyed in Twitter conversations about COVID-19. A pre-trained transformer embedding is used to extract the semantic features and several network architectures are compared. Model performance is evaluated on two new, publicly available Twitter corpora of crisis-related posts. The best performing pessimism and optimism detection models are based on bidirectional long- and short-term memory networks. Experimental results on four periods of the COVID-19 pandemic show how the proposed approach can model optimism and pessimism in the context of a health crisis. There is a total of 150,503 tweets and 51,319 unique users. Conversations are characterised in terms of emotional signals and shifts to unravel empathy and support mechanisms. Conversations with stronger pessimistic signals denoted little emotional shift (i.e. 62.21% of these conversations experienced almost no change in emotion). In turn, only 10.42% of the conversations laying more on the optimistic side maintained the mood. User emotional volatility is further linked with social influence.}
}
@article{BELLA2022100507,
title = {Multi-service threats: Attacking and protecting network printers and VoIP phones alike},
journal = {Internet of Things},
volume = {18},
pages = {100507},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100507},
url = {https://www.sciencedirect.com/science/article/pii/S2542660522000130},
author = {Giampaolo Bella and Pietro Biondi and Stefano Bognanni},
keywords = {Denial of service, Data breach, Security, Privacy, Trust, Insider threat},
abstract = {Printing over a network and calling over VoIP technology are routine at present. This article investigates to what extent these services can be attacked using freeware in the real world if they are not configured securely. In finding out that attacks of high impact, termed the Printjack and Phonejack families, could be mounted at least from insiders, the article also observes that secure configurations do not appear to be widely adopted. Users with the necessary skills may put existing security measures in place with printers, but would need novel measures, which the article prototypes, with phones in order for a pair of peers to call each other securely and without trusting anyone else, including sysadmins.}
}
@article{DYSON2020200894,
title = {Scenario-based creation and digital investigation of ethereum ERC20 tokens},
journal = {Forensic Science International: Digital Investigation},
volume = {32},
pages = {200894},
year = {2020},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2019.200894},
url = {https://www.sciencedirect.com/science/article/pii/S1742287618302263},
author = {Simon F. Dyson and William J. Buchanan and Liam Bell},
keywords = {Blockchain, Cryptocurrency, Ethereum},
abstract = {This paper examines the Ethereum network in the context of an investigation. The validation of data sources is achieved through different client software on both the Ropsten network and the live blockchain. New scenarios are also used test common patterns in order to track for start and end points for Ethereum and ERC20 tokens.}
}
@article{LIU2023109059,
title = {Joint Graph Learning and Matching for Semantic Feature Correspondence},
journal = {Pattern Recognition},
volume = {134},
pages = {109059},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109059},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005398},
author = {He Liu and Tao Wang and Yidong Li and Congyan Lang and Yi Jin and Haibin Ling},
keywords = {Feature correspondence, Attention network, Graph matching, Graph learning},
abstract = {In recent years, powered by the learned discriminative representation via graph neural network (GNN) models, deep graph matching methods have made great progresses in the task of matching semantic features. However, these methods usually rely on heuristically generated graph patterns, which may introduce unreliable relationships to hurt the matching performance. In this paper, we propose a joint graph learning and matching network, named GLAM, to explore reliable graph structures for boosting graph matching. GLAM adopts a pure attention-based framework for both graph learning and graph matching. Specifically, it employs two types of attention mechanisms, self-attention and cross-attention for the task. The self-attention discovers the relationships between features to further update feature representations over the learnt structures; and the cross-attention computes cross-graph correlations between the two feature sets to be matched for feature reconstruction. Moreover, the final matching solution is directly derived from the output of the cross-attention layer, without employing a specific matching decision module. The proposed method is evaluated on three popular visual matching benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms previous state-of-the-art graph matching methods on all benchmarks. Furthermore, the graph patterns learnt by our model are validated to be able to remarkably enhance previous deep graph matching methods by replacing their handcrafted graph structures with the learnt ones.}
}
@article{LIU2024100024,
title = {Large language models for air transportation: A critical review},
journal = {Journal of the Air Transport Research Society},
volume = {2},
pages = {100024},
year = {2024},
issn = {2941-198X},
doi = {https://doi.org/10.1016/j.jatrs.2024.100024},
url = {https://www.sciencedirect.com/science/article/pii/S2941198X24000356},
author = {Yucheng Liu},
keywords = {Large language models, Air transportation, Review, Challenges},
abstract = {In the past decade, Artificial Intelligence (AI) has contributed to the improvement of various aviation aspects, including flight plan optimization, the development of autonomous systems, performing predictive analytics, as well as in passenger / crew assistance systems. The latest AI technology to potentially revolutionize air transportation are so-called Large Language Models (LLMs), which have an outstanding ability to process and generate human-like text. The application areas for LLMs cover nearly all aspects of air transportation, through language processing, content generation, and problem solving. In this study, we discuss the potential of this impact with two major contributions. First, we have performed an experimental evaluation of twelve commonly-used LLMs concerning their performance of air transportation related subjects, covering fact retrieval, complex reasoning abilities, and explanation tasks. Second, we have performed a survey among graduate students at Beihang University, a leading aviation university in China, to explore the experiences and uses of LLMs. We believe that our study makes a significant contribution towards the dissemination and application of LLMs in the air transportation domain.}
}
@article{PEROTTI2021101932,
title = {Estimating cardiomyofiber strain in vivo by solving a computational model},
journal = {Medical Image Analysis},
volume = {68},
pages = {101932},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101932},
url = {https://www.sciencedirect.com/science/article/pii/S1361841520302966},
author = {Luigi E. Perotti and Ilya A. Verzhbinsky and Kévin Moulin and Tyler E. Cork and Michael Loecher and Daniel Balzani and Daniel B. Ennis},
keywords = {In vivo cardiomyofiber strains, Cardiac kinematics, Model based image analysis, Analytical phantom},
abstract = {Since heart contraction results from the electrically activated contraction of millions of cardiomyocytes, a measure of cardiomyocyte shortening mechanistically underlies cardiac contraction. In this work we aim to measure preferential aggregate cardiomyocyte (“myofiber”) strains based on Magnetic Resonance Imaging (MRI) data acquired to measure both voxel-wise displacements through systole and myofiber orientation. In order to reduce the effect of experimental noise on the computed myofiber strains, we recast the strains calculation as the solution of a boundary value problem (BVP). This approach does not require a calibrated material model, and consequently is independent of specific myocardial material properties. The solution to this auxiliary BVP is the displacement field corresponding to assigned values of myofiber strains. The actual myofiber strains are then determined by minimizing the difference between computed and measured displacements. The approach is validated using an analytical phantom, for which the ground-truth solution is known. The method is applied to compute myofiber strains using in vivo displacement and myofiber MRI data acquired in a mid-ventricular left ventricle section in N=8 swine subjects. The proposed method shows a more physiological distribution of myofiber strains compared to standard approaches that directly differentiate the displacement field.}
}
@article{CHERNAKOV2020111588,
title = {Framework for software development of laboratory equipment and setups integrated into large scale DAQ systems (LabBot)},
journal = {Fusion Engineering and Design},
volume = {156},
pages = {111588},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111588},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620301368},
author = {Alexandr Chernakov and Nikita Zhiltsov and Vasiliy Senitchenkov and Nikita Babinov and Alexander Bazhenov and Ivan Bukreev and Paul Chernakov and Anton Chernakov and Anastasia Chironova and Artem Dmitriev and Denis Elets and Nikita Ermakov and Igor Khodunov and Alexander Koval and Gleb Kurskiev and Andrei Litvinov and Alina Mitrofanova and Alexander Mokeev and Eugene Mukhin and Alexey Razdobarin and Dmitry Samsonov and Leonid Snigirev and Valeri Solovei and Ivan Tereschenko and Sergei Tolstyakov and Lidia Varshavchik and Paul Zatylkin},
keywords = {Data acquisition, Framework, Experiment automation, Firmware development, Scientific software},
abstract = {The LabBot Framework project is intended to implement control of experiment and data acquisition without writing special platform codes, while achieving industrial-grade results. The Framework is intended for fast and advanced development of controlling and data acquisition software for laboratory-scale experimental setups; more sophisticated software for middle-scale laboratory equipment; industrial-grade software for self-made commercial instrumentation developed by small- and middle-scale companies. The paper presents review of the Framework top-level architecture and its implementation addressing development of ITER Divertor Thomson Scattering (DTS) diagnostic. The first examples of the Framework implementation include the set-ups for vacuum heating testbench; RF cleaning of diagnostic mirrors as well as DTS equipment, which must be controlled by the ITER CODAC system.}
}
@article{GUIMARAES2024127064,
title = {DODFMiner: An automated tool for Named Entity Recognition from Official Gazettes},
journal = {Neurocomputing},
volume = {568},
pages = {127064},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127064},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011876},
author = {Gabriel M.C. Guimarães and Felipe X.B. {da Silva} and Andrei L. Queiroz and Ricardo M. Marcacini and Thiago P. Faleiros and Vinicius R.P. Borges and Luís P.F. Garcia},
keywords = {Legal documents, Official gazettes, Natural Language Processing, Classification, Named Entity Recognition},
abstract = {Official gazettes are documents published by governments to publicize their actions, spanning long periods of time and making an important transparency mechanism. These documents have information on laws, contracts, and bidding processes, as well as on civil servants and their careers in public service. Automatic information extraction of these documents may contribute to public transparency, with two tasks being especially useful: the classification of the different segments of these documents, the so called acts; and the Named Entity Recognition (NER) within the acts. The variety of official gazettes and their patterns brings up the necessity of constructing different tools for specific gazettes. In this paper, we propose DODFMiner, a command-line interface tool to classify acts and extract named entities from the Official Gazette of the Federal District. The tool follows a 3-step approach: the pre-processing of the input data; text classification using rule-based systems with regular expressions; and NER with Machine Learning algorithms. It allows users to input JSON files and receive CSV as output, providing information that allows users to track government procurements through years, contracts duration and total amount, among others. We also propose a set of experiments to support the choice of models included in the tool, covering the classification and NER steps. Text classification achieved a mean F1-score of 0.778, while to the NER, we compared 3 different architectures, CRF with a mean F1-score of 0.851, CNN-biLSTM-CRF with 0.787 and CNN-CNN-LSTM with 0.841.}
}
@incollection{CONRAD2023459,
title = {Chapter 9 - Domain 8: Software Development Security},
editor = {Eric Conrad and Seth Misenar and Joshua Feldman},
booktitle = {CISSP® Study Guide (Fourth Edition)},
publisher = {Syngress},
edition = {Fourth Edition},
pages = {459-508},
year = {2023},
isbn = {978-0-443-18734-6},
doi = {https://doi.org/10.1016/B978-0-443-18734-6.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443187346000088},
author = {Eric Conrad and Seth Misenar and Joshua Feldman},
keywords = {DevOps, Extreme Programming, Object, Object-Oriented Programming, Procedural languages, Spiral Model, Systems Development Life Cycle, Waterfall Model},
abstract = {This chapter introduces Domain 8 of the CISSP®, Software Development Security. The most important aspects of this domain are related to managing the development of software and applications. Approaches to software development that attempt to reduce the likelihood of defects or flaws are a key topic in this domain. In particular, the Waterfall, Spiral, and Rapid Application Development (RAD) models of software development are considered. Another significant portion of this chapter is dedicated to understanding the principles of Object-Oriented programming and design. A basic discussion of several types of software vulnerabilities and the issues surrounding disclosure of the vulnerabilities are also a topic for this domain. Finally, databases, being a key component of many applications, are considered.}
}
@article{DANG2022102263,
title = {Vessel-CAPTCHA: An efficient learning framework for vessel annotation and segmentation},
journal = {Medical Image Analysis},
volume = {75},
pages = {102263},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.102263},
url = {https://www.sciencedirect.com/science/article/pii/S136184152100308X},
author = {Vien Ngoc Dang and Francesco Galati and Rosa Cortese and Giuseppe {Di Giacomo} and Viola Marconetto and Prateek Mathur and Karim Lekadir and Marco Lorenzi and Ferran Prados and Maria A. Zuluaga},
keywords = {Efficient annotation, Weak supervised learning, Segmentation, Deep learning, Cerebrovascular tree},
abstract = {Deep learning techniques for 3D brain vessel image segmentation have not been as successful as in the segmentation of other organs and tissues. This can be explained by two factors. First, deep learning techniques tend to show poor performances at the segmentation of relatively small objects compared to the size of the full image. Second, due to the complexity of vascular trees and the small size of vessels, it is challenging to obtain the amount of annotated training data typically needed by deep learning methods. To address these problems, we propose a novel annotation-efficient deep learning vessel segmentation framework. The framework avoids pixel-wise annotations, only requiring weak patch-level labels to discriminate between vessel and non-vessel 2D patches in the training set, in a setup similar to the CAPTCHAs used to differentiate humans from bots in web applications. The user-provided weak annotations are used for two tasks: (1) to synthesize pixel-wise pseudo-labels for vessels and background in each patch, which are used to train a segmentation network, and (2) to train a classifier network. The classifier network allows to generate additional weak patch labels, further reducing the annotation burden, and it acts as a second opinion for poor quality images. We use this framework for the segmentation of the cerebrovascular tree in Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The results show that the framework achieves state-of-the-art accuracy, while reducing the annotation time by ∼77% w.r.t. learning-based segmentation methods using pixel-wise labels for training.}
}
@article{LEMSALU202224,
title = {Real-Time CNN-based Computer Vision System for Open-Field Strawberry Harvesting Robot},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {32},
pages = {24-29},
year = {2022},
note = {7th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.109},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322027422},
author = {Madis Lemsalu and Victor Bloch and Juha Backman and Matti Pastell},
keywords = {Convolutional neural networks, computer vision, agricultural robotics, edge computing},
abstract = {Strawberry production in open-field conditions requires a lot of human labor, which is increasingly difficult to recruit. A robotic solution could potentially operate in the field around the clock with minimal supervision. For strawberry farmers, automation of harvesting would eliminate the personnel risk and provide security for operations in the long term. A robot which would be capable of replacing physical human labor in horticultural production requires an accurate and fast perception system. In this paper, we focus on the task of detecting of garden strawberries to guide the picking by a strawberry harvesting robot. We have developed a real-time implementation of strawberry and peduncle detection system that runs on an edge device. This paper outlines the vision system requirements, hardware selection, model selection, training process and results. After consideration of the overall requirements of the system, we decided to use YOLOv5 to detect both the berries and peduncles for the picking system. Training data was collected and annotated, and the detection model was trained. The network had 91.5% average precision (AP) for detecting strawberries and an 43.6% AP for detecting peduncles. One of the reasons for performance discrepancy was the difficulty to detect peduncles from afar. Overall, the vision algorithm reached the performance that was required to guide the robot to a strawberry and detect the corresponding strawberry-peduncle pairs. However, for densely clustered berries the method often failed to detect the correct peduncle and needs to be improved.}
}
@article{AHMAD2021100365,
title = {Machine learning approaches to IoT security: A systematic literature review },
journal = {Internet of Things},
volume = {14},
pages = {100365},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100365},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000093},
author = {Rasheed Ahmad and Izzat Alsmadi},
keywords = {Internet of things (IoT), Large-scale attacks, Machine learning, Deep learning},
abstract = {With the continuous expansion and evolution of IoT applications, attacks on those IoT applications continue to grow rapidly. In this systematic literature review (SLR) paper, our goal is to provide a research asset to researchers on recent research trends in IoT security. As the main driver of our SLR paper, we proposed six research questions related to IoT security and machine learning. This extensive literature survey on the most recent publications in IoT security identified a few key research trends that will drive future research in this field. With the rapid growth of large scale IoT attacks, it is important to develop models that can integrate state of the art techniques and technologies from big data and machine learning. Accuracy and efficiency are key quality factors in finding the best algorithms and models to detect IoT attacks in real or near real-time}
}
@article{SAFAEIPOUR2020101707,
title = {On data-driven curation, learning, and analysis for inferring evolving internet-of-Things (IoT) botnets in the wild},
journal = {Computers & Security},
volume = {91},
pages = {101707},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101707},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819302445},
author = {Morteza {Safaei Pour} and Antonio Mangino and Kurt Friday and Matthias Rathbun and Elias Bou-Harb and Farkhund Iqbal and Sagar Samtani and Jorge Crichigno and Nasir Ghani},
keywords = {Data science, Cyber forensics, Internet-of-things, IoT Security, Internet measurements},
abstract = {The insecurity of the Internet-of-Things (IoT) paradigm continues to wreak havoc in consumer and critical infrastructures. The highly heterogeneous nature of IoT devices and their widespread deployments has led to the rise of several key security and measurement-based challenges, significantly crippling the process of collecting, analyzing and correlating IoT-centric data. To this end, this paper explores macroscopic, passive empirical data to shed light on this evolving threat phenomena. The proposed work aims to classify and infer Internet-scale compromised IoT devices by solely observing one-way network traffic, while also uncovering, reporting and thoroughly analyzing “in the wild” IoT botnets. To prepare a relevant dataset, a novel probabilistic model is developed to cleanse unrelated traffic by removing noise samples (i.e., misconfigured network traffic). Subsequently, several shallow and deep learning models are evaluated in an effort to train an effective multi-window convolutional neural network. By leveraging active and passing measurements when generating the training dataset, the neural network aims to accurately identify compromised IoT devices. Consequently, to infer orchestrated and unsolicited activities that have been generated by well-coordinated IoT botnets, hierarchical agglomerative clustering is employed by scrutinizing a set of innovative and efficient network feature sets. Analyzing 3.6 TB of recently captured darknet traffic revealed a momentous 440,000 compromised IoT devices and generated evidence-based artifacts related to 350 IoT botnets. Moreover, by conducting thorough analysis of such inferred campaigns, we reveal their scanning behaviors, packet inter-arrival times, employed rates and geo-distributions. Although several campaigns exhibit significant differences in these aspects, some are more distinguishable; by being limited to specific geo-locations or by executing scans on random ports besides their core targets. While many of the inferred botnets belong to previously documented campaigns such as Hide and Seek, Hajime and Fbot, newly discovered events portray the evolving nature of such IoT threat phenomena by demonstrating growing cryptojacking capabilities or by targeting industrial control services. To motivate empirical (and operational) IoT cyber security initiatives as well as aid in reproducibility of the obtained results, we make the source codes of all the developed methods and techniques available to the research community at large.}
}
@article{ZAGO2020105400,
title = {UMUDGA: A dataset for profiling algorithmically generated domain names in botnet detection},
journal = {Data in Brief},
volume = {30},
pages = {105400},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.105400},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920302948},
author = {Mattia Zago and Manuel {Gil Pérez} and Gregorio {Martínez Pérez}},
keywords = {Domain Generation Algorithm (DGA), Natural Language Processing (NLP), Machine learning, Data, Network security},
abstract = {In computer security, botnets still represent a significant cyber threat. Concealing techniques such as the dynamic addressing and the domain generation algorithms (DGAs) require an improved and more effective detection process. To this extent, this data descriptor presents a collection of over 30 million manually-labeled algorithmically generated domain names decorated with a feature set ready-to-use for machine learning (ML) analysis. This proposed dataset has been co-submitted with the research article ”UMUDGA: a dataset for profiling DGA-based botnet” [1], and it aims to enable researchers to move forward the data collection, organization, and pre-processing phases, eventually enabling them to focus on the analysis and the production of ML-powered solutions for network intrusion detection. In this research, we selected 50 among the most notorious malware variants to be as exhaustive as possible. Inhere, each family is available both as a list of domains (generated by executing the malware DGAs in a controlled environment with fixed parameters) and as a collection of features (generated by extracting a combination of statistical and natural language processing metrics).}
}
@article{MANSFIELDDEVINE201713,
title = {Weaponising the Internet of Things},
journal = {Network Security},
volume = {2017},
number = {10},
pages = {13-19},
year = {2017},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(17)30104-6},
url = {https://www.sciencedirect.com/science/article/pii/S1353485817301046},
author = {Steve Mansfield-Devine},
abstract = {It seems that as fast as we develop defences for our networks and computing devices we also introduce new products with new flaws that criminals and others can exploit. The Internet of Things (IoT) is a case in point. In the rush to connect everything from dolls to CCTV systems to the Internet, security seems to get overlooked, making us all vulnerable. In this interview with penetration tester Ken Munro, a partner at Pen Test Partners, we discover how the most seemingly innocuous product can be turned into a cyber weapon.}
}
@article{LICORISH2022111156,
title = {Understanding students’ software development projects: Effort, performance, satisfaction, skills and their relation to the adequacy of outcomes developed},
journal = {Journal of Systems and Software},
volume = {186},
pages = {111156},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111156},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002466},
author = {Sherlock A. Licorish and Matthias Galster and Georgia M. Kapitsaki and Amjed Tahir},
keywords = {Course project effort, Student performance, Student satisfaction, Software engineering skills, Project challenges},
abstract = {Given the inclusion of (often team-based) course projects in tertiary software engineering education, it is necessary to investigate software engineering curricula and students’ experiences while undergoing their software engineering training. Previous research efforts have not sufficiently explored students perceptions around the commitment and adequacy of effort spent on software engineering projects, their project performance and skills that are developed during course projects. This gap in skills awareness includes those that are necessary, anticipated and learned, and the challenges to student project success, which may predict project performance. Such insights could inform curricula design, theory and practice, in terms of improving post-study software development success. We conducted a survey involving undergraduate across four universities in New Zealand and Cyprus to explore these issues, where extensive deductive and inductive analyses were performed. Among our findings we observe that students’ commitment of effort on software engineering project seems appropriate. Students are more satisfied with their team’s collaboration performance than technical contributions, but we found that junior students seemed to struggle with teamwork. Further, we observe that the software students developed were of higher quality if they had worked in project teams previously, had stronger technical skills and were involved in timely meetings. This study singles out mechanisms for informing good estimation of effort, mentoring technical competencies and suitable coaching for enhancing project success and student learning.}
}
@article{MAI2023100135,
title = {A novel GPU based Geo-Location Inference Attack on WebGL framework},
journal = {High-Confidence Computing},
volume = {3},
number = {4},
pages = {100135},
year = {2023},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2023.100135},
url = {https://www.sciencedirect.com/science/article/pii/S2667295223000338},
author = {Weixian Mai and Yinhao Xiao},
keywords = {Side channel attack, GPU, WebGL, Differential Privacy},
abstract = {In the past few years, graphics processing units (GPUs) have become an indispensable part of modern computer systems, not only for graphics rendering but also for intensive parallel computing. Given that many tasks running on GPUs contain sensitive information, security concerns have been raised, especially about potential GPU information leakage. Previous works have shown such concerns by showing that attackers can use GPU memory allocations or performance counters to measure victim side effects. However, such an attack has a critical drawback that it requires a victim to install desktop applications or mobile apps yielding it uneasy to be deployed in the real world. In this paper, we solve this drawback by proposing a novel GPU-based side-channel Geo-Privacy inference attack on the WebGL framework, namely, GLINT (stands for Geo-Location Inference Attack). GLINT merely utilizes a lightweight browser extension to measure the time elapsed to render a sequence of frames on well-known map websites, e.g., Google Maps, or Baidu Maps. The measured stream of time series is then employed to infer geologically privacy-sensitive information, such as a search on a specific location. Upon retrieving the stream, we propose a novel online segmentation algorithm for streaming data to determine the start and end points of privacy-sensitive time series. We then combine the DTW algorithm and KNN algorithm on these series to conclude the final inference on a user’s geo-location privacy. We conducted real-world experiments to testify our attack. The experiments show that GeoInfer can correctly infer more than 83% of user searches regardless of the locations and map websites, meaning that our Geo-Privacy inference attack is accurate, practical, and robust. To counter this attack, we implemented a defense strategy based on Differential Privacy to hinder obtaining accurate rendering data. We found that this defense mechanism managed to reduce the average accuracy of the attack model by more than 70%, indicating that the attack was no longer effective. We have fully implemented GLINT and open-sourced it for future follow-up research.}
}
@article{SIINO2024102342,
title = {Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers},
journal = {Information Systems},
volume = {121},
pages = {102342},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001783},
author = {Marco Siino and Ilenia Tinnirello and Marco {La Cascia}},
keywords = {Text preprocessing, Natural Language Processing, Fake news, SVM, Bayes, Transformers, Deep learning, LSTM, Convolutional neural networks},
abstract = {With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models – including modern Transformers – get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique – in place of the worst – can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Naïve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing.}
}
@article{SAIDANI2020106392,
title = {Predicting continuous integration build failures using evolutionary search},
journal = {Information and Software Technology},
volume = {128},
pages = {106392},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106392},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301579},
author = {Islem Saidani and Ali Ouni and Moataz Chouchen and Mohamed Wiem Mkaouer},
keywords = {Continuous integration, Build prediction, Multi-Objective optimization, Search-Based software engineering, Machine learning},
abstract = {Context: Continuous Integration (CI) is a common practice in modern software development and it is increasingly adopted in the open-source as well as the software industry markets. CI aims at supporting developers in integrating code changes constantly and quickly through an automated build process. However, in such context, the build process is typically time and resource-consuming which requires a high maintenance effort to avoid build failure. Objective: The goal of this study is to introduce an automated approach to cut the expenses of CI build time and provide support tools to developers by predicting the CI build outcome. Method: In this paper, we address problem of CI build failure by introducing a novel search-based approach based on Multi-Objective Genetic Programming (MOGP) to build a CI build failure prediction model. Our approach aims at finding the best combination of CI built features and their appropriate threshold values, based on two conflicting objective functions to deal with both failed and passed builds. Results: We evaluated our approach on a benchmark of 56,019 builds from 10 large-scale and long-lived software projects that use the Travis CI build system. The statistical results reveal that our approach outperforms the state-of-the-art techniques based on machine learning by providing a better balance between both failed and passed builds. Furthermore, we use the generated prediction rules to investigate which factors impact the CI build results, and found that features related to (1) specific statistics about the project such as team size, (2) last build information in the current build and (3) the types of changed files are the most influential to indicate the potential failure of a given build. Conclusion: This paper proposes a multi-objective search-based approach for the problem of CI build failure prediction. The performances of the models developed using our MOGP approach were statistically better than models developed using machine learning techniques. The experimental results show that our approach can effectively reduce both false negative rate and false positive rate of CI build failures in highly imbalanced datasets.}
}
@article{ZWIRNMANN20236477,
title = {Towards End-to-End Automated Microscopy Control using Holotomography: Workflow Design and Data Management},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6477-6483},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.862},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323012429},
author = {Henning Zwirnmann and Dennis Knobbe and Sami Haddadin},
keywords = {Biomedical and medical image processing and systems, Bio-signals analysis and interpretation, Bioinformatics, Human centred automation},
abstract = {Microscopy has been a key tool involved in many discoveries in the life sciences over the past centuries. In the last 30 years in particular, enormous progress has been made in developing this measurement technique further to make researchers working with it more effective. To combine gains in reproducibility and efficiency resulting from these advancements in different research areas, we present for the first time a unified and comprehensive concept for an end-to-end automated microscopy workflow. To this end, we employ both robotic and computational methods as well as holotomography microscopy. Considering the physical preparation and cleanup of a measurement, the image acquisition, and the management and analysis of the resulting data, we give a fine-grained workflow description. We present the robotic system to perform the manual process steps and a Python package to standardize the resulting proprietary image (meta)data. For the other tasks, we identify suitable open-source tools to execute them and apply them to our setup. The choice of holotomography as a suitable microscopy technique to realize this workflow is elucidated. We envision that the adoption of an automated workflow paves the way toward a future life science laboratory where microscopy-based research is carried out more efficiently and reproducibly than in the past.}
}
@article{YANG2024e29584,
title = {Factors affecting the use of artificial intelligence generated content by subject librarians: A qualitative study},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e29584},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29584},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024056159},
author = {Xiaowen Yang and Jingjing Ding and Haibo Chen and Hanzhen Ji},
keywords = {Subject librarians, Artificial intelligence generated content, Interpretive phenomenological analysis, Technology acceptance model, Influencing factor},
abstract = {To explore the factors affecting the use of artificial intelligence generated content (AIGC) by subject librarians through understanding their perceptions of AIGC. Interpretive phenomenological analysis (IPA) and technology acceptance model (TAM) were used in semi-structured interviews to explore the external variables of perceived ease of use and perceived usability of AIGC application in subject librarians. The perceptions of subject librarians towards AIGC included performance, risk perceptions, ability enhancement, and affective attitude. Attentions were paid to AIGC's performances in providing customized services, optimizing collection resources and improving cost efficiency. The risk perception involved technical stability, data security, user acceptance and occupational risk, the ability enhancement involved the improvement of personal literacy, innovative ability, and self-confidence through the use of AIGC technology, and the affective attitudes included not only excitement and anticipation for the technical potential of AIGC, but also concerns and skepticism about it, and critical attitudes toward its application in academic settings and the ethical issues it may raise. TAM analysis on the factors affecting the use of AIGC by subject librarians indicates that the external influencing factors of perceived ease of use include personal literacy, innovative ability, self-confidence enhancement and affective attitude; the external influencing factors of perceived usability include precise service, collection resource optimization, cost-effectiveness, technological risk, user acceptance and occupational risk. These factors constitute a theoretical framework for understanding and promoting the acceptance and effective use of AIGC by subject librarians. TAM analysis combined with IPA exploration on the external variables of perceived ease of use and perceived usability of AIGC application can identify the key factors affecting subject librarians' perceptions of AIGC, propose strategies for optimizing librarians' roles, enhancing information recognition ability and privacy protection, thus providing guidance for effective use of AIGC in library.}
}
@article{LEE2024102520,
title = {Data Collection, data mining and transfer of learning based on customer temperament-centered complaint handling system and one-of-a-kind complaint handling dataset},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102520},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102520},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400168X},
author = {Ching-Hung Lee and Xuejiao Zhao},
keywords = {Customer Complaint Handling System, Customer Temperament, Data Mining, Correspondence Analysis, Interactive marketing},
abstract = {One of the most significant sources of information from customers is customer complaints. Successful and effective complaint management can end complaint crises and ensure client loyalty, which is a sign of great service performance. In this paper, we proposed a novel customer temperament-centered and e-CCH system-based data collection and data mining method titled “3D” model for customer complaint data analysis. Three phases are (1) Development and launch of e-Customer Complaint Handling system, (2) Data collection and transfer of learning by e-Customer Complaint Handling system, and (3) Data mining by e-Customer Complaint Handling system. An advanced electronic Customer Complaint Handling System called the e-CCH system was then developed and launched. This system adapts the seasonal associations model based on Hippocrates's customer temperament theory to the whole stages of customer complaint reporting and handling. With this system, we conducted a dataset collection work from restaurant chains of two brands over four years. As a result, we collect thousands of real-world temperament-centred customer complaint cases by four years to form the one-of-a-kind CCH dataset. This one-of-a-kind CCH dataset was open-sourced with detailed customer complaint attributes and heuristic decision-making for valuable industrial handling manner. After further analysis of this dataset, we found that customers with different temperament types tend to have different types of complaints. In addition, adapting the temperament theory to the e-CCH system can classify customer types better and provide personalized solutions. To our best knowledge, this rich and the one-of-a-kind CCH dataset reported in this paper is the first comprehensive study of customer complaint handling in an industrial service management context. Meanwhile, data mining with cross analysis and correspondence analysis and an ChatGPT experiment for transfer of learning based on this yearly and one-of-a-kind industrial customer complaint dataset was analyzed and discussed. In addition, how this dataset may contribute to more realistic complaint-handling theoretic studies for better service failure recovery and interactive marketing is discussed in-depth.}
}
@article{CHO2019316,
title = {A Method of Monitoring and Detecting APT Attacks Based on Unknown Domains},
journal = {Procedia Computer Science},
volume = {150},
pages = {316-323},
year = {2019},
note = {Proceedings of the 13th International Symposium “Intelligent Systems 2018” (INTELS’18), 22-24 October, 2018, St. Petersburg, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919304041},
author = {Do Xuan Cho and Ha Hai Nam},
keywords = {Information Security, APT, unknown domain, attack detection, attack monitoring},
abstract = {The increasing coverage of Internet has created opportunities and advantages for different aspects of society. However, there come new threats and challenges to information security. One of the typical types of attacks that has increasingly occurred is the APT attack (Advanced Persistent Threat). APT is dangerous with clear purposes. APT attacks employ different sophisticated methods and techniques attacking targets in order to steal confidential and sensitive information. In the past, hackers attacked information systems with personal and financial motives. However, there are nowadays other motives such as political ones and they are potentially backed by governments or nations. Nations that own advanced technologies such as United States, India, Russia, UK are also suffering from special purpose attacks. APT is an advanced type of attacks that consists of many stages and concrete strategies. Besides, techniques and technologies employed in APT attack are usually new and developed by hackers in order to break through the monitoring of security software. However, APT is normally implemented through concrete steps and stages. If one of the steps or stages fails, the entire APT attack will fail. This paper presents a method of detecting APT attacks based on monitoring accesses to unknown domains. This detection method results into high effectiveness in the initial stage of APT attacks.}
}
@article{SINGH201928,
title = {Issues and challenges in DNS based botnet detection: A survey},
journal = {Computers & Security},
volume = {86},
pages = {28-52},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819301117},
author = {Manmeet Singh and Maninder Singh and Sanmeet Kaur},
keywords = {Botnet, Botnet detection, DNS-based Botnet detection, Network Security, DGA},
abstract = {Cybercrimes are evolving on a regular basis and as such these crimes are becoming a greater threat day by day. Earlier these threats were very general and unorganized. In the last decade, these attacks have become highly sophisticated in nature. This higher level of coordination is possible mainly due to botnets, which are clusters of infected hosts controlled remotely by an attacker (botmaster). The number of infected machines is continuously rising, thereby resulting in botnets with over a million infected machines. This powerful capability gives the botmaster a lethal weapon to launch various security attacks. As a result, botnet detection techniques received greater research focus. The Domain Name System (DNS) is a large scale distributed database on the Internet, which is being abused as a botnet communication channel. While there are numerous survey and review papers on botnet detection, there are two survey papers on DNS-based botnet detection which are neither comprehensive nor take into consideration various parameters vital for effective comparison. This survey presents a new classification for DNS-based botnet detection techniques and provides a deep analysis of each technique within the category.}
}
@article{GEHRLEIN2020101577,
title = {The impact of markets on moral reasoning: Evidence from an online experiment},
journal = {Journal of Behavioral and Experimental Economics},
volume = {87},
pages = {101577},
year = {2020},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2020.101577},
url = {https://www.sciencedirect.com/science/article/pii/S221480431930535X},
author = {Jonas Gehrlein and Ann-Kathrin Crede and Nana Adrian},
keywords = {Morality, Markets, Deontology, Consequentialism, Otree, Online experiment},
abstract = {This paper investigates the impact of markets on moral reasoning. Whereas the current literature focuses on morally relevant decisions that arise in markets, little is known about whether the exposition to markets shapes subsequent moral reasoning. To close this gap, we run a large-scale online experiment with 3 conditions: In Baseline, participants make a choice in a moral dilemma. In the other two conditions, participants are exposed to either a Non-market or Market environment, before facing the identical choice in the moral dilemma. We hypothesize that being exposed to Market induces cost-benefit considerations, which translate into modified reasoning in the subsequent moral dilemma. Compared to the baseline distribution, we indeed find a substantial effect in Market. However, similar choices can be observed in Non-market. We discuss potential explanations for these results, and suggest avenues for future research.}
}
@article{WRIGHT2022e00365,
title = {FoamPi: An open-source raspberry Pi based apparatus for monitoring polyurethane foam reactions},
journal = {HardwareX},
volume = {12},
pages = {e00365},
year = {2022},
issn = {2468-0672},
doi = {https://doi.org/10.1016/j.ohx.2022.e00365},
url = {https://www.sciencedirect.com/science/article/pii/S2468067222001109},
author = {Harry C. Wright and Duncan D. Cameron and Anthony J. Ryan},
keywords = {Polyurethane, Kinetics, Raspberry pi, Foam, Adiabatic temperature rise},
abstract = {Adiabatic temperature rise is an important method for determining isocyanate conversion in polyurethane foam reactions as well as many other exothermic chemical reactions. Adiabatic temperature rise can be used in conjunction with change in height and mass measurements to gain understanding into the blowing and gelling reactions that occur during polyurethane foaming as well as give important information on cell morphology. FoamPi is an open-source Raspberry Pi device for monitoring polyurethane foaming reactions. The device effectively monitors temperature rise, change in foam height as well as changes in the mass during the reaction. Three Python scripts are also presented. The first logs raw data during the reaction. The second corrects temperature data such that it can be used in adiabatic temperature rise reactions for calculating isocyanate conversion; additionally this script reduces noise in all the data and removes erroneous readings. The final script extracts important information from the corrected data such as maximum temperature change and maximum height change as well as the time to reach these points. Commercial examples of such equipment exist however the price (>£10000) of these equipment make these systems inaccessible for many research laboratories. The FoamPi build presented is inexpensive (£350) and test examples are shown here to indicate the reproducibility of results as well as precision of the FoamPi.}
}
@article{POURRAHMANI2023100888,
title = {A review of the security vulnerabilities and countermeasures in the Internet of Things solutions: A bright future for the Blockchain},
journal = {Internet of Things},
volume = {23},
pages = {100888},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100888},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002111},
author = {Hossein Pourrahmani and Adel Yavarinasab and Amir Mahdi Hosseini Monazzah and Jan {Van herle}},
keywords = {Blockchain, IA, IoT, Security countermeasures, Security vulnerabilities},
abstract = {The current advances in the Internet of Things (IoT) and the solutions being offered by this technology have accounted IoT among the top ten technologies that will transform the global economy by 2030. IoT is a state-of-the-art paradigm that has developed traditional living into a high-tech lifestyle. The current study aims to provide a comprehensive review and analysis of the existing cybersecurity attacks and vulnerabilities in IoT, offering suitable countermeasures with a focus on describing the impact of emerging technologies on IoT devices and protocol layers. The main vulnerabilities across different layers of the IoT reference model are discussed and categorized, and suitable countermeasures (such as separating IT and IoT network traffic, enhancing physical security, implementing encryption and secure messaging protocols, etc.) are suggested. In addition, the hardware, communication, application, web, and cloud vulnerabilities are introduced, then the corresponding safeguards and protections are presented. Furthermore, Information Assurance (IA) has been deliberately defined and the adoption of the NIST framework and IA model is recommended as a metric to ensure security for IoT solutions considering the five pillars of availability, integrity, authentication, confidentiality, and non-repudiation. Finally, Blockchain technology, known for its use in securing cryptocurrencies, is suggested to facilitate secure data exchange, identification, authentication, and communication for IoT devices by various avenues including ensuring the integrity of sensor data, eliminating the need for intermediaries, reducing costs, and enabling direct addressability of IoT devices.}
}
@article{DICKINSON2023221,
title = {Life history matters: Differential effects of abomasal parasites on caribou fitness},
journal = {International Journal for Parasitology},
volume = {53},
number = {4},
pages = {221-231},
year = {2023},
issn = {0020-7519},
doi = {https://doi.org/10.1016/j.ijpara.2023.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020751923000334},
author = {Eleanor R Dickinson and Karin Orsel and Christine Cuyler and Susan J Kutz},
keywords = {, , , , Reproduction, Body condition, Gastrointestinal nematodes},
abstract = {Parasites can impact wildlife populations through their effects on host fitness and survival. The life history strategies of a parasite species can dictate the mechanisms and timing through which it influences the host. However, unravelling this species-specific effect is difficult as parasites generally occur as part of a broader community of co-infecting parasites. Here, we use a unique study system to explore how life histories of different abomasal nematode species may influence host fitness. We examined abomasal nematodes in two adjacent, but isolated, West Greenland caribou (Rangifer tarandus groenlandicus) populations. One herd of caribou were naturally infected with Ostertagia gruehneri, a common and dominant summer nematode of Rangifer sspp., and the other with Marshallagia marshalli (abundant; winter) and Teladorsagia boreoarcticus (less abundant; summer), allowing us to determine if these nematode species have differing effects on host fitness. Using a Partial Least Squares Path Modelling approach, we found that in the caribou infected with O. gruehneri, higher infection intensity was associated with lower body condition, and that animals with lower body condition were less likely to be pregnant. In caribou infected with M. marshalli and T. boreoarcticus, we found that only M. marshalli infection intensity was negatively related to body condition and pregnancy, but that caribou with a calf at heel were more likely to have higher infection intensities of both nematode species. The differing effects of abomasal nematode species on caribou health outcomes in these herds may be due to parasite species-specific seasonal patterns which influence both transmission dynamics and when the parasites have the greatest impact on host condition. These results highlight the importance of considering parasite life history when testing associations between parasitic infection and host fitness.}
}
@article{TSOGBAATAR2021100391,
title = {DeL-IoT: A deep ensemble learning approach to uncover anomalies in IoT},
journal = {Internet of Things},
volume = {14},
pages = {100391},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100391},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000354},
author = {Enkhtur Tsogbaatar and Monowar H. Bhuyan and Yuzo Taenaka and Doudou Fall and Khishigjargal Gonchigsumlaa and Erik Elmroth and Youki Kadobayashi},
keywords = {Anomaly detection, Software-Defined Networking (SDN), Deep ensemble learning, Autoencoders, Probabilistic Neural Networks (PNNs), Internet of Thing (IoT)},
abstract = {Internet of Things (IoT) devices are inherently vulnerable due to insecure design, implementation, and configuration. Aggressive behavior changes, due to increased attacker’s sophistication, and the heterogeneity of the data in IoT have proven that securing IoT devices trigger multiple challenges. It includes complex and dynamic attack detection, data imbalance, data heterogeneity, real-time response, and prediction capability. Most researchers are not focusing on the class imbalance, dynamic attack detection, and data heterogeneity problems together in Software-Defined Networking (SDN) enabled IoT anomaly detection. Thus, to address these challenging tasks, we propose DeL-IoT, a deep ensemble learning framework for IoT anomaly detection and prediction using SDN, having three primary modules including anomaly detection, intelligent flow management, and device status forecasting. The DeL-IoT employs deep and stacked autoencoders to extract handy features for stacking into an ensemble learning model. This framework yields efficient detection of anomalies, manages flows dynamically, and forecasts both short and long-term device status for early action. We validate the proposed DeL-IoT framework with testbed and benchmark datasets. We demonstrate that in even a 1% imbalanced dataset, the performance of our proposed method, deep feature extraction with a deep ensemble learning model, is around 3% better than the single model. The extensive experimental results show that our models have a better and more reliable performance than the competing models showcased in the relevant related work.}
}
@article{SOKOLOWSKA2019394,
title = {Classification of user attitudes in Twitter -beginners guide to selected Machine Learning libraries},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {27},
pages = {394-399},
year = {2019},
note = {16th IFAC Conference on Programmable Devices and Embedded Systems PDES 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.692},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319326412},
author = {Marta Sokolowska and Maciej Mazurek and Marcin Majer and Michal Podpora},
keywords = {Machine Learning, Natural Language Processing, Education, Twitter, Sentiment Analysis, fastText, TensorFlow, Scikit-Learn},
abstract = {This paper presents an interesting use case for learning as well as teaching basics of Machine Learning systems. Starting from a brief historical outline of the ML, the authors propose and compare a set of popular ML libraries in an interesting exemplary implementation, to present their usability. The paper also describes text classification methods, the aim of which is to distinguish positive and negative labels of particular messages within the Twitter social network. The study is summarized by a brief comparison of the quality of the classification of the libraries and methods used, as an assessment of their suitability. Final thoughts on the importance of teaching ML are included.}
}
@article{SAFA2022108422,
title = {TauRunner: A public Python program to propagate neutral and charged leptons},
journal = {Computer Physics Communications},
volume = {278},
pages = {108422},
year = {2022},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2022.108422},
url = {https://www.sciencedirect.com/science/article/pii/S0010465522001412},
author = {Ibrahim Safa and Jeffrey Lazar and Alex Pizzuto and Oswaldo Vasquez and Carlos A. Argüelles and Justin Vandenbroucke},
keywords = {Ultra-high energy, Neutrinos, Neutrino telescope, Simulation, Tau regeneration, Open source},
abstract = {In the past decade IceCube's observations have revealed a flux of astrophysical neutrinos extending to 107GeV. The forthcoming generation of neutrino observatories promises to grant further insight into the high-energy neutrino sky, with sensitivity reaching energies up to 1012GeV. At such high energies, a new set of effects becomes relevant, which was not accounted for in the last generation of neutrino propagation software. Thus, it is important to develop new simulations which efficiently and accurately model lepton behavior at this scale. We present TauRunner, a Python-based package that propagates neutral and charged leptons. TauRunner supports propagation between 10GeV and 1012GeV. The package accounts for all relevant secondary neutrinos produced in charged-current tau neutrino interactions. Additionally, tau energy losses of taus produced in neutrino interactions are taken into account, and treated stochastically. Finally, TauRunner is broadly adaptable to divers experimental setups, allowing for user-specified trajectories and propagation media, neutrino cross sections, and initial spectra.
Program summary
Program title: TauRunner CPC Library link to program files: https://doi.org/10.17632/82nyd9skhj.1 Developer's repository link: https://github.com/icecube/TauRunner Licensing provisions: GNU General Public License 3 Programming language: Python Nature of problem: Propagation of ultra-high energy neutrinos in dense media accounting for various effects associated with ντ and τ± energy losses. Solution method: Monte Carlo methods.}
}
@article{CASINO2021103135,
title = {Intercepting Hail Hydra: Real-time detection of Algorithmically Generated Domains},
journal = {Journal of Network and Computer Applications},
volume = {190},
pages = {103135},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103135},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001545},
author = {Fran Casino and Nikolaos Lykousas and Ivan Homoliak and Constantinos Patsakis and Julio Hernandez-Castro},
keywords = {Malware, Domain Generation Algorithms, Botnets, DNS, Algorithmically Generated Domain},
abstract = {A crucial technical challenge for cybercriminals is to keep control over the potentially millions of infected devices that build up their botnets, without compromising the robustness of their attacks. A single, fixed C&C server, for example, can be trivially detected either by binary or traffic analysis and immediately sink-holed or taken-down by security researchers or law enforcement. Botnets often use Domain Generation Algorithms (DGAs), primarily to evade take-down attempts. DGAs can enlarge the lifespan of a malware campaign, thus potentially enhancing its profitability. They can also contribute to hindering attack accountability. In this work, we introduce HYDRAS, the most comprehensive and representative dataset of Algorithmically-Generated Domains (AGD) available to date. The dataset contains more than 100 DGA families, including both real-world and adversarially designed ones. We analyse the dataset and discuss the possibility of differentiating between benign requests (to real domains) and malicious ones (to AGDs) in real-time. The simultaneous study of so many families and variants introduces several challenges; nonetheless, it alleviates biases found in previous literature employing small datasets which are frequently overfitted, exploiting characteristic features of particular families that do not generalise well. We thoroughly compare our approach with the current state-of-the-art and highlight some methodological shortcomings in the actual state of practice. The outcomes obtained show that our proposed approach significantly outperforms the current state-of-the-art in terms of both classification performance and efficiency.}
}
@article{DAHIYA2021193,
title = {A reputation score policy and Bayesian game theory based incentivized mechanism for DDoS attacks mitigation and cyber defense},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {193-204},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330600},
author = {Amrita Dahiya and Brij B. Gupta},
keywords = {Auction, Bayesian game, Critical value condition, DDoS attack, Differential payment, Marginal utility},
abstract = {DDoS attack is one of the most powerful cyber-weapons as it does not wait for a specific server configuration or particular network state to attack or to disrupt any operation of the target machine. Further, it does not require any huge investment and can cause enormous reputational and financial loss to the organization. Additionally, the uneven distribution of resources and incentives on Internet has paved an easy path for attackers to take the repercussions of DDoS attack to a challenging level. Malicious users cannot be assumed to obey network protocols or algorithms. In fact, they tried to take advantage of their knowledge about network to disrupt other users and to gain a maximum share of resources. Therefore, in this paper, we propose a Bayesian game theory-based solution to empower service provider to maximize the social welfare by employing incentives and pricing rules on the users of a network. The service provider and legitimate users are assumed to observe the network for a long time and gain probabilistic knowledge about another user being malicious or not. This probabilistic knowledge is utilized by the service provider and legitimate users to amend their actions to counteract malicious users present in the network. Considering these assumptions and facts, we propose Bayesian pricing and auction mechanism to achieve Bayesian Nash Equilibrium points in different scenarios where probabilistic information proves beneficial for legitimate users and service provider. Further, we propose a reputation assessment and updating mechanism where payment and participation parameters are considered to quantify user’s reliability. Extensive experimentation has been carried out using MatLab. We consider the rate of social welfare degradation and variation in user’s utility as parameters to validate the proposed model.}
}
@article{GAO2023115,
title = {Reinforcement learning based web crawler detection for diversity and dynamics},
journal = {Neurocomputing},
volume = {520},
pages = {115-128},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.059},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014473},
author = {Yang Gao and Zunlei Feng and Xiaoyang Wang and Mingli Song and Xingen Wang and Xinyu Wang and Chun Chen},
keywords = {Web crawler detection, Reinforcement learning, Feature selection, Crawler diversity, Crawler dynamics},
abstract = {Crawler detection is always an important research topic in network security. With the development of web technology, crawlers are constantly updating and changing, and their types are becoming diverse. The diversity and dynamics of crawlers pose significant challenges for feature applicability and model robustness. Existing crawler detection methods can only detect a limited number of crawlers by predefined rules and can not cover all types of crawlers; worse, they can be completely invalidated by the emergence of new types of crawlers. In this paper, we propose a reinforcement learning based web crawler detection method for diversity and dynamics (WC3D), which is composed of a feature selector and a session classifier. The feature selector selects the appropriate feature set for different types of crawlers with deep deterministic policy gradient. The session classifier makes crawler detection and provides rewards to the feature selector. The two modules are trained jointly to optimize the feature selection and session classification processes. Extensive experiments demonstrate the existence of crawler diversity and that the proposed method is still highly robust against the new type of crawlers and achieves state-of-the-art performance even without considering the dynamics of the crawlers.}
}
@article{LI2024112019,
title = {Bug priority change: An empirical study on Apache projects},
journal = {Journal of Systems and Software},
volume = {212},
pages = {112019},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112019},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000621},
author = {Zengyang Li and Guangzong Cai and Qinyi Yu and Peng Liang and Ran Mo and Hui Liu},
keywords = {Bug priority change, Open source software, Empirical study},
abstract = {In issue tracking systems, each bug is assigned a priority level (e.g., Blocker, Critical, Major, Minor, or Trivial in JIRA from highest to lowest), which indicates the urgency level of the bug. In this sense, understanding bug priority changes helps to arrange the work schedule of participants reasonably, and facilitates a better analysis and resolution of bugs. According to the data extracted from JIRA deployed by Apache, a proportion of bugs in each project underwent priority changes after such bugs were reported, which brings uncertainty to the bug fixing process. However, there is a lack of in-depth investigation on the phenomenon of bug priority changes, which may negatively impact the bug fixing process. Thus, we conducted a quantitative empirical study on bugs with priority changes through analyzing 32 non-trivial Apache open source software projects. The results show that: (1) 8.3% of the bugs in the selected projects underwent priority changes; (2) the median priority change time interval is merely a few days for most (28 out of 32) projects, and half (50. 7%) of bug priority changes occurred before bugs were handled; (3) for all selected projects, 87.9% of the bugs with priority changes underwent only one priority change, most priority changes tend to shift the priority to its adjacent priority, and a higher priority has a greater probability to undergo priority change; (4) bugs that require bug-fixing changes of higher complexity or that have more comments are likely to undergo priority changes; and (5) priorities of bugs reported or allocated by a few specific participants are more likely to be modified, and maximally only one participant in each project tends to modify priorities.}
}
@article{GUPTA2022107726,
title = {Smart defense against distributed Denial of service attack in IoT networks using supervised learning classifiers},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107726},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107726},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000404},
author = {B.B. Gupta and Pooja Chaudhary and Xiaojun Chang and Nadia Nedjah},
keywords = {Internet of things (IoT) networks, Distributed Denial of Service (DDoS) attack, Consumer IoT (CIoT) devices, Machine learning algorithms, Botnet, IoT security},
abstract = {From smart home to industrial automation to smart power grid, IoT- based solutions penetrate into every working field. These devices expand the attack surface and turned out to be an easy target for the attacker as resource constraint nature hinders the integration of heavy security solutions. Because IoT devices are less secured and operate mostly in unattended scenario, they perfectly justify the requirements of attacker to form botnet army to trigger Denial of Service attack on massive scale. Therefore, this paper presents a Machine Learning-based attack detection approach to identify the attack traffic in Consumer IoT (CIoT). This approach operates on local IoT network-specific attributes to empower low-cost machine learning classifiers to detect attack, at the local router. The experimental outcomes unveiled that the proposed approach achieved the highest accuracy of 0.99 which confirms that it is robust and reliable in IoT networks.}
}
@article{LEE2019102,
title = {Discovering emerging business ideas based on crowdfunded software projects},
journal = {Decision Support Systems},
volume = {116},
pages = {102-113},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618301702},
author = {Won Sang Lee and So Young Sohn},
keywords = {Crowdfunding, User-centered innovation, Design thinking},
abstract = {User-centered innovation has attracted considerable interest for exploiting emerging business ideas. We suggest a novel framework for discovering emerging business ideas and their combination with user-centered innovation in the software industry based on design thinking processes. We apply topic modeling to projects on Kickstarter which is one of the largest crowdfunding platforms in the world. We adopt conjoint analysis to find which topics are most preferred upon the platform in terms of the amount of funding that they have received. From our findings, the convergence of smart assistant services with various domains, such as tutoring mathematics and seeking job opportunities, is recommended as an emerging idea for software businesses. We also find that the ideas preferred in the US are different from those preferred in other countries. Our findings can be exploited effectively for decision support in establishing a new business model. Finally, this study contributes to discovering emerging business ideas by connecting user-centered innovation with a design thinking perspective.}
}
@article{LUTKENHAUS2019100019,
title = {Mapping the Dutch vaccination debate on Twitter: Identifying communities, narratives, and interactions},
journal = {Vaccine: X},
volume = {1},
pages = {100019},
year = {2019},
issn = {2590-1362},
doi = {https://doi.org/10.1016/j.jvacx.2019.100019},
url = {https://www.sciencedirect.com/science/article/pii/S2590136219300208},
author = {Roel O. Lutkenhaus and Jeroen Jansz and Martine P.A. Bouman},
keywords = {Vaccine hesitancy, Health communication, Social media, Network analysis},
abstract = {In recent years, vaccination rates in the Netherlands have declined slightly, but steadily. The Dutch National Institute for Public Health and the Environment (RIVM) commissioned a Committee for Vaccine Willingness (VWC) to study the societal context of the decline. One of the societal contexts is the Internet, where audiences discuss vaccination and refer to sources of health-related information of varying quality. Working for the VWC, we have explored the Dutch vaccination debate on Twitter in order to: (1) identify online communities in the vaccination debate, (2) identify vaccine-related narratives; and (3) understand how the online communities interact with each other. We identified seven different communities, including (public) health professionals, writers and journalists, anti-establishment, and international vaccination advocates. The debate is spearheaded by the writers & journalists community, while the health- and anti-establishment communities try to influence it. The health community circulates facts, figures and scientific studies, while negative messages about vaccination – either from a homeopathy or conspiracy perspective – are most prevalent in the anti-establishment. The facts and figures shared by the health community hardly reach other communities, whereas the myths introduced by the anti-establishment do spill over to other communities. Our study provides further evidence that negative perceptions about vaccination might be rooted in a wider sentiment of distrust of traditional institutions. We argue that Dutch health organizations should try to address questions, doubts, and worries among the general audience more actively, and present scientific information in a simpler and more attractive way.}
}
@article{TANG2022102724,
title = {Automated evolution of feature logging statement levels using Git histories and degree of interest},
journal = {Science of Computer Programming},
volume = {214},
pages = {102724},
year = {2022},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102724},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321001179},
author = {Yiming Tang and Allan Spektor and Raffi Khatchadourian and Mehdi Bagherzadeh},
keywords = {Logging, Software evolution, Software repository mining, Software transformation, Degree of interest},
abstract = {Logging—used for system events and security breaches to describe more informational yet essential aspects of software features—is pervasive. Given the high transactionality of today's software, logging effectiveness can be reduced by information overload. Log levels help alleviate this problem by correlating a priority to logs that can be later filtered. As software evolves, however, levels of logs documenting surrounding feature implementations may also require modification as features once deemed important may have decreased in urgency and vice-versa. We present an automated approach that assists developers in evolving levels of such (feature) logs. The approach, based on mining Git histories and manipulating a degree of interest (DOI) model,1 transforms source code to revitalize feature log levels based on the “interestingness” of the surrounding code. Built upon JGit and Mylyn, the approach is implemented as an Eclipse IDE plug-in and evaluated on 18 Java projects with ∼3 million lines of code and ∼4K log statements. Our tool successfully analyzes 99.22% of logging statements, increases log level distributions by ∼20%, and increases the focus of logs in bug fix contexts ∼83% of the time. Moreover, pull (patch) requests were integrated into large and popular open-source projects. The results indicate that the approach is promising in assisting developers in evolving feature log levels.}
}
@article{TAO201830,
title = {Massive stereo-based DTM production for Mars on cloud computers},
journal = {Planetary and Space Science},
volume = {154},
pages = {30-58},
year = {2018},
issn = {0032-0633},
doi = {https://doi.org/10.1016/j.pss.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0032063317303252},
author = {Y. Tao and J.-P. Muller and P. Sidiropoulos and Si-Ting Xiong and A.R.D. Putri and S.H.G. Walter and J. Veitch-Michaelis and V. Yershov},
keywords = {Mars, Global DTM, CTX, HiRISE, CASP-GO, Clouds computing},
abstract = {Digital Terrain Model (DTM) creation is essential to improving our understanding of the formation processes of the Martian surface. Although there have been previous demonstrations of open-source or commercial planetary 3D reconstruction software, planetary scientists are still struggling with creating good quality DTMs that meet their science needs, especially when there is a requirement to produce a large number of high quality DTMs using “free” software. In this paper, we describe a new open source system to overcome many of these obstacles by demonstrating results in the context of issues found from experience with several planetary DTM pipelines. We introduce a new fully automated multi-resolution DTM processing chain for NASA Mars Reconnaissance Orbiter (MRO) Context Camera (CTX) and High Resolution Imaging Science Experiment (HiRISE) stereo processing, called the Co-registration Ames Stereo Pipeline (ASP) Gotcha Optimised (CASP-GO), based on the open source NASA ASP. CASP-GO employs tie-point based multi-resolution image co-registration, and Gotcha sub-pixel refinement and densification. CASP-GO pipeline is used to produce planet-wide CTX and HiRISE DTMs that guarantee global geo-referencing compliance with respect to High Resolution Stereo Colour imaging (HRSC), and thence to the Mars Orbiter Laser Altimeter (MOLA); providing refined stereo matching completeness and accuracy. All software and good quality products introduced in this paper are being made open-source to the planetary science community through collaboration with NASA Ames, United States Geological Survey (USGS) and the Jet Propulsion Laboratory (JPL), Advanced Multi-Mission Operations System (AMMOS) Planetary Data System (PDS) Pipeline Service (APPS-PDS4), as well as browseable and visualisable through the iMars web based Geographic Information System (webGIS) system.}
}
@article{HOPP2017368,
title = {Does negative campaign advertising stimulate uncivil communication on social media? Measuring audience response using big data},
journal = {Computers in Human Behavior},
volume = {68},
pages = {368-377},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216307750},
author = {Toby Hopp and Chris J. Vargo},
keywords = {Incivility, Big data, Political participation, Political advertising},
abstract = {Using the 2012 presidential election as a case study, this work set out to understand the relationship between negative political advertising and political incivility on Twitter. Drawing on the stimulation hypothesis and the notion that communication with dissimilar others can encourage incivility, it was predicted that (1) heightened levels of negative campaign advertising would be associated with increased citizen activity on Twitter, (2) increased citizen activity would predict online incivility, and (3) that increases in citizen activity would facilitate a positive indirect relationship between negative advertising volume and citizen incivility. This theoretical model was tested using data collected from over 140,000 individual Twitter users located in 206 Designated Market Areas. The results supported the proposed model. Additional analyses further suggested that the relationship between negative political advertising and citizen incivility was conditioned by contextual levels of economic status. These results are discussed in the context of political advertising and democratic deliberation.}
}
@article{SOUSA2024103768,
title = {MONDEO-Tactics5G: Multistage botnet detection and tactics for 5G/6G networks},
journal = {Computers & Security},
volume = {140},
pages = {103768},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103768},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824000695},
author = {Bruno Sousa and Duarte Dias and Nuno Antunes and Javier Cámara and Ryan Wagner and Bradley Schmerl and David Garlan and Pedro Fidalgo},
keywords = {Botnet, DDoS, Command Control server, Tactics},
abstract = {Mobile malware is a malicious code specifically designed to target mobile devices to perform multiple types of fraud. The number of attacks reported each day is increasing constantly and is causing an impact not only at the end-user level but also at the network operator level. Malware like FluBot contributes to identity theft and data loss but also enables remote Command & Control (C2) operations, which can instrument infected devices to conduct Distributed Denial of Service (DDoS) attacks. Current mobile device-installed solutions are not effective, as the end user can ignore security warnings or install malicious software. This article designs and evaluates MONDEO-Tactics5G - a multistage botnet detection mechanism that does not require software installation on end-user devices, together with tactics for 5G network operators to manage infected devices. We conducted an evaluation that demonstrates high accuracy in detecting FluBot malware, and in the different adaptation strategies to reduce the risk of DDoS while minimising the impact on the clients' satisfaction by avoiding disrupting established sessions.}
}
@article{GARCIAROMERO2024108,
title = {Drug efflux and lipid A modification by 4-L-aminoarabinose are key mechanisms of polymyxin B resistance in the sepsis pathogen Enterobacter bugandensis},
journal = {Journal of Global Antimicrobial Resistance},
volume = {37},
pages = {108-121},
year = {2024},
issn = {2213-7165},
doi = {https://doi.org/10.1016/j.jgar.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S2213716524000626},
author = {Inmaculada García-Romero and Mugdha Srivastava and Julia Monjarás-Feria and Samuel O. Korankye and Lewis MacDonald and Nichollas E. Scott and Miguel A. Valvano},
keywords = {Intrinsic antibiotic resistance, Outer membrane permeability, Cationic antimicrobial peptides, Enterobacter cloacae complex, Neonatal sepsis, KexD},
abstract = {ABSTRACT
Objectives
A concern with the ESKAPE pathogen, Enterobacter bugandensis, and other species of the Enterobacter cloacae complex, is the frequent appearance of multidrug resistance against last-resort antibiotics, such as polymyxins.
Methods
Here, we investigated the responses to polymyxin B (PMB) in two PMB-resistant E. bugandensis clinical isolates by global transcriptomics and deletion mutagenesis.
Results
In both isolates, the genes of the CrrAB-regulated operon, including crrC and kexD, displayed the highest levels of upregulation in response to PMB. ∆crrC and ∆kexD mutants became highly susceptible to PMB and lost the heteroresistant phenotype. Conversely, heterologous expression of CrrC and KexD proteins increased PMB resistance in a sensitive Enterobacter ludwigii clinical isolate and in the Escherichia coli K12 strain, W3110. The efflux pump, AcrABTolC, and the two component regulators, PhoPQ and CrrAB, also contributed to PMB resistance and heteroresistance. Additionally, the lipid A modification with 4-L-aminoarabinose (L-Ara4N), mediated by the arnBCADTEF operon, was critical to determine PMB resistance. Biochemical experiments, supported by mass spectrometry and structural modelling, indicated that CrrC is an inner membrane protein that interacts with the membrane domain of the KexD pump. Similar interactions were modeled for AcrB and AcrD efflux pumps.
Conclusion
Our results support a model where drug efflux potentiated by CrrC interaction with membrane domains of major efflux pumps combined with resistance to PMB entry by the L-Ara4N lipid A modification, under the control of PhoPQ and CrrAB, confers the bacterium high-level resistance and heteroresistance to PMB.}
}
@article{MOUSTAFA2021102994,
title = {A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets},
journal = {Sustainable Cities and Society},
volume = {72},
pages = {102994},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102994},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721002808},
author = {Nour Moustafa},
keywords = {Smart cities, Network datasets, Cybersecurity applications, Machine learning, Edge, Software-Defined Network (SDN), Network Function Virtualization (NFV), Service Orchestration (SO)},
abstract = {While there has been a significant interest in understanding the cyber threat landscape of Internet of Things (IoT) networks, and the design of Artificial Intelligence (AI)-based security approaches, there is a lack of distributed architecture led to generating heterogeneous datasets that contain the actual behaviors of real-world IoT networks and complex cyber threat scenarios to evaluate the credibility of the new systems. This paper presents a novel testbed architecture of IoT network which can be used to evaluate Artificial Intelligence (AI)-based security applications. The platform NSX vCloud NFV was employed to facilitate the execution of Software-Defined Network (SDN), Network Function Virtualization (NFV) and Service Orchestration (SO) to offer dynamic testbed networks, which allow the interaction of edge, fog and cloud tiers. While deploying the architecture, real-world normal and attack scenarios are executed to collect labeled datasets. The generated datasets are named ‘TON_IoT’, as they comprise heterogeneous data sources collected from telemetry datasets of IoT services, Windows and Linux-based datasets, and datasets of network traffic. The TON_IoT network dataset is validated using four machine learning-based intrusion detection algorithms of Gradient Boosting Machine, Random Forest, Naive Bayes, and Deep Neural Networks, revealing a high performance of detection accuracy using the set of training and testing. A comparative summary of the TON_IoT network dataset and other competing network datasets demonstrates its diverse legitimate and anomalous patterns that can be used to better validate new AI-based security solutions. The architecture and datasets can be publicly accessed from TON_IOT Datasets (2020).}
}
@article{DALPONTEAYASTUY2021100333,
title = {Adaptive gamification in Collaborative systems, a systematic mapping study},
journal = {Computer Science Review},
volume = {39},
pages = {100333},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100333},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304330},
author = {María {Dalponte Ayastuy} and Diego Torres and Alejandro Fernández},
keywords = {Adaptive gamification, Collaborative systems, Systematic mapping},
abstract = {Mass collaboration mediated by technology is now commonplace (Wikipedia, Quora, TripAdvisor). Online, mass collaboration is also present in science in the form of Citizen Science. These collaboration models, which have a large community of contributors coordinated to pursue a common goal, are known as Collaborative systems. This article introduces a study of the published research on the application of adaptive gamification to collaborative systems. The study focuses on works that explicitly discuss an approach of personalization or adaptation of the gamification elements in this type of system. It employs a systematic mapping design in which a categorical structure for classifying the research results is proposed based on the topics that emerged from the papers review. The main contributions of this paper are a formalization of the adaptation strategies and the proposal of a new taxonomy for gamification elements adaptation. The results evidence the lack of research literature in the study of adapting gamification in the field of collaborative systems. Considering the underlying cultural diversity in those projects, the adaptability of gamification design and strategies is a promissory research field.}
}
@article{TULADHAR2022122,
title = {The allometric propagation of COVID-19 is explained by human travel},
journal = {Infectious Disease Modelling},
volume = {7},
number = {1},
pages = {122-133},
year = {2022},
issn = {2468-0427},
doi = {https://doi.org/10.1016/j.idm.2021.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468042721000841},
author = {Rohisha Tuladhar and Paolo Grigolini and Fidel Santamaria},
keywords = {COVID-19 propagation, Human mobility, Power law scaling, Allometric model, Scalefree dynamics},
abstract = {We analyzed the number of cumulative positive cases of COVID-19 as a function of time in countries around the World. We tracked the increase in cases from the onset of the pandemic in each region for up to 150 days. We found that in 81 out of 146 regions the trajectory was described with a power-law function for up to 30 days. We also detected scale-free properties in the majority of sub-regions in Australia, Canada, China, and the United States (US). We developed an allometric model that was capable of fitting the initial phase of the pandemic and was the best predictor for the propagation of the illness for up to 100 days. We then determined that the power-law COVID-19 exponent correlated with measurements of human mobility. The COVID-19 exponent correlated with the magnitude of air passengers per country. This correlation persisted when we analyzed the number of air passengers per US states, and even per US metropolitan areas. Furthermore, the COVID-19 exponent correlated with the number of vehicle miles traveled in the US. Together, air and vehicular travel explained 70% of the variability of the COVID-19 exponent. Taken together, our results suggest that the scale-free propagation of the virus is present at multiple geographical scales and is correlated with human mobility. We conclude that models of disease transmission should integrate scale-free dynamics as part of the modeling strategy and not only as an emergent phenomenological property.}
}
@article{MURPHY202211021,
title = {The risk perception of nanotechnology: evidence from twitter},
journal = {RSC Advances},
volume = {12},
number = {18},
pages = {11021-11031},
year = {2022},
issn = {2046-2069},
doi = {https://doi.org/10.1039/d1ra09383e},
url = {https://www.sciencedirect.com/science/article/pii/S2046206922006313},
author = {Finbarr Murphy and Ainaz Alavi and Martin Mullins and Irini Furxhi and Arash Kia and Myles Kingston},
abstract = {ABSTRACT
Nanotechnology governance, particularly in relation to human and environmental concerns, remains a contested domain. In recent years, the creation of both a risk governance framework and council has been actively pursued. Part of the function of a governance framework is the communication to external stakeholders. Existing descriptions on the public perceptions of nanotechnology are generally positive with the attendant economic and societal benefits being forefront in that thinking. Debates on nanomaterials' risk tend to be dominated by expert groupings while the general public is largely unaware of the potential hazards. Communicating via social media has become an integral part of everyday life facilitating public connectedness around specific topics that was not feasible in the pre-digital age. When civilian passive stakeholders become active their frustration can quickly coalesce into a campaign of resistance, and once an issue starts to develop into a campaign it is difficult to ease the momentum. Simmering discussions with moderate local attention can gain international exposure resulting in pressure and it can, in some cases, quickly precipitate legislative action and/or economic consequences. This paper highlights the potential of such a runaway, twitterstorm. We conducted a sentiment analysis of tweets since 2006 focusing on silver, titanium and carbon-based nanomaterials. We further examined the sentiment expressed following the decision by the European Food Safety Authority (EFSA) to phase out the food additive titanium dioxide (E 171). Our analysis shows an engaged, attentive public, alert to announcements from industry and regulatory bodies. We demonstrate that risk governance frameworks, particularly the communication aspect of those structures must include a social media blueprint to counter misinformation and alleviate the potential impact of a social media induced regulatory and economic reaction.}
}
@article{MANSFIELDDEVINE20167,
title = {DDoS goes mainstream: how headline-grabbing attacks could make this threat an organisation's biggest nightmare},
journal = {Network Security},
volume = {2016},
number = {11},
pages = {7-13},
year = {2016},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(16)30104-0},
url = {https://www.sciencedirect.com/science/article/pii/S1353485816301040},
author = {Steve Mansfield-Devine},
abstract = {In mid-October, a distributed denial of service (DDoS) attack hit the headlines in a big way. Targeting DNS service provider Dyn, it rendered a significant portion of the Internet inoperable and left many high-profile web services unreachable for several hours. But while this was arguably the most visible DDoS attack in history, it's only one among many. In this interview, Paul Nicholson, responsible for global product marketing and strategy at A10 Networks, talks about how DDoS is becoming an ever-growing threat and what organisations can do about it. We’ve just witnessed the biggest distributed denial of service (DDoS) attacks in history, which turned seemingly harmless devices such as video recorders into cyber-weapons. With both the scale and frequency of attacks increasing, many organisations are left wondering how they can protect themselves and how those defences should be deployed, whether on-premise or in the cloud. In this interview with Paul Nicholson of A10 Networks, we examine how DDoS is becoming an ever-growing threat and what organisations can do about it.}
}
@article{NAIR2019647,
title = {Medical Cyber Physical Systems and Its Issues},
journal = {Procedia Computer Science},
volume = {165},
pages = {647-655},
year = {2019},
note = {2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.01.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920300673},
author = {Meghna Manoj Nair and Amit Kumar Tyagi and Richa Goyal},
keywords = {Cyber Physical System, Medical Cyber Physical System, Smart Devices, Cyber Security in E-healthcare, Issues, Challenges in MCPS},
abstract = {In the previous decade, many technologies have attracted attention from several research communities. Internet of Things (IoT) is main invention of the recent/ past decade. When these smart devices or internet connected devices are interact together, then they create a cyber infrastructure. These cyber infrastructures face several serious concerns privacy, trust, security, etc. These smart devices make an automatic environment (executed without the intervention of a human) in applications likedefense, manufacturing, e-healthcare, etc. In e-healthcare, these devices built the structure of Medical Cyber Physical System (MCPS). MCPS are facing several critical issues and challenges in current era, i.e., several attacks, issues and challenges which we require to overcome in current and next decade to provide efficient and reliable service to patients. MCPS is need of smart healthcare and require attention from several research communities towards its raised issue. Hence, this article provides a detailed study about CPS, MCPS, mitigated attacks on same architecture (CPS and MCPS), issues and challenges in CPS/ MCPS, including several research gaps in CPS/ MCPS (with opportunities for future researchers).}
}
@article{LIU2023103885,
title = {Pore-scale spatiotemporal dynamics of microbial-induced calcium carbonate growth and distribution in porous media},
journal = {International Journal of Greenhouse Gas Control},
volume = {125},
pages = {103885},
year = {2023},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2023.103885},
url = {https://www.sciencedirect.com/science/article/pii/S1750583623000555},
author = {Na Liu and Malin Haugen and Benyamine Benali and David Landa-Marbán and Martin A. Fernø},
keywords = {Hydrodynamics, Supersaturation, Crystal morphology, Porosity reduction},
abstract = {The naturally occurring bio-geochemical microbial-induced calcium carbonate precipitation (MICP) process is an eco-friendly technology for rehabilitating construction materials, reinforcement of soils and sand, heavy metals immobilization and sealing subsurface leakage pathways. We report pore-scale spatiotemporal dynamics of the MICP process in porous media, relevant for reduced environmental risk by leakage during CO2 geological storage. Effects of hydrodynamics and supersaturation on the MICP with Sporosarcina pasteurii stains were studied using a high-pressure, rock-on-a-chip microfluidic device. Bacterial cell numbers and variation in cementation concentration controlled the crystal size and pore-scale distribution by influencing the local supersaturation. Local pore structure determined crystal nucleation, where low velocity regions tended to nucleate more crystals. CaCO3 crystallization was observed at subsurface pressure (100 barg) with a reduced sealing performance due to the low microbial activity from elevated pressure. We identify that hydrodynamics and supersaturation determine crystal nucleation and growth in porous systems, providing important experimental evidence for subsurface environmental applications and validation of upscaled MICP models.}
}
@article{TONNES2021116,
title = {Deterministic Arterial Input Function selection in DCE-MRI for automation of quantitative perfusion calculation of colorectal cancer},
journal = {Magnetic Resonance Imaging},
volume = {75},
pages = {116-123},
year = {2021},
issn = {0730-725X},
doi = {https://doi.org/10.1016/j.mri.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0730725X20302423},
author = {Christian Tönnes and Sonja Janssen and Alena-Kathrin Golla and Tanja Uhrig and Khanlian Chung and Lothar R. Schad and Frank Gerrit Zöllner},
keywords = {Arterial input function, Quantitative perfusion, Dynamic contrast enhanced MRI, Colorectal cancer, Segmentation},
abstract = {Development of a deterministic algorithm for automated detection of the Arterial Input Function (AIF) in DCE-MRI of colorectal cancer. Using a filter pipeline to determine the AIF region of interest. Comparison to algorithms from literature with mean squared error and quantitative perfusion parameter Ktrans. The AIF found by our algorithm has a lower mean squared error (0.0022 ± 0.0021) in reference to the manual annotation than comparable algorithms. The error of Ktrans (21.52 ± 17.2%) is lower than that of other algorithms. Our algorithm generates reproducible results and thus supports a robust and comparable perfusion analysis.}
}
@article{BODAGHI2020674,
title = {The characteristics of rumor spreaders on Twitter: A quantitative analysis on real data},
journal = {Computer Communications},
volume = {160},
pages = {674-687},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420304060},
author = {Amirhosein Bodaghi and Jonice Oliveira},
keywords = {Social networks, User characteristics, Twitter, Theory of planned behavior, Subjective norm, Following to follower},
abstract = {In this paper we study a dozen of rumors on Twitter to find new insights in user characteristics and macro patterns in the process of rumor spreading. The collection and curation of data has left us with 12 rumor datasets out of 56,852 tweets from 43,919 users. The analysis over data shows users with lower ratio of following-to-follower are more probable to spark the rumor diffusion while users with the higher ratio are those who keep the flame alive. Furthermore, most users participate in the process of rumor spreading only once which implies the nature of rumor spreading is not a recurrent activity. However, among those users who engage with multi posts, the extreme change of state from rumor spreader to anti-rumor spreader happens to users with higher ratio of following-to-follower. We discuss these findings by employing the theory of planned behavior. Finally, analyzing the process of rumor spreading at the macro level revealed the existence of two distinctive patterns. Further investigations showed the extent of time gap between the beginning of rumor and anti-rumor diffusion plays the major role in emerging of these patterns. This phenomenon is explained by the shift in subjective norm toward rumors on social media.}
}
@incollection{BAYKOUCHEVA2015127,
title = {14 - Measuring attention: social media and altmetrics},
editor = {Svetla Baykoucheva},
booktitle = {Managing Scientific Information and Research Data},
publisher = {Chandos Publishing},
pages = {127-136},
year = {2015},
isbn = {978-0-08-100195-0},
doi = {https://doi.org/10.1016/B978-0-08-100195-0.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780081001950000147},
author = {Svetla Baykoucheva},
keywords = {altmetrics, alternative metrics, social media, research evaluation, research ­impact, social networks.},
abstract = {Scholars are communicating in many different spheres today, using social media, mobile technology, and cloud computing. Until now, research has been evaluated using citation metrics such as the impact factor (IF) and h-index. As scholarly communication has shifted now mostly to online, other methods are needed to bring attention to research and measure its impact. Altmetrics, a new field that is creating and using such alternative metrics, takes into account not just citation counts of articles published in peer-reviewed journals. It involves collecting information from different sources and measuring interest in articles, people, journals, datasets, presentations, and other artifacts by monitoring views, downloads, “likes,” and mentions in social networks and the news media. Researchers engaging in social networks now rely on recommendations from their peers about newly published articles. Altmetrics tools allow them to see what others are reading, saving, and commenting on. This chapter presents an overview of the area of altmetrics and discusses how it is affecting the dissemination of scientific information and its evaluation.}
}
@article{XU2021113574,
title = {Prediction of initial coin offering success based on team knowledge and expert evaluation},
journal = {Decision Support Systems},
volume = {147},
pages = {113574},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113574},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000841},
author = {Wei Xu and Ting Wang and Runyu Chen and J. Leon Zhao},
keywords = {Initial coin offerings, Cryptocurrency, Heterogeneous knowledge, Text analytics},
abstract = {Initial coin offering (ICO) is a new financing method that has been widely used in cryptocurrency projects. However, it has been reported that nearly 30% of cryptocurrency projects fail during ICO, indicating an important gap in research and an opportunity for more advanced research on ICO project assessment. This study reveals that previous studies primarily used project-related factors to predict ICO success while neglecting social factors such as team information and expert evaluation. Inspired by the knowledge-based theory (KBT) of the firm, we set out to examine the impact of heterogeneous team knowledge and expert evaluation on ICO success. One primary contribution of this study is the design of novel knowledge measures based on KBT. In addition, we propose a deep-learning model – an attention-based bidirectional recurrent neural network (A-BiRNN) – to automatically extract features from online comments. We validate the proposed model on a real-world dataset, and experiments show that the accuracy of the proposed prediction model outperforms those of existing models by more than 6%, highlighting the effectiveness of the proposed approach in predicting ICO success. This study's results provide useful ideas for both investors and ICO platforms to assess the quality of cryptocurrency projects, thus improving information symmetry in ICO markets. Also, this study demonstrates the value of applying KBT in assessing firm performance in ICO markets. The generalized value of the proposed approach should be tested in more business contexts, such as crowdfunding and peer-to-peer (P2P) lending.}
}
@article{LEMAITRE2024100753,
title = {flepiMoP: The evolution of a flexible infectious disease modeling pipeline during the COVID-19 pandemic},
journal = {Epidemics},
volume = {47},
pages = {100753},
year = {2024},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2024.100753},
url = {https://www.sciencedirect.com/science/article/pii/S1755436524000148},
author = {Joseph C. Lemaitre and Sara L. Loo and Joshua Kaminsky and Elizabeth C. Lee and Clifton McKee and Claire Smith and Sung-mok Jung and Koji Sato and Erica Carcelen and Alison Hill and Justin Lessler and Shaun Truelove},
keywords = {COVID-19, Influenza, Respiratory syncytial virus, Compartmental model, Forecasting, Scenario Planning, Pipeline, Open-source Software},
abstract = {The COVID-19 pandemic led to an unprecedented demand for projections of disease burden and healthcare utilization under scenarios ranging from unmitigated spread to strict social distancing policies. In response, members of the Johns Hopkins Infectious Disease Dynamics Group developed flepiMoP (formerly called the COVID Scenario Modeling Pipeline), a comprehensive open-source software pipeline designed for creating and simulating compartmental models of infectious disease transmission and inferring parameters through these models. The framework has been used extensively to produce short-term forecasts and longer-term scenario projections of COVID-19 at the state and county level in the US, for COVID-19 in other countries at various geographic scales, and more recently for seasonal influenza. In this paper, we highlight how the flepiMoP has evolved throughout the COVID-19 pandemic to address changing epidemiological dynamics, new interventions, and shifts in policy-relevant model outputs. As the framework has reached a mature state, we provide a detailed overview of flepiMoP’s key features and remaining limitations, thereby distributing flepiMoP and its documentation as a flexible and powerful tool for researchers and public health professionals to rapidly build and deploy large-scale complex infectious disease models for any pathogen and demographic setup.}
}
@article{SOTELOMONGE2019114,
title = {Traffic-flow analysis for source-side DDoS recognition on 5G environments},
journal = {Journal of Network and Computer Applications},
volume = {136},
pages = {114-131},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S108480451930089X},
author = {Marco Antonio {Sotelo Monge} and Andrés {Herranz González} and Borja {Lorenzo Fernández} and Diego {Maestre Vidal} and Guillermo {Rius García} and Jorge {Maestre Vidal}},
keywords = {5G, Denial of service, Intrusion detection systems, Source-side detection, Knowledge acquisition},
abstract = {This paper introduces a novel approach for detecting the participation of a protected network device in flooding-based Distributed Denial of Service attacks. With this purpose, the traffic flows are inspected at source-side looking for discordant behaviors. In contrast to most previous solutions, the proposal assumes the non-stationarity and heterogeneity inherent in the emergent communication environment. In particular, the approach takes advantage of the monitorization and knowledge acquisition capabilities implemented in the SELFNET (H2020-ICT-2014-2/671672) project, which facilitates its implementation as a self-organizing solution on 5G mobile networks. Monitorization, feature extraction and knowledge acquisition tasks are carried out on centralized control plane, hence the proposed architecture minimizes the impact on operational performance and prompts the end-points mobility. The preliminary results observed when considering different metrics, adjustment parameters, and a dataset with traffic observed in 61 real devices proven efficiency when distinguishing normal activities from DDoS behaviors of different intensity. With an optimal granularity selection, the highest AUC reached values close to 1.0 when measured under the most intense attacks, hence demonstrating optimal TPR and FPR relationships by adapting to the instantiated use cases.}
}
@article{JAIN2022103652,
title = {Imperfect ImaGANation: Implications of GANs exacerbating biases on facial data augmentation and snapchat face lenses},
journal = {Artificial Intelligence},
volume = {304},
pages = {103652},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103652},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221002034},
author = {Niharika Jain and Alberto Olmo and Sailik Sengupta and Lydia Manikonda and Subbarao Kambhampati},
keywords = {Generative adversarial networks (GANs), Societal impacts, Algorithmic bias, Data augmentation, Social media},
abstract = {In this paper, we show that popular Generative Adversarial Network (GAN) variants exacerbate biases along the axes of gender and skin tone in the generated data. The use of synthetic data generated by GANs is widely used for a variety of tasks ranging from data augmentation to stylizing images. While practitioners celebrate this method as an economical way to obtain synthetic data to train data-hungry machine learning models or provide new features to users of mobile applications, it is unclear whether they recognize the perils of such techniques when applied to real world datasets biased along latent dimensions. Although one expects GANs to replicate the distribution of the original data, in real-world settings with limited data and finite network capacity, GANs suffer from mode collapse. First, we show readily-accessible GAN variants such as DCGANs ‘imagine’ faces of synthetic engineering professors that have masculine facial features and fair skin tones. When using popular GAN architectures that attempt to address mode-collapse, we observe that these variants either provide a false sense of security or suffer from other inherent limitations due to their design choice. Second, we show that a conditional GAN variant transforms input images of female and nonwhite faces to have more masculine features and lighter skin when asked to generate faces of engineering professors. Worse yet, prevalent filters on Snapchat end up consistently lightening the skin tones in people of color when trying to make face images appear more feminine. Thus, our study is meant to serve as a cautionary tale for practitioners and educate them about the side-effect of bias amplification when applying GAN-based techniques.}
}
@article{NGHIEM2021115284,
title = {Detecting cryptocurrency pump-and-dump frauds using market and social signals},
journal = {Expert Systems with Applications},
volume = {182},
pages = {115284},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115284},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421007156},
author = {Huy Nghiem and Goran Muric and Fred Morstatter and Emilio Ferrara},
keywords = {Cryptocurrency, Pump and dump, Fraud, Social media, Finance},
abstract = {The cryptocurrency market has gained significant traction in the last decade, becoming an alternative finance platform to traditional stock market trading. Despite its rapid evolution, legal regulations have not yet caught up to the cryptocurrency market’s progress, attracting the attention of scammers looking to exploit legal loopholes for profits. Pump-and-dump schemes, a well-worn fraud device, has regained relevance in this new territory. In a typical pump-and-dump scheme, scammers organize and leverage media channels to artificially inflate the price of an alternative cryptocurrency, only to quickly sell them to profit off unsuspecting buyers. The disruptive nature of pump-and-dump schemes necessitates a system to reliably forecast pump targets and the magnitude of its success. In this paper, we propose an approach to predict the target cryptocurrency for each pump before its announcement using market and social media signals using Neural Network-based architectures while offering interpretable insights into their black-box nature. Additionally, we construct models that are capable of forecasting the highest price induced by the pump after the cryptocurrency’s identity is revealed within 6.1% error margin. We examine the optimal temporal windows and describe the limitations of social data to predict the manipulations in cryptocurrency trade. Our experimental results serve as proof of a feasible forecasting expert system for identifying cryptocurrency pump-and-dump frauds using publicly available data.}
}
@article{CHAGANTI202396,
title = {A survey on Blockchain solutions in DDoS attacks mitigation: Techniques, open challenges and future directions},
journal = {Computer Communications},
volume = {197},
pages = {96-112},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422004145},
author = {Rajasekhar Chaganti and Bharat Bhushan and Vinayakumar Ravi},
keywords = {Denial of service attack, IoT botnet, Software defined networks, Smart contract, Blockchain, DDoS attacks, Internet service provider},
abstract = {With the proliferation of new technologies such as the Internet of Things (IoT) and Software-Defined Networking (SDN) in recent years, the Distributed Denial of Service (DDoS) attack vector has broadened and opened new opportunities for more sophisticated DDoS attacks on the targeted victims. The new attack vector includes unsecured and vulnerable IoT devices connected to the internet, and denial of service vulnerabilities like southbound channel saturation in the SDN architecture. Given the high-volume and pervasive nature of these attacks, it is beneficial for stakeholders to collaborate in detecting and mitigating the denial of service attacks promptly. Blockchain technology is considered to improve the security aspects owing to the decentralized design, secured distributed storage, and privacy. A thorough exploration and classification of blockchain techniques used for DDoS attack mitigation are not explored in the prior art. This paper reviews and categorizes state-of-the-art DDoS mitigation solutions based on blockchain technology. The DDoS mitigation techniques are classified based on the solution deployment location i.e. network-based, near attacker location, near victim location, and hybrid solutions in the network architecture with emphasis on the IoT and SDN architectures. Additionally, based on our study, the research challenges and future directions to implement the blockchain based DDoS mitigation solutions are discussed.}
}
@article{MU2024122791,
title = {Predicting and analyzing the popularity of false rumors in Weibo},
journal = {Expert Systems with Applications},
volume = {243},
pages = {122791},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122791},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032931},
author = {Yida Mu and Pu Niu and Kalina Bontcheva and Nikolaos Aletras},
keywords = {Social media, Rumor popularity, Misinformation analysis},
abstract = {Malicious online rumors with high popularity, if left undetected, can spread very quickly with damaging societal implications. The development of reliable computational methods for early prediction of the popularity of false rumors is very much needed, as a complement to related work on automated rumor detection and fact-checking. Besides, detecting false rumors with higher popularity in the early stage allows social media platforms to timely deliver fact-checking information to end users. To this end, we (1) propose a new regression task to predict the future popularity of false rumors given both post and user-level information; (2) introduce a new publicly available dataset in Chinese that includes 19,256 false rumor cases from Weibo, the corresponding profile information of the original spreaders and a rumor popularity score as a function of the shares, replies and reports it has received; (3) develop a new open-source domain adapted pre-trained language model, i.e., BERT-Weibo-Rumor and evaluate its performance against several supervised classifiers using post and user-level information. Our best performing model (KG-Fusion) achieves the lowest RMSE score (1.54) and highest Pearson’s r (0.636), outperforming competitive baselines by leveraging textual information from both the post and the user profile. Our analysis unveils that popular rumors consist of more conjunctions and punctuation marks, while less popular rumors contain more words related to the social context and personal pronouns. Our dataset is publicly available: https://github.com/YIDAMU/Weibo_Rumor_Popularity.}
}
@incollection{2016155,
title = {Index},
editor = {Kim Holmberg},
booktitle = {Altmetrics for Information Professionals},
publisher = {Chandos Publishing},
pages = {155-159},
year = {2016},
isbn = {978-0-08-100273-5},
doi = {https://doi.org/10.1016/B978-0-08-100273-5.09988-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002735099889}
}
@article{KONDAMUDI2023101571,
title = {A comprehensive survey of fake news in social networks: Attributes, features, and detection approaches},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {6},
pages = {101571},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101571},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001258},
author = {Medeswara Rao Kondamudi and Somya Ranjan Sahoo and Lokesh Chouhan and Nandakishor Yadav},
keywords = {Online social networks, Fake news classification, Fake news identification techniques},
abstract = {The explosion of online social networks in recent decades has significantly improved in which the way individuals communicate with one another. People trust social networks bluntly without knowing the origin and genuinity of the information passed through these networks. Sometimes, unreliable information on online social networks misleads the viewers, and it brings unremovable stains to humanity. Online social networks transform even the original information of the government, which create confusion among the people and people loses confidence over the government. Various types of research have been conducted to identify fake news with high efficiency. In this survey, we describe the basic theories of fake news, investigate and analyze the perspective on fake news, attribute misleading information, an in-depth analysis of disinformation, and methods that have been established for detection. To our knowledge, this research article will assist in facilitating collaborative activities among technical experts, political campaigns, online purchases, and other disciplines that are being used to investigate fake messages.}
}
@article{KOUBAA2023e21624,
title = {Humans are still better than ChatGPT: Case of the IEEEXtreme competition},
journal = {Heliyon},
volume = {9},
number = {11},
pages = {e21624},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e21624},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023088321},
author = {Anis Koubaa and Basit Qureshi and Adel Ammar and Zahid Khan and Wadii Boulila and Lahouari Ghouti},
keywords = {ChatGPT, GPT-4, GPT-3.5, GPT performance, GPT limitations, OpenAI, NLP, Computer programming},
abstract = {Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark—a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we found that the average score obtained by ChatGPT on the set of IEEExtreme programming problems is 3.9 to 5.8 times lower than the average human score, depending on the programming language. This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.}
}
@article{WANG2024108549,
title = {An automatic distillation sequence synthesis framework based on a preorder traversal algorithm},
journal = {Computers & Chemical Engineering},
volume = {181},
pages = {108549},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108549},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423004192},
author = {Anqing Wang and Alexander Guzman-Urbina and Hajime Ohno and Yasuhiro Fukushima},
keywords = {Process synthesis, Distillation sequences, Process optimization, Mixed-integer linear programming},
abstract = {Distillation energy consumption dominates the process industry; hence, the selection of the distillation sequence will substantially affect the separation energy consumption. We propose a framework for the automatic synthesis and optimization of distillation sequences by integrating the Aspen Plus with the MATLAB programming platform. The framework combines the concept of binary trees in data structures and uses a preorder traversal algorithm to generate a network superstructure in the simulator. In the next step, it automatically formulates and solves a mixed-integer linear programming model based on calculation results; finally, the distillation model is optimized using an improved quadratic interpolation algorithm. Two case studies for C5 alkane and dimethyl carbonate separation showed that the optimal solution obtained by the framework reduced the total annual costs by 13.18 % and 2.88 %, respectively, compared with the processes without systematic optimization. The framework also allowed screening out promising alternatives by rapid evaluation of different process routes.}
}
@article{DAVILA2021110951,
title = {A systematic literature review and taxonomy of modern code review},
journal = {Journal of Systems and Software},
volume = {177},
pages = {110951},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110951},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000480},
author = {Nicole Davila and Ingrid Nunes},
keywords = {Modern code review, Software verification, Software quality, Systematic literature review},
abstract = {Context:
Modern Code Review (MCR) is a widely known practice of software quality assurance. However, the existing body of knowledge of MCR is currently not understood as a whole.
Objective:
Our goal is to identify the state of the art on MCR, providing a structured overview and an in-depth analysis of the research done in this field.
Methods:
We performed a systematic literature review, selecting publications from four digital libraries.
Results:
A total of 139 papers were selected and analyzed in three main categories. Foundational studies are those that analyze existing or collected data from the adoption of MCR. Proposals consist of techniques and tools to support MCR, while evaluations are studies to assess an approach or compare a set of them.
Conclusion:
The most represented category is foundational studies, mainly aiming to understand the motivations for adopting MCR, its challenges and benefits, and which influence factors lead to which MCR outcomes. The most common types of proposals are code reviewer recommender and support to code checking. Evaluations of MCR-supporting approaches have been done mostly offline, without involving human subjects. Five main research gaps have been identified, which point out directions for future work in the area.}
}
@article{STEFFENSEN2020634,
title = {TrollBOT: A Spontaneous Networking Tool Facilitating Rapid Prototyping of Wirelessly Communicating Products},
journal = {Procedia CIRP},
volume = {91},
pages = {634-638},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.111},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120308763},
author = {Torjus Steffensen and Sampsa Kohtala and Håvard Vestad and Martin Steinert},
keywords = {Prototyping, Makerspaces, Arduino},
abstract = {In early stages of product development, prototyping is an invaluable tool which allows designers to generate learnings and uncover unknown challenges which can be used to further construct design requirements. While generous use of prototyping early in the design process might reduce the risk of premature design decisions, it also demands significant investments in terms of resources such as time, material, and skills. Tools that allow designers to rapidly implement and test new functionalities are therefore desired. With wirelessly communicating products having become ubiquitous in modern society, designers should be comfortable designing products utilizing these technologies. In this paper we present an Arduino library, named TrollBOT, that facilitates rapid implementation of wireless communication between two or more Arduinos. The Arduinos form nodes in a tree topology using inexpensive nRF24-based radio transceivers. The library is constructed in such a way that a minimal amount of new language syntax must be learned. All nodes can be programmed from a single master node in an intuitive manner, significantly reducing the amount of code that needs to be written as compared to similar existing solutions.}
}
@article{RAZAVIAN2023111560,
title = {The vision of on-demand architectural knowledge systems as a decision-making companion},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111560},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111560},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002369},
author = {Maryam Razavian and Barbara Paech and Antony Tang},
keywords = {Software architecture knowledge, Knowledge management systems, Decision-making, Human aspects},
abstract = {Cobbler’s children do not wear shoes. Software engineers build sophisticated software but we often cannot find the needed information and knowledge for ourselves. Issues are the amount of development information that can be captured, organizing that information to make them useable for other developers as well as human decision-making issues. Current architectural knowledge management systems cannot handle these issues properly. In this paper, we outline a research agenda for intelligent tools to support the knowledge management and decision making of architects. The research agenda consists of a vision and research challenges on the way to realize this vision. We call our vision on-demand architectural knowledge systems (ODAKS). Based on literature review, analysis, and synthesis of past research works, we derive our vision of ODAKS as decision-making companions to architects. ODAKS organize and provide relevant information and knowledge to the architect through an assistive conversation. ODAKS use probing to understand the architects’ goals and their questions, they suggest relevant knowledge and present reflective hints to mitigate human decision-making issues, such as cognitive bias, cognitive limitations, as well as design process aspects, such as problem-solution co-evolution and the balance between intuitive and rational decision-making. We present the main features of ODAKS, investigate current potential technologies for the implementation of ODAKS and discuss the main research challenges.}
}
@article{CUI2023128724,
title = {“Born in Rome” or “Sleeping Beauty”: Emergence of hashtag popularity on the Chinese microblog Sina Weibo},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {619},
pages = {128724},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.128724},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123002790},
author = {Hao Cui and János Kertész},
keywords = {Attention dynamics, Repost network, Online social network, Hot search list},
abstract = {To understand the emergence of hashtag popularity in online social networking complex systems, we study the largest Chinese microblogging site Sina Weibo, which has a Hot Search List (HSL) showing in real time the ranking of the 50 most popular hashtags based on search activity. We investigate the prehistory of successful hashtags from 17 July 2020 to 17 September 2020 by mapping out the related interaction network preceding the selection to HSL. We have found that the circadian activity pattern has an impact on the time needed to get to the HSL. When analyzing this time we distinguish two extreme categories: (a) “Born in Rome”, which means hashtags are mostly first created by superhubs or reach superhubs at an early stage during their propagation and thus gain immediate wide attention from the broad public, and (b) “Sleeping Beauty”, meaning the hashtags gain little attention at the beginning and reach system-wide popularity after a considerable time lag. The evolution of the repost networks of successful hashtags before getting to the HSL show two types of growth patterns: “smooth” and “stepwise”. The former is usually dominated by a superhub and the latter results from consecutive waves of contributions of smaller hubs. The repost networks of unsuccessful hashtags exhibit a simple evolution pattern.}
}
@article{MOTWANI20217973,
title = {Multifactor door locking systems: A review},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {7973-7979},
year = {2021},
note = {3rd International Conference on Materials, Manufacturing and Modelling},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.02.708},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321018617},
author = {Yashraj Motwani and Saambhavi Seth and Devang Dixit and A. Bagubali and R. Rajesh},
keywords = {Security, Door lock, Multifactor authentication, Biometric verification, Keypad, Gsm lock},
abstract = {Security has become very important, but along with that, people also need a system that is not very expensive and can be customized to meet our needs. As conventional door locks can be easily opened, this makes people vulnerable to security threats. This study attempts a comparative analysis of pre-existing researches, made in the field of security control system developed and improvised over the span of time with multifactor authentication technique’s evolvement. The components, hardware complications, work efficiency and algorithms used in each of the model is drawn as a comparison to other to provide an idea of systematic development in this regard. With each passing day, security systems are advancing and new technology is being developed. Security systems or door locking mechanics have evolved from metallic door locks of primitive type keys, to advanced controlling structure with up to four or five step authentications to ensure utmost safety.}
}
@article{GRAJEDA2017S94,
title = {Availability of datasets for digital forensics – And what is missing},
journal = {Digital Investigation},
volume = {22},
pages = {S94-S105},
year = {2017},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1742287617301913},
author = {Cinthya Grajeda and Frank Breitinger and Ibrahim Baggili},
keywords = {Availability, Data collection, Dataset, Origin, Experiment generated, User generated, Repository},
abstract = {This paper targets two main goals. First, we want to provide an overview of available datasets that can be used by researchers and where to find them. Second, we want to stress the importance of sharing datasets to allow researchers to replicate results and improve the state of the art. To answer the first goal, we analyzed 715 peer-reviewed research articles from 2010 to 2015 with focus and relevance to digital forensics to see what datasets are available and focused on three major aspects: (1) the origin of the dataset (e.g., real world vs. synthetic), (2) if datasets were released by researchers and (3) the types of datasets that exist. Additionally, we broadened our results to include the outcome of online search results. We also discuss what we think is missing. Overall, our results show that the majority of datasets are experiment generated (56.4%) followed by real world data (36.7%). On the other hand, 54.4% of the articles use existing datasets while the rest created their own. In the latter case, only 3.8% actually released their datasets. Finally, we conclude that there are many datasets for use out there but finding them can be challenging.}
}
@incollection{DEKA201877,
title = {Chapter Three - NoSQL Web Crawler Application},
editor = {Pethuru Raj and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {109},
pages = {77-100},
year = {2018},
booktitle = {A Deep Dive into NoSQL Databases: The Use Cases and Applications},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300323},
author = {Ganesh Chandra Deka},
keywords = {Robots Exclusion Protocol, Internet Archive, JSONP, Hypercat, Meta crawling, HTML crawling, Mobile crawlers, Bloom filter, Resource Description Framework, Search engine optimization},
abstract = {With the advent of Web technology, the Web is full of unstructured data called Big Data. However, these data are not easy to collect, access, and process at large scale. Web Crawling is an optimization problem. Site-specific crawling of various social media platforms, e-Commerce websites, Blogs, News websites, and Forums is a requirement for various business organizations to answer a search quarry from webpages. Indexing of huge number of webpage requires a cluster with several petabytes of usable disk. Since the NoSQL databases are highly scalable, use of NoSQL database for storing the Crawler data is increasing along with the growing popularity of NoSQL databases. This chapter discusses about the application of NoSQL database in Web Crawler application to store the data collected by the Web Crawler.}
}
@article{PERNISCH2021100658,
title = {Beware of the hierarchy — An analysis of ontology evolution and the materialisation impact for biomedical ontologies},
journal = {Journal of Web Semantics},
volume = {70},
pages = {100658},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100658},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000330},
author = {Romana Pernisch and Daniele Dell’Aglio and Abraham Bernstein},
keywords = {Ontology evolution, Materialisation, Evolution impact, Ontology change},
abstract = {Ontologies are becoming a key component of numerous applications and research fields. But knowledge captured within ontologies is not static. Some ontology updates potentially have a wide ranging impact; others only affect very localised parts of the ontology and their applications. Investigating the impact of the evolution gives us insight into the editing behaviour but also signals ontology engineers and users how the ontology evolution is affecting other applications. However, such research is in its infancy. Hence, we need to investigate the evolution itself and its impact on the simplest of applications: the materialisation. In this work, we define impact measures that capture the effect of changes on the materialisation. In the future, the impact measures introduced in this work can be used to investigate how aware the ontology editors are about consequences of changes. By introducing five different measures, which focus either on the change in the materialisation with respect to the size or on the number of changes applied, we are able to quantify the consequences of ontology changes. To see these measures in action, we investigate the evolution and its impact on materialisation for nine open biomedical ontologies, most of which adhere to the EL++ description logic. Our results show that these ontologies evolve at varying paces but no statistically significant difference between the ontologies with respect to their evolution could be identified. We identify three types of ontologies based on the types of complex changes which are applied to them throughout their evolution. The impact on the materialisation is the same for the investigated ontologies, bringing us to the conclusion that the effect of changes on the materialisation can be generalised to other similar ontologies. Further, we found that the materialised concept inclusion axioms experience most of the impact induced by changes to the class inheritance of the ontology and other changes only marginally touch the materialisation.}
}
@article{PANDEY2023100198,
title = {A comparative study of retrieval-based and generative-based chatbots using Deep Learning and Machine Learning},
journal = {Healthcare Analytics},
volume = {3},
pages = {100198},
year = {2023},
issn = {2772-4425},
doi = {https://doi.org/10.1016/j.health.2023.100198},
url = {https://www.sciencedirect.com/science/article/pii/S2772442523000655},
author = {Sumit Pandey and Srishti Sharma},
keywords = {Artificial Intelligence, Chatbot, Deep Learning, Machine Learning, Mental health},
abstract = {Increased screen time may cause significant health impacts, including harmful effects on mental health. Studies on the association between technological obsessions and their influence on health have been conducted using Deep Learning (DL) and Machine Learning (ML) techniques. The deployment of chatbots in different industries has been proven as a game-changer. We study conversational Artificial Intelligence (AI) systems enabling operators to conduct conversations with machines that resemble those with humans. We design and develop two retrieval-based and generative-based chatbots, each with six designs. Among the retrieval-based chatbots, Vanilla Recurrent Neural Network (RNN) has an accuracy of 83.22%, Long Short Term Memory (LSTM) is 89.87% accurate, Bidirectional LSTM (Bi-LSTM) is 91.57% accurate, Gated Recurrent Unit (GRU) is 65.57% accurate, and Convolution Neural Network (CNN) is 82.33% accurate. In comparison, generative-based chatbots have encoder–decoder designs that are 94.45% accurate. The most significant distinction is that while generative-based chatbots can generate new text, retrieval-based chatbots are restricted to responding to inputs that match the best of the outputs they already know.}
}
@article{CABALLERO202374,
title = {The Rise of GoodFATR: A Novel Accuracy Comparison Methodology for Indicator Extraction Tools},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {74-89},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000535},
author = {Juan Caballero and Gibran Gomez and Srdjan Matic and Gustavo Sánchez and Silvia Sebastián and Arturo Villacañas},
keywords = {Indicators of Compromise, IOC, Cyber Threat Intelligence, RSS, Twitter, Telegram},
abstract = {To adapt to a constantly evolving landscape of cyber threats, organizations actively need to collect Indicators of Compromise (IOCs), i.e., forensic artifacts that signal that a host or network might have been compromised. IOCs can be collected through open-source and commercial structured IOC feeds. But, they can also be extracted from a myriad of unstructured threat reports written in natural language and distributed using a wide array of sources such as blogs and social media. There exist multiple indicator extraction tools that can identify IOCs in natural language reports. But, it is hard to compare their accuracy due to the difficulty of building large ground truth datasets. This work presents a novel majority vote methodology for comparing the accuracy of indicator extraction tools, which does not require a manually-built ground truth. We implement our methodology into GoodFATR, an automated platform for collecting threat reports from a wealth of sources, extracting IOCs from the collected reports using multiple tools, and comparing their accuracy. GoodFATR supports 6 threat report sources: RSS, Twitter, Telegram, Malpedia, APTnotes, and ChainSmith. GoodFATR continuously monitors the sources, downloads new threat reports, extracts 41 indicator types from the collected reports, and filters non-malicious indicators to output the IOCs. We run GoodFATR over 15 months to collect 472,891 reports from the 6 sources; extract 978,151 indicators from the reports; and identify 618,217 IOCs. We analyze the collected data to identify the top IOC contributors and the IOC class distribution. We apply GoodFATR to compare the IOC extraction accuracy of 7 popular open-source tools with GoodFATR’s own indicator extraction module.}
}
@article{MOTZ2022165,
title = {Live Sentiment Analysis Using Multiple Machine Learning and Text Processing Algorithms},
journal = {Procedia Computer Science},
volume = {203},
pages = {165-172},
year = {2022},
note = {17th International Conference on Future Networks and Communications / 19th International Conference on Mobile Systems and Pervasive Computing / 12th International Conference on Sustainable Energy Information Technology (FNC/MobiSPC/SEIT 2022), August 9-11, 2022, Niagara Falls, Ontario, Canada},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922006287},
author = {Andrew Motz and Elizabeth Ranta and Adan Sierra Calderon and Quin Adam and Fadi Alzhouri and Dariush Ebrahimi},
keywords = {Lexicons, Machine Learning, Twitter, Data Streams, Sentiment Analysis},
abstract = {Due to the massive amount of data being generated on the platform, Twitter has been the subject of numerous sentiment analysis studies. Such social network services generate massive unstructured data streams which make working with them very challenging. The aim of this study is to reliably analyze the sentiment of trending tweets in the Twitter API data stream using a combination of different algorithms to achieve a consensus. The methods we implemented include Support-Vector Machine, Naive Bayes, Textblob, and Lexicon Approach. The hypothesis is that using these methods together would enable us to get more accurate results. Using a labeled dataset to test our model, the results show that the combination of these four algorithms all together performed best with an overall accuracy of 68.29%. We conclude that our combination method of analysis is suitable and fast enough for our data stream and also accurate for analyzing sentiment.}
}
@article{STALLONE2021100023,
title = {Applications of Blockchain Technology in marketing—A systematic review of marketing technology companies},
journal = {Blockchain: Research and Applications},
volume = {2},
number = {3},
pages = {100023},
year = {2021},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2021.100023},
url = {https://www.sciencedirect.com/science/article/pii/S209672092100018X},
author = {Valerio Stallone and Martin Wetzels and Michael Klaas},
keywords = {Blockchain, Marketing, Blockchain applications, Blockchain in marketing},
abstract = {Given the emerging nature of integrating Blockchain Technology (BCT) into several business fields concerning the interaction between companies and their customers, this study aims to investigate the applications of BCT in marketing through an accurate procedure of locating, selecting and analyzing existing companies using BCT in marketing. A sample that consists of 800 companies was identified using web-scraping methods. The data set was collected from initial coin offerings (ICO) websites as well as from an existing, older landscape of applications. The data set was then intensively analyzed in order to be categorized into five fields of marketing technology. Advertising and ecommerce outgrew the other fields of social & relationship, content & experience and data in absolute numbers, revealing the focus of practitioners in the past as well as gaps for the future. The authors provided future directions for researchers on and development of tools to systematically generate knowledge and improve the application of BCT and the work of practitioners in marketing.}
}
@article{BHARDWAJ2024100443,
title = {Fortifying home IoT security: A framework for comprehensive examination of vulnerabilities and intrusion detection strategies for smart cities},
journal = {Egyptian Informatics Journal},
volume = {25},
pages = {100443},
year = {2024},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2024.100443},
url = {https://www.sciencedirect.com/science/article/pii/S1110866524000069},
author = {Akashdeep Bhardwaj and Salil Bharany and Anas W. Abulfaraj and Ashraf {Osman Ibrahim} and Wamda Nagmeldin},
keywords = {IoT, Firmware attack, XSS, Brute Force, Cross-site scripting, UPnP, IDS},
abstract = {Smart home devices have brought in a disruptive, revolutionary Internet-based ecosystem that enhanced our daily lives but has pushed private data from inside our homes to external public sources. Threats and attacks mounted against IoT deployments have only increased in recent times. There have been several proposals to secure home automation environments, but there is no full protection against Cybersecurity threats for our home IoT platforms. This research investigates attack attempts on smart home environments, focusing on firmware, brute force, and DoS attacks on the Internet of Things (IoT) network which were successful in bringing down the device in less than a minute. Weak passwords were cracked using Brute Force techniques related to HTTP, SSH, Telnet, and FTP protocols, and an unknown service port to reveal backdoor access. Cross-site scripting vulnerability was detected on IoT devices that could allow running malicious scripts on the devices. The authors also exploited the unknown services to reveal backdoors and access sensitive device details and potentially exploited them to add new ports or rules to turn the IoT devices into a router to attack other devices. To detect and mitigate such attacks, the authors present an IoT-based intrusion detection and prevention system to secure smart home network devices. The authors compared the proposed framework with other similar research based on Precision, Accuracy, F-measure, and Recall. The proposed model outperforms all the other known models reporting a high of 95% for identifying malicious attack packets, while others reported 58% and 71% detection percentage.}
}
@article{FORTH2023100263,
title = {BIM4EarlyLCA: An interactive visualization approach for early design support based on uncertain LCA results using open BIM},
journal = {Developments in the Built Environment},
volume = {16},
pages = {100263},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100263},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300145X},
author = {Kasimir Forth and Alexander Hollberg and André Borrmann},
keywords = {LCA, BIM, Design decision support, Early design stages},
abstract = {To meet the European climate goals in the building sector, a holistic optimization of embodied greenhouse gas (GHG) emissions using the method of life cycle assessments (LCA) are necessary. The early design stages have high impact on the final performance of the buildings and are characterized by high uncertainty due to the lack of information and not yet taken decisions. Furthermore, most current LCA approaches based on Building Information Models (BIM) require high expertise and experience in both BIM and LCA and do not follow an intuitive visualization approach for other stakeholders and non-experts. This paper presents a novel design-decision-making approach for reducing embodied GHG emissions by interactive, model-based visualizations of uncertain LCA results. The proposed workflow is based on open BIM data formats, such as Industry Foundation Classes (IFC) and BIM Collaboration Format (BCF), and is developed for decision support for non-LCA experts in the early design stages. With the help of a user study, the prototypical implementation is tested by 103 participants with different levels of experience in BIM and LCA based on a case study. We evaluate the proposed approach regarding the support of open BIM data formats, different LCA visualization strategies, and the intuitiveness of different approaches to visualizing uncertain LCA results. The user study results show a broad acceptance and need for open BIM data formats and model-based LCA visualization but less for visualizing uncertainties, which needs further research. In conclusion, this interactive, model-based visualization approach using color coding supports non-LCA experts in the design decision-making process in early design stages.}
}
@article{DARIUS2021100174,
title = {Disinformed social movements: A large-scale mapping of conspiracy narratives as online harms during the COVID-19 pandemic},
journal = {Online Social Networks and Media},
volume = {26},
pages = {100174},
year = {2021},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2021.100174},
url = {https://www.sciencedirect.com/science/article/pii/S2468696421000550},
author = {Philipp Darius and Michael Urquhart},
keywords = {Disinformation, Conspiracy theory, Anti-vaccination, Anti-5G, Social network analysis, COVID-19, Social movements, Twitter, Public health, Trust in government},
abstract = {The COVID-19 pandemic caused high uncertainty regarding appropriate treatments and public policy reactions. This uncertainty provided a perfect breeding ground for spreading conspiratorial anti-science narratives based on disinformation. Disinformation on public health may alter the population’s hesitance to vaccinations, counted among the ten most severe threats to global public health by the United Nations. We understand conspiracy narratives as a combination of disinformation, misinformation, and rumour that are especially effective in drawing people to believe in post-factual claims and form disinformed social movements. Conspiracy narratives provide a pseudo-epistemic background for disinformed social movements that allow for self-identification and cognitive certainty in a rapidly changing information environment. This study monitors two established conspiracy narratives and their communities on Twitter, the anti-vaccination and anti-5G communities, before and during the first UK lockdown. The study finds that, despite content moderation efforts by Twitter, conspiracy groups were able to proliferate their networks and influence broader public discourses on Twitter, such as #Lockdown in the United Kingdom.}
}